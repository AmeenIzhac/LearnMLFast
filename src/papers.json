[
    {
        "paperId": "b5c26ab8767d046cb6e32d959fdf726aee89bb62",
        "url": "https://www.semanticscholar.org/paper/b5c26ab8767d046cb6e32d959fdf726aee89bb62",
        "title": "Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning",
        "venue": "AAAI Conference on Artificial Intelligence",
        "year": 2016,
        "citationCount": 15043,
        "openAccessPdf": {
            "url": "https://ojs.aaai.org/index.php/AAAI/article/download/11231/11090",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1602.07261, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2574060",
                "name": "Christian Szegedy"
            },
            {
                "authorId": "2054165706",
                "name": "Sergey Ioffe"
            },
            {
                "authorId": "2657155",
                "name": "Vincent Vanhoucke"
            },
            {
                "authorId": "122113652",
                "name": "Alexander A. Alemi"
            }
        ],
        "abstract": "\n \n Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question: Are there any benefits to combining Inception architectures with residual connections? Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4 networks, we achieve 3.08% top-5 error on the test set of the ImageNet classification (CLS) challenge.\n \n"
    },
    {
        "paperId": "87f40e6f3022adbc1f1905e3e506abad05a9964f",
        "url": "https://www.semanticscholar.org/paper/87f40e6f3022adbc1f1905e3e506abad05a9964f",
        "title": "Distributed Representations of Words and Phrases and their Compositionality",
        "venue": "Neural Information Processing Systems",
        "year": 2013,
        "citationCount": 34686,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1310.4546, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2047446108",
                "name": "Tomas Mikolov"
            },
            {
                "authorId": "1701686",
                "name": "I. Sutskever"
            },
            {
                "authorId": "2118440152",
                "name": "Kai Chen"
            },
            {
                "authorId": "32131713",
                "name": "G. Corrado"
            },
            {
                "authorId": "49959210",
                "name": "J. Dean"
            }
        ],
        "abstract": "The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. \n \nAn inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of \"Canada\" and \"Air\" cannot be easily combined to obtain \"Air Canada\". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible."
    },
    {
        "paperId": "cea967b59209c6be22829699f05b8b1ac4dc092d",
        "url": "https://www.semanticscholar.org/paper/cea967b59209c6be22829699f05b8b1ac4dc092d",
        "title": "Sequence to Sequence Learning with Neural Networks",
        "venue": "Neural Information Processing Systems",
        "year": 2014,
        "citationCount": 21538,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1409.3215, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1701686",
                "name": "I. Sutskever"
            },
            {
                "authorId": "1689108",
                "name": "O. Vinyals"
            },
            {
                "authorId": "2827616",
                "name": "Quoc V. Le"
            }
        ],
        "abstract": "Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT-14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous state of the art. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier."
    },
    {
        "paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
        "url": "https://www.semanticscholar.org/paper/204e3073870fae3d05bcbc2f6a8e263d9b72e776",
        "title": "Attention is All you Need",
        "venue": "Neural Information Processing Systems",
        "year": 2017,
        "citationCount": 159909,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1706.03762, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "40348417",
                "name": "Ashish Vaswani"
            },
            {
                "authorId": "1846258",
                "name": "Noam Shazeer"
            },
            {
                "authorId": "3877127",
                "name": "Niki Parmar"
            },
            {
                "authorId": "39328010",
                "name": "Jakob Uszkoreit"
            },
            {
                "authorId": "145024664",
                "name": "Llion Jones"
            },
            {
                "authorId": "19177000",
                "name": "Aidan N. Gomez"
            },
            {
                "authorId": "40527594",
                "name": "Lukasz Kaiser"
            },
            {
                "authorId": "3443442",
                "name": "I. Polosukhin"
            }
        ],
        "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data."
    },
    {
        "paperId": "231af7dc01a166cac3b5b01ca05778238f796e41",
        "url": "https://www.semanticscholar.org/paper/231af7dc01a166cac3b5b01ca05778238f796e41",
        "title": "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium",
        "venue": "Neural Information Processing Systems",
        "year": 2017,
        "citationCount": 16327,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "2445103",
                "name": "M. Heusel"
            },
            {
                "authorId": "19219270",
                "name": "Hubert Ramsauer"
            },
            {
                "authorId": "2465270",
                "name": "Thomas Unterthiner"
            },
            {
                "authorId": "37082831",
                "name": "Bernhard Nessler"
            },
            {
                "authorId": "3308557",
                "name": "Sepp Hochreiter"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "442e10a3c6640ded9408622005e3c2a8906ce4c2",
        "url": "https://www.semanticscholar.org/paper/442e10a3c6640ded9408622005e3c2a8906ce4c2",
        "title": "A Unified Approach to Interpreting Model Predictions",
        "venue": "Neural Information Processing Systems",
        "year": 2017,
        "citationCount": 28982,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1705.07874, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "23451726",
                "name": "Scott M. Lundberg"
            },
            {
                "authorId": "2180463",
                "name": "Su-In Lee"
            }
        ],
        "abstract": "Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches."
    },
    {
        "paperId": "497e4b08279d69513e4d2313a7fd9a55dfb73273",
        "url": "https://www.semanticscholar.org/paper/497e4b08279d69513e4d2313a7fd9a55dfb73273",
        "title": "LightGBM: A Highly Efficient Gradient Boosting Decision Tree",
        "venue": "Neural Information Processing Systems",
        "year": 2017,
        "citationCount": 12813,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "35286545",
                "name": "Guolin Ke"
            },
            {
                "authorId": "47580728",
                "name": "Qi Meng"
            },
            {
                "authorId": "50256971",
                "name": "Thomas Finley"
            },
            {
                "authorId": null,
                "name": "Taifeng Wang"
            },
            {
                "authorId": null,
                "name": "Wei Chen"
            },
            {
                "authorId": "3029546",
                "name": "Weidong Ma"
            },
            {
                "authorId": "3006308",
                "name": "Qiwei Ye"
            },
            {
                "authorId": "2110264337",
                "name": "Tie-Yan Liu"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "6b7d6e6416343b2a122f8416e69059ce919026ef",
        "url": "https://www.semanticscholar.org/paper/6b7d6e6416343b2a122f8416e69059ce919026ef",
        "title": "Inductive Representation Learning on Large Graphs",
        "venue": "Neural Information Processing Systems",
        "year": 2017,
        "citationCount": 17944,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1706.02216, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "49437682",
                "name": "William L. Hamilton"
            },
            {
                "authorId": "4058003",
                "name": "Z. Ying"
            },
            {
                "authorId": "1702139",
                "name": "J. Leskovec"
            }
        ],
        "abstract": "Low-dimensional embeddings of nodes in large graphs have proved extremely useful in a variety of prediction tasks, from content recommendation to identifying protein functions. However, most existing approaches require that all nodes in the graph are present during training of the embeddings; these previous approaches are inherently transductive and do not naturally generalize to unseen nodes. Here we present GraphSAGE, a general, inductive framework that leverages node feature information (e.g., text attributes) to efficiently generate node embeddings for previously unseen data. Instead of training individual embeddings for each node, we learn a function that generates embeddings by sampling and aggregating features from a node's local neighborhood. Our algorithm outperforms strong baselines on three inductive node-classification benchmarks: we classify the category of unseen nodes in evolving information graphs based on citation and Reddit post data, and we show that our algorithm generalizes to completely unseen graphs using a multi-graph dataset of protein-protein interactions."
    },
    {
        "paperId": "8674494bd7a076286b905912d26d47f7501c4046",
        "url": "https://www.semanticscholar.org/paper/8674494bd7a076286b905912d26d47f7501c4046",
        "title": "PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space",
        "venue": "Neural Information Processing Systems",
        "year": 2017,
        "citationCount": 12891,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1706.02413, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "144329939",
                "name": "C. Qi"
            },
            {
                "authorId": "47782132",
                "name": "L. Yi"
            },
            {
                "authorId": "144914140",
                "name": "Hao Su"
            },
            {
                "authorId": "51352814",
                "name": "L. Guibas"
            }
        ],
        "abstract": "Few prior works study deep learning on point sets. PointNet by Qi et al. is a pioneer in this direction. However, by design PointNet does not capture local structures induced by the metric space points live in, limiting its ability to recognize fine-grained patterns and generalizability to complex scenes. In this work, we introduce a hierarchical neural network that applies PointNet recursively on a nested partitioning of the input point set. By exploiting metric space distances, our network is able to learn local features with increasing contextual scales. With further observation that point sets are usually sampled with varying densities, which results in greatly decreased performance for networks trained on uniform densities, we propose novel set learning layers to adaptively combine features from multiple scales. Experiments show that our network called PointNet++ is able to learn deep point set features efficiently and robustly. In particular, results significantly better than state-of-the-art have been obtained on challenging benchmarks of 3D point clouds."
    },
    {
        "paperId": "edf73ab12595c6709f646f542a0d2b33eb20a3f4",
        "url": "https://www.semanticscholar.org/paper/edf73ab12595c6709f646f542a0d2b33eb20a3f4",
        "title": "Improved Training of Wasserstein GANs",
        "venue": "Neural Information Processing Systems",
        "year": 2017,
        "citationCount": 10378,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1704.00028, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2708454",
                "name": "Ishaan Gulrajani"
            },
            {
                "authorId": "2054472270",
                "name": "Faruk Ahmed"
            },
            {
                "authorId": "2877311",
                "name": "Martín Arjovsky"
            },
            {
                "authorId": "3074927",
                "name": "Vincent Dumoulin"
            },
            {
                "authorId": "1760871",
                "name": "Aaron C. Courville"
            }
        ],
        "abstract": "Generative Adversarial Networks (GANs) are powerful generative models, but suffer from training instability. The recently proposed Wasserstein GAN (WGAN) makes progress toward stable training of GANs, but sometimes can still generate only low-quality samples or fail to converge. We find that these problems are often due to the use of weight clipping in WGAN to enforce a Lipschitz constraint on the critic, which can lead to undesired behavior. We propose an alternative to clipping weights: penalize the norm of gradient of the critic with respect to its input. Our proposed method performs better than standard WGAN and enables stable training of a wide variety of GAN architectures with almost no hyperparameter tuning, including 101-layer ResNets and language models over discrete data. We also achieve high quality generations on CIFAR-10 and LSUN bedrooms."
    },
    {
        "paperId": "3c8a456509e6c0805354bd40a35e3f2dbf8069b1",
        "url": "https://www.semanticscholar.org/paper/3c8a456509e6c0805354bd40a35e3f2dbf8069b1",
        "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library",
        "venue": "Neural Information Processing Systems",
        "year": 2019,
        "citationCount": 48432,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1912.01703, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3407277",
                "name": "Adam Paszke"
            },
            {
                "authorId": "39793298",
                "name": "Sam Gross"
            },
            {
                "authorId": "1403239967",
                "name": "Francisco Massa"
            },
            {
                "authorId": "1977806",
                "name": "Adam Lerer"
            },
            {
                "authorId": "2065251344",
                "name": "James Bradbury"
            },
            {
                "authorId": "114250963",
                "name": "Gregory Chanan"
            },
            {
                "authorId": "2059271276",
                "name": "Trevor Killeen"
            },
            {
                "authorId": "3370429",
                "name": "Zeming Lin"
            },
            {
                "authorId": "3365851",
                "name": "N. Gimelshein"
            },
            {
                "authorId": "3029482",
                "name": "L. Antiga"
            },
            {
                "authorId": "3050846",
                "name": "Alban Desmaison"
            },
            {
                "authorId": "1473151134",
                "name": "Andreas Köpf"
            },
            {
                "authorId": "2052812305",
                "name": "E. Yang"
            },
            {
                "authorId": "2253681376",
                "name": "Zachary DeVito"
            },
            {
                "authorId": "10707709",
                "name": "Martin Raison"
            },
            {
                "authorId": "41203992",
                "name": "Alykhan Tejani"
            },
            {
                "authorId": "22236100",
                "name": "Sasank Chilamkurthy"
            },
            {
                "authorId": "32163737",
                "name": "Benoit Steiner"
            },
            {
                "authorId": "152599430",
                "name": "Lu Fang"
            },
            {
                "authorId": "2113829116",
                "name": "Junjie Bai"
            },
            {
                "authorId": "2127604",
                "name": "Soumith Chintala"
            }
        ],
        "abstract": "Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it was designed from first principles to support an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several commonly used benchmarks."
    },
    {
        "paperId": "5c126ae3421f05768d8edd97ecd44b1364e2c99a",
        "url": "https://www.semanticscholar.org/paper/5c126ae3421f05768d8edd97ecd44b1364e2c99a",
        "title": "Denoising Diffusion Probabilistic Models",
        "venue": "Neural Information Processing Systems",
        "year": 2020,
        "citationCount": 25438,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2006.11239, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2126278",
                "name": "Jonathan Ho"
            },
            {
                "authorId": "1623995772",
                "name": "Ajay Jain"
            },
            {
                "authorId": "1689992",
                "name": "P. Abbeel"
            }
        ],
        "abstract": "We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at this https URL"
    },
    {
        "paperId": "659bf9ce7175e1ec266ff54359e2bd76e0b7ff31",
        "url": "https://www.semanticscholar.org/paper/659bf9ce7175e1ec266ff54359e2bd76e0b7ff31",
        "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks",
        "venue": "Neural Information Processing Systems",
        "year": 2020,
        "citationCount": 10167,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2005.11401, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "145222654",
                "name": "Patrick Lewis"
            },
            {
                "authorId": "3439053",
                "name": "Ethan Perez"
            },
            {
                "authorId": "1716179427",
                "name": "Aleksandara Piktus"
            },
            {
                "authorId": "40052301",
                "name": "F. Petroni"
            },
            {
                "authorId": "2067091563",
                "name": "Vladimir Karpukhin"
            },
            {
                "authorId": "39589154",
                "name": "Naman Goyal"
            },
            {
                "authorId": "103131985",
                "name": "Heinrich Kuttler"
            },
            {
                "authorId": "35084211",
                "name": "M. Lewis"
            },
            {
                "authorId": "144105277",
                "name": "Wen-tau Yih"
            },
            {
                "authorId": "2620211",
                "name": "Tim Rocktäschel"
            },
            {
                "authorId": "48662861",
                "name": "Sebastian Riedel"
            },
            {
                "authorId": "1743722",
                "name": "Douwe Kiela"
            }
        ],
        "abstract": "Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline."
    },
    {
        "paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0",
        "url": "https://www.semanticscholar.org/paper/90abbc2cf38462b954ae1b772fac9532e2ccd8b0",
        "title": "Language Models are Few-Shot Learners",
        "venue": "Neural Information Processing Systems",
        "year": 2020,
        "citationCount": 51925,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2005.14165, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "31035595",
                "name": "Tom B. Brown"
            },
            {
                "authorId": "2056658938",
                "name": "Benjamin Mann"
            },
            {
                "authorId": "39849748",
                "name": "Nick Ryder"
            },
            {
                "authorId": "2065894334",
                "name": "Melanie Subbiah"
            },
            {
                "authorId": "152724169",
                "name": "J. Kaplan"
            },
            {
                "authorId": "6515819",
                "name": "Prafulla Dhariwal"
            },
            {
                "authorId": "2072676",
                "name": "Arvind Neelakantan"
            },
            {
                "authorId": "67311962",
                "name": "Pranav Shyam"
            },
            {
                "authorId": "144864359",
                "name": "Girish Sastry"
            },
            {
                "authorId": "119609682",
                "name": "Amanda Askell"
            },
            {
                "authorId": "144517868",
                "name": "Sandhini Agarwal"
            },
            {
                "authorId": "1404060687",
                "name": "Ariel Herbert-Voss"
            },
            {
                "authorId": "2064404342",
                "name": "Gretchen Krueger"
            },
            {
                "authorId": "103143311",
                "name": "T. Henighan"
            },
            {
                "authorId": "48422824",
                "name": "R. Child"
            },
            {
                "authorId": "1992922591",
                "name": "A. Ramesh"
            },
            {
                "authorId": "2052152920",
                "name": "Daniel M. Ziegler"
            },
            {
                "authorId": "49387725",
                "name": "Jeff Wu"
            },
            {
                "authorId": "2059411355",
                "name": "Clemens Winter"
            },
            {
                "authorId": "144239765",
                "name": "Christopher Hesse"
            },
            {
                "authorId": "2108828435",
                "name": "Mark Chen"
            },
            {
                "authorId": "2064673055",
                "name": "Eric Sigler"
            },
            {
                "authorId": "1380985420",
                "name": "Ma-teusz Litwin"
            },
            {
                "authorId": "145565184",
                "name": "Scott Gray"
            },
            {
                "authorId": "1490681878",
                "name": "Benjamin Chess"
            },
            {
                "authorId": "2115193883",
                "name": "Jack Clark"
            },
            {
                "authorId": "133740015",
                "name": "Christopher Berner"
            },
            {
                "authorId": "52238703",
                "name": "Sam McCandlish"
            },
            {
                "authorId": "38909097",
                "name": "Alec Radford"
            },
            {
                "authorId": "1701686",
                "name": "I. Sutskever"
            },
            {
                "authorId": "2698777",
                "name": "Dario Amodei"
            }
        ],
        "abstract": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general."
    },
    {
        "paperId": "64ea8f180d0682e6c18d1eb688afdb2027c02794",
        "url": "https://www.semanticscholar.org/paper/64ea8f180d0682e6c18d1eb688afdb2027c02794",
        "title": "Diffusion Models Beat GANs on Image Synthesis",
        "venue": "Neural Information Processing Systems",
        "year": 2021,
        "citationCount": 10194,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2105.05233, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "6515819",
                "name": "Prafulla Dhariwal"
            },
            {
                "authorId": "38967461",
                "name": "Alex Nichol"
            }
        ],
        "abstract": "We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128$\\times$128, 4.59 on ImageNet 256$\\times$256, and 7.72 on ImageNet 512$\\times$512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256$\\times$256 and 3.85 on ImageNet 512$\\times$512. We release our code at https://github.com/openai/guided-diffusion"
    },
    {
        "paperId": "1b6e810ce0afd0dd093f789d2b2742d047e316d5",
        "url": "https://www.semanticscholar.org/paper/1b6e810ce0afd0dd093f789d2b2742d047e316d5",
        "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models",
        "venue": "Neural Information Processing Systems",
        "year": 2022,
        "citationCount": 14321,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2201.11903, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "119640649",
                "name": "Jason Wei"
            },
            {
                "authorId": "2275277634",
                "name": "Xuezhi Wang"
            },
            {
                "authorId": "1714772",
                "name": "Dale Schuurmans"
            },
            {
                "authorId": "40377863",
                "name": "Maarten Bosma"
            },
            {
                "authorId": "2226805",
                "name": "Ed H. Chi"
            },
            {
                "authorId": "144956443",
                "name": "F. Xia"
            },
            {
                "authorId": "1998340269",
                "name": "Quoc Le"
            },
            {
                "authorId": "65855107",
                "name": "Denny Zhou"
            }
        ],
        "abstract": "We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier."
    },
    {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "url": "https://www.semanticscholar.org/paper/d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback",
        "venue": "Neural Information Processing Systems",
        "year": 2022,
        "citationCount": 17257,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2203.02155, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "31793034",
                "name": "Long Ouyang"
            },
            {
                "authorId": "49387725",
                "name": "Jeff Wu"
            },
            {
                "authorId": "2115903168",
                "name": "Xu Jiang"
            },
            {
                "authorId": "2061137049",
                "name": "Diogo Almeida"
            },
            {
                "authorId": "2064084601",
                "name": "Carroll L. Wainwright"
            },
            {
                "authorId": "2051714782",
                "name": "Pamela Mishkin"
            },
            {
                "authorId": null,
                "name": "Chong Zhang"
            },
            {
                "authorId": "144517868",
                "name": "Sandhini Agarwal"
            },
            {
                "authorId": "2117680841",
                "name": "Katarina Slama"
            },
            {
                "authorId": "2064770039",
                "name": "Alex Ray"
            },
            {
                "authorId": "47971768",
                "name": "John Schulman"
            },
            {
                "authorId": "2052366271",
                "name": "Jacob Hilton"
            },
            {
                "authorId": "2151735262",
                "name": "Fraser Kelton"
            },
            {
                "authorId": "2142365973",
                "name": "Luke E. Miller"
            },
            {
                "authorId": "2151735251",
                "name": "Maddie Simens"
            },
            {
                "authorId": "119609682",
                "name": "Amanda Askell"
            },
            {
                "authorId": "2930640",
                "name": "Peter Welinder"
            },
            {
                "authorId": "145791315",
                "name": "P. Christiano"
            },
            {
                "authorId": "2990741",
                "name": "Jan Leike"
            },
            {
                "authorId": "49407415",
                "name": "Ryan J. Lowe"
            }
        ],
        "abstract": "Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent."
    },
    {
        "paperId": "395de0bd3837fdf4b4b5e5f04835bcc69c279481",
        "url": "https://www.semanticscholar.org/paper/395de0bd3837fdf4b4b5e5f04835bcc69c279481",
        "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2019,
        "citationCount": 12030,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/2020.acl-main.703.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1910.13461, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "35084211",
                "name": "M. Lewis"
            },
            {
                "authorId": "11323179",
                "name": "Yinhan Liu"
            },
            {
                "authorId": "39589154",
                "name": "Naman Goyal"
            },
            {
                "authorId": "2320509",
                "name": "Marjan Ghazvininejad"
            },
            {
                "authorId": "113947684",
                "name": "Abdel-rahman Mohamed"
            },
            {
                "authorId": "39455775",
                "name": "Omer Levy"
            },
            {
                "authorId": "1759422",
                "name": "Veselin Stoyanov"
            },
            {
                "authorId": "1982950",
                "name": "Luke Zettlemoyer"
            }
        ],
        "abstract": "We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance."
    },
    {
        "paperId": "d6f2f611da110b5b5061731be3fc4c7f45d8ee23",
        "url": "https://www.semanticscholar.org/paper/d6f2f611da110b5b5061731be3fc4c7f45d8ee23",
        "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2015,
        "citationCount": 19838,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/1502.01852",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1502.01852, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "39353098",
                "name": "Kaiming He"
            },
            {
                "authorId": "1771551",
                "name": "X. Zhang"
            },
            {
                "authorId": "3080683",
                "name": "Shaoqing Ren"
            },
            {
                "authorId": null,
                "name": "Jian Sun"
            }
        ],
        "abstract": "Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on the learnable activation and advanced initialization, we achieve 4.94% top-5 test error on the ImageNet 2012 classification dataset. This is a 26% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66% [33]). To our knowledge, our result is the first to surpass the reported human-level performance (5.1%, [26]) on this dataset."
    },
    {
        "paperId": "79cfb51a51fc093f66aac8e858afe2e14d4a1f20",
        "url": "https://www.semanticscholar.org/paper/79cfb51a51fc093f66aac8e858afe2e14d4a1f20",
        "title": "Focal Loss for Dense Object Detection",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2017,
        "citationCount": 28997,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/1708.02002",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICCV.2017.324?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICCV.2017.324, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "33493200",
                "name": "Tsung-Yi Lin"
            },
            {
                "authorId": "47316088",
                "name": "Priya Goyal"
            },
            {
                "authorId": "2983898",
                "name": "Ross B. Girshick"
            },
            {
                "authorId": "39353098",
                "name": "Kaiming He"
            },
            {
                "authorId": "3127283",
                "name": "Piotr Dollár"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "c8b25fab5608c3e033d34b4483ec47e68ba109b7",
        "url": "https://www.semanticscholar.org/paper/c8b25fab5608c3e033d34b4483ec47e68ba109b7",
        "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2021,
        "citationCount": 28055,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2103.14030",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2103.14030, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2109371439",
                "name": "Ze Liu"
            },
            {
                "authorId": "51091819",
                "name": "Yutong Lin"
            },
            {
                "authorId": "2112823372",
                "name": "Yue Cao"
            },
            {
                "authorId": "1823518756",
                "name": "Han Hu"
            },
            {
                "authorId": "2107995927",
                "name": "Yixuan Wei"
            },
            {
                "authorId": "2148904543",
                "name": "Zheng Zhang"
            },
            {
                "authorId": "145676588",
                "name": "Stephen Lin"
            },
            {
                "authorId": "2261753424",
                "name": "B. Guo"
            }
        ],
        "abstract": "This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with Shifted windows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The hierarchical design and the shifted window approach also prove beneficial for all-MLP architectures. The code and models are publicly available at https://github.com/microsoft/Swin-Transformer."
    },
    {
        "paperId": "7470a1702c8c86e6f28d32cfa315381150102f5b",
        "url": "https://www.semanticscholar.org/paper/7470a1702c8c86e6f28d32cfa315381150102f5b",
        "title": "Segment Anything",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2023,
        "citationCount": 10923,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2304.02643, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2064802835",
                "name": "A. Kirillov"
            },
            {
                "authorId": "13131689",
                "name": "Eric Mintun"
            },
            {
                "authorId": "2065647966",
                "name": "Nikhila Ravi"
            },
            {
                "authorId": "2053590350",
                "name": "Hanzi Mao"
            },
            {
                "authorId": "2213549340",
                "name": "Chloé Rolland"
            },
            {
                "authorId": "47029037",
                "name": "Laura Gustafson"
            },
            {
                "authorId": "15727192",
                "name": "Tete Xiao"
            },
            {
                "authorId": "153188991",
                "name": "Spencer Whitehead"
            },
            {
                "authorId": "39668247",
                "name": "A. Berg"
            },
            {
                "authorId": "3317278",
                "name": "Wan-Yen Lo"
            },
            {
                "authorId": "3127283",
                "name": "Piotr Dollár"
            },
            {
                "authorId": "2983898",
                "name": "Ross B. Girshick"
            }
        ],
        "abstract": "We introduce the Segment Anything (SA) project: a new task, model, and dataset for image segmentation. Using our efficient model in a data collection loop, we built the largest segmentation dataset to date (by far), with over 1 billion masks on 11M licensed and privacy respecting images. The model is designed and trained to be promptable, so it can transfer zero-shot to new image distributions and tasks. We evaluate its capabilities on numerous tasks and find that its zero-shot performance is impressive – often competitive with or even superior to prior fully supervised results. We are releasing the Segment Anything Model (SAM) and corresponding dataset (SA-1B) of 1B masks and 11M images at segment-anything.com to foster research into foundation models for computer vision. We recommend reading the full paper at: arxiv.org/abs/2304.02643."
    },
    {
        "paperId": "a538b05ebb01a40323997629e171c91aa28b8e2f",
        "url": "https://www.semanticscholar.org/paper/a538b05ebb01a40323997629e171c91aa28b8e2f",
        "title": "Rectified Linear Units Improve Restricted Boltzmann Machines",
        "venue": "International Conference on Machine Learning",
        "year": 2010,
        "citationCount": 18277,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "2073603971",
                "name": "Vinod Nair"
            },
            {
                "authorId": "1695689",
                "name": "Geoffrey E. Hinton"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "4d8f2d14af5991d4f0d050d22216825cac3157bd",
        "url": "https://www.semanticscholar.org/paper/4d8f2d14af5991d4f0d050d22216825cac3157bd",
        "title": "Show, Attend and Tell: Neural Image Caption Generation with Visual Attention",
        "venue": "International Conference on Machine Learning",
        "year": 2015,
        "citationCount": 10558,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1502.03044, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2117101253",
                "name": "Ke Xu"
            },
            {
                "authorId": "2503659",
                "name": "Jimmy Ba"
            },
            {
                "authorId": "3450996",
                "name": "Ryan Kiros"
            },
            {
                "authorId": "1979489",
                "name": "Kyunghyun Cho"
            },
            {
                "authorId": "1760871",
                "name": "Aaron C. Courville"
            },
            {
                "authorId": "145124475",
                "name": "R. Salakhutdinov"
            },
            {
                "authorId": "1804104",
                "name": "R. Zemel"
            },
            {
                "authorId": "1751762",
                "name": "Yoshua Bengio"
            }
        ],
        "abstract": "Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-the-art performance on three benchmark datasets: Flickr9k, Flickr30k and MS COCO."
    },
    {
        "paperId": "995c5f5e62614fcb4d2796ad2faab969da51713e",
        "url": "https://www.semanticscholar.org/paper/995c5f5e62614fcb4d2796ad2faab969da51713e",
        "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
        "venue": "International Conference on Machine Learning",
        "year": 2015,
        "citationCount": 45617,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1502.03167, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2054165706",
                "name": "Sergey Ioffe"
            },
            {
                "authorId": "2574060",
                "name": "Christian Szegedy"
            }
        ],
        "abstract": "Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82% top-5 test error, exceeding the accuracy of human raters."
    },
    {
        "paperId": "f35de4f9b1a7c4d3fa96a0d2ab1bf8937671f6b6",
        "url": "https://www.semanticscholar.org/paper/f35de4f9b1a7c4d3fa96a0d2ab1bf8937671f6b6",
        "title": "Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning",
        "venue": "International Conference on Machine Learning",
        "year": 2015,
        "citationCount": 10628,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1506.02142, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2681954",
                "name": "Y. Gal"
            },
            {
                "authorId": "1744700",
                "name": "Zoubin Ghahramani"
            }
        ],
        "abstract": "Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs -- extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout's uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout's uncertainty in deep reinforcement learning."
    },
    {
        "paperId": "c889d6f98e6d79b89c3a6adf8a921f88fa6ba518",
        "url": "https://www.semanticscholar.org/paper/c889d6f98e6d79b89c3a6adf8a921f88fa6ba518",
        "title": "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks",
        "venue": "International Conference on Machine Learning",
        "year": 2017,
        "citationCount": 13402,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1703.03400, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "46881670",
                "name": "Chelsea Finn"
            },
            {
                "authorId": "1689992",
                "name": "P. Abbeel"
            },
            {
                "authorId": "1736651",
                "name": "S. Levine"
            }
        ],
        "abstract": "We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies."
    },
    {
        "paperId": "4f2eda8077dc7a69bb2b4e0a1a086cf054adb3f9",
        "url": "https://www.semanticscholar.org/paper/4f2eda8077dc7a69bb2b4e0a1a086cf054adb3f9",
        "title": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks",
        "venue": "International Conference on Machine Learning",
        "year": 2019,
        "citationCount": 21490,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1905.11946, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "120805419",
                "name": "Mingxing Tan"
            },
            {
                "authorId": "2827616",
                "name": "Quoc V. Le"
            }
        ],
        "abstract": "Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet. \nTo go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.4% top-1 / 97.1% top-5 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7%), Flowers (98.8%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters. Source code is at this https URL."
    },
    {
        "paperId": "7af72a461ed7cda180e7eab878efd5f35d79bbf4",
        "url": "https://www.semanticscholar.org/paper/7af72a461ed7cda180e7eab878efd5f35d79bbf4",
        "title": "A Simple Framework for Contrastive Learning of Visual Representations",
        "venue": "International Conference on Machine Learning",
        "year": 2020,
        "citationCount": 22189,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2002.05709, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "145358498",
                "name": "Ting Chen"
            },
            {
                "authorId": "40464924",
                "name": "Simon Kornblith"
            },
            {
                "authorId": "144739074",
                "name": "Mohammad Norouzi"
            },
            {
                "authorId": "1695689",
                "name": "Geoffrey E. Hinton"
            }
        ],
        "abstract": "This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy, outperforming AlexNet with 100X fewer labels."
    },
    {
        "paperId": "6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4",
        "url": "https://www.semanticscholar.org/paper/6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4",
        "title": "Learning Transferable Visual Models From Natural Language Supervision",
        "venue": "International Conference on Machine Learning",
        "year": 2021,
        "citationCount": 40671,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2103.00020, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "38909097",
                "name": "Alec Radford"
            },
            {
                "authorId": "2110935237",
                "name": "Jong Wook Kim"
            },
            {
                "authorId": "2004021329",
                "name": "Chris Hallacy"
            },
            {
                "authorId": "1992922591",
                "name": "A. Ramesh"
            },
            {
                "authorId": "40087786",
                "name": "Gabriel Goh"
            },
            {
                "authorId": "144517868",
                "name": "Sandhini Agarwal"
            },
            {
                "authorId": "144864359",
                "name": "Girish Sastry"
            },
            {
                "authorId": "119609682",
                "name": "Amanda Askell"
            },
            {
                "authorId": "2051714782",
                "name": "Pamela Mishkin"
            },
            {
                "authorId": "2115193883",
                "name": "Jack Clark"
            },
            {
                "authorId": "2064404342",
                "name": "Gretchen Krueger"
            },
            {
                "authorId": "1701686",
                "name": "I. Sutskever"
            }
        ],
        "abstract": "State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP."
    },
    {
        "paperId": "5f5dc5b9a2ba710937e2c413b37b053cd673df02",
        "url": "https://www.semanticscholar.org/paper/5f5dc5b9a2ba710937e2c413b37b053cd673df02",
        "title": "Auto-Encoding Variational Bayes",
        "venue": "International Conference on Learning Representations",
        "year": 2013,
        "citationCount": 17251,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1312.6114, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1726807",
                "name": "Diederik P. Kingma"
            },
            {
                "authorId": "1678311",
                "name": "M. Welling"
            }
        ],
        "abstract": "Abstract: How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results."
    },
    {
        "paperId": "d891dc72cbd40ffaeefdc79f2e7afe1e530a23ad",
        "url": "https://www.semanticscholar.org/paper/d891dc72cbd40ffaeefdc79f2e7afe1e530a23ad",
        "title": "Intriguing properties of neural networks",
        "venue": "International Conference on Learning Representations",
        "year": 2013,
        "citationCount": 15952,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1312.6199, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2574060",
                "name": "Christian Szegedy"
            },
            {
                "authorId": "2563432",
                "name": "Wojciech Zaremba"
            },
            {
                "authorId": "1701686",
                "name": "I. Sutskever"
            },
            {
                "authorId": "143627859",
                "name": "Joan Bruna"
            },
            {
                "authorId": "1761978",
                "name": "D. Erhan"
            },
            {
                "authorId": "153440022",
                "name": "I. Goodfellow"
            },
            {
                "authorId": "2276554",
                "name": "R. Fergus"
            }
        ],
        "abstract": "Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. \nFirst, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks. \nSecond, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. We can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input."
    },
    {
        "paperId": "f6b51c8753a871dc94ff32152c00c01e94f90f09",
        "url": "https://www.semanticscholar.org/paper/f6b51c8753a871dc94ff32152c00c01e94f90f09",
        "title": "Efficient Estimation of Word Representations in Vector Space",
        "venue": "International Conference on Learning Representations",
        "year": 2013,
        "citationCount": 33352,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1301.3781, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2047446108",
                "name": "Tomas Mikolov"
            },
            {
                "authorId": "2118440152",
                "name": "Kai Chen"
            },
            {
                "authorId": "32131713",
                "name": "G. Corrado"
            },
            {
                "authorId": "49959210",
                "name": "J. Dean"
            }
        ],
        "abstract": "We propose two novel model architectures for computing continuous vector\nrepresentations of words from very large data sets. The quality of these\nrepresentations is measured in a word similarity task, and the results are\ncompared to the previously best performing techniques based on different types\nof neural networks. We observe large improvements in accuracy at much lower\ncomputational cost, i.e. it takes less than a day to learn high quality word\nvectors from a 1.6 billion words data set. Furthermore, we show that these\nvectors provide state-of-the-art performance on our test set for measuring\nsyntactic and semantic word similarities."
    },
    {
        "paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8",
        "url": "https://www.semanticscholar.org/paper/a6cb366736791bcccc5c8639de5a8f9636bf87e8",
        "title": "Adam: A Method for Stochastic Optimization",
        "venue": "International Conference on Learning Representations",
        "year": 2014,
        "citationCount": 160974,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1412.6980, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1726807",
                "name": "Diederik P. Kingma"
            },
            {
                "authorId": "2503659",
                "name": "Jimmy Ba"
            }
        ],
        "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm."
    },
    {
        "paperId": "bee044c8e8903fb67523c1f8c105ab4718600cdb",
        "url": "https://www.semanticscholar.org/paper/bee044c8e8903fb67523c1f8c105ab4718600cdb",
        "title": "Explaining and Harnessing Adversarial Examples",
        "venue": "International Conference on Learning Representations",
        "year": 2014,
        "citationCount": 20930,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1412.6572, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "153440022",
                "name": "I. Goodfellow"
            },
            {
                "authorId": "1789737",
                "name": "Jonathon Shlens"
            },
            {
                "authorId": "2574060",
                "name": "Christian Szegedy"
            }
        ],
        "abstract": "Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset."
    },
    {
        "paperId": "eb42cf88027de515750f230b23b1a057dc782108",
        "url": "https://www.semanticscholar.org/paper/eb42cf88027de515750f230b23b1a057dc782108",
        "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
        "venue": "International Conference on Learning Representations",
        "year": 2014,
        "citationCount": 107964,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1409.1556, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "34838386",
                "name": "K. Simonyan"
            },
            {
                "authorId": "1688869",
                "name": "Andrew Zisserman"
            }
        ],
        "abstract": "In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision."
    },
    {
        "paperId": "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5",
        "url": "https://www.semanticscholar.org/paper/fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5",
        "title": "Neural Machine Translation by Jointly Learning to Align and Translate",
        "venue": "International Conference on Learning Representations",
        "year": 2014,
        "citationCount": 28606,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1409.0473, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3335364",
                "name": "Dzmitry Bahdanau"
            },
            {
                "authorId": "1979489",
                "name": "Kyunghyun Cho"
            },
            {
                "authorId": "1751762",
                "name": "Yoshua Bengio"
            }
        ],
        "abstract": "Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition."
    },
    {
        "paperId": "024006d4c2a89f7acacc6e4438d156525b60a98f",
        "url": "https://www.semanticscholar.org/paper/024006d4c2a89f7acacc6e4438d156525b60a98f",
        "title": "Continuous control with deep reinforcement learning",
        "venue": "International Conference on Learning Representations",
        "year": 2015,
        "citationCount": 14621,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1509.02971, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2542999",
                "name": "T. Lillicrap"
            },
            {
                "authorId": "2323922",
                "name": "Jonathan J. Hunt"
            },
            {
                "authorId": "1863250",
                "name": "A. Pritzel"
            },
            {
                "authorId": "2801204",
                "name": "N. Heess"
            },
            {
                "authorId": "1968210",
                "name": "Tom Erez"
            },
            {
                "authorId": "2109481",
                "name": "Yuval Tassa"
            },
            {
                "authorId": "145824029",
                "name": "David Silver"
            },
            {
                "authorId": "1688276",
                "name": "Daan Wierstra"
            }
        ],
        "abstract": "We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs."
    },
    {
        "paperId": "8388f1be26329fa45e5807e968a641ce170ea078",
        "url": "https://www.semanticscholar.org/paper/8388f1be26329fa45e5807e968a641ce170ea078",
        "title": "Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks",
        "venue": "International Conference on Learning Representations",
        "year": 2015,
        "citationCount": 14775,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1511.06434, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "38909097",
                "name": "Alec Radford"
            },
            {
                "authorId": "2096458",
                "name": "Luke Metz"
            },
            {
                "authorId": "2127604",
                "name": "Soumith Chintala"
            }
        ],
        "abstract": "In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations."
    },
    {
        "paperId": "36eff562f65125511b5dfab68ce7f7a943c27478",
        "url": "https://www.semanticscholar.org/paper/36eff562f65125511b5dfab68ce7f7a943c27478",
        "title": "Semi-Supervised Classification with Graph Convolutional Networks",
        "venue": "International Conference on Learning Representations",
        "year": 2016,
        "citationCount": 32722,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1609.02907, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "41016725",
                "name": "Thomas Kipf"
            },
            {
                "authorId": "1678311",
                "name": "M. Welling"
            }
        ],
        "abstract": "We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin."
    },
    {
        "paperId": "33998aff64ce51df8dee45989cdca4b6b1329ec4",
        "url": "https://www.semanticscholar.org/paper/33998aff64ce51df8dee45989cdca4b6b1329ec4",
        "title": "Graph Attention Networks",
        "venue": "International Conference on Learning Representations",
        "year": 2017,
        "citationCount": 23897,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1710.10903, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3444569",
                "name": "Petar Velickovic"
            },
            {
                "authorId": "7153363",
                "name": "Guillem Cucurull"
            },
            {
                "authorId": "8742492",
                "name": "Arantxa Casanova"
            },
            {
                "authorId": "144290131",
                "name": "Adriana Romero"
            },
            {
                "authorId": "2392269716",
                "name": "Pietro Liò"
            },
            {
                "authorId": "1751762",
                "name": "Yoshua Bengio"
            }
        ],
        "abstract": "We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs remain unseen during training)."
    },
    {
        "paperId": "4feef0fd284feb1233399b400eb897f59ec92755",
        "url": "https://www.semanticscholar.org/paper/4feef0fd284feb1233399b400eb897f59ec92755",
        "title": "mixup: Beyond Empirical Risk Minimization",
        "venue": "International Conference on Learning Representations",
        "year": 2017,
        "citationCount": 11054,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1710.09412, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2108880169",
                "name": "Hongyi Zhang"
            },
            {
                "authorId": "5723508",
                "name": "Moustapha Cissé"
            },
            {
                "authorId": "2921469",
                "name": "Yann Dauphin"
            },
            {
                "authorId": "1401804750",
                "name": "David Lopez-Paz"
            }
        ],
        "abstract": "Large deep neural networks are powerful, but exhibit undesirable behaviors such as memorization and sensitivity to adversarial examples. In this work, we propose mixup, a simple learning principle to alleviate these issues. In essence, mixup trains a neural network on convex combinations of pairs of examples and their labels. By doing so, mixup regularizes the neural network to favor simple linear behavior in-between training examples. Our experiments on the ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show that mixup improves the generalization of state-of-the-art neural network architectures. We also find that mixup reduces the memorization of corrupt labels, increases the robustness to adversarial examples, and stabilizes the training of generative adversarial networks."
    },
    {
        "paperId": "7aa38b85fa8cba64d6a4010543f6695dbf5f1386",
        "url": "https://www.semanticscholar.org/paper/7aa38b85fa8cba64d6a4010543f6695dbf5f1386",
        "title": "Towards Deep Learning Models Resistant to Adversarial Attacks",
        "venue": "International Conference on Learning Representations",
        "year": 2017,
        "citationCount": 13667,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1706.06083, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "143826246",
                "name": "A. Ma̧dry"
            },
            {
                "authorId": "17775913",
                "name": "Aleksandar Makelov"
            },
            {
                "authorId": "152772922",
                "name": "Ludwig Schmidt"
            },
            {
                "authorId": "2754804",
                "name": "Dimitris Tsipras"
            },
            {
                "authorId": "2869958",
                "name": "Adrian Vladu"
            }
        ],
        "abstract": "Recent work has demonstrated that deep neural networks are vulnerable to adversarial examples---inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. Code and pre-trained models are available at this https URL and this https URL."
    },
    {
        "paperId": "d07284a6811f1b2745d91bdb06b040b57f226882",
        "url": "https://www.semanticscholar.org/paper/d07284a6811f1b2745d91bdb06b040b57f226882",
        "title": "Decoupled Weight Decay Regularization",
        "venue": "International Conference on Learning Representations",
        "year": 2017,
        "citationCount": 29140,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "1678656",
                "name": "I. Loshchilov"
            },
            {
                "authorId": "144661829",
                "name": "F. Hutter"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "014576b866078524286802b1d0e18628520aa886",
        "url": "https://www.semanticscholar.org/paper/014576b866078524286802b1d0e18628520aa886",
        "title": "Denoising Diffusion Implicit Models",
        "venue": "International Conference on Learning Representations",
        "year": 2020,
        "citationCount": 10094,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2010.02502, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "51453887",
                "name": "Jiaming Song"
            },
            {
                "authorId": "2057110631",
                "name": "Chenlin Meng"
            },
            {
                "authorId": "2066211828",
                "name": "Stefano Ermon"
            }
        ],
        "abstract": "Denoising diffusion probabilistic models (DDPMs) have achieved high quality image generation without adversarial training, yet they require simulating a Markov chain for many steps to produce a sample. To accelerate sampling, we present denoising diffusion implicit models (DDIMs), a more efficient class of iterative implicit probabilistic models with the same training procedure as DDPMs. In DDPMs, the generative process is defined as the reverse of a Markovian diffusion process. We construct a class of non-Markovian diffusion processes that lead to the same training objective, but whose reverse process can be much faster to sample from. We empirically demonstrate that DDIMs can produce high quality samples $10 \\times$ to $50 \\times$ faster in terms of wall-clock time compared to DDPMs, allow us to trade off computation for sample quality, and can perform semantically meaningful image interpolation directly in the latent space."
    },
    {
        "paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a",
        "url": "https://www.semanticscholar.org/paper/268d347e8a55b5eb82fb5e7d2f800e33c75ab18a",
        "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
        "venue": "International Conference on Learning Representations",
        "year": 2020,
        "citationCount": 54421,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2010.11929, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2841331",
                "name": "Alexey Dosovitskiy"
            },
            {
                "authorId": "39611591",
                "name": "Lucas Beyer"
            },
            {
                "authorId": "144629422",
                "name": "Alexander Kolesnikov"
            },
            {
                "authorId": "3319373",
                "name": "Dirk Weissenborn"
            },
            {
                "authorId": "2743563",
                "name": "Xiaohua Zhai"
            },
            {
                "authorId": "2465270",
                "name": "Thomas Unterthiner"
            },
            {
                "authorId": "2274215058",
                "name": "Mostafa Dehghani"
            },
            {
                "authorId": "46352821",
                "name": "M. Minderer"
            },
            {
                "authorId": "2280399",
                "name": "G. Heigold"
            },
            {
                "authorId": "1802148",
                "name": "S. Gelly"
            },
            {
                "authorId": "39328010",
                "name": "Jakob Uszkoreit"
            },
            {
                "authorId": "2815290",
                "name": "N. Houlsby"
            }
        ],
        "abstract": "While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train."
    },
    {
        "paperId": "a8ca46b171467ceb2d7652fbfb67fe701ad86092",
        "url": "https://www.semanticscholar.org/paper/a8ca46b171467ceb2d7652fbfb67fe701ad86092",
        "title": "LoRA: Low-Rank Adaptation of Large Language Models",
        "venue": "International Conference on Learning Representations",
        "year": 2021,
        "citationCount": 15140,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2106.09685, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2157840220",
                "name": "J. E. Hu"
            },
            {
                "authorId": "1752875",
                "name": "Yelong Shen"
            },
            {
                "authorId": "104100507",
                "name": "Phillip Wallis"
            },
            {
                "authorId": "1388725932",
                "name": "Zeyuan Allen-Zhu"
            },
            {
                "authorId": "2110486765",
                "name": "Yuanzhi Li"
            },
            {
                "authorId": "2135571585",
                "name": "Shean Wang"
            },
            {
                "authorId": "2109136147",
                "name": "Weizhu Chen"
            }
        ],
        "abstract": "An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA."
    },
    {
        "paperId": "9ad3179098eb9ac7f0c81bf89b6bdcde91752880",
        "url": "https://www.semanticscholar.org/paper/9ad3179098eb9ac7f0c81bf89b6bdcde91752880",
        "title": "SURF: Speeded Up Robust Features",
        "venue": "European Conference on Computer Vision",
        "year": 2006,
        "citationCount": 10464,
        "openAccessPdf": {
            "url": "https://link.springer.com/content/pdf/10.1007/11744023_32.pdf",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/11744023_32?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/11744023_32, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2195306",
                "name": "Herbert Bay"
            },
            {
                "authorId": "1704728",
                "name": "T. Tuytelaars"
            },
            {
                "authorId": "1681236",
                "name": "L. Gool"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "1a2a770d23b4a171fa81de62a78a3deb0588f238",
        "url": "https://www.semanticscholar.org/paper/1a2a770d23b4a171fa81de62a78a3deb0588f238",
        "title": "Visualizing and Understanding Convolutional Networks",
        "venue": "European Conference on Computer Vision",
        "year": 2013,
        "citationCount": 16621,
        "openAccessPdf": {
            "url": "https://link.springer.com/content/pdf/10.1007/978-3-319-10590-1_53.pdf",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1311.2901, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "48799969",
                "name": "Matthew D. Zeiler"
            },
            {
                "authorId": "2276554",
                "name": "R. Fergus"
            }
        ],
        "abstract": "Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark Krizhevsky et al. [18]. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we explore both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. Used in a diagnostic role, these visualizations allow us to find model architectures that outperform Krizhevsky et al on the ImageNet classification benchmark. We also perform an ablation study to discover the performance contribution from different model layers. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets."
    },
    {
        "paperId": "71b7178df5d2b112d07e45038cb5637208659ff7",
        "url": "https://www.semanticscholar.org/paper/71b7178df5d2b112d07e45038cb5637208659ff7",
        "title": "Microsoft COCO: Common Objects in Context",
        "venue": "European Conference on Computer Vision",
        "year": 2014,
        "citationCount": 49175,
        "openAccessPdf": {
            "url": "https://link.springer.com/content/pdf/10.1007%2F978-3-319-10602-1_48.pdf",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1405.0312, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "33493200",
                "name": "Tsung-Yi Lin"
            },
            {
                "authorId": "145854440",
                "name": "M. Maire"
            },
            {
                "authorId": "50172592",
                "name": "Serge J. Belongie"
            },
            {
                "authorId": "48966748",
                "name": "James Hays"
            },
            {
                "authorId": "1690922",
                "name": "P. Perona"
            },
            {
                "authorId": "1770537",
                "name": "Deva Ramanan"
            },
            {
                "authorId": "3127283",
                "name": "Piotr Dollár"
            },
            {
                "authorId": "1699161",
                "name": "C. L. Zitnick"
            }
        ],
        "abstract": "We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model."
    },
    {
        "paperId": "4d7a9197433acbfb24ef0e9d0f33ed1699e4a5b0",
        "url": "https://www.semanticscholar.org/paper/4d7a9197433acbfb24ef0e9d0f33ed1699e4a5b0",
        "title": "SSD: Single Shot MultiBox Detector",
        "venue": "European Conference on Computer Vision",
        "year": 2015,
        "citationCount": 33193,
        "openAccessPdf": {
            "url": "https://link.springer.com/content/pdf/10.1007/978-3-319-46448-0_2.pdf",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1512.02325, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "46641573",
                "name": "W. Liu"
            },
            {
                "authorId": "1838674",
                "name": "Dragomir Anguelov"
            },
            {
                "authorId": "1761978",
                "name": "D. Erhan"
            },
            {
                "authorId": "2574060",
                "name": "Christian Szegedy"
            },
            {
                "authorId": "144828948",
                "name": "Scott E. Reed"
            },
            {
                "authorId": "2084646762",
                "name": "Cheng-Yang Fu"
            },
            {
                "authorId": "39668247",
                "name": "A. Berg"
            }
        ],
        "abstract": "We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. Our SSD model is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stage and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, MS COCO, and ILSVRC datasets confirm that SSD has comparable accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. Compared to other single stage methods, SSD has much better accuracy, even with a smaller input image size. For $300\\times 300$ input, SSD achieves 72.1% mAP on VOC2007 test at 58 FPS on a Nvidia Titan X and for $500\\times 500$ input, SSD achieves 75.1% mAP, outperforming a comparable state of the art Faster R-CNN model. Code is available at this https URL ."
    },
    {
        "paperId": "77f0a39b8e02686fd85b01971f8feb7f60971f80",
        "url": "https://www.semanticscholar.org/paper/77f0a39b8e02686fd85b01971f8feb7f60971f80",
        "title": "Identity Mappings in Deep Residual Networks",
        "venue": "European Conference on Computer Vision",
        "year": 2016,
        "citationCount": 10835,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1603.05027, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "39353098",
                "name": "Kaiming He"
            },
            {
                "authorId": "1771551",
                "name": "X. Zhang"
            },
            {
                "authorId": "3080683",
                "name": "Shaoqing Ren"
            },
            {
                "authorId": null,
                "name": "Jian Sun"
            }
        ],
        "abstract": "Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which makes training easier and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR-10 (4.62 % error) and CIFAR-100, and a 200-layer ResNet on ImageNet. Code is available at: https://github.com/KaimingHe/resnet-1k-layers."
    },
    {
        "paperId": "915c4bb289b3642489e904c65a47fa56efb60658",
        "url": "https://www.semanticscholar.org/paper/915c4bb289b3642489e904c65a47fa56efb60658",
        "title": "Perceptual Losses for Real-Time Style Transfer and Super-Resolution",
        "venue": "European Conference on Computer Vision",
        "year": 2016,
        "citationCount": 11018,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1603.08155, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2115231104",
                "name": "Justin Johnson"
            },
            {
                "authorId": "3304525",
                "name": "Alexandre Alahi"
            },
            {
                "authorId": "48004138",
                "name": "Li Fei-Fei"
            }
        ],
        "abstract": "We consider image transformation problems, where an input image is transformed into an output image. Recent methods for such problems typically train feed-forward convolutional neural networks using a per-pixel loss between the output and ground-truth images. Parallel work has shown that high-quality images can be generated by defining and optimizing perceptual loss functions based on high-level features extracted from pretrained networks. We combine the benefits of both approaches, and propose the use of perceptual loss functions for training feed-forward networks for image transformation tasks. We show results on image style transfer, where a feed-forward network is trained to solve the optimization problem proposed by Gatys et al. in real-time. Compared to the optimization-based method, our network gives similar qualitative results but is three orders of magnitude faster. We also experiment with single-image super-resolution, where replacing a per-pixel loss with a perceptual loss gives visually pleasing results."
    },
    {
        "paperId": "9217e28b2273eb3b26e4e9b7b498b4661e6e09f5",
        "url": "https://www.semanticscholar.org/paper/9217e28b2273eb3b26e4e9b7b498b4661e6e09f5",
        "title": "Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation",
        "venue": "European Conference on Computer Vision",
        "year": 2018,
        "citationCount": 15272,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1802.02611, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "34192119",
                "name": "Liang-Chieh Chen"
            },
            {
                "authorId": "1844940337",
                "name": "Yukun Zhu"
            },
            {
                "authorId": "2776496",
                "name": "G. Papandreou"
            },
            {
                "authorId": "3302320",
                "name": "Florian Schroff"
            },
            {
                "authorId": "2595180",
                "name": "Hartwig Adam"
            }
        ],
        "abstract": "Spatial pyramid pooling module or encode-decoder structure are used in deep neural networks for semantic segmentation task. The former networks are able to encode multi-scale contextual information by probing the incoming features with filters or pooling operations at multiple rates and multiple effective fields-of-view, while the latter networks can capture sharper object boundaries by gradually recovering the spatial information. In this work, we propose to combine the advantages from both methods. Specifically, our proposed model, DeepLabv3+, extends DeepLabv3 by adding a simple yet effective decoder module to refine the segmentation results especially along object boundaries. We further explore the Xception model and apply the depthwise separable convolution to both Atrous Spatial Pyramid Pooling and decoder modules, resulting in a faster and stronger encoder-decoder network. We demonstrate the effectiveness of the proposed model on PASCAL VOC 2012 and Cityscapes datasets, achieving the test set performance of 89.0\\% and 82.1\\% without any post-processing. Our paper is accompanied with a publicly available reference implementation of the proposed models in Tensorflow at \\url{this https URL}."
    },
    {
        "paperId": "de95601d9e3b20ec51aa33e1f27b1880d2c44ef2",
        "url": "https://www.semanticscholar.org/paper/de95601d9e3b20ec51aa33e1f27b1880d2c44ef2",
        "title": "CBAM: Convolutional Block Attention Module",
        "venue": "European Conference on Computer Vision",
        "year": 2018,
        "citationCount": 20913,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1807.06521, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2262209",
                "name": "Sanghyun Woo"
            },
            {
                "authorId": "2109216573",
                "name": "Jongchan Park"
            },
            {
                "authorId": "1926578",
                "name": "Joon-Young Lee"
            },
            {
                "authorId": "2398271",
                "name": "In-So Kweon"
            }
        ],
        "abstract": "We propose Convolutional Block Attention Module (CBAM), a simple yet effective attention module for feed-forward convolutional neural networks. Given an intermediate feature map, our module sequentially infers attention maps along two separate dimensions, channel and spatial, then the attention maps are multiplied to the input feature map for adaptive feature refinement. Because CBAM is a lightweight and general module, it can be integrated into any CNN architectures seamlessly with negligible overheads and is end-to-end trainable along with base CNNs. We validate our CBAM through extensive experiments on ImageNet-1K, MS COCO detection, and VOC 2007 detection datasets. Our experiments show consistent improvements in classification and detection performances with various models, demonstrating the wide applicability of CBAM. The code and models will be publicly available."
    },
    {
        "paperId": "962dc29fdc3fbdc5930a10aba114050b82fe5a3e",
        "url": "https://www.semanticscholar.org/paper/962dc29fdc3fbdc5930a10aba114050b82fe5a3e",
        "title": "End-to-End Object Detection with Transformers",
        "venue": "European Conference on Computer Vision",
        "year": 2020,
        "citationCount": 16281,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2005.12872, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3422899",
                "name": "Nicolas Carion"
            },
            {
                "authorId": "1403239967",
                "name": "Francisco Massa"
            },
            {
                "authorId": "2282478",
                "name": "Gabriel Synnaeve"
            },
            {
                "authorId": "1746841",
                "name": "Nicolas Usunier"
            },
            {
                "authorId": "144843400",
                "name": "Alexander Kirillov"
            },
            {
                "authorId": "2134433",
                "name": "Sergey Zagoruyko"
            }
        ],
        "abstract": "We present a new method that views object detection as a direct set prediction problem. Our approach streamlines the detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the final set of predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster RCNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily generalized to produce panoptic segmentation in a unified manner. We show that it significantly outperforms competitive baselines. Training code and pretrained models are available at this https URL."
    },
    {
        "paperId": "0b544dfe355a5070b60986319a3f51fb45d1348e",
        "url": "https://www.semanticscholar.org/paper/0b544dfe355a5070b60986319a3f51fb45d1348e",
        "title": "Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2014,
        "citationCount": 25199,
        "openAccessPdf": {
            "url": "https://aclanthology.org/D14-1179.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1406.1078, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1979489",
                "name": "Kyunghyun Cho"
            },
            {
                "authorId": "3158246",
                "name": "B. V. Merrienboer"
            },
            {
                "authorId": "1854385",
                "name": "Çaglar Gülçehre"
            },
            {
                "authorId": "3335364",
                "name": "Dzmitry Bahdanau"
            },
            {
                "authorId": "2076086",
                "name": "Fethi Bougares"
            },
            {
                "authorId": "144518416",
                "name": "Holger Schwenk"
            },
            {
                "authorId": "1751762",
                "name": "Yoshua Bengio"
            }
        ],
        "abstract": "In this paper, we propose a novel neural network model called RNN Encoder‐ Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixedlength vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder‐Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases."
    },
    {
        "paperId": "1f6ba0782862ec12a5ec6d7fb608523d55b0c6ba",
        "url": "https://www.semanticscholar.org/paper/1f6ba0782862ec12a5ec6d7fb608523d55b0c6ba",
        "title": "Convolutional Neural Networks for Sentence Classification",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2014,
        "citationCount": 13889,
        "openAccessPdf": {
            "url": "https://aclanthology.org/D14-1181.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1408.5882, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "38367242",
                "name": "Yoon Kim"
            }
        ],
        "abstract": "We report on a series of experiments with convolutional neural networks (CNN) trained on top of pre-trained word vectors for sentence-level classification tasks. We show that a simple CNN with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks. Learning task-specific vectors through fine-tuning offers further gains in performance. We additionally propose a simple modification to the architecture to allow for the use of both task-specific and static vectors. The CNN models discussed herein improve upon the state of the art on 4 out of 7 tasks, which include sentiment analysis and question classification."
    },
    {
        "paperId": "f37e1b62a767a307c046404ca96bc140b3e68cb5",
        "url": "https://www.semanticscholar.org/paper/f37e1b62a767a307c046404ca96bc140b3e68cb5",
        "title": "GloVe: Global Vectors for Word Representation",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2014,
        "citationCount": 33719,
        "openAccessPdf": {
            "url": "https://doi.org/10.3115/v1/d14-1162",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/D14-1162, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "143845796",
                "name": "Jeffrey Pennington"
            },
            {
                "authorId": "2166511",
                "name": "R. Socher"
            },
            {
                "authorId": "144783904",
                "name": "Christopher D. Manning"
            }
        ],
        "abstract": "Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition."
    },
    {
        "paperId": "93d63ec754f29fa22572615320afe0521f7ec66d",
        "url": "https://www.semanticscholar.org/paper/93d63ec754f29fa22572615320afe0521f7ec66d",
        "title": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2019,
        "citationCount": 15348,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/D19-1410.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1908.10084, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2959414",
                "name": "Nils Reimers"
            },
            {
                "authorId": "1730400",
                "name": "Iryna Gurevych"
            }
        ],
        "abstract": "BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations (~65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods."
    },
    {
        "paperId": "e8b12467bdc20bde976750b8a28decdb33246d1d",
        "url": "https://www.semanticscholar.org/paper/e8b12467bdc20bde976750b8a28decdb33246d1d",
        "title": "Histograms of oriented gradients for human detection",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2005,
        "citationCount": 35105,
        "openAccessPdf": {
            "url": "https://hal.inria.fr/inria-00548512/file/hog_cvpr2005.pdf",
            "status": "GREEN",
            "license": "other-oa",
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR.2005.177?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR.2005.177, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "48950628",
                "name": "Navneet Dalal"
            },
            {
                "authorId": "1756114",
                "name": "B. Triggs"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "6fc6803df5f9ae505cae5b2f178ade4062c768d0",
        "url": "https://www.semanticscholar.org/paper/6fc6803df5f9ae505cae5b2f178ade4062c768d0",
        "title": "Fully convolutional networks for semantic segmentation",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2014,
        "citationCount": 40584,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/1411.4038",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1411.4038, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1782282",
                "name": "Evan Shelhamer"
            },
            {
                "authorId": "2117314646",
                "name": "Jonathan Long"
            },
            {
                "authorId": "1753210",
                "name": "Trevor Darrell"
            }
        ],
        "abstract": "Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build “fully convolutional” networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet [20], the VGG net [31], and GoogLeNet [32]) into fully convolutional networks and transfer their learned representations by fine-tuning [3] to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20% relative improvement to 62.2% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes less than one fifth of a second for a typical image."
    },
    {
        "paperId": "e15cf50aa89fee8535703b9f9512fca5bfc43327",
        "url": "https://www.semanticscholar.org/paper/e15cf50aa89fee8535703b9f9512fca5bfc43327",
        "title": "Going deeper with convolutions",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2014,
        "citationCount": 46105,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/1409.4842",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1409.4842, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2574060",
                "name": "Christian Szegedy"
            },
            {
                "authorId": "2157222093",
                "name": "Wei Liu"
            },
            {
                "authorId": "39978391",
                "name": "Yangqing Jia"
            },
            {
                "authorId": "3142556",
                "name": "P. Sermanet"
            },
            {
                "authorId": "144828948",
                "name": "Scott E. Reed"
            },
            {
                "authorId": "1838674",
                "name": "Dragomir Anguelov"
            },
            {
                "authorId": "1761978",
                "name": "D. Erhan"
            },
            {
                "authorId": "2657155",
                "name": "Vincent Vanhoucke"
            },
            {
                "authorId": "39863668",
                "name": "Andrew Rabinovich"
            }
        ],
        "abstract": "We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection."
    },
    {
        "paperId": "23ffaa0fe06eae05817f527a47ac3291077f9e58",
        "url": "https://www.semanticscholar.org/paper/23ffaa0fe06eae05817f527a47ac3291077f9e58",
        "title": "Rethinking the Inception Architecture for Computer Vision",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2015,
        "citationCount": 29753,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/1512.00567",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1512.00567, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2574060",
                "name": "Christian Szegedy"
            },
            {
                "authorId": "2657155",
                "name": "Vincent Vanhoucke"
            },
            {
                "authorId": "2054165706",
                "name": "Sergey Ioffe"
            },
            {
                "authorId": "1789737",
                "name": "Jonathon Shlens"
            },
            {
                "authorId": "3282833",
                "name": "Z. Wojna"
            }
        ],
        "abstract": "Convolutional networks are at the core of most state of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we are exploring ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21:2% top-1 and 5:6% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3:5% top-5 error and 17:3% top-1 error on the validation set and 3:6% top-5 error on the official test set."
    },
    {
        "paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d",
        "url": "https://www.semanticscholar.org/paper/2c03df8b48bf3fa39054345bafabfeff15bfd11d",
        "title": "Deep Residual Learning for Image Recognition",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2015,
        "citationCount": 215984,
        "openAccessPdf": {
            "url": "https://repositorio.unal.edu.co/bitstream/unal/81443/1/98670607.2022.pdf",
            "status": "GREEN",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1512.03385, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "39353098",
                "name": "Kaiming He"
            },
            {
                "authorId": "1771551",
                "name": "X. Zhang"
            },
            {
                "authorId": "3080683",
                "name": "Shaoqing Ren"
            },
            {
                "authorId": null,
                "name": "Jian Sun"
            }
        ],
        "abstract": "Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation."
    },
    {
        "paperId": "31f9eb39d840821979e5df9f34a6e92dd9c879f2",
        "url": "https://www.semanticscholar.org/paper/31f9eb39d840821979e5df9f34a6e92dd9c879f2",
        "title": "Learning Deep Features for Discriminative Localization",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2015,
        "citationCount": 10155,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/1512.04150",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1512.04150, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "145291669",
                "name": "Bolei Zhou"
            },
            {
                "authorId": "2556428",
                "name": "A. Khosla"
            },
            {
                "authorId": "2677488",
                "name": "Àgata Lapedriza"
            },
            {
                "authorId": "143868587",
                "name": "A. Oliva"
            },
            {
                "authorId": "143805211",
                "name": "A. Torralba"
            }
        ],
        "abstract": "In this work, we revisit the global average pooling layer proposed in [13], and shed light on how it explicitly enables the convolutional neural network (CNN) to have remarkable localization ability despite being trained on imagelevel labels. While this technique was previously proposed as a means for regularizing training, we find that it actually builds a generic localizable deep representation that exposes the implicit attention of CNNs on an image. Despite the apparent simplicity of global average pooling, we are able to achieve 37.1% top-5 error for object localization on ILSVRC 2014 without training on any bounding box annotation. We demonstrate in a variety of experiments that our network is able to localize the discriminative image regions despite just being trained for solving classification task1."
    },
    {
        "paperId": "5aa26299435bdf7db874ef1640a6c3b5a4a2c394",
        "url": "https://www.semanticscholar.org/paper/5aa26299435bdf7db874ef1640a6c3b5a4a2c394",
        "title": "FaceNet: A unified embedding for face recognition and clustering",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2015,
        "citationCount": 14204,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1503.03832",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1503.03832, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3302320",
                "name": "Florian Schroff"
            },
            {
                "authorId": "2741985",
                "name": "Dmitry Kalenichenko"
            },
            {
                "authorId": "2066819269",
                "name": "James Philbin"
            }
        ],
        "abstract": "Despite significant recent advances in the field of face recognition [10, 14, 15, 17], implementing face verification and recognition efficiently at scale presents serious challenges to current approaches. In this paper we present a system, called FaceNet, that directly learns a mapping from face images to a compact Euclidean space where distances directly correspond to a measure offace similarity. Once this space has been produced, tasks such as face recognition, verification and clustering can be easily implemented using standard techniques with FaceNet embeddings asfeature vectors. Our method uses a deep convolutional network trained to directly optimize the embedding itself, rather than an intermediate bottleneck layer as in previous deep learning approaches. To train, we use triplets of roughly aligned matching / non-matching face patches generated using a novel online triplet mining method. The benefit of our approach is much greater representational efficiency: we achieve state-of-the-artface recognition performance using only 128-bytes perface. On the widely used Labeled Faces in the Wild (LFW) dataset, our system achieves a new record accuracy of 99.63%. On YouTube Faces DB it achieves 95.12%. Our system cuts the error rate in comparison to the best published result [15] by 30% on both datasets."
    },
    {
        "paperId": "f8e79ac0ea341056ef20f2616628b3e964764cfd",
        "url": "https://www.semanticscholar.org/paper/f8e79ac0ea341056ef20f2616628b3e964764cfd",
        "title": "You Only Look Once: Unified, Real-Time Object Detection",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2015,
        "citationCount": 42306,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1506.02640",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1506.02640, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "40497777",
                "name": "Joseph Redmon"
            },
            {
                "authorId": "2038685",
                "name": "S. Divvala"
            },
            {
                "authorId": "2983898",
                "name": "Ross B. Girshick"
            },
            {
                "authorId": "143787583",
                "name": "Ali Farhadi"
            }
        ],
        "abstract": "We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is less likely to predict false positives on background. Finally, YOLO learns very general representations of objects. It outperforms other detection methods, including DPM and R-CNN, when generalizing from natural images to other domains like artwork."
    },
    {
        "paperId": "1031a69923b80ad01cf3fbb703d10757a80e699b",
        "url": "https://www.semanticscholar.org/paper/1031a69923b80ad01cf3fbb703d10757a80e699b",
        "title": "Pyramid Scene Parsing Network",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2016,
        "citationCount": 13412,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1612.01105",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1612.01105, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3459894",
                "name": "Hengshuang Zhao"
            },
            {
                "authorId": "1788070",
                "name": "Jianping Shi"
            },
            {
                "authorId": "50844674",
                "name": "Xiaojuan Qi"
            },
            {
                "authorId": "31843833",
                "name": "Xiaogang Wang"
            },
            {
                "authorId": "1729056",
                "name": "Jiaya Jia"
            }
        ],
        "abstract": "Scene parsing is challenging for unrestricted open vocabulary and diverse scenes. In this paper, we exploit the capability of global context information by different-region-based context aggregation through our pyramid pooling module together with the proposed pyramid scene parsing network (PSPNet). Our global prior representation is effective to produce good quality results on the scene parsing task, while PSPNet provides a superior framework for pixel-level prediction. The proposed approach achieves state-of-the-art performance on various datasets. It came first in ImageNet scene parsing challenge 2016, PASCAL VOC 2012 benchmark and Cityscapes benchmark. A single PSPNet yields the new record of mIoU accuracy 85.4% on PASCAL VOC 2012 and accuracy 80.2% on Cityscapes."
    },
    {
        "paperId": "2a94c84383ee3de5e6211d43d16e7de387f68878",
        "url": "https://www.semanticscholar.org/paper/2a94c84383ee3de5e6211d43d16e7de387f68878",
        "title": "Feature Pyramid Networks for Object Detection",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2016,
        "citationCount": 25137,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1612.03144",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1612.03144, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "33493200",
                "name": "Tsung-Yi Lin"
            },
            {
                "authorId": "3127283",
                "name": "Piotr Dollár"
            },
            {
                "authorId": "2983898",
                "name": "Ross B. Girshick"
            },
            {
                "authorId": "39353098",
                "name": "Kaiming He"
            },
            {
                "authorId": "1790580",
                "name": "Bharath Hariharan"
            },
            {
                "authorId": "50172592",
                "name": "Serge J. Belongie"
            }
        ],
        "abstract": "Feature pyramids are a basic component in recognition systems for detecting objects at different scales. But pyramid representations have been avoided in recent object detectors that are based on deep convolutional networks, partially because they are slow to compute and memory intensive. In this paper, we exploit the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost. A top-down architecture with lateral connections is developed for building high-level semantic feature maps at all scales. This architecture, called a Feature Pyramid Network (FPN), shows significant improvement as a generic feature extractor in several applications. Using a basic Faster R-CNN system, our method achieves state-of-the-art single-model results on the COCO detection benchmark without bells and whistles, surpassing all existing single-model entries including those from the COCO 2016 challenge winners. In addition, our method can run at 5 FPS on a GPU and thus is a practical and accurate solution to multi-scale object detection. Code will be made publicly available."
    },
    {
        "paperId": "5694e46284460a648fe29117cbc55f6c9be3fa3c",
        "url": "https://www.semanticscholar.org/paper/5694e46284460a648fe29117cbc55f6c9be3fa3c",
        "title": "Densely Connected Convolutional Networks",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2016,
        "citationCount": 40912,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1608.06993",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1608.06993, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "143983679",
                "name": "Gao Huang"
            },
            {
                "authorId": "2109168016",
                "name": "Zhuang Liu"
            },
            {
                "authorId": "7446832",
                "name": "Kilian Q. Weinberger"
            }
        ],
        "abstract": "Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections&#x2014;one between each layer and its subsequent layer&#x2014;our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less memory and computation to achieve high performance. Code and pre-trained models are available at https://github.com/liuzhuang13/DenseNet."
    },
    {
        "paperId": "5b6ec746d309b165f9f9def873a2375b6fb40f3d",
        "url": "https://www.semanticscholar.org/paper/5b6ec746d309b165f9f9def873a2375b6fb40f3d",
        "title": "Xception: Deep Learning with Depthwise Separable Convolutions",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2016,
        "citationCount": 16597,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1610.02357",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1610.02357, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1565641737",
                "name": "François Chollet"
            }
        ],
        "abstract": "We present an interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and significantly outperforms Inception V3 on a larger image classification dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameters as Inception V3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters."
    },
    {
        "paperId": "7d39d69b23424446f0400ef603b2e3e22d0309d6",
        "url": "https://www.semanticscholar.org/paper/7d39d69b23424446f0400ef603b2e3e22d0309d6",
        "title": "YOLO9000: Better, Faster, Stronger",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2016,
        "citationCount": 16993,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1612.08242",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1612.08242, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "40497777",
                "name": "Joseph Redmon"
            },
            {
                "authorId": "143787583",
                "name": "Ali Farhadi"
            }
        ],
        "abstract": "We introduce YOLO9000, a state-of-the-art, real-time object detection system that can detect over 9000 object categories. First we propose various improvements to the YOLO detection method, both novel and drawn from prior work. The improved model, YOLOv2, is state-of-the-art on standard detection tasks like PASCAL VOC and COCO. Using a novel, multi-scale training method the same YOLOv2 model can run at varying sizes, offering an easy tradeoff between speed and accuracy. At 67 FPS, YOLOv2 gets 76.8 mAP on VOC 2007. At 40 FPS, YOLOv2 gets 78.6 mAP, outperforming state-of-the-art methods like Faster RCNN with ResNet and SSD while still running significantly faster. Finally we propose a method to jointly train on object detection and classification. Using this method we train YOLO9000 simultaneously on the COCO detection dataset and the ImageNet classification dataset. Our joint training allows YOLO9000 to predict detections for object classes that dont have labelled detection data. We validate our approach on the ImageNet detection task. YOLO9000 gets 19.7 mAP on the ImageNet detection validation set despite only having detection data for 44 of the 200 classes. On the 156 classes not in COCO, YOLO9000 gets 16.0 mAP. YOLO9000 predicts detections for more than 9000 different object categories, all in real-time."
    },
    {
        "paperId": "8acbe90d5b852dadea7810345451a99608ee54c7",
        "url": "https://www.semanticscholar.org/paper/8acbe90d5b852dadea7810345451a99608ee54c7",
        "title": "Image-to-Image Translation with Conditional Adversarial Networks",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2016,
        "citationCount": 21354,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1611.07004",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1611.07004, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2094770",
                "name": "Phillip Isola"
            },
            {
                "authorId": "2436356",
                "name": "Jun-Yan Zhu"
            },
            {
                "authorId": "1822702",
                "name": "Tinghui Zhou"
            },
            {
                "authorId": "1763086",
                "name": "Alexei A. Efros"
            }
        ],
        "abstract": "We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Moreover, since the release of the pix2pix software associated with this paper, hundreds of twitter users have posted their own artistic experiments using our system. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without handengineering our loss functions either."
    },
    {
        "paperId": "c8c494ee5488fe20e0aa01bddf3fc4632086d654",
        "url": "https://www.semanticscholar.org/paper/c8c494ee5488fe20e0aa01bddf3fc4632086d654",
        "title": "The Cityscapes Dataset for Semantic Urban Scene Understanding",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2016,
        "citationCount": 12791,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1604.01685",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1604.01685, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2841796",
                "name": "Marius Cordts"
            },
            {
                "authorId": "144187309",
                "name": "Mohamed Omran"
            },
            {
                "authorId": "39940699",
                "name": "Sebastian Ramos"
            },
            {
                "authorId": "3393153",
                "name": "Timo Rehfeld"
            },
            {
                "authorId": "1765022",
                "name": "Markus Enzweiler"
            },
            {
                "authorId": "1798000",
                "name": "Rodrigo Benenson"
            },
            {
                "authorId": "145582788",
                "name": "Uwe Franke"
            },
            {
                "authorId": "145920814",
                "name": "S. Roth"
            },
            {
                "authorId": "48920094",
                "name": "B. Schiele"
            }
        ],
        "abstract": "Visual understanding of complex urban street scenes is an enabling factor for a wide range of applications. Object detection has benefited enormously from large-scale datasets, especially in the context of deep learning. For semantic urban scene understanding, however, no current dataset adequately captures the complexity of real-world urban scenes. To address this, we introduce Cityscapes, a benchmark suite and large-scale dataset to train and test approaches for pixel-level and instance-level semantic labeling. Cityscapes is comprised of a large, diverse set of stereo video sequences recorded in streets from 50 different cities. 5000 of these images have high quality pixel-level annotations, 20 000 additional images have coarse annotations to enable methods that leverage large volumes of weakly-labeled data. Crucially, our effort exceeds previous attempts in terms of dataset size, annotation richness, scene variability, and complexity. Our accompanying empirical study provides an in-depth analysis of the dataset characteristics, as well as a performance evaluation of several state-of-the-art approaches based on our benchmark."
    },
    {
        "paperId": "d997beefc0922d97202789d2ac307c55c2c52fba",
        "url": "https://www.semanticscholar.org/paper/d997beefc0922d97202789d2ac307c55c2c52fba",
        "title": "PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2016,
        "citationCount": 16315,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1612.00593",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1612.00593, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "144329939",
                "name": "C. Qi"
            },
            {
                "authorId": "144914140",
                "name": "Hao Su"
            },
            {
                "authorId": "2216377",
                "name": "Kaichun Mo"
            },
            {
                "authorId": "51352814",
                "name": "L. Guibas"
            }
        ],
        "abstract": "Point cloud is an important type of geometric data structure. Due to its irregular format, most researchers transform such data to regular 3D voxel grids or collections of images. This, however, renders data unnecessarily voluminous and causes issues. In this paper, we design a novel type of neural network that directly consumes point clouds, which well respects the permutation invariance of points in the input. Our network, named PointNet, provides a unified architecture for applications ranging from object classification, part segmentation, to scene semantic parsing. Though simple, PointNet is highly efficient and effective. Empirically, it shows strong performance on par or even better than state of the art. Theoretically, we provide analysis towards understanding of what the network has learnt and why the network is robust with respect to input perturbation and corruption."
    },
    {
        "paperId": "df0c54fe61f0ffb9f0e36a17c2038d9a1964cba3",
        "url": "https://www.semanticscholar.org/paper/df0c54fe61f0ffb9f0e36a17c2038d9a1964cba3",
        "title": "Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2016,
        "citationCount": 11610,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/1609.04802",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1609.04802, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1779917",
                "name": "C. Ledig"
            },
            {
                "authorId": "2073063",
                "name": "Lucas Theis"
            },
            {
                "authorId": "3108066",
                "name": "Ferenc Huszár"
            },
            {
                "authorId": "145372820",
                "name": "Jose Caballero"
            },
            {
                "authorId": "49931957",
                "name": "Andrew P. Aitken"
            },
            {
                "authorId": "41203992",
                "name": "Alykhan Tejani"
            },
            {
                "authorId": "1853456",
                "name": "J. Totz"
            },
            {
                "authorId": "34627233",
                "name": "Zehan Wang"
            },
            {
                "authorId": "46810836",
                "name": "Wenzhe Shi"
            }
        ],
        "abstract": "Despite the breakthroughs in accuracy and speed of single image super-resolution using faster and deeper convolutional neural networks, one central problem remains largely unsolved: how do we recover the finer texture details when we super-resolve at large upscaling factors? The behavior of optimization-based super-resolution methods is principally driven by the choice of the objective function. Recent work has largely focused on minimizing the mean squared reconstruction error. The resulting estimates have high peak signal-to-noise ratios, but they are often lacking high-frequency details and are perceptually unsatisfying in the sense that they fail to match the fidelity expected at the higher resolution. In this paper, we present SRGAN, a generative adversarial network (GAN) for image super-resolution (SR). To our knowledge, it is the first framework capable of inferring photo-realistic natural images for 4x upscaling factors. To achieve this, we propose a perceptual loss function which consists of an adversarial loss and a content loss. The adversarial loss pushes our solution to the natural image manifold using a discriminator network that is trained to differentiate between the super-resolved images and original photo-realistic images. In addition, we use a content loss motivated by perceptual similarity instead of similarity in pixel space. Our deep residual network is able to recover photo-realistic textures from heavily downsampled images on public benchmarks. An extensive mean-opinion-score (MOS) test shows hugely significant gains in perceptual quality using SRGAN. The MOS scores obtained with SRGAN are closer to those of the original high-resolution images than to those obtained with any state-of-the-art method."
    },
    {
        "paperId": "f6e0856b4a9199fa968ac00da612a9407b5cb85c",
        "url": "https://www.semanticscholar.org/paper/f6e0856b4a9199fa968ac00da612a9407b5cb85c",
        "title": "Aggregated Residual Transformations for Deep Neural Networks",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2016,
        "citationCount": 11230,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/1611.05431",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1611.05431, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1817030",
                "name": "Saining Xie"
            },
            {
                "authorId": "2983898",
                "name": "Ross B. Girshick"
            },
            {
                "authorId": "3127283",
                "name": "Piotr Dollár"
            },
            {
                "authorId": "144035504",
                "name": "Z. Tu"
            },
            {
                "authorId": "39353098",
                "name": "Kaiming He"
            }
        ],
        "abstract": "We present a simple, highly modularized network architecture for image classification. Our network is constructed by repeating a building block that aggregates a set of transformations with the same topology. Our simple design results in a homogeneous, multi-branch architecture that has only a few hyper-parameters to set. This strategy exposes a new dimension, which we call cardinality (the size of the set of transformations), as an essential factor in addition to the dimensions of depth and width. On the ImageNet-1K dataset, we empirically show that even under the restricted condition of maintaining complexity, increasing cardinality is able to improve classification accuracy. Moreover, increasing cardinality is more effective than going deeper or wider when we increase the capacity. Our models, named ResNeXt, are the foundations of our entry to the ILSVRC 2016 classification task in which we secured 2nd place. We further investigate ResNeXt on an ImageNet-5K set and the COCO detection set, also showing better results than its ResNet counterpart. The code and models are publicly available online."
    },
    {
        "paperId": "ceb2ebef0b41e31c1a21b28c2734123900c005e2",
        "url": "https://www.semanticscholar.org/paper/ceb2ebef0b41e31c1a21b28c2734123900c005e2",
        "title": "A Style-Based Generator Architecture for Generative Adversarial Networks",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2018,
        "citationCount": 12050,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1812.04948",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1812.04948, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2976930",
                "name": "Tero Karras"
            },
            {
                "authorId": "36436218",
                "name": "S. Laine"
            },
            {
                "authorId": "1761103",
                "name": "Timo Aila"
            }
        ],
        "abstract": "We propose an alternative generator architecture for generative adversarial networks, borrowing from style transfer literature. The new architecture leads to an automatically learned, unsupervised separation of high-level attributes (e.g., pose and identity when trained on human faces) and stochastic variation in the generated images (e.g., freckles, hair), and it enables intuitive, scale-specific control of the synthesis. The new generator improves the state-of-the-art in terms of traditional distribution quality metrics, leads to demonstrably better interpolation properties, and also better disentangles the latent factors of variation. To quantify interpolation quality and disentanglement, we propose two new, automated methods that are applicable to any generator architecture. Finally, we introduce a new, highly varied and high-quality dataset of human faces."
    },
    {
        "paperId": "add2f205338d70e10ce5e686df4a690e2851bdfc",
        "url": "https://www.semanticscholar.org/paper/add2f205338d70e10ce5e686df4a690e2851bdfc",
        "title": "Momentum Contrast for Unsupervised Visual Representation Learning",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2019,
        "citationCount": 13902,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1911.05722",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1911.05722, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "39353098",
                "name": "Kaiming He"
            },
            {
                "authorId": "146884473",
                "name": "Haoqi Fan"
            },
            {
                "authorId": "98264506",
                "name": "Yuxin Wu"
            },
            {
                "authorId": "1817030",
                "name": "Saining Xie"
            },
            {
                "authorId": "2983898",
                "name": "Ross B. Girshick"
            }
        ],
        "abstract": "We present Momentum Contrast (MoCo) for unsupervised visual representation learning. From a perspective on contrastive learning as dictionary look-up, we build a dynamic dictionary with a queue and a moving-averaged encoder. This enables building a large and consistent dictionary on-the-fly that facilitates contrastive unsupervised learning. MoCo provides competitive results under the common linear protocol on ImageNet classification. More importantly, the representations learned by MoCo transfer well to downstream tasks. MoCo can outperform its supervised pre-training counterpart in 7 detection/segmentation tasks on PASCAL VOC, COCO, and other datasets, sometimes surpassing it by large margins. This suggests that the gap between unsupervised and supervised representation learning has been largely closed in many vision tasks."
    },
    {
        "paperId": "c10075b3746a9f3dd5811970e93c8ca3ad39b39d",
        "url": "https://www.semanticscholar.org/paper/c10075b3746a9f3dd5811970e93c8ca3ad39b39d",
        "title": "High-Resolution Image Synthesis with Latent Diffusion Models",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2021,
        "citationCount": 20793,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2112.10752",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2112.10752, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1660819540",
                "name": "Robin Rombach"
            },
            {
                "authorId": "119843260",
                "name": "A. Blattmann"
            },
            {
                "authorId": "2053482699",
                "name": "Dominik Lorenz"
            },
            {
                "authorId": "35175531",
                "name": "Patrick Esser"
            },
            {
                "authorId": "1796707",
                "name": "B. Ommer"
            }
        ],
        "abstract": "By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve new state of the art scores for image inpainting and class-conditional image synthesis and highly competitive performance on various tasks, including unconditional image generation, text-to-image synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs."
    },
    {
        "paperId": "c0883f5930a232a9c1ad601c978caede29155979",
        "url": "https://www.semanticscholar.org/paper/c0883f5930a232a9c1ad601c978caede29155979",
        "title": "“Why Should I Trust You?”: Explaining the Predictions of Any Classifier",
        "venue": "North American Chapter of the Association for Computational Linguistics",
        "year": 2016,
        "citationCount": 19543,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/N16-3020.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1602.04938, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "78846919",
                "name": "Marco Tulio Ribeiro"
            },
            {
                "authorId": "34650964",
                "name": "Sameer Singh"
            },
            {
                "authorId": "1730156",
                "name": "Carlos Guestrin"
            }
        ],
        "abstract": "Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally varound the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted."
    },
    {
        "paperId": "3febb2bed8865945e7fddc99efd791887bb7e14f",
        "url": "https://www.semanticscholar.org/paper/3febb2bed8865945e7fddc99efd791887bb7e14f",
        "title": "Deep Contextualized Word Representations",
        "venue": "North American Chapter of the Association for Computational Linguistics",
        "year": 2018,
        "citationCount": 11955,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/N18-1202.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1802.05365, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "39139825",
                "name": "Matthew E. Peters"
            },
            {
                "authorId": "50043859",
                "name": "Mark Neumann"
            },
            {
                "authorId": "2136562",
                "name": "Mohit Iyyer"
            },
            {
                "authorId": "40642935",
                "name": "Matt Gardner"
            },
            {
                "authorId": "143997772",
                "name": "Christopher Clark"
            },
            {
                "authorId": "2544107",
                "name": "Kenton Lee"
            },
            {
                "authorId": "1982950",
                "name": "Luke Zettlemoyer"
            }
        ],
        "abstract": "We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals."
    },
    {
        "paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992",
        "url": "https://www.semanticscholar.org/paper/df2b0e26d0599ce3e70df8a9da02e51594e0e992",
        "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
        "venue": "North American Chapter of the Association for Computational Linguistics",
        "year": 2019,
        "citationCount": 107607,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1810.04805, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "39172707",
                "name": "Jacob Devlin"
            },
            {
                "authorId": "1744179",
                "name": "Ming-Wei Chang"
            },
            {
                "authorId": "2544107",
                "name": "Kenton Lee"
            },
            {
                "authorId": "3259253",
                "name": "Kristina Toutanova"
            }
        ],
        "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."
    }
]