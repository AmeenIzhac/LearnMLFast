[
    {
        "paperId": "bbde9ef4a4b4da0620b14a25c5a4a3d6bd4780e5",
        "url": "https://www.semanticscholar.org/paper/bbde9ef4a4b4da0620b14a25c5a4a3d6bd4780e5",
        "title": "Activity Recognition from Accelerometer Data",
        "venue": "AAAI Conference on Artificial Intelligence",
        "year": 2005,
        "citationCount": 1753,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "2458826",
                "name": "N. Ravi"
            },
            {
                "authorId": "2074513371",
                "name": "Nikhil Dandekar"
            },
            {
                "authorId": "31527485",
                "name": "P. Mysore"
            },
            {
                "authorId": "144885169",
                "name": "M. Littman"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "6633814eb1e066d146bdae16d0c1c8344c60778c",
        "url": "https://www.semanticscholar.org/paper/6633814eb1e066d146bdae16d0c1c8344c60778c",
        "title": "Corpus-based and Knowledge-based Measures of Text Semantic Similarity",
        "venue": "AAAI Conference on Artificial Intelligence",
        "year": 2006,
        "citationCount": 1425,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "145557251",
                "name": "Rada Mihalcea"
            },
            {
                "authorId": "1947728",
                "name": "Courtney Corley"
            },
            {
                "authorId": "1723976",
                "name": "C. Strapparava"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "c8221c054459e37edbf313668523d667fe5c1536",
        "url": "https://www.semanticscholar.org/paper/c8221c054459e37edbf313668523d667fe5c1536",
        "title": "Maximum Entropy Inverse Reinforcement Learning",
        "venue": "AAAI Conference on Artificial Intelligence",
        "year": 2008,
        "citationCount": 3233,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "1753269",
                "name": "Brian D. Ziebart"
            },
            {
                "authorId": "2348228804",
                "name": "Andrew L. Maas"
            },
            {
                "authorId": "1756566",
                "name": "J. Bagnell"
            },
            {
                "authorId": "144021446",
                "name": "A. Dey"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "2a3f862199883ceff5e3c74126f0c80770653e05",
        "url": "https://www.semanticscholar.org/paper/2a3f862199883ceff5e3c74126f0c80770653e05",
        "title": "Knowledge Graph Embedding by Translating on Hyperplanes",
        "venue": "AAAI Conference on Artificial Intelligence",
        "year": 2014,
        "citationCount": 3848,
        "openAccessPdf": {
            "url": "https://ojs.aaai.org/index.php/AAAI/article/download/8870/8729",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1609/aaai.v28i1.8870?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1609/aaai.v28i1.8870, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2118452539",
                "name": "Zhen Wang"
            },
            {
                "authorId": "2108090984",
                "name": "Jianwen Zhang"
            },
            {
                "authorId": "2592554",
                "name": "Jianlin Feng"
            },
            {
                "authorId": "35773227",
                "name": "Zheng Chen"
            }
        ],
        "abstract": "\n \n We deal with embedding a large scale knowledge graph composed of entities and relations into a continuous vector space. TransE is a promising method proposed recently, which is very efficient while achieving state-of-the-art predictive performance. We discuss some mapping properties of relations which should be considered in embedding, such as reflexive, one-to-many, many-to-one, and many-to-many. We note that TransE does not do well in dealing with these properties. Some complex models are capable of preserving these mapping properties but sacrifice efficiency in the process. To make a good trade-off between model capacity and efficiency, in this paper we propose TransH which models a relation as a hyperplane together with a translation operation on it. In this way, we can well preserve the above mapping properties of relations with almost the same model complexity of TransE. Additionally, as a practical knowledge graph is often far from completed, how to construct negative examples to reduce false negative labels in training is very important. Utilizing the one-to-many/many-to-one mapping property of a relation, we propose a simple trick to reduce the possibility of false negative labeling. We conduct extensive experiments on link prediction, triplet classification and fact extraction on benchmark datasets like WordNet and Freebase. Experiments show TransH delivers significant improvements over TransE on predictive accuracy with comparable capability to scale up.\n \n"
    },
    {
        "paperId": "0ba86604228b555475496e200f31878df3aabd6e",
        "url": "https://www.semanticscholar.org/paper/0ba86604228b555475496e200f31878df3aabd6e",
        "title": "Never-Ending Learning",
        "venue": "AAAI Conference on Artificial Intelligence",
        "year": 2015,
        "citationCount": 1101,
        "openAccessPdf": {
            "url": "https://dl.acm.org/doi/pdf/10.1145/3191513",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3191513?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3191513, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "40975594",
                "name": "Tom Michael Mitchell"
            },
            {
                "authorId": "50056360",
                "name": "William W. Cohen"
            },
            {
                "authorId": "1842532",
                "name": "Estevam Hruschka"
            },
            {
                "authorId": "2406435",
                "name": "Partha P. Talukdar"
            },
            {
                "authorId": "2119660368",
                "name": "Bo Yang"
            },
            {
                "authorId": "31779043",
                "name": "J. Betteridge"
            },
            {
                "authorId": "143818235",
                "name": "Andrew Carlson"
            },
            {
                "authorId": "40135250",
                "name": "Bhavana Dalvi"
            },
            {
                "authorId": "40642935",
                "name": "Matt Gardner"
            },
            {
                "authorId": "16411658",
                "name": "B. Kisiel"
            },
            {
                "authorId": "2517825",
                "name": "Jayant Krishnamurthy"
            },
            {
                "authorId": "1914797",
                "name": "N. Lao"
            },
            {
                "authorId": "2406799",
                "name": "Kathryn Mazaitis"
            },
            {
                "authorId": "35645263",
                "name": "Thahir Mohamed"
            },
            {
                "authorId": "3115592",
                "name": "Ndapandula Nakashole"
            },
            {
                "authorId": "144888672",
                "name": "Emmanouil Antonios Platanios"
            },
            {
                "authorId": "1863425",
                "name": "Alan Ritter"
            },
            {
                "authorId": "32402038",
                "name": "M. Samadi"
            },
            {
                "authorId": "1717452",
                "name": "Burr Settles"
            },
            {
                "authorId": "2108772203",
                "name": "Richard C. Wang"
            },
            {
                "authorId": "2129412",
                "name": "Derry Tanti Wijaya"
            },
            {
                "authorId": "1726095131",
                "name": "A. Gupta"
            },
            {
                "authorId": "39717886",
                "name": "Xinlei Chen"
            },
            {
                "authorId": "2407368",
                "name": "Abulhair Saparov"
            },
            {
                "authorId": "2062798496",
                "name": "Malcolm Greaves"
            },
            {
                "authorId": "122360608",
                "name": "Joel Welling"
            }
        ],
        "abstract": "Whereas people learn many different types of knowledge from diverse experiences over many years, most current machine learning systems acquire just a single function or data model from just a single data set. We propose a neverending learning paradigm for machine learning, to better reflect the more ambitious and encompassing type of learning performed by humans. As a case study, we describe the Never-Ending Language Learner (NELL), which achieves some of the desired properties of a never-ending learner, and we discuss lessons learned. NELL has been learning to read the web 24 hours/day since January 2010, and so far has acquired a knowledge base with over 80 million confidenceweighted beliefs (e.g., servedWith(tea, biscuits)). NELL has also learned millions of features and parameters that enable it to read these beliefs from the web. Additionally, it has learned to reason over these beliefs to infer new beliefs, and is able to extend its ontology by synthesizing new relational predicates. NELL can be tracked online at http://rtw.ml.cmu.edu, and followed on Twitter at @CMUNELL."
    },
    {
        "paperId": "17f5c7411eeeeedf25b0db99a9130aa353aee4ba",
        "url": "https://www.semanticscholar.org/paper/17f5c7411eeeeedf25b0db99a9130aa353aee4ba",
        "title": "Building End-To-End Dialogue Systems Using Generative Hierarchical Neural Network Models",
        "venue": "AAAI Conference on Artificial Intelligence",
        "year": 2015,
        "citationCount": 1798,
        "openAccessPdf": {
            "url": "https://ojs.aaai.org/index.php/AAAI/article/download/9883/9742",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1507.04808, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "35224828",
                "name": "Iulian Serban"
            },
            {
                "authorId": "2041695",
                "name": "Alessandro Sordoni"
            },
            {
                "authorId": "1751762",
                "name": "Yoshua Bengio"
            },
            {
                "authorId": "1760871",
                "name": "Aaron C. Courville"
            },
            {
                "authorId": "145134886",
                "name": "Joelle Pineau"
            }
        ],
        "abstract": "\n \n We investigate the task of building open domain, conversational dialogue systems based on large dialogue corpora using generative models. Generative models produce system responses that are autonomously generated word-by-word, opening up the possibility for realistic, flexible interactions. In support of this goal, we extend the recently proposed hierarchical recurrent encoder-decoder neural network to the dialogue domain, and demonstrate that this model is competitive with state-of-the-art neural language models and back-off n-gram models. We investigate the limitations of this and similar approaches, and show how its performance can be improved by bootstrapping the learning from a larger question-answer pair corpus and from pretrained word embeddings.\n \n"
    },
    {
        "paperId": "3b9732bb07dc99bde5e1f9f75251c6ea5039373e",
        "url": "https://www.semanticscholar.org/paper/3b9732bb07dc99bde5e1f9f75251c6ea5039373e",
        "title": "Deep Reinforcement Learning with Double Q-Learning",
        "venue": "AAAI Conference on Artificial Intelligence",
        "year": 2015,
        "citationCount": 8480,
        "openAccessPdf": {
            "url": "https://ojs.aaai.org/index.php/AAAI/article/download/10295/10154",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1509.06461, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "7634925",
                "name": "H. V. Hasselt"
            },
            {
                "authorId": "35099444",
                "name": "A. Guez"
            },
            {
                "authorId": "145824029",
                "name": "David Silver"
            }
        ],
        "abstract": "\n \n The popular Q-learning algorithm is known to overestimate action values under certain conditions. It was not previously known whether, in practice, such overestimations are common, whether they harm performance, and whether they can generally be prevented. In this paper, we answer all these questions affirmatively. In particular, we first show that the recent DQN algorithm, which combines Q-learning with a deep neural network, suffers from substantial overestimations in some games in the Atari 2600 domain. We then show that the idea behind the Double Q-learning algorithm, which was introduced in a tabular setting, can be generalized to work with large-scale function approximation. We propose a specific adaptation to the DQN algorithm and show that the resulting algorithm not only reduces the observed overestimations, as hypothesized, but that this also leads to much better performance on several games.\n \n"
    },
    {
        "paperId": "891ce1687e2befddd19f54e4eef1d3f39c8dbaf7",
        "url": "https://www.semanticscholar.org/paper/891ce1687e2befddd19f54e4eef1d3f39c8dbaf7",
        "title": "Character-Aware Neural Language Models",
        "venue": "AAAI Conference on Artificial Intelligence",
        "year": 2015,
        "citationCount": 1700,
        "openAccessPdf": {
            "url": "https://ojs.aaai.org/index.php/AAAI/article/download/10362/10221",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1508.06615, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "38367242",
                "name": "Yoon Kim"
            },
            {
                "authorId": "2262249",
                "name": "Yacine Jernite"
            },
            {
                "authorId": "1746662",
                "name": "D. Sontag"
            },
            {
                "authorId": "2531268",
                "name": "Alexander M. Rush"
            }
        ],
        "abstract": "\n \n We describe a simple neural language model that relies only on character-level inputs. Predictions are still made at the word-level. Our model employs a convolutional neural network (CNN) and a highway net work over characters, whose output is given to a long short-term memory (LSTM) recurrent neural network language model (RNN-LM). On the English Penn Treebank the model is on par with the existing state-of-the-art despite having 60% fewer parameters. On languages with rich morphology (Arabic, Czech, French, German, Spanish, Russian), the model outperforms word-level/morpheme-level LSTM baselines, again with fewer parameters. The results suggest that on many languages, character inputs are sufficient for language modeling. Analysis of word representations obtained from the character composition part of the model reveals that the model is able to encode, from characters only, both semantic and orthographic information.\n \n"
    },
    {
        "paperId": "8d115c3b2ee80e0754360a154a9369bc1658b607",
        "url": "https://www.semanticscholar.org/paper/8d115c3b2ee80e0754360a154a9369bc1658b607",
        "title": "Obtaining Well Calibrated Probabilities Using Bayesian Binning",
        "venue": "AAAI Conference on Artificial Intelligence",
        "year": 2015,
        "citationCount": 1660,
        "openAccessPdf": {
            "url": "https://ojs.aaai.org/index.php/AAAI/article/download/9602/9461",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1609/aaai.v29i1.9602?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1609/aaai.v29i1.9602, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1739626",
                "name": "Mahdi Pakdaman Naeini"
            },
            {
                "authorId": "1726406",
                "name": "G. Cooper"
            },
            {
                "authorId": "1731761",
                "name": "M. Hauskrecht"
            }
        ],
        "abstract": "Learning probabilistic predictive models that are well calibrated is critical for many prediction and decision-making tasks in artificial intelligence. In this paper we present a new non-parametric calibration method called Bayesian Binning into Quantiles (BBQ) which addresses key limitations of existing calibration methods. The method post processes the output of a binary classification algorithm; thus, it can be readily combined with many existing classification algorithms. The method is computationally tractable, and empirically accurate, as evidenced by the set of experiments reported here on both real and simulated datasets."
    },
    {
        "paperId": "93ee8e1c05d11d63aa3d61653b2c8bae75e0aecd",
        "url": "https://www.semanticscholar.org/paper/93ee8e1c05d11d63aa3d61653b2c8bae75e0aecd",
        "title": "The Network Data Repository with Interactive Graph Analytics and Visualization",
        "venue": "AAAI Conference on Artificial Intelligence",
        "year": 2015,
        "citationCount": 2765,
        "openAccessPdf": {
            "url": "https://ojs.aaai.org/index.php/AAAI/article/download/9277/9136",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1609/aaai.v29i1.9277?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1609/aaai.v29i1.9277, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1862090",
                "name": "Ryan A. Rossi"
            },
            {
                "authorId": "47699955",
                "name": "Nesreen Ahmed"
            }
        ],
        "abstract": "\n \n NetworkRepository (NR) is the first interactive data repository with a web-based platform for visual interactive analytics. Unlike other data repositories (e.g., UCI ML Data Repository, and SNAP), the network data repository (networkrepository.com) allows users to not only download, but to interactively analyze and visualize such data using our web-based interactive graph analytics platform. Users can in real-time analyze, visualize, compare, and explore data along many different dimensions. The aim of NR is to make it easy to discover key insights into the data extremely fast with little effort while also providing a medium for users to share data, visualizations, and insights. Other key factors that differentiate NR from the current data repositories is the number of graph datasets, their size, and variety. While other data repositories are static, they also lack a means for users to collaboratively discuss a particular dataset, corrections, or challenges with using the data for certain applications. In contrast, NR incorporates many social and collaborative aspects that facilitate scientific research, e.g., users can discuss each graph, post observations, and visualizations.\n \n"
    },
    {
        "paperId": "955fe2ee26d888ae22749b0853981b8b581b133d",
        "url": "https://www.semanticscholar.org/paper/955fe2ee26d888ae22749b0853981b8b581b133d",
        "title": "Holographic Embeddings of Knowledge Graphs",
        "venue": "AAAI Conference on Artificial Intelligence",
        "year": 2015,
        "citationCount": 1247,
        "openAccessPdf": {
            "url": "https://ojs.aaai.org/index.php/AAAI/article/download/10314/10173",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1510.04935, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1729762",
                "name": "Maximilian Nickel"
            },
            {
                "authorId": "1690976",
                "name": "L. Rosasco"
            },
            {
                "authorId": "1685292",
                "name": "T. Poggio"
            }
        ],
        "abstract": "\n \n Learning embeddings of entities and relations is an efficient and versatile method to perform machine learning on relational data such as knowledge graphs. In this work, we propose holographic embeddings (HolE) to learn compositional vector space representations of entire knowledge graphs. The proposed method is related to holographic models of associative memory in that it employs circular correlation to create compositional representations. By using correlation as the compositional operator, HolE can capture rich interactions but simultaneously remains efficient to compute, easy to train, and scalable to very large datasets. Experimentally, we show that holographic embeddings are able to outperform state-of-the-art methods for link prediction on knowledge graphs and relational learning benchmark datasets.\n \n"
    },
    {
        "paperId": "994afdf0db0cb0456f4f76468380822c2f532726",
        "url": "https://www.semanticscholar.org/paper/994afdf0db0cb0456f4f76468380822c2f532726",
        "title": "Learning Entity and Relation Embeddings for Knowledge Graph Completion",
        "venue": "AAAI Conference on Artificial Intelligence",
        "year": 2015,
        "citationCount": 3811,
        "openAccessPdf": {
            "url": "https://ojs.aaai.org/index.php/AAAI/article/download/9491/9350",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1609/aaai.v29i1.9491?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1609/aaai.v29i1.9491, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2427350",
                "name": "Yankai Lin"
            },
            {
                "authorId": "49293587",
                "name": "Zhiyuan Liu"
            },
            {
                "authorId": "1753344",
                "name": "Maosong Sun"
            },
            {
                "authorId": "2152797839",
                "name": "Yang Liu"
            },
            {
                "authorId": "144809121",
                "name": "Xuan Zhu"
            }
        ],
        "abstract": "\n \n Knowledge graph completion aims to perform link prediction between entities. In this paper, we consider the approach of knowledge graph embeddings. Recently, models such as TransE and TransH build entity and relation embeddings by regarding a relation as translation from head entity to tail entity. We note that these models simply put both entities and relations within the same semantic space. In fact, an entity may have multiple aspects and various relations may focus on different aspects of entities, which makes a common space insufficient for modeling. In this paper, we propose TransR to build entity and relation embeddings in separate entity space and relation spaces. Afterwards, we learn embeddings by first projecting entities from entity space to corresponding relation space and then building translations between projected entities. In experiments, we evaluate our models on three tasks including link prediction, triple classification and relational fact extraction. Experimental results show significant and consistent improvements compared to state-of-the-art baselines including TransE and TransH.\n \n"
    },
    {
        "paperId": "bbb4a5e24d8b227e553b9efd6f1d62a56b7fba92",
        "url": "https://www.semanticscholar.org/paper/bbb4a5e24d8b227e553b9efd6f1d62a56b7fba92",
        "title": "VBPR: Visual Bayesian Personalized Ranking from Implicit Feedback",
        "venue": "AAAI Conference on Artificial Intelligence",
        "year": 2015,
        "citationCount": 1055,
        "openAccessPdf": {
            "url": "https://ojs.aaai.org/index.php/AAAI/article/download/9973/9832",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1510.01784, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2933399",
                "name": "Ruining He"
            },
            {
                "authorId": "35660011",
                "name": "Julian McAuley"
            }
        ],
        "abstract": "\n \n Modern recommender systems model people and items by discovering or `teasing apart' the underlying dimensions that encode the properties of items and users' preferences toward them. Critically, such dimensions are uncovered based on user feedback, often in implicit form (such as purchase histories, browsing logs, etc.); in addition, some recommender systems make use of side information, such as product attributes, temporal information, or review text.However one important feature that is typically ignored by existing personalized recommendation and ranking methods is the visual appearance of the items being considered. In this paper we propose a scalable factorization model to incorporate visual signals into predictors of people's opinions, which we apply to a selection of large, real-world datasets. We make use of visual features extracted from product images using (pre-trained) deep networks, on top of which we learn an additional layer that uncovers the visual dimensions that best explain the variation in people's feedback. This not only leads to significantly more accurate personalized ranking methods, but also helps to alleviate cold start issues, and qualitatively to analyze the visual dimensions that influence people's opinions.\n \n"
    },
    {
        "paperId": "d079a2f877f554e00f71a6975435d8325987bdf5",
        "url": "https://www.semanticscholar.org/paper/d079a2f877f554e00f71a6975435d8325987bdf5",
        "title": "Return of Frustratingly Easy Domain Adaptation",
        "venue": "AAAI Conference on Artificial Intelligence",
        "year": 2015,
        "citationCount": 1957,
        "openAccessPdf": {
            "url": "https://ojs.aaai.org/index.php/AAAI/article/download/10306/10165",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1511.05547, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2636783",
                "name": "Baochen Sun"
            },
            {
                "authorId": "33221685",
                "name": "Jiashi Feng"
            },
            {
                "authorId": "2903226",
                "name": "Kate Saenko"
            }
        ],
        "abstract": "\n \n Unlike human learning, machine learning often fails to handle changes between training (source) and test (target) input distributions. Such domain shifts, common in practical scenarios, severely damage the performance of conventional machine learning methods. Supervised domain adaptation methods have been proposed for the case when the target data have labels, including some that perform very well despite being ``frustratingly easy'' to implement. However, in practice, the target domain is often unlabeled, requiring unsupervised adaptation. We propose a simple, effective, and efficient method for unsupervised domain adaptation called CORrelation ALignment (CORAL). CORAL minimizes domain shift by aligning the second-order statistics of source and target distributions, without requiring any target labels. Even though it is extraordinarily simple--it can be implemented in four lines of Matlab code--CORAL performs remarkably well in extensive evaluations on standard benchmark datasets.\n \n"
    },
    {
        "paperId": "eba36ac75bf22edf9a1bfd33244d459c75b98305",
        "url": "https://www.semanticscholar.org/paper/eba36ac75bf22edf9a1bfd33244d459c75b98305",
        "title": "Recurrent Convolutional Neural Networks for Text Classification",
        "venue": "AAAI Conference on Artificial Intelligence",
        "year": 2015,
        "citationCount": 2491,
        "openAccessPdf": {
            "url": "https://ojs.aaai.org/index.php/AAAI/article/download/9513/9372",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1609/aaai.v29i1.9513?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1609/aaai.v29i1.9513, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "38431523",
                "name": "Siwei Lai"
            },
            {
                "authorId": "8540973",
                "name": "Liheng Xu"
            },
            {
                "authorId": "2200096",
                "name": "Kang Liu"
            },
            {
                "authorId": "1390572170",
                "name": "Jun Zhao"
            }
        ],
        "abstract": "\n \n Text classification is a foundational task in many NLP applications. Traditional text classifiers often rely on many human-designed features, such as dictionaries, knowledge bases and special tree kernels. In contrast to traditional methods, we introduce a recurrent convolutional neural network for text classification without human-designed features. In our model, we apply a recurrent structure to capture contextual information as far as possible when learning word representations, which may introduce considerably less noise compared to traditional window-based neural networks. We also employ a max-pooling layer that automatically judges which words play key roles in text classification to capture the key components in texts. We conduct experiments on four commonly used datasets. The experimental results show that the proposed method outperforms the state-of-the-art methods on several datasets, particularly on document-level datasets.\n \n"
    },
    {
        "paperId": "0f7ef24660d2524fbda2a3822d070f38dcc239ef",
        "url": "https://www.semanticscholar.org/paper/0f7ef24660d2524fbda2a3822d070f38dcc239ef",
        "title": "Deep Spatio-Temporal Residual Networks for Citywide Crowd Flows Prediction",
        "venue": "AAAI Conference on Artificial Intelligence",
        "year": 2016,
        "citationCount": 2270,
        "openAccessPdf": {
            "url": "https://ojs.aaai.org/index.php/AAAI/article/download/10735/10594",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1610.00081, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "50562026",
                "name": "Junbo Zhang"
            },
            {
                "authorId": "144384718",
                "name": "Yu Zheng"
            },
            {
                "authorId": "7418601",
                "name": "Dekang Qi"
            }
        ],
        "abstract": "\n \n Forecasting the flow of crowds is of great importance to traffic management and public safety, and very challenging as it is affected by many complex factors, such as inter-region traffic, events, and weather. We propose a deep-learning-based approach, called ST-ResNet, to collectively forecast the inflow and outflow of crowds in each and every region of a city. We design an end-to-end structure of ST-ResNet based on unique properties of spatio-temporal data. More specifically, we employ the residual neural network framework to model the temporal closeness, period, and trend properties of crowd traffic. For each property, we design a branch of residual convolutional units, each of which models the spatial properties of crowd traffic. ST-ResNet learns to dynamically aggregate the output of the three residual neural networks based on data, assigning different weights to different branches and regions. The aggregation is further combined with external factors, such as weather and day of the week, to predict the final traffic of crowds in each and every region. Experiments on two types of crowd flows in Beijing and New York City (NYC) demonstrate that the proposed ST-ResNet outperforms six well-known methods.\n \n"
    },
    {
        "paperId": "15b26d8cb35d7e795c8832fe08794224ee1e9f84",
        "url": "https://www.semanticscholar.org/paper/15b26d8cb35d7e795c8832fe08794224ee1e9f84",
        "title": "The Option-Critic Architecture",
        "venue": "AAAI Conference on Artificial Intelligence",
        "year": 2016,
        "citationCount": 1175,
        "openAccessPdf": {
            "url": "https://ojs.aaai.org/index.php/AAAI/article/download/10916/10775",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1609.05140, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "145180695",
                "name": "Pierre-Luc Bacon"
            },
            {
                "authorId": "40638357",
                "name": "J. Harb"
            },
            {
                "authorId": "144368601",
                "name": "Doina Precup"
            }
        ],
        "abstract": "\n \n Temporal abstraction is key to scaling up learning and planning in reinforcement learning. While planning with temporally extended actions is well understood, creating such abstractions autonomously from data has remained challenging.We tackle this problem in the framework of options [Sutton,Precup and Singh, 1999; Precup, 2000]. We derive policy gradient theorems for options and propose a new option-critic architecture capable of learning both the internal policies and the termination conditions of options, in tandem with the policy over options, and without the need to provide any additional rewards or subgoals. Experimental results in both discrete and continuous environments showcase the flexibility and efficiency of the framework.\n \n"
    },
    {
        "paperId": "1a37f07606d60df365d74752857e8ce909f700b3",
        "url": "https://www.semanticscholar.org/paper/1a37f07606d60df365d74752857e8ce909f700b3",
        "title": "Deep Neural Networks for Learning Graph Representations",
        "venue": "AAAI Conference on Artificial Intelligence",
        "year": 2016,
        "citationCount": 1009,
        "openAccessPdf": {
            "url": "https://ojs.aaai.org/index.php/AAAI/article/download/10179/10038",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1609/aaai.v30i1.10179?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1609/aaai.v30i1.10179, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2105817",
                "name": "Shaosheng Cao"
            },
            {
                "authorId": "143844110",
                "name": "Wei Lu"
            },
            {
                "authorId": "3101288",
                "name": "Qiongkai Xu"
            }
        ],
        "abstract": "\n \n In this paper, we propose a novel model for learning graph representations, which generates a low-dimensional vector representation for each vertex by capturing the graph structural information. Different from other previous research efforts, we adopt a random surfing model to capture graph structural information directly, instead of using the sampling-based method for generating linear sequences proposed by Perozzi et al. (2014). The advantages of our approach will be illustrated from both theorical and empirical perspectives. We also give a new perspective for the matrix factorization method proposed by Levy and Goldberg (2014), in which the pointwise mutual information (PMI) matrix is considered as an analytical solution to the objective function of the skip-gram model with negative sampling proposed by Mikolov et al. (2013). Unlike their approach which involves the use of the SVD for finding the low-dimensitonal projections from the PMI matrix, however, the stacked denoising autoencoder is introduced in our model to extract complex features and model non-linearities. To demonstrate the effectiveness of our model, we conduct experiments on clustering and visualization tasks, employing the learned vertex representations as features. Empirical results on datasets of varying sizes show that our model outperforms other stat-of-the-art models in such tasks.\n \n"
    },
    {
        "paperId": "1bc49abe5145055f1fa259bd4e700b1eb6b7f08d",
        "url": "https://www.semanticscholar.org/paper/1bc49abe5145055f1fa259bd4e700b1eb6b7f08d",
        "title": "SummaRuNNer: A Recurrent Neural Network Based Sequence Model for Extractive Summarization of Documents",
        "venue": "AAAI Conference on Artificial Intelligence",
        "year": 2016,
        "citationCount": 1308,
        "openAccessPdf": {
            "url": "https://ojs.aaai.org/index.php/AAAI/article/download/10958/10817",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1611.04230, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1701451",
                "name": "Ramesh Nallapati"
            },
            {
                "authorId": "9091828",
                "name": "Feifei Zhai"
            },
            {
                "authorId": "145218984",
                "name": "Bowen Zhou"
            }
        ],
        "abstract": "\n \n We present SummaRuNNer, a Recurrent Neural Network (RNN) based sequence model for extractive summarization of documents and show that it achieves performance better than or comparable to state-of-the-art. Our model has the additional advantage of being very interpretable, since it allows visualization of its predictions broken up by abstract features such as information content, salience and novelty. Another novel contribution of our work is abstractive training of our extractive model that can train on human generated reference summaries alone, eliminating the need for sentence-level extractive labels.\n \n"
    },
    {
        "paperId": "26aa6fe2028b5eefbaa40ab54ef725bbbe7d9810",
        "url": "https://www.semanticscholar.org/paper/26aa6fe2028b5eefbaa40ab54ef725bbbe7d9810",
        "title": "ConceptNet 5.5: An Open Multilingual Graph of General Knowledge",
        "venue": "AAAI Conference on Artificial Intelligence",
        "year": 2016,
        "citationCount": 3131,
        "openAccessPdf": {
            "url": "https://ojs.aaai.org/index.php/AAAI/article/download/11164/11023",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1612.03975, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "145696762",
                "name": "R. Speer"
            },
            {
                "authorId": "2060230787",
                "name": "Joshua Chin"
            },
            {
                "authorId": "2232845",
                "name": "Catherine Havasi"
            }
        ],
        "abstract": "\n \n Machine learning about language can be improved by supplying it with specific knowledge and sources of external information. We present here a new version of the linked open data resource ConceptNet that is particularly well suited to be used with modern NLP techniques such as word embeddings. ConceptNet is a knowledge graph that connects words and phrases of natural language with labeled edges. Its knowledge is collected from many sources that include expert-created resources, crowd-sourcing, and games with a purpose. It is designed to represent the general knowledge involved in understanding language, improving natural language applications by allowing the application to better understand the meanings behind the words people use. When ConceptNet is combined with word embeddings acquired from distributional semantics (such as word2vec), it provides applications with understanding that they would not acquire from distributional semantics alone, nor from narrower resources such as WordNet or DBPedia. We demonstrate this with state-of-the-art results on intrinsic evaluations of word relatedness that translate into improvements on applications of word vectors, including solving SAT-style analogies.\n \n"
    },
    {
        "paperId": "32c4e19f4a757f6c6984416b97d69e287d1d0ecd",
        "url": "https://www.semanticscholar.org/paper/32c4e19f4a757f6c6984416b97d69e287d1d0ecd",
        "title": "SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient",
        "venue": "AAAI Conference on Artificial Intelligence",
        "year": 2016,
        "citationCount": 2510,
        "openAccessPdf": {
            "url": "https://ojs.aaai.org/index.php/AAAI/article/download/10804/10663",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1609.05473, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3469209",
                "name": "Lantao Yu"
            },
            {
                "authorId": "2108309275",
                "name": "Weinan Zhang"
            },
            {
                "authorId": "39055225",
                "name": "Jun Wang"
            },
            {
                "authorId": "1811427",
                "name": "Yong Yu"
            }
        ],
        "abstract": "\n \n As a new way of training generative models, Generative Adversarial Net (GAN) that uses a discriminative model to guide the training of the generative model has enjoyed considerable success in generating real-valued data. However, it has limitations when the goal is for generating sequences of discrete tokens. A major reason lies in that the discrete outputs from the generative model make it difficult to pass the gradient update from the discriminative model to the generative model. Also, the discriminative model can only assess a complete sequence, while for a partially generated sequence, it is non-trivial to balance its current score and the future one once the entire sequence has been generated. In this paper, we propose a sequence generation framework, called SeqGAN, to solve the problems. Modeling the data generator as a stochastic policy in reinforcement learning (RL), SeqGAN bypasses the generator differentiation problem by directly performing gradient policy update. The RL reward signal comes from the GAN discriminator judged on a complete sequence, and is passed back to the intermediate state-action steps using Monte Carlo search. Extensive experiments on synthetic data and real-world tasks demonstrate significant improvements over strong baselines.\n \n"
    },
    {
        "paperId": "609e0f0e60ddfe83fdc71bf5397205323888289d",
        "url": "https://www.semanticscholar.org/paper/609e0f0e60ddfe83fdc71bf5397205323888289d",
        "title": "A Hierarchical Latent Variable Encoder-Decoder Model for Generating Dialogues",
        "venue": "AAAI Conference on Artificial Intelligence",
        "year": 2016,
        "citationCount": 1134,
        "openAccessPdf": {
            "url": "https://ojs.aaai.org/index.php/AAAI/article/download/10983/10842",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1605.06069, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "35224828",
                "name": "Iulian Serban"
            },
            {
                "authorId": "2041695",
                "name": "Alessandro Sordoni"
            },
            {
                "authorId": "2054294",
                "name": "Ryan Lowe"
            },
            {
                "authorId": "1778839",
                "name": "Laurent Charlin"
            },
            {
                "authorId": "145134886",
                "name": "Joelle Pineau"
            },
            {
                "authorId": "1760871",
                "name": "Aaron C. Courville"
            },
            {
                "authorId": "1751762",
                "name": "Yoshua Bengio"
            }
        ],
        "abstract": "\n \n Sequential data often possesses hierarchical structures with complex dependencies between sub-sequences, such as found between the utterances in a dialogue. To model these dependencies in a generative framework, we propose a neural network-based generative architecture, with stochastic latent variables that span a variable number of time steps. We apply the proposed model to the task of dialogue response generation and compare it with other recent neural-network architectures. We evaluate the model performance through a human evaluation study. The experiments demonstrate that our model improves upon recently proposed models and that the latent variables facilitate both the generation of meaningful, long and diverse responses and maintaining dialogue state.\n \n"
    },
    {
        "paperId": "addb41821f0e6c3f89289061a5598e9a70b637a4",
        "url": "https://www.semanticscholar.org/paper/addb41821f0e6c3f89289061a5598e9a70b637a4",
        "title": "An End-to-End Spatio-Temporal Attention Model for Human Action Recognition from Skeleton Data",
        "venue": "AAAI Conference on Artificial Intelligence",
        "year": 2016,
        "citationCount": 1030,
        "openAccessPdf": {
            "url": "https://ojs.aaai.org/index.php/AAAI/article/download/11212/11071",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1611.06067, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3384254",
                "name": "Sijie Song"
            },
            {
                "authorId": "40093162",
                "name": "Cuiling Lan"
            },
            {
                "authorId": "1757173",
                "name": "Junliang Xing"
            },
            {
                "authorId": "1634494276",
                "name": "Wenjun Zeng"
            },
            {
                "authorId": "41127426",
                "name": "Jiaying Liu"
            }
        ],
        "abstract": "\n \n Human action recognition is an important task in computer vision. Extracting discriminative spatial and temporal features to model the spatial and temporal evolutions of different actions plays a key role in accomplishing this task. In this work, we propose an end-to-end spatial and temporal attention model for human action recognition from skeleton data. We build our model on top of the Recurrent Neural Networks (RNNs) with Long Short-Term Memory (LSTM), which learns to selectively focus on discriminative joints of skeleton within each frame of the inputs and pays different levels of attention to the outputs of different frames. Furthermore, to ensure effective training of the network, we propose a regularized cross-entropy loss to drive the model learning process and develop a joint training strategy accordingly. Experimental results demonstrate the effectiveness of the proposed model, both on the small human action recognition dataset of SBU and the currently largest NTU dataset.\n \n"
    },
    {
        "paperId": "b5c26ab8767d046cb6e32d959fdf726aee89bb62",
        "url": "https://www.semanticscholar.org/paper/b5c26ab8767d046cb6e32d959fdf726aee89bb62",
        "title": "Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning",
        "venue": "AAAI Conference on Artificial Intelligence",
        "year": 2016,
        "citationCount": 15043,
        "openAccessPdf": {
            "url": "https://ojs.aaai.org/index.php/AAAI/article/download/11231/11090",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1602.07261, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2574060",
                "name": "Christian Szegedy"
            },
            {
                "authorId": "2054165706",
                "name": "Sergey Ioffe"
            },
            {
                "authorId": "2657155",
                "name": "Vincent Vanhoucke"
            },
            {
                "authorId": "122113652",
                "name": "Alexander A. Alemi"
            }
        ],
        "abstract": "\n \n Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question: Are there any benefits to combining Inception architectures with residual connections? Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4 networks, we achieve 3.08% top-5 error on the test set of the ImageNet classification (CLS) challenge.\n \n"
    },
    {
        "paperId": "0ab3f7ecbdc5a33565a234215604a6ca9d155a33",
        "url": "https://www.semanticscholar.org/paper/0ab3f7ecbdc5a33565a234215604a6ca9d155a33",
        "title": "Rainbow: Combining Improvements in Deep Reinforcement Learning",
        "venue": "AAAI Conference on Artificial Intelligence",
        "year": 2017,
        "citationCount": 2484,
        "openAccessPdf": {
            "url": "https://ojs.aaai.org/index.php/AAAI/article/download/11796/11655",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1710.02298, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "39357484",
                "name": "Matteo Hessel"
            },
            {
                "authorId": "3321484",
                "name": "Joseph Modayil"
            },
            {
                "authorId": "7634925",
                "name": "H. V. Hasselt"
            },
            {
                "authorId": "1725157",
                "name": "T. Schaul"
            },
            {
                "authorId": "2273072",
                "name": "Georg Ostrovski"
            },
            {
                "authorId": "2605877",
                "name": "Will Dabney"
            },
            {
                "authorId": "48257711",
                "name": "Dan Horgan"
            },
            {
                "authorId": "1808897",
                "name": "Bilal Piot"
            },
            {
                "authorId": "37666967",
                "name": "M. G. Azar"
            },
            {
                "authorId": "145824029",
                "name": "David Silver"
            }
        ],
        "abstract": "\n \n The deep reinforcement learning community has made several independent improvements to the DQN algorithm. However, it is unclear which of these extensions are complementary and can be fruitfully combined. This paper examines six extensions to the DQN algorithm and empirically studies their combination. Our experiments show that the combination provides state-of-the-art performance on the Atari 2600 benchmark, both in terms of data efficiency and final performance. We also provide results from a detailed ablation study that shows the contribution of each component to overall performance.\n \n"
    },
    {
        "paperId": "2788a2461ed0067e2f7aaa63c449a24a237ec341",
        "url": "https://www.semanticscholar.org/paper/2788a2461ed0067e2f7aaa63c449a24a237ec341",
        "title": "Random Erasing Data Augmentation",
        "venue": "AAAI Conference on Artificial Intelligence",
        "year": 2017,
        "citationCount": 3962,
        "openAccessPdf": {
            "url": "https://ojs.aaai.org/index.php/AAAI/article/download/7000/6854",
            "status": "GOLD",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1708.04896, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2069512103",
                "name": "Zhun Zhong"
            },
            {
                "authorId": "144802394",
                "name": "Liang Zheng"
            },
            {
                "authorId": "3374337",
                "name": "Guoliang Kang"
            },
            {
                "authorId": "8086812",
                "name": "Shaozi Li"
            },
            {
                "authorId": "7179962",
                "name": "Yi Yang"
            }
        ],
        "abstract": "In this paper, we introduce Random Erasing, a new data augmentation method for training the convolutional neural network (CNN). In training, Random Erasing randomly selects a rectangle region in an image and erases its pixels with random values. In this process, training images with various levels of occlusion are generated, which reduces the risk of over-fitting and makes the model robust to occlusion. Random Erasing is parameter learning free, easy to implement, and can be integrated with most of the CNN-based recognition models. Albeit simple, Random Erasing is complementary to commonly used data augmentation techniques such as random cropping and flipping, and yields consistent improvement over strong baselines in image classification, object detection and person re-identification. Code is available at: https://github.com/zhunzhong07/Random-Erasing."
    },
    {
        "paperId": "2b292ff89d808fba10579871591a22f1649cd039",
        "url": "https://www.semanticscholar.org/paper/2b292ff89d808fba10579871591a22f1649cd039",
        "title": "Counterfactual Multi-Agent Policy Gradients",
        "venue": "AAAI Conference on Artificial Intelligence",
        "year": 2017,
        "citationCount": 2326,
        "openAccessPdf": {
            "url": "https://ojs.aaai.org/index.php/AAAI/article/download/11794/11653",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1705.08926, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "145356667",
                "name": "Jakob N. Foerster"
            },
            {
                "authorId": "38698094",
                "name": "Gregory Farquhar"
            },
            {
                "authorId": "2285516",
                "name": "Triantafyllos Afouras"
            },
            {
                "authorId": "39683441",
                "name": "Nantas Nardelli"
            },
            {
                "authorId": "1766767",
                "name": "Shimon Whiteson"
            }
        ],
        "abstract": "\n \n Many real-world problems, such as network packet routing and the coordination of autonomous vehicles, are naturally modelled as cooperative multi-agent systems. There is a great need for new reinforcement learning methods that can efficiently learn decentralised policies for such systems. To this end, we propose a new multi-agent actor-critic method called counterfactual multi-agent (COMA) policy gradients. COMA uses a centralised critic to estimate the Q-function and decentralised actors to optimise the agents' policies. In addition, to address the challenges of multi-agent credit assignment, it uses a counterfactual baseline that marginalises out a single agent's action, while keeping the other agents' actions fixed. COMA also uses a critic representation that allows the counterfactual baseline to be computed efficiently in a single forward pass. We evaluate COMA in the testbed of StarCraft unit micromanagement, using a decentralised variant with significant partial observability. COMA significantly improves average performance over other multi-agent actor-critic methods in this setting, and the best performing agents are competitive with state-of-the-art centralised controllers that get access to the full state.\n \n"
    },
    {
        "paperId": "33690ff21ef1efb576410e656f2e60c89d0307d6",
        "url": "https://www.semanticscholar.org/paper/33690ff21ef1efb576410e656f2e60c89d0307d6",
        "title": "Deep Reinforcement Learning that Matters",
        "venue": "AAAI Conference on Artificial Intelligence",
        "year": 2017,
        "citationCount": 2133,
        "openAccessPdf": {
            "url": "https://ojs.aaai.org/index.php/AAAI/article/download/11694/11553",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1709.06560, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "40068904",
                "name": "Peter Henderson"
            },
            {
                "authorId": "18014232",
                "name": "Riashat Islam"
            },
            {
                "authorId": "143902541",
                "name": "Philip Bachman"
            },
            {
                "authorId": "145134886",
                "name": "Joelle Pineau"
            },
            {
                "authorId": "144368601",
                "name": "Doina Precup"
            },
            {
                "authorId": "2462512",
                "name": "D. Meger"
            }
        ],
        "abstract": "\n \n In recent years, significant progress has been made in solving challenging problems across various domains using deep reinforcement learning (RL). Reproducing existing work and accurately judging the improvements offered by novel methods is vital to sustaining this progress. Unfortunately, reproducing results for state-of-the-art deep RL methods is seldom straightforward. In particular, non-determinism in standard benchmark environments, combined with variance intrinsic to the methods, can make reported results tough to interpret. Without significance metrics and tighter standardization of experimental reporting, it is difficult to determine whether improvements over the prior state-of-the-art are meaningful. In this paper, we investigate challenges posed by reproducibility, proper experimental techniques, and reporting procedures. We illustrate the variability in reported metrics and results when comparing against common baselines and suggest guidelines to make future results in deep RL more reproducible. We aim to spur discussion about how to ensure continued progress in the field by minimizing wasted effort stemming from results that are non-reproducible and easily misinterpreted.\n \n"
    },
    {
        "paperId": "33d6aa6c41ce3000161d9b5eea910a5b78e14330",
        "url": "https://www.semanticscholar.org/paper/33d6aa6c41ce3000161d9b5eea910a5b78e14330",
        "title": "Robust Loss Functions under Label Noise for Deep Neural Networks",
        "venue": "AAAI Conference on Artificial Intelligence",
        "year": 2017,
        "citationCount": 1080,
        "openAccessPdf": {
            "url": "https://ojs.aaai.org/index.php/AAAI/article/download/10894/10753",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1712.09482, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3201314",
                "name": "Aritra Ghosh"
            },
            {
                "authorId": "2065156735",
                "name": "Himanshu Kumar"
            },
            {
                "authorId": "143637056",
                "name": "P. Sastry"
            }
        ],
        "abstract": "\n \n In many applications of classifier learning, training data suffers from label noise. Deep networks are learned using huge training data where the problem of noisy labels is particularly relevant. The current techniques proposed for learning deep networks under label noise focus on modifying the network architecture and on algorithms for estimating true labels from noisy labels. An alternate approach would be to look for loss functions that are inherently noise-tolerant. For binary classification there exist theoretical results on loss functions that are robust to label noise. In this paper, we provide some sufficient conditions on a loss function so that risk minimization under that loss function would be inherently tolerant to label noise for multiclass classification problems. These results generalize the existing results on noise-tolerant loss functions for binary classification. We study some of the widely used loss functions in deep networks and show that the loss function based on mean absolute value of error is inherently robust to label noise. Thus standard back propagation is enough to learn the true classifier even under label noise. Through experiments, we illustrate the robustness of risk minimization with such loss functions for learning neural networks.\n \n"
    },
    {
        "paperId": "5383ee00a5c0c3372081fccfeceb812b7854c9ea",
        "url": "https://www.semanticscholar.org/paper/5383ee00a5c0c3372081fccfeceb812b7854c9ea",
        "title": "Spatial As Deep: Spatial CNN for Traffic Scene Understanding",
        "venue": "AAAI Conference on Artificial Intelligence",
        "year": 2017,
        "citationCount": 1122,
        "openAccessPdf": {
            "url": "https://ojs.aaai.org/index.php/AAAI/article/download/12301/12160",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1712.06080, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "14214933",
                "name": "Xingang Pan"
            },
            {
                "authorId": "1788070",
                "name": "Jianping Shi"
            },
            {
                "authorId": "47571885",
                "name": "Ping Luo"
            },
            {
                "authorId": "31843833",
                "name": "Xiaogang Wang"
            },
            {
                "authorId": "50295995",
                "name": "Xiaoou Tang"
            }
        ],
        "abstract": "\n \n Convolutional neural networks (CNNs) are usually built by stacking convolutional operations layer-by-layer. Although CNN has shown strong capability to extract semantics from raw pixels, its capacity to capture spatial relationships of pixels across rows and columns of an image is not fully explored. These relationships are important to learn semantic objects with strong shape priors but weak appearance coherences, such as traffic lanes, which are often occluded or not even painted on the road surface as shown in Fig. 1 (a). In this paper, we propose Spatial CNN (SCNN), which generalizes traditional deep layer-by-layer convolutions to slice-by-slice convolutions within feature maps, thus enabling message passings between pixels across rows and columns in a layer. Such SCNN is particular suitable for long continuous shape structure or large objects, with strong spatial relationship but less appearance clues, such as traffic lanes, poles, and wall. We apply SCNN on a newly released very challenging traffic lane detection dataset and Cityscapse dataset. The results show that SCNN could learn the spatial relationship for structure output and significantly improves the performance. We show that SCNN outperforms the recurrent neural network (RNN) based ReNet and MRF+CNN (MRFNet) in the lane detection dataset by 8.7% and 4.6% respectively. Moreover, our SCNN won the 1st place on the TuSimple Benchmark Lane Detection Challenge, with an accuracy of 96.53%.\n \n"
    },
    {
        "paperId": "5c717445179991e002333182df3b233fe502e357",
        "url": "https://www.semanticscholar.org/paper/5c717445179991e002333182df3b233fe502e357",
        "title": "Learning to Generalize: Meta-Learning for Domain Generalization",
        "venue": "AAAI Conference on Artificial Intelligence",
        "year": 2017,
        "citationCount": 1611,
        "openAccessPdf": {
            "url": "https://ojs.aaai.org/index.php/AAAI/article/download/11596/11455",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1710.03463, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2108338672",
                "name": "Da Li"
            },
            {
                "authorId": "2653152",
                "name": "Yongxin Yang"
            },
            {
                "authorId": "2264012952",
                "name": "Yi-Zhe Song"
            },
            {
                "authorId": "1697755",
                "name": "Timothy M. Hospedales"
            }
        ],
        "abstract": "\n \n Domain shift refers to the well known problem that a model trained in one source domain performs poorly when appliedto a target domain with different statistics. Domain Generalization (DG) techniques attempt to alleviate this issue by producing models which by design generalize well to novel testing domains. We propose a novel meta-learning method for domain generalization. Rather than designing a specific model that is robust to domain shift as in most previous DG work, we propose a model agnostic training procedure for DG. Our algorithm simulates train/test domain shift during training by synthesizing virtual testing domains within each mini-batch. The meta-optimization objective requires that steps to improve training domain performance should also improve testing domain performance. This meta-learning procedure trains models with good generalization ability to novel domains. We evaluate our method and achieve state of the art results on a recent cross-domain image classification benchmark, as well demonstrating its potential on two classic reinforcement learning tasks.\n \n"
    },
    {
        "paperId": "7cfa5c97164129ce3630511f639040d28db1d4b7",
        "url": "https://www.semanticscholar.org/paper/7cfa5c97164129ce3630511f639040d28db1d4b7",
        "title": "FiLM: Visual Reasoning with a General Conditioning Layer",
        "venue": "AAAI Conference on Artificial Intelligence",
        "year": 2017,
        "citationCount": 2852,
        "openAccessPdf": {
            "url": "https://ojs.aaai.org/index.php/AAAI/article/download/11671/11530",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1709.07871, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3439053",
                "name": "Ethan Perez"
            },
            {
                "authorId": "3367628",
                "name": "Florian Strub"
            },
            {
                "authorId": "153559313",
                "name": "H. D. Vries"
            },
            {
                "authorId": "3074927",
                "name": "Vincent Dumoulin"
            },
            {
                "authorId": "1760871",
                "name": "Aaron C. Courville"
            }
        ],
        "abstract": "\n \n We introduce a general-purpose conditioning method for neural networks called FiLM: Feature-wise Linear Modulation. FiLM layers influence neural network computation via a simple, feature-wise affine transformation based on conditioning information. We show that FiLM layers are highly effective for visual reasoning - answering image-related questions which require a multi-step, high-level process - a task which has proven difficult for standard deep learning methods that do not explicitly model reasoning. Specifically, we show on visual reasoning tasks that FiLM layers 1) halve state-of-the-art error for the CLEVR benchmark, 2) modulate features in a coherent manner, 3) are robust to ablations and architectural modifications, and 4) generalize well to challenging, new data from few examples or even zero-shot.\n \n"
    },
    {
        "paperId": "9697d32ed0a16da167f2bdba05ef96d0da066eb5",
        "url": "https://www.semanticscholar.org/paper/9697d32ed0a16da167f2bdba05ef96d0da066eb5",
        "title": "Convolutional 2D Knowledge Graph Embeddings",
        "venue": "AAAI Conference on Artificial Intelligence",
        "year": 2017,
        "citationCount": 2890,
        "openAccessPdf": {
            "url": "https://ojs.aaai.org/index.php/AAAI/article/download/11573/11432",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1707.01476, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3239480",
                "name": "Tim Dettmers"
            },
            {
                "authorId": "3051815",
                "name": "Pasquale Minervini"
            },
            {
                "authorId": "1918552",
                "name": "Pontus Stenetorp"
            },
            {
                "authorId": "48662861",
                "name": "Sebastian Riedel"
            }
        ],
        "abstract": "\n \n Link prediction for knowledge graphs is the task of predicting missing relationships between entities. Previous work on link prediction has focused on shallow, fast models which can scale to large knowledge graphs. However, these models learn less expressive features than deep, multi-layer models  which potentially limits performance. In this work we introduce ConvE, a multi-layer convolutional network model for link prediction, and report state-of-the-art results for several established datasets. We also show that the model is highly parameter efficient, yielding the same performance as DistMult and R-GCN with 8x and 17x fewer parameters. Analysis of our model suggests that it is particularly effective at modelling nodes with high indegree  which are common in highly-connected, complex knowledge graphs such as Freebase and YAGO3. In addition, it has been noted that the WN18 and FB15k datasets suffer from test set leakage, due to inverse relations from the training set being present in the test set  however, the extent of this issue has so far not been quantified. We find this problem to be severe: a simple rule-based model can achieve state-of-the-art results on both WN18 and FB15k. To ensure that models are evaluated on datasets where simply exploiting inverse relations cannot yield competitive results, we investigate and validate several commonly used datasets  deriving robust variants where necessary. We then perform experiments on these robust datasets for our own and several previously proposed models, and find that ConvE achieves state-of-the-art Mean Reciprocal Rank across all datasets.\n \n"
    },
    {
        "paperId": "e3b0ea7209731c47b582215c6c67f9c691ad9863",
        "url": "https://www.semanticscholar.org/paper/e3b0ea7209731c47b582215c6c67f9c691ad9863",
        "title": "Deep Q-learning From Demonstrations",
        "venue": "AAAI Conference on Artificial Intelligence",
        "year": 2017,
        "citationCount": 1203,
        "openAccessPdf": {
            "url": "https://ojs.aaai.org/index.php/AAAI/article/download/11757/11616",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1609/aaai.v32i1.11757?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1609/aaai.v32i1.11757, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "143772943",
                "name": "Todd Hester"
            },
            {
                "authorId": "7515048",
                "name": "Matej Vecerk"
            },
            {
                "authorId": "1721354",
                "name": "O. Pietquin"
            },
            {
                "authorId": "1975889",
                "name": "Marc Lanctot"
            },
            {
                "authorId": "1725157",
                "name": "T. Schaul"
            },
            {
                "authorId": "1808897",
                "name": "Bilal Piot"
            },
            {
                "authorId": "48257711",
                "name": "Dan Horgan"
            },
            {
                "authorId": "34660073",
                "name": "John Quan"
            },
            {
                "authorId": "2533110",
                "name": "A. Sendonaris"
            },
            {
                "authorId": "2561924",
                "name": "Ian Osband"
            },
            {
                "authorId": "1387885286",
                "name": "Gabriel Dulac-Arnold"
            },
            {
                "authorId": "70495322",
                "name": "J. Agapiou"
            },
            {
                "authorId": "1700356",
                "name": "Joel Z. Leibo"
            },
            {
                "authorId": "2203658",
                "name": "A. Gruslys"
            }
        ],
        "abstract": "\n \n Deep reinforcement learning (RL) has achieved several high profile successes in difficult decision-making problems. However, these algorithms typically require a huge amount of data before they reach reasonable performance. In fact, their performance during learning can be extremely poor. This may be acceptable for a simulator, but it severely limits the applicability of deep RL to many real-world tasks, where the agent must learn in the real environment. In this paper we study a setting where the agent may access data from previous control of the system. We present an algorithm, Deep Q-learning from Demonstrations (DQfD), that leverages small sets of demonstration data to massively accelerate the learning process even from relatively small amounts of demonstration data and is able to automatically assess the necessary ratio of demonstration data while learning thanks to a prioritized replay mechanism. DQfD works by combining temporal difference updates with supervised classification of the demonstrators actions. We show that DQfD has better initial performance than Prioritized Dueling Double Deep Q-Networks (PDD DQN) as it starts with better scores on the first million steps on 41 of 42 games and on average it takes PDD DQN 83 million steps to catch up to DQfDs performance. DQfD learns to out-perform the best demonstration given in 14 of 42 games. In addition, DQfD leverages human demonstrations to achieve state-of-the-art results for 11 games. Finally, we show that DQfD performs better than three related algorithms for incorporating demonstration data into DQN.\n \n"
    },
    {
        "paperId": "0c7e1338a9c7914a3b9a5bdc42b457b3f272160e",
        "url": "https://www.semanticscholar.org/paper/0c7e1338a9c7914a3b9a5bdc42b457b3f272160e",
        "title": "Session-based Recommendation with Graph Neural Networks",
        "venue": "AAAI Conference on Artificial Intelligence",
        "year": 2018,
        "citationCount": 1751,
        "openAccessPdf": {
            "url": "https://aaai.org/ojs/index.php/AAAI/article/download/3804/3682",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1811.00855, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "50425438",
                "name": "Shu Wu"
            },
            {
                "authorId": "30582299",
                "name": "Yuyuan Tang"
            },
            {
                "authorId": "2653121",
                "name": "Yanqiao Zhu"
            },
            {
                "authorId": "123865558",
                "name": "Liang Wang"
            },
            {
                "authorId": "144076239",
                "name": "Xing Xie"
            },
            {
                "authorId": "143874948",
                "name": "T. Tan"
            }
        ],
        "abstract": "The problem of session-based recommendation aims to predict user actions based on anonymous sessions. Previous methods model a session as a sequence and estimate user representations besides item representations to make recommendations. Though achieved promising results, they are insufficient to obtain accurate user vectors in sessions and neglect complex transitions of items. To obtain accurate item embedding and take complex transitions of items into account, we propose a novel method, i.e. Session-based Recommendation with Graph Neural Networks, SR-GNN for brevity. In the proposed method, session sequences are modeled as graphstructured data. Based on the session graph, GNN can capture complex transitions of items, which are difficult to be revealed by previous conventional sequential methods. Each session is then represented as the composition of the global preference and the current interest of that session using an attention network. Extensive experiments conducted on two real datasets show that SR-GNN evidently outperforms the state-of-the-art session-based recommendation methods consistently."
    },
    {
        "paperId": "1d8f4f76ac6534627ef8a1c24b9937d8ab2a5c5f",
        "url": "https://www.semanticscholar.org/paper/1d8f4f76ac6534627ef8a1c24b9937d8ab2a5c5f",
        "title": "Anchors: High-Precision Model-Agnostic Explanations",
        "venue": "AAAI Conference on Artificial Intelligence",
        "year": 2018,
        "citationCount": 2211,
        "openAccessPdf": {
            "url": "https://ojs.aaai.org/index.php/AAAI/article/download/11491/11350",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1609/aaai.v32i1.11491?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1609/aaai.v32i1.11491, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "78846919",
                "name": "Marco Tulio Ribeiro"
            },
            {
                "authorId": "34650964",
                "name": "Sameer Singh"
            },
            {
                "authorId": "1730156",
                "name": "Carlos Guestrin"
            }
        ],
        "abstract": "\n \n We introduce a novel model-agnostic system that explains the behavior of complex models with high-precision rules called anchors, representing local, \"sufficient\" conditions for predictions. We propose an algorithm to efficiently compute these explanations for any black-box model with high-probability guarantees. We demonstrate the flexibility of anchors by explaining a myriad of different models for different domains and tasks. In a user study, we show that anchors enable users to predict how a model would behave on unseen instances with less effort and higher precision, as compared to existing linear explanations or no explanations.\n \n"
    },
    {
        "paperId": "36652428740cd30d245d55889f01a7fb04a91c93",
        "url": "https://www.semanticscholar.org/paper/36652428740cd30d245d55889f01a7fb04a91c93",
        "title": "Deeper Insights into Graph Convolutional Networks for Semi-Supervised Learning",
        "venue": "AAAI Conference on Artificial Intelligence",
        "year": 2018,
        "citationCount": 3147,
        "openAccessPdf": {
            "url": "https://ojs.aaai.org/index.php/AAAI/article/download/11604/11463",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1801.07606, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "35692225",
                "name": "Qimai Li"
            },
            {
                "authorId": "40592359",
                "name": "Zhichao Han"
            },
            {
                "authorId": "19195265",
                "name": "Xiao-Ming Wu"
            }
        ],
        "abstract": "\n \n Many interesting problems in machine learning are being revisited with new deep learning tools. For graph-based semi-supervised learning, a recent important development is graph convolutional networks (GCNs), which nicely integrate local vertex features and graph topology in the convolutional layers. Although the GCN model compares favorably with other state-of-the-art methods, its mechanisms are not clear and it still requires considerable amount of labeled data for validation and model selection. In this paper, we develop deeper insights into the GCN model and address its fundamental limits. First, we show that the graph convolution of the GCN model is actually a special form of Laplacian smoothing, which is the key reason why GCNs work, but it also brings potential concerns of over-smoothing with many convolutional layers. Second, to overcome the limits of the GCN model with shallow architectures, we propose both co-training and self-training approaches to train GCNs. Our approaches significantly improve GCNs in learning with very few labels, and exempt them from requiring additional labels for validation. Extensive experiments on benchmarks have verified our theory and proposals.\n \n"
    },
    {
        "paperId": "50bdda28de3dcf82a0e10f9ec13eea248b19edb5",
        "url": "https://www.semanticscholar.org/paper/50bdda28de3dcf82a0e10f9ec13eea248b19edb5",
        "title": "Regularized Evolution for Image Classifier Architecture Search",
        "venue": "AAAI Conference on Artificial Intelligence",
        "year": 2018,
        "citationCount": 3204,
        "openAccessPdf": {
            "url": "https://ojs.aaai.org/index.php/AAAI/article/download/4405/4283",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1802.01548, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2892780",
                "name": "Esteban Real"
            },
            {
                "authorId": "1737322",
                "name": "A. Aggarwal"
            },
            {
                "authorId": "2145438541",
                "name": "Yanping Huang"
            },
            {
                "authorId": "2827616",
                "name": "Quoc V. Le"
            }
        ],
        "abstract": "The effort devoted to hand-crafting neural network image classifiers has motivated the use of architecture search to discover them automatically. Although evolutionary algorithms have been repeatedly applied to neural network topologies, the image classifiers thus discovered have remained inferior to human-crafted ones. Here, we evolve an image classifier AmoebaNet-Athat surpasses hand-designs for the first time. To do this, we modify the tournament selection evolutionary algorithm by introducing an age property to favor the younger genotypes. Matching size, AmoebaNet-A has comparable accuracy to current state-of-the-art ImageNet models discovered with more complex architecture-search methods. Scaled to larger size, AmoebaNet-A sets a new state-of-theart 83.9% top-1 / 96.6% top-5 ImageNet accuracy. In a controlled comparison against a well known reinforcement learning algorithm, we give evidence that evolution can obtain results faster with the same hardware, especially at the earlier stages of the search. This is relevant when fewer compute resources are available. Evolution is, thus, a simple method to effectively discover high-quality architectures."
    },
    {
        "paperId": "510d98681e5e85fb1265513728f16e2543ae1b4b",
        "url": "https://www.semanticscholar.org/paper/510d98681e5e85fb1265513728f16e2543ae1b4b",
        "title": "Hypergraph Neural Networks",
        "venue": "AAAI Conference on Artificial Intelligence",
        "year": 2018,
        "citationCount": 1703,
        "openAccessPdf": {
            "url": "https://ojs.aaai.org/index.php/AAAI/article/download/4235/4113",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1809.09401, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "95444098",
                "name": "Yifan Feng"
            },
            {
                "authorId": "30156979",
                "name": "Haoxuan You"
            },
            {
                "authorId": "2476328",
                "name": "Zizhao Zhang"
            },
            {
                "authorId": "145592290",
                "name": "R. Ji"
            },
            {
                "authorId": "35350470",
                "name": "Yue Gao"
            }
        ],
        "abstract": "In this paper, we present a hypergraph neural networks (HGNN) framework for data representation learning, which can encode high-order data correlation in a hypergraph structure. Confronting the challenges of learning representation for complex data in real practice, we propose to incorporate such data structure in a hypergraph, which is more flexible on data modeling, especially when dealing with complex data. In this method, a hyperedge convolution operation is designed to handle the data correlation during representation learning. In this way, traditional hypergraph learning procedure can be conducted using hyperedge convolution operations efficiently. HGNN is able to learn the hidden layer representation considering the high-order data structure, which is a general framework considering the complex data correlations. We have conducted experiments on citation network classification and visual object recognition tasks and compared HGNN with graph convolutional networks and other traditional methods. Experimental results demonstrate that the proposed HGNN method outperforms recent state-of-theart methods. We can also reveal from the results that the proposed HGNN is superior when dealing with multi-modal data compared with existing methods."
    },
    {
        "paperId": "6017e81c5ede6c38b306a3df9738aeb04baa7619",
        "url": "https://www.semanticscholar.org/paper/6017e81c5ede6c38b306a3df9738aeb04baa7619",
        "title": "Graph Convolutional Networks for Text Classification",
        "venue": "AAAI Conference on Artificial Intelligence",
        "year": 2018,
        "citationCount": 1976,
        "openAccessPdf": {
            "url": "https://ojs.aaai.org/index.php/AAAI/article/download/4725/4603",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1809.05679, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "100680875",
                "name": "Liang Yao"
            },
            {
                "authorId": "145449667",
                "name": "Chengsheng Mao"
            },
            {
                "authorId": "1683396",
                "name": "Yuan Luo"
            }
        ],
        "abstract": "Text classification is an important and classical problem in natural language processing. There have been a number of studies that applied convolutional neural networks (convolution on regular grid, e.g., sequence) to classification. However, only a limited number of studies have explored the more flexible graph convolutional neural networks (convolution on non-grid, e.g., arbitrary graph) for the task. In this work, we propose to use graph convolutional networks for text classification. We build a single text graph for a corpus based on word co-occurrence and document word relations, then learn a Text Graph Convolutional Network (Text GCN) for the corpus. Our Text GCN is initialized with one-hot representation for word and document, it then jointly learns the embeddings for both words and documents, as supervised by the known class labels for documents. Our experimental results on multiple benchmark datasets demonstrate that a vanilla Text GCN without any external word embeddings or knowledge outperforms state-of-the-art methods for text classification. On the other hand, Text GCN also learns predictive word and document embeddings. In addition, experimental results show that the improvement of Text GCN over state-of-the-art comparison methods become more prominent as we lower the percentage of training data, suggesting the robustness of Text GCN to less training data in text classification."
    },
    {
        "paperId": "6ea57a2aea08ce0628c93f77bdc24c2f3e9cc6da",
        "url": "https://www.semanticscholar.org/paper/6ea57a2aea08ce0628c93f77bdc24c2f3e9cc6da",
        "title": "Weisfeiler and Leman Go Neural: Higher-order Graph Neural Networks",
        "venue": "AAAI Conference on Artificial Intelligence",
        "year": 2018,
        "citationCount": 1849,
        "openAccessPdf": {
            "url": "https://ojs.aaai.org/index.php/AAAI/article/download/4384/4262",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1810.02244, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "143622465",
                "name": "Christopher Morris"
            },
            {
                "authorId": "8787552",
                "name": "Martin Ritzert"
            },
            {
                "authorId": "3410500",
                "name": "Matthias Fey"
            },
            {
                "authorId": "49437682",
                "name": "William L. Hamilton"
            },
            {
                "authorId": "9572099",
                "name": "J. E. Lenssen"
            },
            {
                "authorId": "3329062",
                "name": "Gaurav Rattan"
            },
            {
                "authorId": "1744396",
                "name": "Martin Grohe"
            }
        ],
        "abstract": "In recent years, graph neural networks (GNNs) have emerged as a powerful neural architecture to learn vector representations of nodes and graphs in a supervised, end-to-end fashion. Up to now, GNNs have only been evaluated empiricallyshowing promising results. The following work investigates GNNs from a theoretical point of view and relates them to the 1-dimensional Weisfeiler-Leman graph isomorphism heuristic (1-WL). We show that GNNs have the same expressiveness as the 1-WL in terms of distinguishing non-isomorphic (sub-)graphs. Hence, both algorithms also have the same shortcomings. Based on this, we propose a generalization of GNNs, so-called k-dimensional GNNs (k-GNNs), which can take higher-order graph structures at multiple scales into account. These higher-order structures play an essential role in the characterization of social networks and molecule graphs. Our experimental evaluation confirms our theoretical findings as well as confirms that higher-order information is useful in the task of graph classification and regression."
    },
    {
        "paperId": "70d5ceb59118334e1a6eed33a149234413147b92",
        "url": "https://www.semanticscholar.org/paper/70d5ceb59118334e1a6eed33a149234413147b92",
        "title": "Deep Interest Evolution Network for Click-Through Rate Prediction",
        "venue": "AAAI Conference on Artificial Intelligence",
        "year": 2018,
        "citationCount": 1206,
        "openAccessPdf": {
            "url": "https://ojs.aaai.org/index.php/AAAI/article/download/4545/4423",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1809.03672, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "35066946",
                "name": "Guorui Zhou"
            },
            {
                "authorId": "51431610",
                "name": "Na Mou"
            },
            {
                "authorId": "2109755002",
                "name": "Ying Fan"
            },
            {
                "authorId": "37915354",
                "name": "Qi Pi"
            },
            {
                "authorId": "9455314",
                "name": "Weijie Bian"
            },
            {
                "authorId": "144161025",
                "name": "Chang Zhou"
            },
            {
                "authorId": "150345697",
                "name": "Xiaoqiang Zhu"
            },
            {
                "authorId": "20029557",
                "name": "Kun Gai"
            }
        ],
        "abstract": "Click-through rate (CTR) prediction, whose goal is to estimate the probability of a user clicking on the item, has become one of the core tasks in the advertising system. For CTR prediction model, it is necessary to capture the latent user interest behind the user behavior data. Besides, considering the changing of the external environment and the internal cognition, user interest evolves over time dynamically. There are several CTR prediction methods for interest modeling, while most of them regard the representation of behavior as the interest directly, and lack specially modeling for latent interest behind the concrete behavior. Moreover, little work considers the changing trend of the interest. In this paper, we propose a novel model, named Deep Interest Evolution Network (DIEN), for CTR prediction. Specifically, we design interest extractor layer to capture temporal interests from history behavior sequence. At this layer, we introduce an auxiliary loss to supervise interest extracting at each step. As user interests are diverse, especially in the e-commerce system, we propose interest evolving layer to capture interest evolving process that is relative to the target item. At interest evolving layer, attention mechanism is embedded into the sequential structure novelly, and the effects of relative interests are strengthened during interest evolution. In the experiments on both public and industrial datasets, DIEN significantly outperforms the state-of-the-art solutions. Notably, DIEN has been deployed in the display advertisement system of Taobao, and obtained 20.7% improvement on CTR."
    },
    {
        "paperId": "839c4dd710ae7a234424aeda2f1423e0ce61bd5e",
        "url": "https://www.semanticscholar.org/paper/839c4dd710ae7a234424aeda2f1423e0ce61bd5e",
        "title": "Deep Multi-View Spatial-Temporal Network for Taxi Demand Prediction",
        "venue": "AAAI Conference on Artificial Intelligence",
        "year": 2018,
        "citationCount": 1198,
        "openAccessPdf": {
            "url": "https://ojs.aaai.org/index.php/AAAI/article/download/11836/11695",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1802.08714, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "18307037",
                "name": "Huaxiu Yao"
            },
            {
                "authorId": "32996440",
                "name": "Fei Wu"
            },
            {
                "authorId": "33511250",
                "name": "Jintao Ke"
            },
            {
                "authorId": "48784944",
                "name": "Xianfeng Tang"
            },
            {
                "authorId": "9371052",
                "name": "Yitian Jia"
            },
            {
                "authorId": "2031544270",
                "name": "Siyu Lu"
            },
            {
                "authorId": "2925921",
                "name": "Pinghua Gong"
            },
            {
                "authorId": "144030870",
                "name": "Jieping Ye"
            },
            {
                "authorId": "2109640666",
                "name": "Z. Li"
            }
        ],
        "abstract": "\n \n Taxi demand prediction is an important building block to enabling intelligent transportation systems in a smart city. An accurate prediction model can help the city pre-allocate resources to meet travel demand and to reduce empty taxis on streets which waste energy and worsen the traffic congestion. With the increasing popularity of taxi requesting services such as Uber and Didi Chuxing (in China), we are able to collect large-scale taxi demand data continuously. How to utilize such big data to improve the demand prediction is an interesting and critical real-world problem. Traditional demand prediction methods mostly rely on time series forecasting techniques, which fail to model the complex non-linear spatial and temporal relations. Recent advances in deep learning have shown superior performance on traditionally challenging tasks such as image classification by learning the complex features and correlations from large-scale data. This breakthrough has inspired researchers to explore deep learning techniques on traffic prediction problems. However, existing methods on traffic prediction have only considered spatial relation (e.g., using CNN) or temporal relation (e.g., using LSTM) independently. We propose a Deep Multi-View Spatial-Temporal Network (DMVST-Net) framework to model both spatial and temporal relations. Specifically, our proposed model consists of three views: temporal view (modeling correlations between future demand values with near time points via LSTM), spatial view (modeling local spatial correlation via local CNN), and semantic view (modeling correlations among regions sharing similar temporal patterns). Experiments on large-scale real taxi demand data demonstrate effectiveness of our approach over state-of-the-art methods.\n \n"
    },
    {
        "paperId": "d81fc968196e06ccafd7ea4c008b13e1cad1be64",
        "url": "https://www.semanticscholar.org/paper/d81fc968196e06ccafd7ea4c008b13e1cad1be64",
        "title": "An End-to-End Deep Learning Architecture for Graph Classification",
        "venue": "AAAI Conference on Artificial Intelligence",
        "year": 2018,
        "citationCount": 1661,
        "openAccessPdf": {
            "url": "https://ojs.aaai.org/index.php/AAAI/article/download/11782/11641",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1609/aaai.v32i1.11782?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1609/aaai.v32i1.11782, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3098251",
                "name": "Muhan Zhang"
            },
            {
                "authorId": "7217944",
                "name": "Zhicheng Cui"
            },
            {
                "authorId": "40059761",
                "name": "Marion Neumann"
            },
            {
                "authorId": "9527255",
                "name": "Yixin Chen"
            }
        ],
        "abstract": "\n \n Neural networks are typically designed to deal with data in tensor forms. In this paper, we propose a novel neural network architecture accepting graphs of arbitrary structure. Given a dataset containing graphs in the form of (G,y) where G is a graph and y is its class, we aim to develop neural networks that read the graphs directly and learn a classification function. There are two main challenges: 1) how to extract useful features characterizing the rich information encoded in a graph for classification purpose, and 2) how to sequentially read a graph in a meaningful and consistent order. To address the first challenge, we design a localized graph convolution model and show its connection with two graph kernels. To address the second challenge, we design a novel SortPooling layer which sorts graph vertices in a consistent order so that traditional neural networks can be trained on the graphs. Experiments on benchmark graph classification datasets demonstrate that the proposed architecture achieves highly competitive performance with state-of-the-art graph kernels and other graph neural network methods. Moreover, the architecture allows end-to-end gradient-based training with original graphs, without the need to first transform graphs into vectors.\n \n"
    },
    {
        "paperId": "efeaa6e3114d6d6ae5c3041b66ac9a9ae9bf52bf",
        "url": "https://www.semanticscholar.org/paper/efeaa6e3114d6d6ae5c3041b66ac9a9ae9bf52bf",
        "title": "Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition",
        "venue": "AAAI Conference on Artificial Intelligence",
        "year": 2018,
        "citationCount": 4733,
        "openAccessPdf": {
            "url": "https://ojs.aaai.org/index.php/AAAI/article/download/12328/12187",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1801.07455, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2111431444",
                "name": "Sijie Yan"
            },
            {
                "authorId": "3331521",
                "name": "Yuanjun Xiong"
            },
            {
                "authorId": "1807606",
                "name": "Dahua Lin"
            }
        ],
        "abstract": "\n \n Dynamics of human body skeletons convey significant information for human action recognition. Conventional approaches for modeling skeletons usually rely on hand-crafted parts or traversal rules, thus resulting in limited expressive power and difficulties of generalization. In this work, we propose a novel model of dynamic skeletons called Spatial-Temporal Graph Convolutional Networks (ST-GCN), which moves beyond the limitations of previous methods by automatically learning both the spatial and temporal patterns from data. This formulation not only leads to greater expressive power but also stronger generalization capability. On two large datasets, Kinetics and NTU-RGBD, it achieves substantial improvements over mainstream methods.\n \n"
    },
    {
        "paperId": "04f4e55e14150b7c48b0287ba77c7443df76ed45",
        "url": "https://www.semanticscholar.org/paper/04f4e55e14150b7c48b0287ba77c7443df76ed45",
        "title": "PIQA: Reasoning about Physical Commonsense in Natural Language",
        "venue": "AAAI Conference on Artificial Intelligence",
        "year": 2019,
        "citationCount": 2471,
        "openAccessPdf": {
            "url": "https://ojs.aaai.org/index.php/AAAI/article/download/6239/6095",
            "status": "GOLD",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1911.11641, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3312309",
                "name": "Yonatan Bisk"
            },
            {
                "authorId": "2545335",
                "name": "Rowan Zellers"
            },
            {
                "authorId": "39227408",
                "name": "Ronan Le Bras"
            },
            {
                "authorId": "48441311",
                "name": "Jianfeng Gao"
            },
            {
                "authorId": "1699545",
                "name": "Yejin Choi"
            }
        ],
        "abstract": "To apply eyeshadow without a brush, should I use a cotton swab or a toothpick? Questions requiring this kind of physical commonsense pose a challenge to today's natural language understanding systems. While recent pretrained models (such as BERT) have made progress on question answering over more abstract domains  such as news articles and encyclopedia entries, where text is plentiful  in more physical domains, text is inherently limited due to reporting bias. Can AI systems learn to reliably answer physical commonsense questions without experiencing the physical world?In this paper, we introduce the task of physical commonsense reasoning and a corresponding benchmark dataset Physical Interaction: Question Answering or PIQA. Though humans find the dataset easy (95% accuracy), large pretrained models struggle (75%). We provide analysis about the dimensions of knowledge that existing models lack, which offers significant opportunities for future research."
    },
    {
        "paperId": "22418b7b7d5d6d18b81f232f60c22a15d1c8a38a",
        "url": "https://www.semanticscholar.org/paper/22418b7b7d5d6d18b81f232f60c22a15d1c8a38a",
        "title": "FFA-Net: Feature Fusion Attention Network for Single Image Dehazing",
        "venue": "AAAI Conference on Artificial Intelligence",
        "year": 2019,
        "citationCount": 1644,
        "openAccessPdf": {
            "url": "https://ojs.aaai.org/index.php/AAAI/article/download/6865/6719",
            "status": "GOLD",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1911.07559, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2113426177",
                "name": "Xu Qin"
            },
            {
                "authorId": "2108080348",
                "name": "Zhiling Wang"
            },
            {
                "authorId": "2876022",
                "name": "Yuanchao Bai"
            },
            {
                "authorId": "145037181",
                "name": "Xiaodong Xie"
            },
            {
                "authorId": "2642975",
                "name": "Huizhu Jia"
            }
        ],
        "abstract": "In this paper, we propose an end-to-end feature fusion at-tention network (FFA-Net) to directly restore the haze-free image. The FFA-Net architecture consists of three key components:1) A novel Feature Attention (FA) module combines Channel Attention with Pixel Attention mechanism, considering that different channel-wise features contain totally different weighted information and haze distribution is uneven on the different image pixels. FA treats different features and pixels unequally, which provides additional flexibility in dealing with different types of information, expanding the representational ability of CNNs. 2) A basic block structure consists of Local Residual Learning and Feature Attention, Local Residual Learning allowing the less important information such as thin haze region or low-frequency to be bypassed through multiple local residual connections, let main network architecture focus on more effective information. 3) An Attention-based different levels Feature Fusion (FFA) structure, the feature weights are adaptively learned from the Feature Attention (FA) module, giving more weight to important features. This structure can also retain the information of shallow layers and pass it into deep layers.The experimental results demonstrate that our proposed FFA-Net surpasses previous state-of-the-art single image dehazing methods by a very large margin both quantitatively and qualitatively, boosting the best published PSNR metric from 30.23 dB to 36.39 dB on the SOTS indoor test dataset. Code has been made available at GitHub."
    },
    {
        "paperId": "362e416c5f55b056a6c5930d55d8e3588efce9b9",
        "url": "https://www.semanticscholar.org/paper/362e416c5f55b056a6c5930d55d8e3588efce9b9",
        "title": "EvolveGCN: Evolving Graph Convolutional Networks for Dynamic Graphs",
        "venue": "AAAI Conference on Artificial Intelligence",
        "year": 2019,
        "citationCount": 1281,
        "openAccessPdf": {
            "url": "https://ojs.aaai.org/index.php/AAAI/article/download/5984/5840",
            "status": "GOLD",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1902.10191, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "143692776",
                "name": "A. Pareja"
            },
            {
                "authorId": "2972684",
                "name": "Giacomo Domeniconi"
            },
            {
                "authorId": null,
                "name": "Jie Chen"
            },
            {
                "authorId": "40411766",
                "name": "Tengfei Ma"
            },
            {
                "authorId": "2231831",
                "name": "T. Suzumura"
            },
            {
                "authorId": "3166546",
                "name": "H. Kanezashi"
            },
            {
                "authorId": "3164907",
                "name": "Tim Kaler"
            },
            {
                "authorId": "73770272",
                "name": "Charles E. Leisersen"
            }
        ],
        "abstract": "Graph representation learning resurges as a trending research subject owing to the widespread use of deep learning for Euclidean data, which inspire various creative designs of neural networks in the non-Euclidean domain, particularly graphs. With the success of these graph neural networks (GNN) in the static setting, we approach further practical scenarios where the graph dynamically evolves. Existing approaches typically resort to node embeddings and use a recurrent neural network (RNN, broadly speaking) to regulate the embeddings and learn the temporal dynamics. These methods require the knowledge of a node in the full time span (including both training and testing) and are less applicable to the frequent change of the node set. In some extreme scenarios, the node sets at different time steps may completely differ. To resolve this challenge, we propose EvolveGCN, which adapts the graph convolutional network (GCN) model along the temporal dimension without resorting to node embeddings. The proposed approach captures the dynamism of the graph sequence through using an RNN to evolve the GCN parameters. Two architectures are considered for the parameter evolution. We evaluate the proposed approach on tasks including link prediction, edge classification, and node classification. The experimental results indicate a generally higher performance of EvolveGCN compared with related approaches. The code is available at https://github.com/IBM/EvolveGCN."
    },
    {
        "paperId": "36cf500079b82e3adf4a3afe3356c1c03426bdcd",
        "url": "https://www.semanticscholar.org/paper/36cf500079b82e3adf4a3afe3356c1c03426bdcd",
        "title": "Attention Based Spatial-Temporal Graph Convolutional Networks for Traffic Flow Forecasting",
        "venue": "AAAI Conference on Artificial Intelligence",
        "year": 2019,
        "citationCount": 2798,
        "openAccessPdf": {
            "url": "https://aaai.org/ojs/index.php/AAAI/article/download/3881/3759",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1609/AAAI.V33I01.3301922?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1609/AAAI.V33I01.3301922, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1910814",
                "name": "S. Guo"
            },
            {
                "authorId": "2624174",
                "name": "Youfang Lin"
            },
            {
                "authorId": "2065659433",
                "name": "Ning Feng"
            },
            {
                "authorId": "2116333016",
                "name": "Chao Song"
            },
            {
                "authorId": "39699556",
                "name": "Huaiyu Wan"
            }
        ],
        "abstract": "Forecasting the traffic flows is a critical issue for researchers and practitioners in the field of transportation. However, it is very challenging since the traffic flows usually show high nonlinearities and complex patterns. Most existing traffic flow prediction methods, lacking abilities of modeling the dynamic spatial-temporal correlations of traffic data, thus cannot yield satisfactory prediction results. In this paper, we propose a novel attention based spatial-temporal graph convolutional network (ASTGCN) model to solve traffic flow forecasting problem. ASTGCN mainly consists of three independent components to respectively model three temporal properties of traffic flows, i.e., recent, daily-periodic and weekly-periodic dependencies. More specifically, each component contains two major parts: 1) the spatial-temporal attention mechanism to effectively capture the dynamic spatialtemporal correlations in traffic data; 2) the spatial-temporal convolution which simultaneously employs graph convolutions to capture the spatial patterns and common standard convolutions to describe the temporal features. The output of the three components are weighted fused to generate the final prediction results. Experiments on two real-world datasets from the Caltrans Performance Measurement System (PeMS) demonstrate that the proposed ASTGCN model outperforms the state-of-the-art baselines."
    },
    {
        "paperId": "63a243afcb133569a962c41e9db956c076c5c4f3",
        "url": "https://www.semanticscholar.org/paper/63a243afcb133569a962c41e9db956c076c5c4f3",
        "title": "Distance-IoU Loss: Faster and Better Learning for Bounding Box Regression",
        "venue": "AAAI Conference on Artificial Intelligence",
        "year": 2019,
        "citationCount": 4449,
        "openAccessPdf": {
            "url": "https://ojs.aaai.org/index.php/AAAI/article/download/6999/6853",
            "status": "GOLD",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1911.08287, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2115549022",
                "name": "Zhaohui Zheng"
            },
            {
                "authorId": "2152209342",
                "name": "Ping Wang"
            },
            {
                "authorId": "2157221967",
                "name": "Wei Liu"
            },
            {
                "authorId": "2175139382",
                "name": "Jinze Li"
            },
            {
                "authorId": "1420127255",
                "name": "Rongguang Ye"
            },
            {
                "authorId": "2404143",
                "name": "Dongwei Ren"
            }
        ],
        "abstract": "Bounding box regression is the crucial step in object detection. In existing methods, while n-norm loss is widely adopted for bounding box regression, it is not tailored to the evaluation metric, i.e., Intersection over Union (IoU). Recently, IoU loss and generalized IoU (GIoU) loss have been proposed to benefit the IoU metric, but still suffer from the problems of slow convergence and inaccurate regression. In this paper, we propose a Distance-IoU (DIoU) loss by incorporating the normalized distance between the predicted box and the target box, which converges much faster in training than IoU and GIoU losses. Furthermore, this paper summarizes three geometric factors in bounding box regression, i.e., overlap area, central point distance and aspect ratio, based on which a Complete IoU (CIoU) loss is proposed, thereby leading to faster convergence and better performance. By incorporating DIoU and CIoU losses into state-of-the-art object detection algorithms, e.g., YOLO v3, SSD and Faster R-CNN, we achieve notable performance gains in terms of not only IoU metric but also GIoU metric. Moreover, DIoU can be easily adopted into non-maximum suppression (NMS) to act as the criterion, further boosting performance improvement. The source code and trained models are available at https://github.com/Zzh-tju/DIoU."
    },
    {
        "paperId": "6648b4db5f12c30941ea78c695e77aded19672bb",
        "url": "https://www.semanticscholar.org/paper/6648b4db5f12c30941ea78c695e77aded19672bb",
        "title": "Unified Vision-Language Pre-Training for Image Captioning and VQA",
        "venue": "AAAI Conference on Artificial Intelligence",
        "year": 2019,
        "citationCount": 1005,
        "openAccessPdf": {
            "url": "https://ojs.aaai.org/index.php/AAAI/article/download/7005/6859",
            "status": "GOLD",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1909.11059, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2116644664",
                "name": "Luowei Zhou"
            },
            {
                "authorId": "2542427",
                "name": "Hamid Palangi"
            },
            {
                "authorId": "39089563",
                "name": "Lei Zhang"
            },
            {
                "authorId": "35431603",
                "name": "Houdong Hu"
            },
            {
                "authorId": "3587688",
                "name": "Jason J. Corso"
            },
            {
                "authorId": "1800422",
                "name": "Jianfeng Gao"
            }
        ],
        "abstract": "This paper presents a unified Vision-Language Pre-training (VLP) model. The model is unified in that (1) it can be fine-tuned for either vision-language generation (e.g., image captioning) or understanding (e.g., visual question answering) tasks, and (2) it uses a shared multi-layer transformer network for both encoding and decoding, which differs from many existing methods where the encoder and decoder are implemented using separate models. The unified VLP model is pre-trained on a large amount of image-text pairs using the unsupervised learning objectives of two tasks: bidirectional and sequence-to-sequence (seq2seq) masked vision-language prediction. The two tasks differ solely in what context the prediction conditions on. This is controlled by utilizing specific self-attention masks for the shared transformer network. To the best of our knowledge, VLP is the first reported model that achieves state-of-the-art results on both vision-language generation and understanding tasks, as disparate as image captioning and visual question answering, across three challenging benchmark datasets: COCO Captions, Flickr30k Captions, and VQA 2.0. The code and the pre-trained models are available at https://github.com/LuoweiZhou/VLP."
    },
    {
        "paperId": "89a816719613e220a64ab2590c938c23bbfe187e",
        "url": "https://www.semanticscholar.org/paper/89a816719613e220a64ab2590c938c23bbfe187e",
        "title": "CheXpert: A Large Chest Radiograph Dataset with Uncertainty Labels and Expert Comparison",
        "venue": "AAAI Conference on Artificial Intelligence",
        "year": 2019,
        "citationCount": 3049,
        "openAccessPdf": {
            "url": "https://aaai.org/ojs/index.php/AAAI/article/download/3834/3712",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1901.07031, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "46559852",
                "name": "J. Irvin"
            },
            {
                "authorId": "2706258",
                "name": "Pranav Rajpurkar"
            },
            {
                "authorId": "2065881358",
                "name": "M. Ko"
            },
            {
                "authorId": "2119041079",
                "name": "Yifan Yu"
            },
            {
                "authorId": "1415086867",
                "name": "Silviana Ciurea-Ilcus"
            },
            {
                "authorId": "1517032702",
                "name": "Chris Chute"
            },
            {
                "authorId": "66977188",
                "name": "H. Marklund"
            },
            {
                "authorId": "1411409755",
                "name": "Behzad Haghgoo"
            },
            {
                "authorId": "2054772900",
                "name": "Robyn L. Ball"
            },
            {
                "authorId": "3474704",
                "name": "K. Shpanskaya"
            },
            {
                "authorId": "52088792",
                "name": "J. Seekins"
            },
            {
                "authorId": "8372530",
                "name": "D. Mong"
            },
            {
                "authorId": "143970433",
                "name": "S. Halabi"
            },
            {
                "authorId": "3240442",
                "name": "J. Sandberg"
            },
            {
                "authorId": "2109320578",
                "name": "Ricky Jones"
            },
            {
                "authorId": "145758092",
                "name": "D. Larson"
            },
            {
                "authorId": "2356307",
                "name": "C. Langlotz"
            },
            {
                "authorId": "2082429577",
                "name": "B. Patel"
            },
            {
                "authorId": "4204731",
                "name": "M. Lungren"
            },
            {
                "authorId": "34699434",
                "name": "A. Ng"
            }
        ],
        "abstract": "Large, labeled datasets have driven deep learning methods to achieve expert-level performance on a variety of medical imaging tasks. We present CheXpert, a large dataset that contains 224,316 chest radiographs of 65,240 patients. We design a labeler to automatically detect the presence of 14 observations in radiology reports, capturing uncertainties inherent in radiograph interpretation. We investigate different approaches to using the uncertainty labels for training convolutional neural networks that output the probability of these observations given the available frontal and lateral radiographs. On a validation set of 200 chest radiographic studies which were manually annotated by 3 board-certified radiologists, we find that different uncertainty approaches are useful for different pathologies. We then evaluate our best model on a test set composed of 500 chest radiographic studies annotated by a consensus of 5 board-certified radiologists, and compare the performance of our model to that of 3 additional radiologists in the detection of 5 selected pathologies. On Cardiomegaly, Edema, and Pleural Effusion, the model ROC and PR curves lie above all 3 radiologist operating points. We release the dataset to the public as a standard benchmark to evaluate performance of chest radiograph interpretation models."
    },
    {
        "paperId": "94194703e83b5447f519fd8bcbb903916e05aaf9",
        "url": "https://www.semanticscholar.org/paper/94194703e83b5447f519fd8bcbb903916e05aaf9",
        "title": "Measuring and Relieving the Over-smoothing Problem for Graph Neural Networks from the Topological View",
        "venue": "AAAI Conference on Artificial Intelligence",
        "year": 2019,
        "citationCount": 1260,
        "openAccessPdf": {
            "url": "https://ojs.aaai.org/index.php/AAAI/article/download/5747/5603",
            "status": "GOLD",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1909.03211, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "49025779",
                "name": "Deli Chen"
            },
            {
                "authorId": "2427350",
                "name": "Yankai Lin"
            },
            {
                "authorId": "1470680578",
                "name": "Wei Li"
            },
            {
                "authorId": "144326610",
                "name": "Peng Li"
            },
            {
                "authorId": "2108485135",
                "name": "Jie Zhou"
            },
            {
                "authorId": "11774802",
                "name": "Xu Sun"
            }
        ],
        "abstract": "Graph Neural Networks (GNNs) have achieved promising performance on a wide range of graph-based tasks. Despite their success, one severe limitation of GNNs is the over-smoothing issue (indistinguishable representations of nodes in different classes). In this work, we present a systematic and quantitative study on the over-smoothing issue of GNNs. First, we introduce two quantitative metrics, MAD and MADGap, to measure the smoothness and over-smoothness of the graph nodes representations, respectively. Then, we verify that smoothing is the nature of GNNs and the critical factor leading to over-smoothness is the low information-to-noise ratio of the message received by the nodes, which is partially determined by the graph topology. Finally, we propose two methods to alleviate the over-smoothing issue from the topological view: (1) MADReg which adds a MADGap-based regularizer to the training objective; (2) AdaEdge which optimizes the graph topology based on the model predictions. Extensive experiments on 7 widely-used graph datasets with 10 typical GNN models show that the two proposed methods are effective for relieving the over-smoothing issue, thus improving the performance of various GNN models."
    },
    {
        "paperId": "9fe69cf5c104b2205cdb7908df8cdb389256b4b5",
        "url": "https://www.semanticscholar.org/paper/9fe69cf5c104b2205cdb7908df8cdb389256b4b5",
        "title": "TabNet: Attentive Interpretable Tabular Learning",
        "venue": "AAAI Conference on Artificial Intelligence",
        "year": 2019,
        "citationCount": 1771,
        "openAccessPdf": {
            "url": "https://ojs.aaai.org/index.php/AAAI/article/download/16826/16633",
            "status": "GOLD",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1908.07442, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2676352",
                "name": "Sercan . Arik"
            },
            {
                "authorId": "1945962",
                "name": "Tomas Pfister"
            }
        ],
        "abstract": "We propose a novel high-performance and interpretable canonical deep tabular data learning architecture, TabNet. TabNet uses sequential attention to choose which features to reason from at each decision step, enabling interpretability and more efficient learning as the learning capacity is used for the most salient features. We demonstrate that TabNet outperforms other variants on a wide range of non-performance-saturated tabular datasets and yields interpretable feature attributions plus insights into its global behavior. Finally, we demonstrate self-supervised learning for tabular data, significantly improving performance when unlabeled data is abundant."
    },
    {
        "paperId": "ae04f3d011511ad8ed7ffdf9fcfb7f11e6899ca2",
        "url": "https://www.semanticscholar.org/paper/ae04f3d011511ad8ed7ffdf9fcfb7f11e6899ca2",
        "title": "Is BERT Really Robust? A Strong Baseline for Natural Language Attack on Text Classification and Entailment",
        "venue": "AAAI Conference on Artificial Intelligence",
        "year": 2019,
        "citationCount": 1246,
        "openAccessPdf": {
            "url": "https://ojs.aaai.org/index.php/AAAI/article/download/6311/6167",
            "status": "GOLD",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1907.11932, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2068347799",
                "name": "Di Jin"
            },
            {
                "authorId": "8752221",
                "name": "Zhijing Jin"
            },
            {
                "authorId": "10638646",
                "name": "Joey Tianyi Zhou"
            },
            {
                "authorId": "1679873",
                "name": "Peter Szolovits"
            }
        ],
        "abstract": "Machine learning algorithms are often vulnerable to adversarial examples that have imperceptible alterations from the original counterparts but can fool the state-of-the-art models. It is helpful to evaluate or even improve the robustness of these models by exposing the maliciously crafted adversarial examples. In this paper, we present TextFooler, a simple but strong baseline to generate adversarial text. By applying it to two fundamental natural language tasks, text classification and textual entailment, we successfully attacked three target models, including the powerful pre-trained BERT, and the widely used convolutional and recurrent neural networks. We demonstrate three advantages of this framework: (1) effectiveit outperforms previous attacks by success rate and perturbation rate, (2) utility-preservingit preserves semantic content, grammaticality, and correct types classified by humans, and (3) efficientit generates adversarial text with computational complexity linear to the text length.1"
    },
    {
        "paperId": "bc6dfc6bda2d929fec91042dce1831fd07999b39",
        "url": "https://www.semanticscholar.org/paper/bc6dfc6bda2d929fec91042dce1831fd07999b39",
        "title": "Improved Knowledge Distillation via Teacher Assistant",
        "venue": "AAAI Conference on Artificial Intelligence",
        "year": 2019,
        "citationCount": 1261,
        "openAccessPdf": {
            "url": "https://ojs.aaai.org/index.php/AAAI/article/download/5963/5819",
            "status": "GOLD",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1902.03393, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "145156788",
                "name": "Seyed Iman Mirzadeh"
            },
            {
                "authorId": "1682124",
                "name": "Mehrdad Farajtabar"
            },
            {
                "authorId": "2112839418",
                "name": "Ang Li"
            },
            {
                "authorId": "153898744",
                "name": "Nir Levine"
            },
            {
                "authorId": "2063980545",
                "name": "Akihiro Matsukawa"
            },
            {
                "authorId": "144600887",
                "name": "H. Ghasemzadeh"
            }
        ],
        "abstract": "Despite the fact that deep neural networks are powerful models and achieve appealing results on many tasks, they are too large to be deployed on edge devices like smartphones or embedded sensor nodes. There have been efforts to compress these networks, and a popular method is knowledge distillation, where a large (teacher) pre-trained network is used to train a smaller (student) network. However, in this paper, we show that the student network performance degrades when the gap between student and teacher is large. Given a fixed student network, one cannot employ an arbitrarily large teacher, or in other words, a teacher can effectively transfer its knowledge to students up to a certain size, not smaller. To alleviate this shortcoming, we introduce multi-step knowledge distillation, which employs an intermediate-sized network (teacher assistant) to bridge the gap between the student and the teacher. Moreover, we study the effect of teacher assistant size and extend the framework to multi-step distillation. Theoretical analysis and extensive experiments on CIFAR-10,100 and ImageNet datasets and on CNN and ResNet architectures substantiate the effectiveness of our proposed approach."
    },
    {
        "paperId": "edada2363969e3929366df06aad8a8e9c73ba32f",
        "url": "https://www.semanticscholar.org/paper/edada2363969e3929366df06aad8a8e9c73ba32f",
        "title": "R3Det: Refined Single-Stage Detector with Feature Refinement for Rotating Object",
        "venue": "AAAI Conference on Artificial Intelligence",
        "year": 2019,
        "citationCount": 1034,
        "openAccessPdf": {
            "url": "https://ojs.aaai.org/index.php/AAAI/article/download/16426/16233",
            "status": "GOLD",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1908.05612, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "143989318",
                "name": "Xue Yang"
            },
            {
                "authorId": "2112214992",
                "name": "Qingqing Liu"
            },
            {
                "authorId": "3063894",
                "name": "Junchi Yan"
            },
            {
                "authorId": "1807512632",
                "name": "Ang Li"
            }
        ],
        "abstract": "Rotation detection is a challenging task due to the difficulties of locating the multi-angle objects and separating them effectively from the background. Though considerable progress has been made, for practical settings, there still exist challenges for rotating objects with large aspect ratio, dense distribution and category extremely imbalance. In this paper, we propose an end-to-end refined single-stage rotation detector for fast and accurate object detection by using a progressive regression approach from coarse to fine granularity. Considering the shortcoming of feature misalignment in existing refined single-stage detector, we design a feature refinement module to improve detection performance by getting more accurate features. The key idea of feature refinement module is to re-encode the position information of the current refined bounding box to the corresponding feature points through pixel-wise feature interpolation to realize feature reconstruction and alignment. For more accurate rotation estimation, an approximate SkewIoU loss is proposed to solve the problem that the calculation of SkewIoU is not derivable. Experiments on three popular remote sensing public datasets DOTA, HRSC2016, UCAS-AOD as well as one scene text dataset ICDAR2015 show the effectiveness of our approach. The source code is available at https://github.com/Thinklab-SJTU/R3Det_Tensorflow and is also integrated in our open source rotation detection benchmark: https://github.com/yangxue0827/RotationDetection."
    },
    {
        "paperId": "fc41d75288a81dd7e30087480f51821fc6572d95",
        "url": "https://www.semanticscholar.org/paper/fc41d75288a81dd7e30087480f51821fc6572d95",
        "title": "GMAN: A Graph Multi-Attention Network for Traffic Prediction",
        "venue": "AAAI Conference on Artificial Intelligence",
        "year": 2019,
        "citationCount": 1685,
        "openAccessPdf": {
            "url": "https://ojs.aaai.org/index.php/AAAI/article/download/5477/5333",
            "status": "GOLD",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1911.08415, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "24849962",
                "name": "Chuanpan Zheng"
            },
            {
                "authorId": "34964296",
                "name": "Xiaoliang Fan"
            },
            {
                "authorId": "115877724",
                "name": "Cheng Wang"
            },
            {
                "authorId": "39899794",
                "name": "Jianzhong Qi"
            }
        ],
        "abstract": "Long-term traffic prediction is highly challenging due to the complexity of traffic systems and the constantly changing nature of many impacting factors. In this paper, we focus on the spatio-temporal factors, and propose a graph multi-attention network (GMAN) to predict traffic conditions for time steps ahead at different locations on a road network graph. GMAN adapts an encoder-decoder architecture, where both the encoder and the decoder consist of multiple spatio-temporal attention blocks to model the impact of the spatio-temporal factors on traffic conditions. The encoder encodes the input traffic features and the decoder predicts the output sequence. Between the encoder and the decoder, a transform attention layer is applied to convert the encoded traffic features to generate the sequence representations of future time steps as the input of the decoder. The transform attention mechanism models the direct relationships between historical and future time steps that helps to alleviate the error propagation problem among prediction time steps. Experimental results on two real-world traffic prediction tasks (i.e., traffic volume prediction and traffic speed prediction) demonstrate the superiority of GMAN. In particular, in the 1 hour ahead prediction, GMAN outperforms state-of-the-art methods by up to 4% improvement in MAE measure. The source code is available at https://github.com/zhengchuanpan/GMAN."
    },
    {
        "paperId": "5b9d8bcc46b766b47389c912a8e026f81b91b0d8",
        "url": "https://www.semanticscholar.org/paper/5b9d8bcc46b766b47389c912a8e026f81b91b0d8",
        "title": "Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting",
        "venue": "AAAI Conference on Artificial Intelligence",
        "year": 2020,
        "citationCount": 6027,
        "openAccessPdf": {
            "url": "https://ojs.aaai.org/index.php/AAAI/article/download/17325/17132",
            "status": "GOLD",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2012.07436, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2395143",
                "name": "Haoyi Zhou"
            },
            {
                "authorId": "2437353",
                "name": "Shanghang Zhang"
            },
            {
                "authorId": "1752536568",
                "name": "Jieqi Peng"
            },
            {
                "authorId": "2108433304",
                "name": "Shuai Zhang"
            },
            {
                "authorId": "1492113939",
                "name": "Jianxin Li"
            },
            {
                "authorId": "144467554",
                "name": "Hui Xiong"
            },
            {
                "authorId": "2108278607",
                "name": "Wan Zhang"
            }
        ],
        "abstract": "Many real-world applications require the prediction of long sequence time-series, such as electricity consumption planning. Long sequence time-series forecasting (LSTF) demands a high prediction capacity of the model, which is the ability to capture precise long-range dependency coupling between output and input efficiently. Recent studies have shown the potential of Transformer to increase the prediction capacity. However, there are several severe issues with Transformer that prevent it from being directly applicable to LSTF, including quadratic time complexity, high memory usage, and inherent limitation of the encoder-decoder architecture. To address these issues, we design an efficient transformer-based model for LSTF, named Informer, with three distinctive characteristics: (i) a ProbSparse self-attention mechanism, which achieves O(L log L) in time complexity and memory usage, and has comparable performance on sequences' dependency alignment. (ii) the self-attention distilling highlights dominating attention by halving cascading layer input, and efficiently handles extreme long input sequences. (iii) the generative style decoder, while conceptually simple, predicts the long time-series sequences at one forward operation rather than a step-by-step way, which drastically improves the inference speed of long-sequence predictions. Extensive experiments on four large-scale datasets demonstrate that Informer significantly outperforms existing methods and provides a new solution to the LSTF problem."
    },
    {
        "paperId": "efc669f75ad0ed2033e7c499ae2d6c8dab7489ce",
        "url": "https://www.semanticscholar.org/paper/efc669f75ad0ed2033e7c499ae2d6c8dab7489ce",
        "title": "Voxel R-CNN: Towards High Performance Voxel-based 3D Object Detection",
        "venue": "AAAI Conference on Artificial Intelligence",
        "year": 2020,
        "citationCount": 1036,
        "openAccessPdf": {
            "url": "https://ojs.aaai.org/index.php/AAAI/article/download/16207/16014",
            "status": "GOLD",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2012.15712, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2087218319",
                "name": "Jiajun Deng"
            },
            {
                "authorId": "2072683588",
                "name": "Shaoshuai Shi"
            },
            {
                "authorId": "51249725",
                "name": "Pei-Cian Li"
            },
            {
                "authorId": "38272296",
                "name": "Wen-gang Zhou"
            },
            {
                "authorId": "2124819888",
                "name": "Yanyong Zhang"
            },
            {
                "authorId": "2108508109",
                "name": "Houqiang Li"
            }
        ],
        "abstract": "Recent advances on 3D object detection heavily rely on how the 3D data are represented, i.e., voxel-based or point-based representation. Many existing high performance 3D detectors are point-based because this structure can better retain precise point positions. Nevertheless, point-level features lead to high computation overheads due to unordered storage. In contrast, the voxel-based structure is better suited for feature extraction but often yields lower accuracy because the input data are divided into grids. In this paper, we take a slightly different viewpoint --- we find that precise positioning of raw points is not essential for high performance 3D object detection and that the coarse voxel granularity can also offer sufficient detection accuracy. Bearing this view in mind, we devise a simple but effective voxel-based framework, named Voxel R-CNN. By taking full advantage of voxel features in a two-stage approach, our method achieves comparable detection accuracy with state-of-the-art point-based models, but at a fraction of the computation cost. Voxel R-CNN consists of a 3D backbone network, a 2D bird-eye-view (BEV) Region Proposal Network, and a detect head. A voxel RoI pooling is devised to extract RoI features directly from voxel features for further refinement. Extensive experiments are conducted on the widely used KITTI Dataset and the more recent Waymo Open Dataset. Our results show that compared to existing voxel-based methods, Voxel R-CNN delivers a higher detection accuracy while maintaining a real-time frame processing rate, i.e., at a speed of 25 FPS on an NVIDIA RTX 2080 Ti GPU. The code is available at https://github.com/djiajunustc/Voxel-R-CNN."
    },
    {
        "paperId": "f420663fe8c69ed5ea5236201a1f4c734cd145a7",
        "url": "https://www.semanticscholar.org/paper/f420663fe8c69ed5ea5236201a1f4c734cd145a7",
        "title": "Spatial-Temporal Synchronous Graph Convolutional Networks: A New Framework for Spatial-Temporal Network Data Forecasting",
        "venue": "AAAI Conference on Artificial Intelligence",
        "year": 2020,
        "citationCount": 1503,
        "openAccessPdf": {
            "url": "https://ojs.aaai.org/index.php/AAAI/article/download/5438/5294",
            "status": "GOLD",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1609/AAAI.V34I01.5438?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1609/AAAI.V34I01.5438, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2116333016",
                "name": "Chao Song"
            },
            {
                "authorId": "2624174",
                "name": "Youfang Lin"
            },
            {
                "authorId": "1910814",
                "name": "S. Guo"
            },
            {
                "authorId": "39699556",
                "name": "Huaiyu Wan"
            }
        ],
        "abstract": "Spatial-temporal network data forecasting is of great importance in a huge amount of applications for traffic management and urban planning. However, the underlying complex spatial-temporal correlations and heterogeneities make this problem challenging. Existing methods usually use separate components to capture spatial and temporal correlations and ignore the heterogeneities in spatial-temporal data. In this paper, we propose a novel model, named Spatial-Temporal Synchronous Graph Convolutional Networks (STSGCN), for spatial-temporal network data forecasting. The model is able to effectively capture the complex localized spatial-temporal correlations through an elaborately designed spatial-temporal synchronous modeling mechanism. Meanwhile, multiple modules for different time periods are designed in the model to effectively capture the heterogeneities in localized spatial-temporal graphs. Extensive experiments are conducted on four real-world datasets, which demonstrates that our method achieves the state-of-the-art performance and consistently outperforms other baselines."
    },
    {
        "paperId": "21d2742e38f7167354dafcf7f565d3894b31d008",
        "url": "https://www.semanticscholar.org/paper/21d2742e38f7167354dafcf7f565d3894b31d008",
        "title": "Graph Neural Network-Based Anomaly Detection in Multivariate Time Series",
        "venue": "AAAI Conference on Artificial Intelligence",
        "year": 2021,
        "citationCount": 1141,
        "openAccessPdf": {
            "url": "https://ojs.aaai.org/index.php/AAAI/article/download/16523/16330",
            "status": "GOLD",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2106.06947, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2063950812",
                "name": "Ailin Deng"
            },
            {
                "authorId": "2319222667",
                "name": "Bryan Hooi"
            }
        ],
        "abstract": "Given high-dimensional time series data (e.g., sensor data), how can we detect anomalous events, such as system faults and attacks? More challengingly, how can we do this in a way that captures complex inter-sensor relationships, and detects and explains anomalies which deviate from these relationships? Recently, deep learning approaches have enabled improvements in anomaly detection in high-dimensional datasets; however, existing methods do not explicitly learn the structure of existing relationships between variables, or use them to predict the expected behavior of time series. Our approach combines a structure learning approach with graph neural networks, additionally using attention weights to provide explainability for the detected anomalies. Experiments on two real-world sensor datasets with ground truth anomalies show that our method detects anomalies more accurately than baseline approaches, accurately captures correlations between sensors, and allows users to deduce the root cause of a detected anomaly."
    },
    {
        "paperId": "5f404dbba07619cc7f28d75d03f124a52290046e",
        "url": "https://www.semanticscholar.org/paper/5f404dbba07619cc7f28d75d03f124a52290046e",
        "title": "Are Transformers Effective for Time Series Forecasting?",
        "venue": "AAAI Conference on Artificial Intelligence",
        "year": 2022,
        "citationCount": 2843,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2205.13504",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2205.13504, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "51286000",
                "name": "Ailing Zeng"
            },
            {
                "authorId": "122004634",
                "name": "Mu-Hwa Chen"
            },
            {
                "authorId": "47058944",
                "name": "L. Zhang"
            },
            {
                "authorId": "2149106517",
                "name": "Qiang Xu"
            }
        ],
        "abstract": "Recently, there has been a surge of Transformer-based solutions for the long-term time series forecasting (LTSF) task. Despite the growing performance over the past few years, we question the validity of this line of research in this work. Specifically, Transformers is arguably the most successful solution to extract the semantic correlations among the elements in a long sequence. However, in time series modeling, we are to extract the temporal relations in an ordered set of continuous points. While employing positional encoding and using tokens to embed sub-series in Transformers facilitate preserving some ordering information, the nature of the permutation-invariant self-attention mechanism inevitably results in temporal information loss. \nTo validate our claim, we introduce a set of embarrassingly simple one-layer linear models named LTSF-Linear for comparison. Experimental results on nine real-life datasets show that LTSF-Linear surprisingly outperforms existing sophisticated Transformer-based LTSF models in all cases, and often by a large margin. Moreover, we conduct comprehensive empirical studies to explore the impacts of various design elements of LTSF models on their temporal relation extraction capability. We hope this surprising finding opens up new research directions for the LTSF task. We also advocate revisiting the validity of Transformer-based solutions for other time series analysis tasks (e.g., anomaly detection) in the future."
    },
    {
        "paperId": "58842cdca3ea68f7b9e638b288fc247a6f26dafc",
        "url": "https://www.semanticscholar.org/paper/58842cdca3ea68f7b9e638b288fc247a6f26dafc",
        "title": "T2I-Adapter: Learning Adapters to Dig out More Controllable Ability for Text-to-Image Diffusion Models",
        "venue": "AAAI Conference on Artificial Intelligence",
        "year": 2023,
        "citationCount": 1387,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2302.08453",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2302.08453, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1993674386",
                "name": "Chong Mou"
            },
            {
                "authorId": "47119707",
                "name": "Xintao Wang"
            },
            {
                "authorId": "1604613100",
                "name": "Liangbin Xie"
            },
            {
                "authorId": "47539632",
                "name": "Jing Zhang"
            },
            {
                "authorId": "2539841",
                "name": "Zhongang Qi"
            },
            {
                "authorId": "1387190008",
                "name": "Ying Shan"
            },
            {
                "authorId": "3284850",
                "name": "Xiaohu Qie"
            }
        ],
        "abstract": "The incredible generative ability of large-scale text-to-image (T2I) models has demonstrated strong power of learning complex structures and meaningful semantics. However, relying solely on text prompts cannot fully take advantage of the knowledge learned by the model, especially when flexible and accurate controlling (e.g., structure and color) is needed. In this paper, we aim to ``dig out\" the capabilities that T2I models have implicitly learned, and then explicitly use them to control the generation more granularly. Specifically, we propose to learn low-cost T2I-Adapters to align internal knowledge in T2I models with external control signals, while freezing the original large T2I models. In this way, we can train various adapters according to different conditions, achieving rich control and editing effects in the color and structure of the generation results. Further, the proposed T2I-Adapters have attractive properties of practical value, such as composability and generalization ability. Extensive experiments demonstrate that our T2I-Adapter has promising generation quality and a wide range of applications. Our code is available at https://github.com/TencentARC/T2I-Adapter."
    },
    {
        "paperId": "aade40af0d85b0b4fe15c97f6222d5c2e4d6d9b3",
        "url": "https://www.semanticscholar.org/paper/aade40af0d85b0b4fe15c97f6222d5c2e4d6d9b3",
        "title": "Graph of Thoughts: Solving Elaborate Problems with Large Language Models",
        "venue": "AAAI Conference on Artificial Intelligence",
        "year": 2023,
        "citationCount": 1024,
        "openAccessPdf": {
            "url": "https://ojs.aaai.org/index.php/AAAI/article/download/29720/31236",
            "status": "GOLD",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2308.09687, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2919642",
                "name": "Maciej Besta"
            },
            {
                "authorId": "2007657725",
                "name": "Nils Blach"
            },
            {
                "authorId": "100980110",
                "name": "Ale Kubek"
            },
            {
                "authorId": "3308719",
                "name": "Robert Gerstenberger"
            },
            {
                "authorId": "10790558",
                "name": "Lukas Gianinazzi"
            },
            {
                "authorId": "2232599373",
                "name": "Joanna Gajda"
            },
            {
                "authorId": "2232599003",
                "name": "Tomasz Lehmann"
            },
            {
                "authorId": "18356904",
                "name": "Michal Podstawski"
            },
            {
                "authorId": "50544096",
                "name": "H. Niewiadomski"
            },
            {
                "authorId": "10836474",
                "name": "P. Nyczyk"
            },
            {
                "authorId": "2174028998",
                "name": "Torsten Hoefler"
            }
        ],
        "abstract": "We introduce Graph of Thoughts (GoT): a framework that\nadvances prompting capabilities in large language models\n(LLMs) beyond those offered by paradigms such as \nChain-of-Thought or Tree of Thoughts (ToT). The key idea and \nprimary advantage of GoT is the ability to model the information \ngenerated by an LLM as an arbitrary graph, where units of \ninformation (\"LLM thoughts\") are vertices, and edges correspond\nto dependencies between these vertices. This approach enables \ncombining arbitrary LLM thoughts into synergistic outcomes, \ndistilling the essence of whole networks of thoughts,\nor enhancing thoughts using feedback loops. We illustrate\nthat GoT offers advantages over state of the art on different\ntasks, for example increasing the quality of sorting by 62%\nover ToT, while simultaneously reducing costs by >31%.\nWe ensure that GoT is extensible with new thought \ntransformations and thus can be used to spearhead new prompting\nschemes. This work brings the LLM reasoning closer to human \nthinking or brain mechanisms such as recurrence, both\nof which form complex networks"
    },
    {
        "paperId": "4f847b4ddc105d73bc78f3e7220e6c1f71a7dfb6",
        "url": "https://www.semanticscholar.org/paper/4f847b4ddc105d73bc78f3e7220e6c1f71a7dfb6",
        "title": "Saliency Based on Information Maximization",
        "venue": "Neural Information Processing Systems",
        "year": 2005,
        "citationCount": 1266,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "2866780",
                "name": "Neil D. B. Bruce"
            },
            {
                "authorId": "1727853",
                "name": "John K. Tsotsos"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "78947497cbbffc691aac3f590d972130259af9ce",
        "url": "https://www.semanticscholar.org/paper/78947497cbbffc691aac3f590d972130259af9ce",
        "title": "Distance Metric Learning for Large Margin Nearest Neighbor Classification",
        "venue": "Neural Information Processing Systems",
        "year": 2005,
        "citationCount": 5906,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.5555/1577069.1577078?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.5555/1577069.1577078, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "7446832",
                "name": "Kilian Q. Weinberger"
            },
            {
                "authorId": "1796044",
                "name": "L. Saul"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "b6a2d80854651a56e0f023543131744f14f20ab4",
        "url": "https://www.semanticscholar.org/paper/b6a2d80854651a56e0f023543131744f14f20ab4",
        "title": "Sparse Gaussian Processes using Pseudo-inputs",
        "venue": "Neural Information Processing Systems",
        "year": 2005,
        "citationCount": 1971,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "2081889",
                "name": "Edward Snelson"
            },
            {
                "authorId": "1744700",
                "name": "Zoubin Ghahramani"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "cddd92203c8deb022a29b512b11050da531c5f3b",
        "url": "https://www.semanticscholar.org/paper/cddd92203c8deb022a29b512b11050da531c5f3b",
        "title": "Learning Depth from Single Monocular Images",
        "venue": "Neural Information Processing Systems",
        "year": 2005,
        "citationCount": 1219,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "1681995",
                "name": "Ashutosh Saxena"
            },
            {
                "authorId": "2112610462",
                "name": "Sung H. Chung"
            },
            {
                "authorId": "34699434",
                "name": "A. Ng"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "f7060dcaeda81c52db2267838c4c80b470ca780e",
        "url": "https://www.semanticscholar.org/paper/f7060dcaeda81c52db2267838c4c80b470ca780e",
        "title": "Laplacian Score for Feature Selection",
        "venue": "Neural Information Processing Systems",
        "year": 2005,
        "citationCount": 2223,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "3945955",
                "name": "Xiaofei He"
            },
            {
                "authorId": "1724421",
                "name": "Deng Cai"
            },
            {
                "authorId": "1770745",
                "name": "P. Niyogi"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "355d44f53428b1ac4fb2ab468d593c720640e5bd",
        "url": "https://www.semanticscholar.org/paper/355d44f53428b1ac4fb2ab468d593c720640e5bd",
        "title": "Greedy Layer-Wise Training of Deep Networks",
        "venue": "Neural Information Processing Systems",
        "year": 2006,
        "citationCount": 5418,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.7551/mitpress/7503.003.0024?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.7551/mitpress/7503.003.0024, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1751762",
                "name": "Yoshua Bengio"
            },
            {
                "authorId": "3087941",
                "name": "Pascal Lamblin"
            },
            {
                "authorId": "32384143",
                "name": "D. Popovici"
            },
            {
                "authorId": "1777528",
                "name": "H. Larochelle"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "38aff6df1accc456f6cda7d16d4b9ecf418ef21e",
        "url": "https://www.semanticscholar.org/paper/38aff6df1accc456f6cda7d16d4b9ecf418ef21e",
        "title": "Map-Reduce for Machine Learning on Multicore",
        "venue": "Neural Information Processing Systems",
        "year": 2006,
        "citationCount": 1280,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "2551676",
                "name": "Cheng-Tao Chu"
            },
            {
                "authorId": "2109828167",
                "name": "Sang Kyun Kim"
            },
            {
                "authorId": "47904256",
                "name": "Yi-An Lin"
            },
            {
                "authorId": "2117163611",
                "name": "YuanYuan Yu"
            },
            {
                "authorId": "1720184",
                "name": "Gary R. Bradski"
            },
            {
                "authorId": "34699434",
                "name": "A. Ng"
            },
            {
                "authorId": "1746638",
                "name": "K. Olukotun"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "932c2a02d462abd75af018125413b1ceaa1ee3f4",
        "url": "https://www.semanticscholar.org/paper/932c2a02d462abd75af018125413b1ceaa1ee3f4",
        "title": "Efficient Learning of Sparse Representations with an Energy-Based Model",
        "venue": "Neural Information Processing Systems",
        "year": 2006,
        "citationCount": 1345,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.7551/mitpress/7503.003.0147?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.7551/mitpress/7503.003.0147, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1706809",
                "name": "Marc'Aurelio Ranzato"
            },
            {
                "authorId": "2060013",
                "name": "Christopher S. Poultney"
            },
            {
                "authorId": "3295092",
                "name": "S. Chopra"
            },
            {
                "authorId": "1688882",
                "name": "Yann LeCun"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "96c6bc559b79d8fd518f431c707e8b44ce3bc4de",
        "url": "https://www.semanticscholar.org/paper/96c6bc559b79d8fd518f431c707e8b44ce3bc4de",
        "title": "Analysis of Representations for Domain Adaptation",
        "venue": "Neural Information Processing Systems",
        "year": 2006,
        "citationCount": 2441,
        "openAccessPdf": {
            "url": "http://www.iro.umontreal.ca/~lisa/seminaires/21-02-2007-2.pdf",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.7551/mitpress/7503.003.0022?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.7551/mitpress/7503.003.0022, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1401829700",
                "name": "Shai Ben-David"
            },
            {
                "authorId": "2116927",
                "name": "John Blitzer"
            },
            {
                "authorId": "1693407",
                "name": "K. Crammer"
            },
            {
                "authorId": "145366908",
                "name": "Fernando C Pereira"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "9bca4d7b932e0854c3325f1578cfd17341dd8ea8",
        "url": "https://www.semanticscholar.org/paper/9bca4d7b932e0854c3325f1578cfd17341dd8ea8",
        "title": "A Kernel Method for the Two-Sample-Problem",
        "venue": "Neural Information Processing Systems",
        "year": 2006,
        "citationCount": 2518,
        "openAccessPdf": {
            "url": "https://papers.nips.cc/paper/3110-a-kernel-method-for-the-two-sample-problem.pdf",
            "status": "GREEN",
            "license": "other-oa",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/0805.2368, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1708497",
                "name": "A. Gretton"
            },
            {
                "authorId": "1704422",
                "name": "Karsten M. Borgwardt"
            },
            {
                "authorId": "1733256",
                "name": "M. Rasch"
            },
            {
                "authorId": "1707625",
                "name": "B. Scholkopf"
            },
            {
                "authorId": "46234526",
                "name": "Alex Smola"
            }
        ],
        "abstract": "We propose two statistical tests to determine if two samples are from different distributions. Our test statistic is in both cases the distance between the means of the two samples mapped into a reproducing kernel Hilbert space (RKHS). The first test is based on a large deviation bound for the test statistic, while the second is based on the asymptotic distribution of this statistic. The test statistic can be computed in O(m2) time. We apply our approach to a variety of problems, including attribute matching for databases using the Hungarian marriage method, where our test performs strongly. We also demonstrate excellent performance when comparing distributions over graphs, for which no alternative tests currently exist."
    },
    {
        "paperId": "9f53740cc80ecf013b3646d10c6357e80e5e6b1e",
        "url": "https://www.semanticscholar.org/paper/9f53740cc80ecf013b3646d10c6357e80e5e6b1e",
        "title": "Learning with Hypergraphs: Clustering, Classification, and Embedding",
        "venue": "Neural Information Processing Systems",
        "year": 2006,
        "citationCount": 1486,
        "openAccessPdf": {
            "url": "https://papers.nips.cc/paper/3128-learning-with-hypergraphs-clustering-classification-and-embedding.pdf",
            "status": "GREEN",
            "license": "other-oa",
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.7551/mitpress/7503.003.0205?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.7551/mitpress/7503.003.0205, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "24982365",
                "name": "Dengyong Zhou"
            },
            {
                "authorId": "8192534",
                "name": "Jiayuan Huang"
            },
            {
                "authorId": "1707625",
                "name": "B. Scholkopf"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "af8835c8960e539cc33f5375861efaedec1fb0b2",
        "url": "https://www.semanticscholar.org/paper/af8835c8960e539cc33f5375861efaedec1fb0b2",
        "title": "Correcting Sample Selection Bias by Unlabeled Data",
        "venue": "Neural Information Processing Systems",
        "year": 2006,
        "citationCount": 1865,
        "openAccessPdf": {
            "url": "https://papers.nips.cc/paper/3075-correcting-sample-selection-bias-by-unlabeled-data.pdf",
            "status": "GREEN",
            "license": "other-oa",
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.7551/mitpress/7503.003.0080?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.7551/mitpress/7503.003.0080, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "8192534",
                "name": "Jiayuan Huang"
            },
            {
                "authorId": "46234526",
                "name": "Alex Smola"
            },
            {
                "authorId": "1708497",
                "name": "A. Gretton"
            },
            {
                "authorId": "1704422",
                "name": "Karsten M. Borgwardt"
            },
            {
                "authorId": "1707625",
                "name": "B. Scholkopf"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "e64a9960734215e2b1866ea3cb723ffa5585ac14",
        "url": "https://www.semanticscholar.org/paper/e64a9960734215e2b1866ea3cb723ffa5585ac14",
        "title": "Efficient sparse coding algorithms",
        "venue": "Neural Information Processing Systems",
        "year": 2006,
        "citationCount": 2855,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.7551/mitpress/7503.003.0105?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.7551/mitpress/7503.003.0105, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1697141",
                "name": "Honglak Lee"
            },
            {
                "authorId": "2078284037",
                "name": "Alexis Battle"
            },
            {
                "authorId": "2979876",
                "name": "Rajat Raina"
            },
            {
                "authorId": "34699434",
                "name": "A. Ng"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "f412bb31ec9ef8bbef70eefc7ffd04420c1365d9",
        "url": "https://www.semanticscholar.org/paper/f412bb31ec9ef8bbef70eefc7ffd04420c1365d9",
        "title": "Graph-Based Visual Saliency",
        "venue": "Neural Information Processing Systems",
        "year": 2006,
        "citationCount": 3820,
        "openAccessPdf": {
            "url": "https://authors.library.caltech.edu/records/0jwq7-qyh36/files/3095-graph-based-visual-saliency.pdf?download=1",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.7551/mitpress/7503.003.0073?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.7551/mitpress/7503.003.0073, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "39810944",
                "name": "Jonathan Harel"
            },
            {
                "authorId": "145624227",
                "name": "C. Koch"
            },
            {
                "authorId": "1690922",
                "name": "P. Perona"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "10d10df314c1b58f5c83629e73a35185876cd4e2",
        "url": "https://www.semanticscholar.org/paper/10d10df314c1b58f5c83629e73a35185876cd4e2",
        "title": "Multi-task Gaussian Process Prediction",
        "venue": "Neural Information Processing Systems",
        "year": 2007,
        "citationCount": 1280,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "30561807",
                "name": "Edwin V. Bonilla"
            },
            {
                "authorId": "1890071",
                "name": "K. M. Chai"
            },
            {
                "authorId": "145715698",
                "name": "Christopher K. I. Williams"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "202cbbf671743aefd380d2f23987bd46b9caaf97",
        "url": "https://www.semanticscholar.org/paper/202cbbf671743aefd380d2f23987bd46b9caaf97",
        "title": "Sparse deep belief net model for visual area V2",
        "venue": "Neural Information Processing Systems",
        "year": 2007,
        "citationCount": 1099,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "1697141",
                "name": "Honglak Lee"
            },
            {
                "authorId": "1779306",
                "name": "Chaitanya Ekanadham"
            },
            {
                "authorId": "34699434",
                "name": "A. Ng"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "5936754b5762260bf102ac95d7b26cfc9d31956a",
        "url": "https://www.semanticscholar.org/paper/5936754b5762260bf102ac95d7b26cfc9d31956a",
        "title": "The Tradeoffs of Large Scale Learning",
        "venue": "Neural Information Processing Systems",
        "year": 2007,
        "citationCount": 1712,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "52184096",
                "name": "L. Bottou"
            },
            {
                "authorId": "1698617",
                "name": "O. Bousquet"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "7a59fde27461a3ef4a21a249cc403d0d96e4a0d7",
        "url": "https://www.semanticscholar.org/paper/7a59fde27461a3ef4a21a249cc403d0d96e4a0d7",
        "title": "Random Features for Large-Scale Kernel Machines",
        "venue": "Neural Information Processing Systems",
        "year": 2007,
        "citationCount": 4591,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "31558848",
                "name": "A. Rahimi"
            },
            {
                "authorId": "9229182",
                "name": "B. Recht"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "c13aa63ccd5cf972a0a8c6b236c1dfad95b19b4e",
        "url": "https://www.semanticscholar.org/paper/c13aa63ccd5cf972a0a8c6b236c1dfad95b19b4e",
        "title": "Supervised Topic Models",
        "venue": "Neural Information Processing Systems",
        "year": 2007,
        "citationCount": 1803,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1003.0783, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1796335",
                "name": "D. Blei"
            },
            {
                "authorId": "40411909",
                "name": "Jon D. McAuliffe"
            }
        ],
        "abstract": "We introduce supervised latent Dirichlet allocation (sLDA), a statistical model of labelled documents. The model accommodates a variety of response types. We derive a maximum-likelihood procedure for parameter estimation, which relies on variational approximations to handle intractable posterior expectations. Prediction problems motivate this research: we use the fitted model to predict response values for new documents. We test sLDA on two real-world problems: movie ratings predicted from reviews, and web page popularity predicted from text descriptions. We illustrate the benefits of sLDA versus modern regularized regression, as well as versus an unsupervised LDA analysis followed by a separate regression."
    },
    {
        "paperId": "d9b9fb207013bf8afb064f23f3dffc7edd005f73",
        "url": "https://www.semanticscholar.org/paper/d9b9fb207013bf8afb064f23f3dffc7edd005f73",
        "title": "Mixed Membership Stochastic Blockmodels",
        "venue": "Neural Information Processing Systems",
        "year": 2007,
        "citationCount": 2178,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/0705.4485, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2247552",
                "name": "E. Airoldi"
            },
            {
                "authorId": "1796335",
                "name": "D. Blei"
            },
            {
                "authorId": "1684961",
                "name": "S. Fienberg"
            },
            {
                "authorId": "143977260",
                "name": "E. Xing"
            }
        ],
        "abstract": "Observations consisting of measurements on relationships for pairs of objects arise in many settings, such as protein interaction and gene regulatory networks, collections of author-recipient email, and social networks. Analyzing such data with probabilisic models can be delicate because the simple exchangeability assumptions underlying many boilerplate models no longer hold. In this paper, we describe a latent variable model of such data called the mixed membership stochastic blockmodel. This model extends blockmodels for relational data to ones which capture mixed membership latent relational structure, thus providing an object-specific low-dimensional representation. We develop a general variational inference algorithm for fast approximate posterior inference. We explore applications to social and protein interaction networks."
    },
    {
        "paperId": "e19971e7d100386b9b4cf4ea1a0782b62fe036e5",
        "url": "https://www.semanticscholar.org/paper/e19971e7d100386b9b4cf4ea1a0782b62fe036e5",
        "title": "Probabilistic Matrix Factorization",
        "venue": "Neural Information Processing Systems",
        "year": 2007,
        "citationCount": 4724,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "145124475",
                "name": "R. Salakhutdinov"
            },
            {
                "authorId": "1714004",
                "name": "A. Mnih"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "477e46461e3442220cd505f02b59195be016c747",
        "url": "https://www.semanticscholar.org/paper/477e46461e3442220cd505f02b59195be016c747",
        "title": "Nonlinear causal discovery with additive noise models",
        "venue": "Neural Information Processing Systems",
        "year": 2008,
        "citationCount": 1153,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "35211061",
                "name": "P. Hoyer"
            },
            {
                "authorId": "1700657",
                "name": "D. Janzing"
            },
            {
                "authorId": "1696158",
                "name": "J. Mooij"
            },
            {
                "authorId": "31443876",
                "name": "J. Peters"
            },
            {
                "authorId": "1707625",
                "name": "B. Scholkopf"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "9d65ba8bb20ae6dd001b9833c525c279dfe18916",
        "url": "https://www.semanticscholar.org/paper/9d65ba8bb20ae6dd001b9833c525c279dfe18916",
        "title": "Supervised Dictionary Learning",
        "venue": "Neural Information Processing Systems",
        "year": 2008,
        "citationCount": 1194,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/0809.3083, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2599292",
                "name": "J. Mairal"
            },
            {
                "authorId": "144570279",
                "name": "F. Bach"
            },
            {
                "authorId": "144189388",
                "name": "J. Ponce"
            },
            {
                "authorId": "1699339",
                "name": "G. Sapiro"
            },
            {
                "authorId": "1688869",
                "name": "Andrew Zisserman"
            }
        ],
        "abstract": "It is now well established that sparse signal models are well suited for restoration tasks and can be effectively learned from audio, image, and video data. Recent research has been aimed at learning discriminative sparse models instead of purely reconstructive ones. This paper proposes a new step in that direction, with a novel sparse representation for signals belonging to different classes in terms of a shared dictionary and discriminative class models. The linear version of the proposed model admits a simple probabilistic interpretation, while its most general variant admits an interpretation in terms of kernels. An optimization framework for learning all the components of the proposed model is presented, along with experimental results on standard handwritten digit and texture classification tasks."
    },
    {
        "paperId": "a9fc84f8abe740cdc7ee82e69444d1d00dbe0ceb",
        "url": "https://www.semanticscholar.org/paper/a9fc84f8abe740cdc7ee82e69444d1d00dbe0ceb",
        "title": "A Scalable Hierarchical Distributed Language Model",
        "venue": "Neural Information Processing Systems",
        "year": 2008,
        "citationCount": 1021,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "1714004",
                "name": "A. Mnih"
            },
            {
                "authorId": "1695689",
                "name": "Geoffrey E. Hinton"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "00facc08b06e5a8ce32278e8d0072b59fcf9f252",
        "url": "https://www.semanticscholar.org/paper/00facc08b06e5a8ce32278e8d0072b59fcf9f252",
        "title": "Reading Tea Leaves: How Humans Interpret Topic Models",
        "venue": "Neural Information Processing Systems",
        "year": 2009,
        "citationCount": 2444,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "80936017",
                "name": "Jonathan D. Chang"
            },
            {
                "authorId": "2267532838",
                "name": "Jordan L. Boyd-Graber"
            },
            {
                "authorId": "21007048",
                "name": "S. Gerrish"
            },
            {
                "authorId": "2108881999",
                "name": "Chong Wang"
            },
            {
                "authorId": "1796335",
                "name": "D. Blei"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "017438e9f62834694a4ea85f87f48bfcbd0490d7",
        "url": "https://www.semanticscholar.org/paper/017438e9f62834694a4ea85f87f48bfcbd0490d7",
        "title": "Robust Principal Component Analysis: Exact Recovery of Corrupted Low-Rank Matrices via Convex Optimization",
        "venue": "Neural Information Processing Systems",
        "year": 2009,
        "citationCount": 1427,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "143607492",
                "name": "John Wright"
            },
            {
                "authorId": "1701028",
                "name": "Arvind Ganesh"
            },
            {
                "authorId": "144304418",
                "name": "Shankar R. Rao"
            },
            {
                "authorId": "2390079",
                "name": "YiGang Peng"
            },
            {
                "authorId": "50032052",
                "name": "Yi Ma"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "0f6911bc1e6abee8bbf9dd3f8d54d40466429da7",
        "url": "https://www.semanticscholar.org/paper/0f6911bc1e6abee8bbf9dd3f8d54d40466429da7",
        "title": "Zero-shot Learning with Semantic Output Codes",
        "venue": "Neural Information Processing Systems",
        "year": 2009,
        "citationCount": 1118,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1184/R1/6476456.V1?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1184/R1/6476456.V1, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1752252",
                "name": "Mark Palatucci"
            },
            {
                "authorId": "48855558",
                "name": "D. Pomerleau"
            },
            {
                "authorId": "1695689",
                "name": "Geoffrey E. Hinton"
            },
            {
                "authorId": "40975594",
                "name": "Tom Michael Mitchell"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "5f56320c5979faeab78dbd9ddb7db755ba4550f3",
        "url": "https://www.semanticscholar.org/paper/5f56320c5979faeab78dbd9ddb7db755ba4550f3",
        "title": "A unified framework for high-dimensional analysis of $M$-estimators with decomposable regularizers",
        "venue": "Neural Information Processing Systems",
        "year": 2009,
        "citationCount": 1427,
        "openAccessPdf": {
            "url": "https://doi.org/10.1214/12-sts400",
            "status": "HYBRID",
            "license": "unspecified-oa",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1010.2731, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "145537739",
                "name": "S. Negahban"
            },
            {
                "authorId": "145969795",
                "name": "Pradeep Ravikumar"
            },
            {
                "authorId": "1721860",
                "name": "M. Wainwright"
            },
            {
                "authorId": "144923779",
                "name": "Bin Yu"
            }
        ],
        "abstract": "High-dimensional statistical inference deals with models in which the the number of parameters p is comparable to or larger than the sample size n. Since it is usually impossible to obtain consistent procedures unless p/n  0, a line of recent work has studied models with various types of structure (e.g., sparse vectors; block-structured matrices; low-rank matrices; Markov assumptions). In such settings, a general approach to estimation is to solve a regularized convex program (known as a regularized M-estimator) which combines a loss function (measuring how well the model fits the data) with some regularization function that encourages the assumed structure. The goal of this paper is to provide a unified framework for establishing consistency and convergence rates for such regularized M-estimators under high-dimensional scaling. We state one main theorem and show how it can be used to re-derive several existing results, and also to obtain several new results on consistency and convergence rates. Our analysis also identifies two key properties of loss and regularization functions, referred to as restricted strong convexity and decomposability, that ensure the corresponding regularized M-estimators have fast convergence rates."
    },
    {
        "paperId": "6953420c593842697dd09bc2cf7ffbbaf67a6e8e",
        "url": "https://www.semanticscholar.org/paper/6953420c593842697dd09bc2cf7ffbbaf67a6e8e",
        "title": "Whose Vote Should Count More: Optimal Integration of Labels from Labelers of Unknown Expertise",
        "venue": "Neural Information Processing Systems",
        "year": 2009,
        "citationCount": 1291,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "143973061",
                "name": "J. Whitehill"
            },
            {
                "authorId": "12114845",
                "name": "P. Ruvolo"
            },
            {
                "authorId": "4072965",
                "name": "Tingfan Wu"
            },
            {
                "authorId": "153807404",
                "name": "J. Bergsma"
            },
            {
                "authorId": "1741200",
                "name": "J. Movellan"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "9009461b81bed5dbf69432ff679f9fba1ef7adc5",
        "url": "https://www.semanticscholar.org/paper/9009461b81bed5dbf69432ff679f9fba1ef7adc5",
        "title": "Fast Image Deconvolution using Hyper-Laplacian Priors",
        "venue": "Neural Information Processing Systems",
        "year": 2009,
        "citationCount": 1442,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "1707347",
                "name": "Dilip Krishnan"
            },
            {
                "authorId": "2276554",
                "name": "R. Fergus"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "bf38dfb13352449b965c08282b66d3ffc5a0539f",
        "url": "https://www.semanticscholar.org/paper/bf38dfb13352449b965c08282b66d3ffc5a0539f",
        "title": "Unsupervised feature learning for audio classification using convolutional deep belief networks",
        "venue": "Neural Information Processing Systems",
        "year": 2009,
        "citationCount": 1168,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "1697141",
                "name": "Honglak Lee"
            },
            {
                "authorId": "2061523260",
                "name": "Peter T. Pham"
            },
            {
                "authorId": "1918282",
                "name": "Yan Largman"
            },
            {
                "authorId": "34699434",
                "name": "A. Ng"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "1baee4ae5e5eaf75e322b53afa3cbdea89dcc2d0",
        "url": "https://www.semanticscholar.org/paper/1baee4ae5e5eaf75e322b53afa3cbdea89dcc2d0",
        "title": "Efficient and Robust Feature Selection via Joint 2, 1-Norms Minimization",
        "venue": "Neural Information Processing Systems",
        "year": 2010,
        "citationCount": 1976,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "144962210",
                "name": "F. Nie"
            },
            {
                "authorId": "1748032",
                "name": "Heng Huang"
            },
            {
                "authorId": "144380249",
                "name": "Xiao Cai"
            },
            {
                "authorId": "1737469",
                "name": "C. Ding"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "2d8cbd7370b4ce666edd864e66f83ebf20963516",
        "url": "https://www.semanticscholar.org/paper/2d8cbd7370b4ce666edd864e66f83ebf20963516",
        "title": "Online Learning for Latent Dirichlet Allocation",
        "venue": "Neural Information Processing Systems",
        "year": 2010,
        "citationCount": 1746,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "28552618",
                "name": "M. Hoffman"
            },
            {
                "authorId": "1796335",
                "name": "D. Blei"
            },
            {
                "authorId": "144570279",
                "name": "F. Bach"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "6286a82f72f632672c1890f3dd6bbb15b8e5168b",
        "url": "https://www.semanticscholar.org/paper/6286a82f72f632672c1890f3dd6bbb15b8e5168b",
        "title": "Object Bank: A High-Level Image Representation for Scene Classification & Semantic Feature Sparsification",
        "venue": "Neural Information Processing Systems",
        "year": 2010,
        "citationCount": 1079,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "2040091191",
                "name": "Li-Jia Li"
            },
            {
                "authorId": "144914140",
                "name": "Hao Su"
            },
            {
                "authorId": "143977260",
                "name": "E. Xing"
            },
            {
                "authorId": "48004138",
                "name": "Li Fei-Fei"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "644a079073969a92674f69483c4a85679d066545",
        "url": "https://www.semanticscholar.org/paper/644a079073969a92674f69483c4a85679d066545",
        "title": "Double Q-learning",
        "venue": "Neural Information Processing Systems",
        "year": 2010,
        "citationCount": 1698,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "7634925",
                "name": "H. V. Hasselt"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "6b22036526adbfa8f93ff1a4749414d475950146",
        "url": "https://www.semanticscholar.org/paper/6b22036526adbfa8f93ff1a4749414d475950146",
        "title": "Extended Bayesian Information Criteria for Gaussian Graphical Models",
        "venue": "Neural Information Processing Systems",
        "year": 2010,
        "citationCount": 1006,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1011.6640, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2531127",
                "name": "Rina Foygel"
            },
            {
                "authorId": "2107277",
                "name": "M. Drton"
            }
        ],
        "abstract": "Gaussian graphical models with sparsity in the inverse covariance matrix are of significant interest in many modern applications. For the problem of recovering the graphical structure, information criteria provide useful optimization objectives for algorithms searching through sets of graphs or for selection of tuning parameters of other methods such as the graphical lasso, which is a likelihood penalization technique. In this paper we establish the consistency of an extended Bayesian information criterion for Gaussian graphical models in a scenario where both the number of variables p and the sample size n grow. Compared to earlier work on the regression case, our treatment allows for growth in the number of non-zero parameters in the true model, which is necessary in order to cover connected graphs. We demonstrate the performance of this criterion on simulated data when used in conjunction with the graphical lasso, and verify that the criterion indeed performs better than either cross-validation or the ordinary Bayesian information criterion when p and the number of non-zero parameters q both scale with n."
    },
    {
        "paperId": "83e9565cede81b2b88a9fa241833135da142f4d3",
        "url": "https://www.semanticscholar.org/paper/83e9565cede81b2b88a9fa241833135da142f4d3",
        "title": "Parallelized Stochastic Gradient Descent",
        "venue": "Neural Information Processing Systems",
        "year": 2010,
        "citationCount": 1421,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "8195063",
                "name": "Martin A. Zinkevich"
            },
            {
                "authorId": "2965406",
                "name": "Markus Weimer"
            },
            {
                "authorId": "46234526",
                "name": "Alex Smola"
            },
            {
                "authorId": "47681372",
                "name": "Lihong Li"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "a049555721f17ed79a97fd492c8fc9a3f8f8aa17",
        "url": "https://www.semanticscholar.org/paper/a049555721f17ed79a97fd492c8fc9a3f8f8aa17",
        "title": "Self-Paced Learning for Latent Variable Models",
        "venue": "Neural Information Processing Systems",
        "year": 2010,
        "citationCount": 1544,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "3717791",
                "name": "M. P. Kumar"
            },
            {
                "authorId": "1409971380",
                "name": "Ben Packer"
            },
            {
                "authorId": "1736370",
                "name": "D. Koller"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "f115acbd94451f58a5bc5062c1d7e6707e5be1f0",
        "url": "https://www.semanticscholar.org/paper/f115acbd94451f58a5bc5062c1d7e6707e5be1f0",
        "title": "Monte-Carlo Planning in Large POMDPs",
        "venue": "Neural Information Processing Systems",
        "year": 2010,
        "citationCount": 1259,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "145824029",
                "name": "David Silver"
            },
            {
                "authorId": "144056327",
                "name": "J. Veness"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "f4c7ff4b8613f700aa9f89a2c0653b6ffcf658be",
        "url": "https://www.semanticscholar.org/paper/f4c7ff4b8613f700aa9f89a2c0653b6ffcf658be",
        "title": "Learning To Count Objects in Images",
        "venue": "Neural Information Processing Systems",
        "year": 2010,
        "citationCount": 1298,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "1740145",
                "name": "V. Lempitsky"
            },
            {
                "authorId": "1688869",
                "name": "Andrew Zisserman"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "03911c85305d42aa2eeb02be82ef6fb7da644dd0",
        "url": "https://www.semanticscholar.org/paper/03911c85305d42aa2eeb02be82ef6fb7da644dd0",
        "title": "Algorithms for Hyper-Parameter Optimization",
        "venue": "Neural Information Processing Systems",
        "year": 2011,
        "citationCount": 5180,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "32837403",
                "name": "J. Bergstra"
            },
            {
                "authorId": "2103302",
                "name": "R. Bardenet"
            },
            {
                "authorId": "1751762",
                "name": "Yoshua Bengio"
            },
            {
                "authorId": "143674326",
                "name": "B. Kgl"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "3449b65008b27f6e60a73d80c1fd990f0481126b",
        "url": "https://www.semanticscholar.org/paper/3449b65008b27f6e60a73d80c1fd990f0481126b",
        "title": "Torch7: A Matlab-like Environment for Machine Learning",
        "venue": "Neural Information Processing Systems",
        "year": 2011,
        "citationCount": 1592,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "2939803",
                "name": "R. Collobert"
            },
            {
                "authorId": "2645384",
                "name": "K. Kavukcuoglu"
            },
            {
                "authorId": "2256269",
                "name": "C. Farabet"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "36f49b05d764bf5c10428b082c2d96c13c4203b9",
        "url": "https://www.semanticscholar.org/paper/36f49b05d764bf5c10428b082c2d96c13c4203b9",
        "title": "Hogwild: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent",
        "venue": "Neural Information Processing Systems",
        "year": 2011,
        "citationCount": 2331,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1106.5730, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "9229182",
                "name": "B. Recht"
            },
            {
                "authorId": "1803218",
                "name": "Christopher R"
            },
            {
                "authorId": "144731788",
                "name": "Stephen J. Wright"
            },
            {
                "authorId": "47657030",
                "name": "Feng Niu"
            }
        ],
        "abstract": "Stochastic Gradient Descent (SGD) is a popular algorithm that can achieve state-of-the-art performance on a variety of machine learning tasks. Several researchers have recently proposed schemes to parallelize SGD, but all require performance-destroying memory locking and synchronization. This work aims to show using novel theoretical analysis, algorithms, and implementation that SGD can be implemented without any locking. We present an update scheme called HOGWILD! which allows processors access to shared memory with the possibility of overwriting each other's work. We show that when the associated optimization problem is sparse, meaning most gradient updates only modify small parts of the decision variable, then HOGWILD! achieves a nearly optimal rate of convergence. We demonstrate experimentally that HOGWILD! outperforms alternative schemes that use locking by an order of magnitude."
    },
    {
        "paperId": "3d22552235f5c51d33b657fb816050820c6e055e",
        "url": "https://www.semanticscholar.org/paper/3d22552235f5c51d33b657fb816050820c6e055e",
        "title": "Linearized Alternating Direction Method with Adaptive Penalty for Low-Rank Representation",
        "venue": "Neural Information Processing Systems",
        "year": 2011,
        "citationCount": 1264,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1109.0367, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "33383055",
                "name": "Zhouchen Lin"
            },
            {
                "authorId": "34469457",
                "name": "Risheng Liu"
            },
            {
                "authorId": "4642456",
                "name": "Zhixun Su"
            }
        ],
        "abstract": "Many machine learning and signal processing problems can be formulated as linearly constrained convex programs, which could be efficiently solved by the alternating direction method (ADM). However, usually the subproblems in ADM are easily solvable only when the linear mappings in the constraints are identities. To address this issue, we propose a linearized ADM (LADM) method by linearizing the quadratic penalty term and adding a proximal term when solving the sub-problems. For fast convergence, we also allow the penalty to change adaptively according a novel update rule. We prove the global convergence of LADM with adaptive penalty (LADMAP). As an example, we apply LADMAP to solve low-rank representation (LRR), which is an important subspace clustering technique yet suffers from high computation cost. By combining LADMAP with a skinny SVD representation technique, we are able to reduce the complexity O(n3) of the original ADM based method to O(rn2), where r and n are the rank and size of the representation matrix, respectively, hence making LRR possible for large scale applications. Numerical experiments verify that for LRR our LADMAP based methods are much faster than state-of-the-art algorithms."
    },
    {
        "paperId": "5a9ef216bf11f222438fff130c778267d39a9564",
        "url": "https://www.semanticscholar.org/paper/5a9ef216bf11f222438fff130c778267d39a9564",
        "title": "Practical Variational Inference for Neural Networks",
        "venue": "Neural Information Processing Systems",
        "year": 2011,
        "citationCount": 1779,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "1753223",
                "name": "Alex Graves"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "6bea71fa6deb19c67e9586428f8f240e789fb3df",
        "url": "https://www.semanticscholar.org/paper/6bea71fa6deb19c67e9586428f8f240e789fb3df",
        "title": "Improved Algorithms for Linear Stochastic Bandits",
        "venue": "Neural Information Processing Systems",
        "year": 2011,
        "citationCount": 1895,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "1388837087",
                "name": "Yasin Abbasi-Yadkori"
            },
            {
                "authorId": "2153912",
                "name": "D. Pl"
            },
            {
                "authorId": "40868287",
                "name": "Csaba Szepesvari"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "8e080b98efbe65c02a116439205ca2344b9f7cd4",
        "url": "https://www.semanticscholar.org/paper/8e080b98efbe65c02a116439205ca2344b9f7cd4",
        "title": "Im2Text: Describing Images Using 1 Million Captioned Photographs",
        "venue": "Neural Information Processing Systems",
        "year": 2011,
        "citationCount": 1537,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "2004053",
                "name": "Vicente Ordonez"
            },
            {
                "authorId": "145564333",
                "name": "Girish Kulkarni"
            },
            {
                "authorId": "1685538",
                "name": "Tamara L. Berg"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "ab867c140d2947511979c87e7ae580d9d3f0aeab",
        "url": "https://www.semanticscholar.org/paper/ab867c140d2947511979c87e7ae580d9d3f0aeab",
        "title": "An Empirical Evaluation of Thompson Sampling",
        "venue": "Neural Information Processing Systems",
        "year": 2011,
        "citationCount": 1554,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "1730609",
                "name": "O. Chapelle"
            },
            {
                "authorId": "47681372",
                "name": "Lihong Li"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "ba9233dd7c81765b7afb2cc1a6e5e9a075518d8c",
        "url": "https://www.semanticscholar.org/paper/ba9233dd7c81765b7afb2cc1a6e5e9a075518d8c",
        "title": "Co-regularized Multi-view Spectral Clustering",
        "venue": "Neural Information Processing Systems",
        "year": 2011,
        "citationCount": 1245,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "50333123",
                "name": "Abhishek Kumar"
            },
            {
                "authorId": "145593549",
                "name": "Piyush Rai"
            },
            {
                "authorId": "1722360",
                "name": "Hal Daum"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "c81c20109c809cfc47565a9477c04ee005d424bf",
        "url": "https://www.semanticscholar.org/paper/c81c20109c809cfc47565a9477c04ee005d424bf",
        "title": "Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials",
        "venue": "Neural Information Processing Systems",
        "year": 2011,
        "citationCount": 3601,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1210.5644, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2562966",
                "name": "Philipp Krhenbhl"
            },
            {
                "authorId": "145231047",
                "name": "V. Koltun"
            }
        ],
        "abstract": "Most state-of-the-art techniques for multi-class image segmentation and labeling use conditional random fields defined over pixels or image regions. While region-level models often feature dense pairwise connectivity, pixel-level models are considerably larger and have only permitted sparse graph structures. In this paper, we consider fully connected CRF models defined on the complete set of pixels in an image. The resulting graphs have billions of edges, making traditional inference algorithms impractical. Our main contribution is a highly efficient approximate inference algorithm for fully connected CRF models in which the pairwise edge potentials are defined by a linear combination of Gaussian kernels. Our experiments demonstrate that dense connectivity at the pixel level substantially improves segmentation and labeling accuracy."
    },
    {
        "paperId": "09193e19b59fc8f05bee9d6efbfb1607ca5b6501",
        "url": "https://www.semanticscholar.org/paper/09193e19b59fc8f05bee9d6efbfb1607ca5b6501",
        "title": "Deep Neural Networks Segment Neuronal Membranes in Electron Microscopy Images",
        "venue": "Neural Information Processing Systems",
        "year": 2012,
        "citationCount": 1491,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "1895356",
                "name": "D. Ciresan"
            },
            {
                "authorId": "33354551",
                "name": "A. Giusti"
            },
            {
                "authorId": "6803671",
                "name": "L. Gambardella"
            },
            {
                "authorId": "145341374",
                "name": "J. Schmidhuber"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "2e2089ae76fe914706e6fa90081a79c8fe01611e",
        "url": "https://www.semanticscholar.org/paper/2e2089ae76fe914706e6fa90081a79c8fe01611e",
        "title": "Practical Bayesian Optimization of Machine Learning Algorithms",
        "venue": "Neural Information Processing Systems",
        "year": 2012,
        "citationCount": 8760,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1206.2944, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "144108062",
                "name": "Jasper Snoek"
            },
            {
                "authorId": "1777528",
                "name": "H. Larochelle"
            },
            {
                "authorId": "1722180",
                "name": "Ryan P. Adams"
            }
        ],
        "abstract": "The use of machine learning algorithms frequently involves careful tuning of learning parameters and model hyperparameters. Unfortunately, this tuning is often a \"black art\" requiring expert experience, rules of thumb, or sometimes brute-force search. There is therefore great appeal for automatic approaches that can optimize the performance of any given learning algorithm to the problem at hand. In this work, we consider this problem through the framework of Bayesian optimization, in which a learning algorithm's generalization performance is modeled as a sample from a Gaussian process (GP). We show that certain choices for the nature of the GP, such as the type of kernel and the treatment of its hyperparameters, can play a crucial role in obtaining a good optimizer that can achieve expertlevel performance. We describe new algorithms that take into account the variable cost (duration) of learning algorithm experiments and that can leverage the presence of multiple cores for parallel experimentation. We show that these proposed algorithms improve on previous automatic procedures and can reach or surpass human expert-level optimization for many algorithms including latent Dirichlet allocation, structured SVMs and convolutional neural networks."
    },
    {
        "paperId": "3127190433230b3dc1abd0680bb58dced4bcd90e",
        "url": "https://www.semanticscholar.org/paper/3127190433230b3dc1abd0680bb58dced4bcd90e",
        "title": "Large Scale Distributed Deep Networks",
        "venue": "Neural Information Processing Systems",
        "year": 2012,
        "citationCount": 4050,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "49959210",
                "name": "J. Dean"
            },
            {
                "authorId": "32131713",
                "name": "G. Corrado"
            },
            {
                "authorId": "3089272",
                "name": "R. Monga"
            },
            {
                "authorId": "2118440152",
                "name": "Kai Chen"
            },
            {
                "authorId": "145139947",
                "name": "M. Devin"
            },
            {
                "authorId": "2827616",
                "name": "Quoc V. Le"
            },
            {
                "authorId": "1715548",
                "name": "Mark Z. Mao"
            },
            {
                "authorId": "1706809",
                "name": "Marc'Aurelio Ranzato"
            },
            {
                "authorId": "33666044",
                "name": "A. Senior"
            },
            {
                "authorId": "2080690",
                "name": "P. Tucker"
            },
            {
                "authorId": "143781496",
                "name": "Ke Yang"
            },
            {
                "authorId": "34699434",
                "name": "A. Ng"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "89147b52cfdf7d8f778763d04d845f7e1bb275ab",
        "url": "https://www.semanticscholar.org/paper/89147b52cfdf7d8f778763d04d845f7e1bb275ab",
        "title": "Learning to Discover Social Circles in Ego Networks",
        "venue": "Neural Information Processing Systems",
        "year": 2012,
        "citationCount": 2216,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "35660011",
                "name": "Julian McAuley"
            },
            {
                "authorId": "1702139",
                "name": "J. Leskovec"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "a2017ec2c60d542af5e9993176ba68f89529dbce",
        "url": "https://www.semanticscholar.org/paper/a2017ec2c60d542af5e9993176ba68f89529dbce",
        "title": "Image Denoising and Inpainting with Deep Neural Networks",
        "venue": "Neural Information Processing Systems",
        "year": 2012,
        "citationCount": 1449,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "2369548",
                "name": "Junyuan Xie"
            },
            {
                "authorId": "2230211",
                "name": "Linli Xu"
            },
            {
                "authorId": "2227868312",
                "name": "Enhong Chen"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "0080118b0eb02af581ff32b85a1bb6aed7081f45",
        "url": "https://www.semanticscholar.org/paper/0080118b0eb02af581ff32b85a1bb6aed7081f45",
        "title": "Sinkhorn Distances: Lightspeed Computation of Optimal Transport",
        "venue": "Neural Information Processing Systems",
        "year": 2013,
        "citationCount": 4869,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1306.0895, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1711979",
                "name": "Marco Cuturi"
            }
        ],
        "abstract": "Optimal transportation distances are a fundamental family of parameterized distances for histograms. Despite their appealing theoretical properties, excellent performance in retrieval tasks and intuitive formulation, their computation involves the resolution of a linear program whose cost is prohibitive whenever the histograms' dimension exceeds a few hundreds. We propose in this work a new family of optimal transportation distances that look at transportation problems from a maximum-entropy perspective. We smooth the classical optimal transportation problem with an entropic regularization term, and show that the resulting optimum is also a distance which can be computed through Sinkhorn-Knopp's matrix scaling algorithm at a speed that is several orders of magnitude faster than that of transportation solvers. We also report improved performance over classical optimal transportation distances on the MNIST benchmark problem."
    },
    {
        "paperId": "1ab5c006caf3bf8c128fdfad80e58277cb8b1455",
        "url": "https://www.semanticscholar.org/paper/1ab5c006caf3bf8c128fdfad80e58277cb8b1455",
        "title": "Learning with Noisy Labels",
        "venue": "Neural Information Processing Systems",
        "year": 2013,
        "citationCount": 1240,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "145328735",
                "name": "Nagarajan Natarajan"
            },
            {
                "authorId": "1783667",
                "name": "I. Dhillon"
            },
            {
                "authorId": "145969795",
                "name": "Pradeep Ravikumar"
            },
            {
                "authorId": "3064914",
                "name": "Ambuj Tewari"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "2582ab7c70c9e7fcb84545944eba8f3a7f253248",
        "url": "https://www.semanticscholar.org/paper/2582ab7c70c9e7fcb84545944eba8f3a7f253248",
        "title": "Translating Embeddings for Modeling Multi-relational Data",
        "venue": "Neural Information Processing Systems",
        "year": 2013,
        "citationCount": 8359,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "1713934",
                "name": "Antoine Bordes"
            },
            {
                "authorId": "1746841",
                "name": "Nicolas Usunier"
            },
            {
                "authorId": "1405061488",
                "name": "Alberto Garca-Durn"
            },
            {
                "authorId": "145183709",
                "name": "J. Weston"
            },
            {
                "authorId": "2406794",
                "name": "Oksana Yakhnenko"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "43c05444fbc239321f6676f3cd539cac34fde7b8",
        "url": "https://www.semanticscholar.org/paper/43c05444fbc239321f6676f3cd539cac34fde7b8",
        "title": "Accelerating Stochastic Gradient Descent using Predictive Variance Reduction",
        "venue": "Neural Information Processing Systems",
        "year": 2013,
        "citationCount": 2890,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "2108925678",
                "name": "Rie Johnson"
            },
            {
                "authorId": "50728655",
                "name": "Tong Zhang"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "4a7bec5222f4b3f3dc13799582fc3a89c59a5149",
        "url": "https://www.semanticscholar.org/paper/4a7bec5222f4b3f3dc13799582fc3a89c59a5149",
        "title": "Understanding variable importances in forests of randomized trees",
        "venue": "Neural Information Processing Systems",
        "year": 2013,
        "citationCount": 1023,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "1881041",
                "name": "Gilles Louppe"
            },
            {
                "authorId": "1695713",
                "name": "L. Wehenkel"
            },
            {
                "authorId": "31999245",
                "name": "Antonio Sutera"
            },
            {
                "authorId": "50206577",
                "name": "P. Geurts"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "4aa4069693bee00d1b0759ca3df35e59284e9845",
        "url": "https://www.semanticscholar.org/paper/4aa4069693bee00d1b0759ca3df35e59284e9845",
        "title": "DeViSE: A Deep Visual-Semantic Embedding Model",
        "venue": "Neural Information Processing Systems",
        "year": 2013,
        "citationCount": 2964,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "2279670",
                "name": "Andrea Frome"
            },
            {
                "authorId": "32131713",
                "name": "G. Corrado"
            },
            {
                "authorId": "1789737",
                "name": "Jonathon Shlens"
            },
            {
                "authorId": "1751569",
                "name": "Samy Bengio"
            },
            {
                "authorId": "49959210",
                "name": "J. Dean"
            },
            {
                "authorId": "1706809",
                "name": "Marc'Aurelio Ranzato"
            },
            {
                "authorId": "2047446108",
                "name": "Tomas Mikolov"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "50d53cc562225549457cbc782546bfbe1ac6f0cf",
        "url": "https://www.semanticscholar.org/paper/50d53cc562225549457cbc782546bfbe1ac6f0cf",
        "title": "Reasoning With Neural Tensor Networks for Knowledge Base Completion",
        "venue": "Neural Information Processing Systems",
        "year": 2013,
        "citationCount": 2050,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "2166511",
                "name": "R. Socher"
            },
            {
                "authorId": "50536468",
                "name": "Danqi Chen"
            },
            {
                "authorId": "144783904",
                "name": "Christopher D. Manning"
            },
            {
                "authorId": "34699434",
                "name": "A. Ng"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "713f73ce5c3013d9fb796c21b981dc6629af0bd5",
        "url": "https://www.semanticscholar.org/paper/713f73ce5c3013d9fb796c21b981dc6629af0bd5",
        "title": "Deep Neural Networks for Object Detection",
        "venue": "Neural Information Processing Systems",
        "year": 2013,
        "citationCount": 1541,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "2574060",
                "name": "Christian Szegedy"
            },
            {
                "authorId": "1726415",
                "name": "Alexander Toshev"
            },
            {
                "authorId": "1761978",
                "name": "D. Erhan"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "755e9f43ce398ae8737366720c5f82685b0c253e",
        "url": "https://www.semanticscholar.org/paper/755e9f43ce398ae8737366720c5f82685b0c253e",
        "title": "Zero-Shot Learning Through Cross-Modal Transfer",
        "venue": "Neural Information Processing Systems",
        "year": 2013,
        "citationCount": 1509,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1301.3666, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2166511",
                "name": "R. Socher"
            },
            {
                "authorId": "2012435",
                "name": "M. Ganjoo"
            },
            {
                "authorId": "144783904",
                "name": "Christopher D. Manning"
            },
            {
                "authorId": "34699434",
                "name": "A. Ng"
            }
        ],
        "abstract": "This work introduces a model that can recognize objects in images even if no training data is available for the object class. The only necessary knowledge about unseen visual categories comes from unsupervised text corpora. Unlike previous zero-shot learning models, which can only differentiate between unseen classes, our model can operate on a mixture of seen and unseen classes, simultaneously obtaining state of the art performance on classes with thousands of training images and reasonable performance on unseen classes. This is achieved by seeing the distributions of words in texts as a semantic space for understanding what objects look like. Our deep learning model does not require any manually defined semantic or visual features for either words or images. Images are mapped to be close to semantic word vectors corresponding to their classes, and the resulting image embeddings can be used to distinguish whether an image is of a seen or unseen class. We then use novelty detection methods to differentiate unseen classes from seen classes. We demonstrate two novelty detection strategies; the first gives high accuracy on unseen classes, while the second is conservative in its prediction of novelty and keeps the seen classes' accuracy high."
    },
    {
        "paperId": "87f40e6f3022adbc1f1905e3e506abad05a9964f",
        "url": "https://www.semanticscholar.org/paper/87f40e6f3022adbc1f1905e3e506abad05a9964f",
        "title": "Distributed Representations of Words and Phrases and their Compositionality",
        "venue": "Neural Information Processing Systems",
        "year": 2013,
        "citationCount": 34686,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1310.4546, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2047446108",
                "name": "Tomas Mikolov"
            },
            {
                "authorId": "1701686",
                "name": "I. Sutskever"
            },
            {
                "authorId": "2118440152",
                "name": "Kai Chen"
            },
            {
                "authorId": "32131713",
                "name": "G. Corrado"
            },
            {
                "authorId": "49959210",
                "name": "J. Dean"
            }
        ],
        "abstract": "The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. \n \nAn inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of \"Canada\" and \"Air\" cannot be easily combined to obtain \"Air Canada\". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible."
    },
    {
        "paperId": "d770060812fb646b3846a7d398a3066145b5e3c8",
        "url": "https://www.semanticscholar.org/paper/d770060812fb646b3846a7d398a3066145b5e3c8",
        "title": "Do Deep Nets Really Need to be Deep?",
        "venue": "Neural Information Processing Systems",
        "year": 2013,
        "citationCount": 2196,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1312.6184, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2503659",
                "name": "Jimmy Ba"
            },
            {
                "authorId": "145727186",
                "name": "R. Caruana"
            }
        ],
        "abstract": "Currently, deep neural networks are the state of the art on problems such as speech recognition and computer vision. In this paper we empirically demonstrate that shallow feed-forward nets can learn the complex functions previously learned by deep nets and achieve accuracies previously only achievable with deep models. Moreover, in some cases the shallow nets can learn these deep functions using the same number of parameters as the original deep models. On the TIMIT phoneme recognition and CIFAR-10 image recognition tasks, shallow nets can be trained that perform similarly to complex, well-engineered, deeper convolutional models."
    },
    {
        "paperId": "eeff60867041d2ea92d1b38a20c2031d240d8872",
        "url": "https://www.semanticscholar.org/paper/eeff60867041d2ea92d1b38a20c2031d240d8872",
        "title": "Deep content-based music recommendation",
        "venue": "Neural Information Processing Systems",
        "year": 2013,
        "citationCount": 1234,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "3422336",
                "name": "Aron van den Oord"
            },
            {
                "authorId": "48373216",
                "name": "S. Dieleman"
            },
            {
                "authorId": "2621946",
                "name": "B. Schrauwen"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "eff61216e0136886e1158625b1e5a88ed1a7cbce",
        "url": "https://www.semanticscholar.org/paper/eff61216e0136886e1158625b1e5a88ed1a7cbce",
        "title": "Predicting Parameters in Deep Learning",
        "venue": "Neural Information Processing Systems",
        "year": 2013,
        "citationCount": 1379,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1306.0543, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1715051",
                "name": "Misha Denil"
            },
            {
                "authorId": "3355894",
                "name": "B. Shakibi"
            },
            {
                "authorId": "46573521",
                "name": "Laurent Dinh"
            },
            {
                "authorId": "1706809",
                "name": "Marc'Aurelio Ranzato"
            },
            {
                "authorId": "1737568",
                "name": "Nando de Freitas"
            }
        ],
        "abstract": "We demonstrate that there is significant redundancy in the parameterization of several deep learning models. Given only a few weight values for each feature it is possible to accurately predict the remaining values. Moreover, we show that not only can the parameter values be predicted, but many of them need not be learned at all. We train several different architectures by learning only a small number of weights and predicting the rest. In the best case we are able to predict more than 95% of the weights of a network without any drop in accuracy."
    },
    {
        "paperId": "081651b38ff7533550a3adfc1c00da333a8fe86c",
        "url": "https://www.semanticscholar.org/paper/081651b38ff7533550a3adfc1c00da333a8fe86c",
        "title": "How transferable are features in deep neural networks?",
        "venue": "Neural Information Processing Systems",
        "year": 2014,
        "citationCount": 8897,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1411.1792, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2965424",
                "name": "J. Yosinski"
            },
            {
                "authorId": "2552141",
                "name": "J. Clune"
            },
            {
                "authorId": "1751762",
                "name": "Yoshua Bengio"
            },
            {
                "authorId": "1747909",
                "name": "Hod Lipson"
            }
        ],
        "abstract": "Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters and color blobs. Such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A final surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset."
    },
    {
        "paperId": "12ecc2d786080f638a01b9999518e9386baa157d",
        "url": "https://www.semanticscholar.org/paper/12ecc2d786080f638a01b9999518e9386baa157d",
        "title": "Joint Training of a Convolutional Network and a Graphical Model for Human Pose Estimation",
        "venue": "Neural Information Processing Systems",
        "year": 2014,
        "citationCount": 1586,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1406.2984, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2704494",
                "name": "Jonathan Tompson"
            },
            {
                "authorId": "49147969",
                "name": "Arjun Jain"
            },
            {
                "authorId": "1688882",
                "name": "Yann LeCun"
            },
            {
                "authorId": "2428034",
                "name": "C. Bregler"
            }
        ],
        "abstract": "This paper proposes a new hybrid architecture that consists of a deep Convolu-tional Network and a Markov Random Field. We show how this architecture is successfully applied to the challenging problem of articulated human pose estimation in monocular images. The architecture can exploit structural domain constraints such as geometric relationships between body joint locations. We show that joint training of these two model paradigms improves performance and allows us to significantly outperform existing state-of-the-art techniques."
    },
    {
        "paperId": "4daec165c1f4aa1206b0d91c0b26f0287d1ef52d",
        "url": "https://www.semanticscholar.org/paper/4daec165c1f4aa1206b0d91c0b26f0287d1ef52d",
        "title": "SAGA: A Fast Incremental Gradient Method With Support for Non-Strongly Convex Composite Objectives",
        "venue": "Neural Information Processing Systems",
        "year": 2014,
        "citationCount": 1932,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1407.0202, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "34597877",
                "name": "Aaron Defazio"
            },
            {
                "authorId": "144570279",
                "name": "F. Bach"
            },
            {
                "authorId": "1388317459",
                "name": "Simon Lacoste-Julien"
            }
        ],
        "abstract": "In this work we introduce a new optimisation method called SAGA in the spirit of SAG, SDCA, MISO and SVRG, a set of recently proposed incremental gradient algorithms with fast linear convergence rates. SAGA improves on the theory behind SAG and SVRG, with better theoretical convergence rates, and has support for composite objectives where a proximal operator is used on the regulariser. Unlike SDCA, SAGA supports non-strongly convex problems directly, and is adaptive to any inherent strong convexity of the problem. We give experimental results showing the effectiveness of our method."
    },
    {
        "paperId": "66ad2fbc8b73242a889699868611fcf239e3435d",
        "url": "https://www.semanticscholar.org/paper/66ad2fbc8b73242a889699868611fcf239e3435d",
        "title": "Semi-supervised Learning with Deep Generative Models",
        "venue": "Neural Information Processing Systems",
        "year": 2014,
        "citationCount": 2850,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1406.5298, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1726807",
                "name": "Diederik P. Kingma"
            },
            {
                "authorId": "14594344",
                "name": "S. Mohamed"
            },
            {
                "authorId": "1748523",
                "name": "Danilo Jimenez Rezende"
            },
            {
                "authorId": "1678311",
                "name": "M. Welling"
            }
        ],
        "abstract": "The ever-increasing size of modern data sets combined with the difficulty of obtaining label information has made semi-supervised learning one of the problems of significant practical importance in modern data analysis. We revisit the approach to semi-supervised learning with generative models and develop new models that allow for effective generalisation from small labelled data sets to large unlabelled ones. Generative approaches have thus far been either inflexible, inefficient or non-scalable. We show that deep generative models and approximate Bayesian inference exploiting recent advances in variational methods can be used to provide significant improvements, making generative approaches highly competitive for semi-supervised learning."
    },
    {
        "paperId": "67dccc9a856b60bdc4d058d83657a089b8ad4486",
        "url": "https://www.semanticscholar.org/paper/67dccc9a856b60bdc4d058d83657a089b8ad4486",
        "title": "Two-Stream Convolutional Networks for Action Recognition in Videos",
        "venue": "Neural Information Processing Systems",
        "year": 2014,
        "citationCount": 7950,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1406.2199, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "34838386",
                "name": "K. Simonyan"
            },
            {
                "authorId": "1688869",
                "name": "Andrew Zisserman"
            }
        ],
        "abstract": "We investigate architectures of discriminatively trained deep Convolutional Networks (ConvNets) for action recognition in video. The challenge is to capture the complementary information on appearance from still frames and motion between frames. We also aim to generalise the best performing hand-crafted features within a data-driven learning framework. \n \nOur contribution is three-fold. First, we propose a two-stream ConvNet architecture which incorporates spatial and temporal networks. Second, we demonstrate that a ConvNet trained on multi-frame dense optical flow is able to achieve very good performance in spite of limited training data. Finally, we show that multitask learning, applied to two different action classification datasets, can be used to increase the amount of training data and improve the performance on both. Our architecture is trained and evaluated on the standard video actions benchmarks of UCF-101 and HMDB-51, where it is competitive with the state of the art. It also exceeds by a large margin previous attempts to use deep nets for video classification."
    },
    {
        "paperId": "86ee1835a56722b76564119437070782fc90eb19",
        "url": "https://www.semanticscholar.org/paper/86ee1835a56722b76564119437070782fc90eb19",
        "title": "Generative Adversarial Nets",
        "venue": "Neural Information Processing Systems",
        "year": 2014,
        "citationCount": 2278,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1406.2661, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "153440022",
                "name": "I. Goodfellow"
            },
            {
                "authorId": "1403025868",
                "name": "Jean Pouget-Abadie"
            },
            {
                "authorId": "153583218",
                "name": "Mehdi Mirza"
            },
            {
                "authorId": "2113742925",
                "name": "Bing Xu"
            },
            {
                "authorId": "1393680089",
                "name": "David Warde-Farley"
            },
            {
                "authorId": "1955694",
                "name": "Sherjil Ozair"
            },
            {
                "authorId": "1760871",
                "name": "Aaron C. Courville"
            },
            {
                "authorId": "1751762",
                "name": "Yoshua Bengio"
            }
        ],
        "abstract": "We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to  everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples."
    },
    {
        "paperId": "8a756d4d25511d92a45d0f4545fa819de993851d",
        "url": "https://www.semanticscholar.org/paper/8a756d4d25511d92a45d0f4545fa819de993851d",
        "title": "Recurrent Models of Visual Attention",
        "venue": "Neural Information Processing Systems",
        "year": 2014,
        "citationCount": 3874,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1406.6247, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3255983",
                "name": "Volodymyr Mnih"
            },
            {
                "authorId": "2801204",
                "name": "N. Heess"
            },
            {
                "authorId": "1753223",
                "name": "Alex Graves"
            },
            {
                "authorId": "2645384",
                "name": "K. Kavukcuoglu"
            }
        ],
        "abstract": "Applying convolutional neural networks to large images is computationally expensive because the amount of computation scales linearly with the number of image pixels. We present a novel recurrent neural network model that is capable of extracting information from an image or video by adaptively selecting a sequence of regions or locations and only processing the selected regions at high resolution. Like convolutional neural networks, the proposed model has a degree of translation invariance built-in, but the amount of computation it performs can be controlled independently of the input image size. While the model is non-differentiable, it can be trained using reinforcement learning methods to learn task-specific policies. We evaluate our model on several image classification tasks, where it significantly outperforms a convolutional neural network baseline on cluttered images, and on a dynamic visual control problem, where it learns to track a simple object without an explicit training signal for doing so."
    },
    {
        "paperId": "91bdaf3f1226e4065c4296d5c362906ceadfc631",
        "url": "https://www.semanticscholar.org/paper/91bdaf3f1226e4065c4296d5c362906ceadfc631",
        "title": "Deep Learning Face Representation by Joint Identification-Verification",
        "venue": "Neural Information Processing Systems",
        "year": 2014,
        "citationCount": 2287,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1406.4773, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2143795572",
                "name": "Yi Sun"
            },
            {
                "authorId": "2733115",
                "name": "Yuheng Chen"
            },
            {
                "authorId": "31843833",
                "name": "Xiaogang Wang"
            },
            {
                "authorId": "50295995",
                "name": "Xiaoou Tang"
            }
        ],
        "abstract": "The key challenge of face recognition is to develop effective feature representations for reducing intra-personal variations while enlarging inter-personal differences. In this paper, we show that it can be well solved with deep learning and using both face identification and verification signals as supervision. The Deep IDentification-verification features (DeepID2) are learned with carefully designed deep convolutional networks. The face identification task increases the inter-personal variations by drawing DeepID2 features extracted from different identities apart, while the face verification task reduces the intra-personal variations by pulling DeepID2 features extracted from the same identity together, both of which are essential to face recognition. The learned DeepID2 features can be well generalized to new identities unseen in the training data. On the challenging LFW dataset [11], 99.15% face verification accuracy is achieved. Compared with the best previous deep learning result [20] on LFW, the error rate has been significantly reduced by 67%."
    },
    {
        "paperId": "9667f8264745b626c6173b1310e2ff0298b09cfc",
        "url": "https://www.semanticscholar.org/paper/9667f8264745b626c6173b1310e2ff0298b09cfc",
        "title": "Learning Deep Features for Scene Recognition using Places Database",
        "venue": "Neural Information Processing Systems",
        "year": 2014,
        "citationCount": 3066,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "145291669",
                "name": "Bolei Zhou"
            },
            {
                "authorId": "2677488",
                "name": "gata Lapedriza"
            },
            {
                "authorId": "40599257",
                "name": "Jianxiong Xiao"
            },
            {
                "authorId": "143805211",
                "name": "A. Torralba"
            },
            {
                "authorId": "143868587",
                "name": "A. Oliva"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "981ce6b655cc06416ff6bf7fac8c6c2076fd7fac",
        "url": "https://www.semanticscholar.org/paper/981ce6b655cc06416ff6bf7fac8c6c2076fd7fac",
        "title": "Identifying and attacking the saddle point problem in high-dimensional non-convex optimization",
        "venue": "Neural Information Processing Systems",
        "year": 2014,
        "citationCount": 1460,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1406.2572, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2921469",
                "name": "Yann Dauphin"
            },
            {
                "authorId": "1996134",
                "name": "Razvan Pascanu"
            },
            {
                "authorId": "1854385",
                "name": "aglar Glehre"
            },
            {
                "authorId": "1979489",
                "name": "Kyunghyun Cho"
            },
            {
                "authorId": "25769960",
                "name": "S. Ganguli"
            },
            {
                "authorId": "1751762",
                "name": "Yoshua Bengio"
            }
        ],
        "abstract": "A central challenge to many fields of science and engineering involves minimizing non-convex error functions over continuous, high dimensional spaces. Gradient descent or quasi-Newton methods are almost ubiquitously used to perform such minimizations, and it is often thought that a main source of difficulty for these local methods to find the global minimum is the proliferation of local minima with much higher error than the global minimum. Here we argue, based on results from statistical physics, random matrix theory, neural network theory, and empirical evidence, that a deeper and more profound difficulty originates from the proliferation of saddle points, not local minima, especially in high dimensional problems of practical interest. Such saddle points are surrounded by high error plateaus that can dramatically slow down learning, and give the illusory impression of the existence of a local minimum. Motivated by these arguments, we propose a new approach to second-order optimization, the saddle-free Newton method, that can rapidly escape high dimensional saddle points, unlike gradient descent and quasi-Newton methods. We apply this algorithm to deep or recurrent neural network training, and provide numerical evidence for its superior optimization performance."
    },
    {
        "paperId": "9f08b01251cb99f4ffae8c7b3e4468d3af9c98d3",
        "url": "https://www.semanticscholar.org/paper/9f08b01251cb99f4ffae8c7b3e4468d3af9c98d3",
        "title": "Convolutional Neural Network Architectures for Matching Natural Language Sentences",
        "venue": "Neural Information Processing Systems",
        "year": 2014,
        "citationCount": 1366,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1503.03244, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "33968873",
                "name": "Baotian Hu"
            },
            {
                "authorId": "11955007",
                "name": "Zhengdong Lu"
            },
            {
                "authorId": "49404233",
                "name": "Hang Li"
            },
            {
                "authorId": "144159781",
                "name": "Qingcai Chen"
            }
        ],
        "abstract": "Semantic matching is of central importance to many natural language tasks [2,28]. A successful matching algorithm needs to adequately model the internal structures of language objects and the interaction between them. As a step toward this goal, we propose convolutional neural network models for matching two sentences, by adapting the convolutional strategy in vision and speech. The proposed models not only nicely represent the hierarchical structures of sentences with their layer-by-layer composition and pooling, but also capture the rich matching patterns at different levels. Our models are rather generic, requiring no prior knowledge on language, and can hence be applied to matching tasks of different nature and in different languages. The empirical study on a variety of matching tasks demonstrates the efficacy of the proposed model on a variety of matching tasks and its superiority to competitor models."
    },
    {
        "paperId": "a6fd96a900d4130940b488863b71fd09ad41ccb9",
        "url": "https://www.semanticscholar.org/paper/a6fd96a900d4130940b488863b71fd09ad41ccb9",
        "title": "Discriminative Unsupervised Feature Learning with Convolutional Neural Networks",
        "venue": "Neural Information Processing Systems",
        "year": 2014,
        "citationCount": 1047,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1406.6909, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2841331",
                "name": "Alexey Dosovitskiy"
            },
            {
                "authorId": "2060551",
                "name": "Jost Tobias Springenberg"
            },
            {
                "authorId": "3137672",
                "name": "Martin A. Riedmiller"
            },
            {
                "authorId": "1710872",
                "name": "T. Brox"
            }
        ],
        "abstract": "Current methods for training convolutional neural networks depend on large amounts of labeled samples for supervised training. In this paper we present an approach for training a convolutional neural network using only unlabeled data. We train the network to discriminate between a set of surrogate classes. Each surrogate class is formed by applying a variety of transformations to a randomly sampled 'seed' image patch. We find that this simple feature learning algorithm is surprisingly successful when applied to visual object recognition. The feature representation learned by our algorithm achieves classification results matching or outperforming the current state-of-the-art for unsupervised learning on several popular datasets (STL-10, CIFAR-10, Caltech-101)."
    },
    {
        "paperId": "b034b5769ab94acf9fb8ae48c7edb560a300bb63",
        "url": "https://www.semanticscholar.org/paper/b034b5769ab94acf9fb8ae48c7edb560a300bb63",
        "title": "On the Number of Linear Regions of Deep Neural Networks",
        "venue": "Neural Information Processing Systems",
        "year": 2014,
        "citationCount": 1329,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1402.1869, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1784667",
                "name": "Guido Montfar"
            },
            {
                "authorId": "1996134",
                "name": "Razvan Pascanu"
            },
            {
                "authorId": "1979489",
                "name": "Kyunghyun Cho"
            },
            {
                "authorId": "1751762",
                "name": "Yoshua Bengio"
            }
        ],
        "abstract": "We study the complexity of functions computable by deep feedforward neural networks with piecewise linear activations in terms of the symmetries and the number of linear regions that they have. Deep networks are able to sequentially map portions of each layer's input-space to the same output. In this way, deep models compute functions that react equally to complicated patterns of different inputs. The compositional structure of these functions enables them to re-use pieces of computation exponentially often in terms of the network's depth. This paper investigates the complexity of such compositional maps and contributes new theoretical results regarding the advantage of depth for neural networks with piecewise linear activation functions. In particular, our analysis is not specific to a single family of models, and as an example, we employ it for rectifier and maxout networks. We improve complexity bounds from pre-existing work and investigate the behavior of units in higher layers."
    },
    {
        "paperId": "cea967b59209c6be22829699f05b8b1ac4dc092d",
        "url": "https://www.semanticscholar.org/paper/cea967b59209c6be22829699f05b8b1ac4dc092d",
        "title": "Sequence to Sequence Learning with Neural Networks",
        "venue": "Neural Information Processing Systems",
        "year": 2014,
        "citationCount": 21538,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1409.3215, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1701686",
                "name": "I. Sutskever"
            },
            {
                "authorId": "1689108",
                "name": "O. Vinyals"
            },
            {
                "authorId": "2827616",
                "name": "Quoc V. Le"
            }
        ],
        "abstract": "Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT-14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous state of the art. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier."
    },
    {
        "paperId": "dd2cf76ae78a3262a094ac865aa9f60c55472c5d",
        "url": "https://www.semanticscholar.org/paper/dd2cf76ae78a3262a094ac865aa9f60c55472c5d",
        "title": "Depth Map Prediction from a Single Image using a Multi-Scale Deep Network",
        "venue": "Neural Information Processing Systems",
        "year": 2014,
        "citationCount": 4433,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1406.2283, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2060028",
                "name": "D. Eigen"
            },
            {
                "authorId": "1940183",
                "name": "Christian Puhrsch"
            },
            {
                "authorId": "2276554",
                "name": "R. Fergus"
            }
        ],
        "abstract": "Predicting depth is an essential component in understanding the 3D geometry of a scene. While for stereo images local correspondence suffices for estimation, finding depth relations from a single image is less straightforward, requiring integration of both global and local information from various cues. Moreover, the task is inherently ambiguous, with a large source of uncertainty coming from the overall scale. In this paper, we present a new method that addresses this task by employing two deep network stacks: one that makes a coarse global prediction based on the entire image, and another that refines this prediction locally. We also apply a scale-invariant error to help measure depth relations rather than scale. By leveraging the raw datasets as large sources of training data, our method achieves state-of-the-art results on both NYU Depth and KITTI, and matches detailed depth boundaries without the need for superpixelation."
    },
    {
        "paperId": "e5ae8ab688051931b4814f6d32b18391f8d1fa8d",
        "url": "https://www.semanticscholar.org/paper/e5ae8ab688051931b4814f6d32b18391f8d1fa8d",
        "title": "Exploiting Linear Structure Within Convolutional Networks for Efficient Evaluation",
        "venue": "Neural Information Processing Systems",
        "year": 2014,
        "citationCount": 1775,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1404.0736, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "40081727",
                "name": "Emily L. Denton"
            },
            {
                "authorId": "2563432",
                "name": "Wojciech Zaremba"
            },
            {
                "authorId": "143627859",
                "name": "Joan Bruna"
            },
            {
                "authorId": "1688882",
                "name": "Yann LeCun"
            },
            {
                "authorId": "2276554",
                "name": "R. Fergus"
            }
        ],
        "abstract": "We present techniques for speeding up the test-time evaluation of large convolutional networks, designed for object recognition tasks. These models deliver impressive accuracy, but each image evaluation requires millions of floating point operations, making their deployment on smartphones and Internet-scale clusters problematic. The computation is dominated by the convolution operations in the lower layers of the model. We exploit the redundancy present within the convolutional filters to derive approximations that significantly reduce the required computation. Using large state-of-the-art models, we demonstrate speedups of convolutional layers on both CPU and GPU by a factor of 2 x, while keeping the accuracy within 1% of the original model."
    },
    {
        "paperId": "f4c018bcc8ea707b83247866bdc8ccb87cd9f5da",
        "url": "https://www.semanticscholar.org/paper/f4c018bcc8ea707b83247866bdc8ccb87cd9f5da",
        "title": "Neural Word Embedding as Implicit Matrix Factorization",
        "venue": "Neural Information Processing Systems",
        "year": 2014,
        "citationCount": 1983,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "39455775",
                "name": "Omer Levy"
            },
            {
                "authorId": "2089067",
                "name": "Yoav Goldberg"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "0c1f9ca23f4f09ecfc44bcc8ca1c2736624f4652",
        "url": "https://www.semanticscholar.org/paper/0c1f9ca23f4f09ecfc44bcc8ca1c2736624f4652",
        "title": "A Theoretically Grounded Application of Dropout in Recurrent Neural Networks",
        "venue": "Neural Information Processing Systems",
        "year": 2015,
        "citationCount": 1696,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1512.05287, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2681954",
                "name": "Y. Gal"
            },
            {
                "authorId": "1744700",
                "name": "Zoubin Ghahramani"
            }
        ],
        "abstract": "Recurrent neural networks (RNNs) stand at the forefront of many recent developments in deep learning. Yet a major difficulty with these models is their tendency to overfit, with dropout shown to fail when applied to recurrent layers. Recent results at the intersection of Bayesian modelling and deep learning offer a Bayesian interpretation of common deep learning techniques such as dropout. This grounding of dropout in approximate Bayesian inference suggests an extension of the theoretical results, offering insights into the use of dropout with RNN models. We apply this new variational inference based dropout technique in LSTM and GRU models, assessing it on language modelling and sentiment analysis tasks. The new approach outperforms existing techniques, and to the best of our knowledge improves on the single model state-of-the-art in language modelling with the Penn Treebank (73.4 test perplexity). This extends our arsenal of variational tools in deep learning."
    },
    {
        "paperId": "0c3b69b5247ef18fd5bab1109d87a04184ea8f4b",
        "url": "https://www.semanticscholar.org/paper/0c3b69b5247ef18fd5bab1109d87a04184ea8f4b",
        "title": "A Recurrent Latent Variable Model for Sequential Data",
        "venue": "Neural Information Processing Systems",
        "year": 2015,
        "citationCount": 1319,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1506.02216, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "8270717",
                "name": "Junyoung Chung"
            },
            {
                "authorId": "2182706",
                "name": "Kyle Kastner"
            },
            {
                "authorId": "46573521",
                "name": "Laurent Dinh"
            },
            {
                "authorId": "2957685",
                "name": "Kratarth Goel"
            },
            {
                "authorId": "1760871",
                "name": "Aaron C. Courville"
            },
            {
                "authorId": "1751762",
                "name": "Yoshua Bengio"
            }
        ],
        "abstract": "In this paper, we explore the inclusion of latent random variables into the hidden state of a recurrent neural network (RNN) by combining the elements of the variational autoencoder. We argue that through the use of high-level latent random variables, the variational RNN (VRNN)1 can model the kind of variability observed in highly structured sequential data such as natural speech. We empirically evaluate the proposed model against other related sequential models on four speech datasets and one handwriting dataset. Our results show the important roles that latent random variables can play in the RNN dynamics."
    },
    {
        "paperId": "0d0eeb46fc5ec778a62bb94aa2ef261b08e6f8c6",
        "url": "https://www.semanticscholar.org/paper/0d0eeb46fc5ec778a62bb94aa2ef261b08e6f8c6",
        "title": "Texture Synthesis Using Convolutional Neural Networks",
        "venue": "Neural Information Processing Systems",
        "year": 2015,
        "citationCount": 1412,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "1891828",
                "name": "Leon A. Gatys"
            },
            {
                "authorId": "1746183",
                "name": "Alexander S. Ecker"
            },
            {
                "authorId": "1731199",
                "name": "M. Bethge"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "18b47b83a373f33d6b902a3615f42c10f7600d72",
        "url": "https://www.semanticscholar.org/paper/18b47b83a373f33d6b902a3615f42c10f7600d72",
        "title": "Diffusion-Convolutional Neural Networks",
        "venue": "Neural Information Processing Systems",
        "year": 2015,
        "citationCount": 1337,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1511.02136, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "144585309",
                "name": "James Atwood"
            },
            {
                "authorId": "1705427",
                "name": "D. Towsley"
            }
        ],
        "abstract": "We present diffusion-convolutional neural networks (DCNNs), a new model for graph-structured data. Through the introduction of a diffusion-convolution operation, we show how diffusion-based representations can be learned from graph-structured data and used as an effective basis for node classification. DCNNs have several attractive qualities, including a latent representation for graphical data that is invariant under isomorphism, as well as polynomial-time prediction and learning that can be represented as tensor operations and efficiently implemented on the GPU. Through several experiments with real structured datasets, we demonstrate that DCNNs are able to outperform probabilistic relational models and kernel-on-graph methods at relational node classification tasks."
    },
    {
        "paperId": "1eb131a34fbb508a9dd8b646950c65901d6f1a5b",
        "url": "https://www.semanticscholar.org/paper/1eb131a34fbb508a9dd8b646950c65901d6f1a5b",
        "title": "Hidden Technical Debt in Machine Learning Systems",
        "venue": "Neural Information Processing Systems",
        "year": 2015,
        "citationCount": 1239,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "1733143",
                "name": "D. Sculley"
            },
            {
                "authorId": "144510728",
                "name": "Gary Holt"
            },
            {
                "authorId": "145973657",
                "name": "D. Golovin"
            },
            {
                "authorId": "143698521",
                "name": "Eugene Davydov"
            },
            {
                "authorId": "2054375101",
                "name": "Todd Phillips"
            },
            {
                "authorId": "49236095",
                "name": "D. Ebner"
            },
            {
                "authorId": "2055477158",
                "name": "Vinay Chaudhary"
            },
            {
                "authorId": "2114084357",
                "name": "Michael Young"
            },
            {
                "authorId": "40169157",
                "name": "Jean-Franois Crespo"
            },
            {
                "authorId": "47019745",
                "name": "Dan Dennison"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "1ff9a37d766e3a4f39757f5e1b235a42dacf18ff",
        "url": "https://www.semanticscholar.org/paper/1ff9a37d766e3a4f39757f5e1b235a42dacf18ff",
        "title": "Learning both Weights and Connections for Efficient Neural Network",
        "venue": "Neural Information Processing Systems",
        "year": 2015,
        "citationCount": 7288,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1506.02626, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "143840275",
                "name": "Song Han"
            },
            {
                "authorId": "47325862",
                "name": "Jeff Pool"
            },
            {
                "authorId": "2066786849",
                "name": "J. Tran"
            },
            {
                "authorId": "80724002",
                "name": "W. Dally"
            }
        ],
        "abstract": "Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems. Also, conventional networks fix the architecture before training starts; as a result, training cannot improve the architecture. To address these limitations, we describe a method to reduce the storage and computation required by neural networks by an order of magnitude without affecting their accuracy by learning only the important connections. Our method prunes redundant connections using a three-step method. First, we train the network to learn which connections are important. Next, we prune the unimportant connections. Finally, we retrain the network to fine tune the weights of the remaining connections. On the ImageNet dataset, our method reduced the number of parameters of AlexNet by a factor of 9x, from 61 million to 6.7 million, without incurring accuracy loss. Similar experiments with VGG-16 found that the number of parameters can be reduced by 13x, from 138 million to 10.3 million, again with no loss of accuracy."
    },
    {
        "paperId": "3f25e17eb717e5894e0404ea634451332f85d287",
        "url": "https://www.semanticscholar.org/paper/3f25e17eb717e5894e0404ea634451332f85d287",
        "title": "Learning Structured Output Representation using Deep Conditional Generative Models",
        "venue": "Neural Information Processing Systems",
        "year": 2015,
        "citationCount": 3580,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "1729571",
                "name": "Kihyuk Sohn"
            },
            {
                "authorId": "1697141",
                "name": "Honglak Lee"
            },
            {
                "authorId": "3084614",
                "name": "Xinchen Yan"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "4443e2e5bfd112562ecef578f772cee3c692f19f",
        "url": "https://www.semanticscholar.org/paper/4443e2e5bfd112562ecef578f772cee3c692f19f",
        "title": "Variational Dropout and the Local Reparameterization Trick",
        "venue": "Neural Information Processing Systems",
        "year": 2015,
        "citationCount": 1610,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1506.02557, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1726807",
                "name": "Diederik P. Kingma"
            },
            {
                "authorId": "2887364",
                "name": "Tim Salimans"
            },
            {
                "authorId": "1678311",
                "name": "M. Welling"
            }
        ],
        "abstract": "We investigate a local reparameterizaton technique for greatly reducing the variance of stochastic gradients for variational Bayesian inference (SGVB) of a posterior over model parameters, while retaining parallelizability. This local reparameterization translates uncertainty about global parameters into local noise that is independent across datapoints in the minibatch. Such parameterizations can be trivially parallelized and have variance that is inversely proportional to the mini-batch size, generally leading to much faster convergence. Additionally, we explore a connection with dropout: Gaussian dropout objectives correspond to SGVB with local reparameterization, a scale-invariant prior and proportionally fixed posterior variance. Our method allows inference of more flexibly parameterized posteriors; specifically, we propose variational dropout, a generalization of Gaussian dropout where the dropout rates are learned, often leading to better models. The method is demonstrated through several experiments."
    },
    {
        "paperId": "47900aca2f0b50da3010ad59b394c870f0e6c02e",
        "url": "https://www.semanticscholar.org/paper/47900aca2f0b50da3010ad59b394c870f0e6c02e",
        "title": "Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks",
        "venue": "Neural Information Processing Systems",
        "year": 2015,
        "citationCount": 2312,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1506.05751, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "40081727",
                "name": "Emily L. Denton"
            },
            {
                "authorId": "2127604",
                "name": "Soumith Chintala"
            },
            {
                "authorId": "3149531",
                "name": "Arthur Szlam"
            },
            {
                "authorId": "2276554",
                "name": "R. Fergus"
            }
        ],
        "abstract": "In this paper we introduce a generative parametric model capable of producing high quality samples of natural images. Our approach uses a cascade of convolutional networks within a Laplacian pyramid framework to generate images in a coarse-to-fine fashion. At each level of the pyramid, a separate generative convnet model is trained using the Generative Adversarial Nets (GAN) approach [11]. Samples drawn from our model are of significantly higher quality than alternate approaches. In a quantitative assessment by human evaluators, our CIFAR10 samples were mistaken for real images around 40% of the time, compared to 10% for samples drawn from a GAN baseline model. We also show samples from models trained on the higher resolution images of the LSUN scene dataset."
    },
    {
        "paperId": "4aa9f5150b46320f534de4747a2dd0cd7f3fe292",
        "url": "https://www.semanticscholar.org/paper/4aa9f5150b46320f534de4747a2dd0cd7f3fe292",
        "title": "Semi-supervised Sequence Learning",
        "venue": "Neural Information Processing Systems",
        "year": 2015,
        "citationCount": 1269,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1511.01432, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2555924",
                "name": "Andrew M. Dai"
            },
            {
                "authorId": "2827616",
                "name": "Quoc V. Le"
            }
        ],
        "abstract": "We present two approaches to use unlabeled data to improve Sequence Learning with recurrent networks. The first approach is to predict what comes next in a sequence, which is a language model in NLP. The second approach is to use a sequence autoencoder, which reads the input sequence into a vector and predicts the input sequence again. These two algorithms can be used as a \"pretraining\" algorithm for a later supervised sequence learning algorithm. In other words, the parameters obtained from the pretraining step can then be used as a starting point for other supervised training models. In our experiments, we find that long short term memory recurrent networks after pretrained with the two approaches become more stable to train and generalize better. With pretraining, we were able to achieve strong performance in many classification tasks, such as text classification with IMDB, DBpedia or image recognition in CIFAR-10."
    },
    {
        "paperId": "4f10b9f47c5bb6b54dd4f5ca8d9fa2c0bbd7ec5e",
        "url": "https://www.semanticscholar.org/paper/4f10b9f47c5bb6b54dd4f5ca8d9fa2c0bbd7ec5e",
        "title": "End-To-End Memory Networks",
        "venue": "Neural Information Processing Systems",
        "year": 2015,
        "citationCount": 2666,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "2265067",
                "name": "Sainbayar Sukhbaatar"
            },
            {
                "authorId": "3149531",
                "name": "Arthur Szlam"
            },
            {
                "authorId": "145183709",
                "name": "J. Weston"
            },
            {
                "authorId": "2276554",
                "name": "R. Fergus"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "51a55df1f023571a7e07e338ee45a3e3d66ef73e",
        "url": "https://www.semanticscholar.org/paper/51a55df1f023571a7e07e338ee45a3e3d66ef73e",
        "title": "Character-level Convolutional Networks for Text Classification",
        "venue": "Neural Information Processing Systems",
        "year": 2015,
        "citationCount": 6678,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1509.01626, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": null,
                "name": "Xiang Zhang"
            },
            {
                "authorId": "7818229",
                "name": "J. Zhao"
            },
            {
                "authorId": "1688882",
                "name": "Yann LeCun"
            }
        ],
        "abstract": "This article offers an empirical exploration on the use of character-level convolutional networks (ConvNets) for text classification. We constructed several large-scale datasets to show that character-level convolutional networks could achieve state-of-the-art or competitive results. Comparisons are offered against traditional models such as bag of words, n-grams and their TFIDF variants, and deep learning models such as word-based ConvNets and recurrent neural networks."
    },
    {
        "paperId": "5d1bfeed240709725c78bc72ea40e55410b373dc",
        "url": "https://www.semanticscholar.org/paper/5d1bfeed240709725c78bc72ea40e55410b373dc",
        "title": "Convolutional Networks on Graphs for Learning Molecular Fingerprints",
        "venue": "Neural Information Processing Systems",
        "year": 2015,
        "citationCount": 3533,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1509.09292, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1704657",
                "name": "D. Duvenaud"
            },
            {
                "authorId": "1683298",
                "name": "D. Maclaurin"
            },
            {
                "authorId": "1422175619",
                "name": "J. Aguilera-Iparraguirre"
            },
            {
                "authorId": "2344011563",
                "name": "Rafael Gmez-Bombarelli"
            },
            {
                "authorId": "145916942",
                "name": "Timothy D. Hirzel"
            },
            {
                "authorId": "1380248954",
                "name": "Aln Aspuru-Guzik"
            },
            {
                "authorId": "1722180",
                "name": "Ryan P. Adams"
            }
        ],
        "abstract": "We introduce a convolutional neural network that operates directly on graphs. These networks allow end-to-end learning of prediction pipelines whose inputs are graphs of arbitrary size and shape. The architecture we present generalizes standard molecular feature extraction methods based on circular fingerprints. We show that these data-driven features are more interpretable, and have better predictive performance on a variety of tasks."
    },
    {
        "paperId": "6e795c6e9916174ae12349f5dc3f516570c17ce8",
        "url": "https://www.semanticscholar.org/paper/6e795c6e9916174ae12349f5dc3f516570c17ce8",
        "title": "Skip-Thought Vectors",
        "venue": "Neural Information Processing Systems",
        "year": 2015,
        "citationCount": 2465,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1506.06726, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3450996",
                "name": "Ryan Kiros"
            },
            {
                "authorId": "2334885409",
                "name": "Yukun Zhu"
            },
            {
                "authorId": "145124475",
                "name": "R. Salakhutdinov"
            },
            {
                "authorId": "1804104",
                "name": "R. Zemel"
            },
            {
                "authorId": "2422559",
                "name": "R. Urtasun"
            },
            {
                "authorId": "143805211",
                "name": "A. Torralba"
            },
            {
                "authorId": "37895334",
                "name": "S. Fidler"
            }
        ],
        "abstract": "We describe an approach for unsupervised learning of a generic, distributed sentence encoder. Using the continuity of text from books, we train an encoder-decoder model that tries to reconstruct the surrounding sentences of an encoded passage. Sentences that share semantic and syntactic properties are thus mapped to similar vector representations. We next introduce a simple vocabulary expansion method to encode words that were not seen as part of training, allowing us to expand our vocabulary to a million words. After training our model, we extract and evaluate our vectors with linear models on 8 tasks: semantic relatedness, paraphrase detection, image-sentence ranking, question-type classification and 4 benchmark sentiment and subjectivity datasets. The end result is an off-the-shelf encoder that can produce highly generic sentence representations that are robust and perform well in practice."
    },
    {
        "paperId": "775a4e375cc79b53b94e37fa3eedff481823e4a6",
        "url": "https://www.semanticscholar.org/paper/775a4e375cc79b53b94e37fa3eedff481823e4a6",
        "title": "Efficient and Robust Automated Machine Learning",
        "venue": "Neural Information Processing Systems",
        "year": 2015,
        "citationCount": 1819,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "2868444",
                "name": "Matthias Feurer"
            },
            {
                "authorId": "145227684",
                "name": "Aaron Klein"
            },
            {
                "authorId": "2607675",
                "name": "Katharina Eggensperger"
            },
            {
                "authorId": "2060551",
                "name": "Jost Tobias Springenberg"
            },
            {
                "authorId": "2058090778",
                "name": "Manuel Blum"
            },
            {
                "authorId": "144661829",
                "name": "F. Hutter"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "9653d5c2c7844347343d073bbedd96e05d52f69b",
        "url": "https://www.semanticscholar.org/paper/9653d5c2c7844347343d073bbedd96e05d52f69b",
        "title": "Pointer Networks",
        "venue": "Neural Information Processing Systems",
        "year": 2015,
        "citationCount": 3291,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1506.03134, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1689108",
                "name": "O. Vinyals"
            },
            {
                "authorId": "39067762",
                "name": "Meire Fortunato"
            },
            {
                "authorId": "3111912",
                "name": "N. Jaitly"
            }
        ],
        "abstract": "We introduce a new neural architecture to learn the conditional probability of an output sequence with elements that are discrete tokens corresponding to positions in an input sequence. Such problems cannot be trivially addressed by existent approaches such as sequence-to-sequence [1] and Neural Turing Machines [2], because the number of target classes in each step of the output depends on the length of the input, which is variable. Problems such as sorting variable sized sequences, and various combinatorial optimization problems belong to this class. Our model solves the problem of variable size output dictionaries using a recently proposed mechanism of neural attention. It differs from the previous attention attempts in that, instead of using attention to blend hidden units of an encoder to a context vector at each decoder step, it uses attention as a pointer to select a member of the input sequence as the output. We call this architecture a Pointer Net (Ptr-Net). We show Ptr-Nets can be used to learn approximate solutions to three challenging geometric problems - finding planar convex hulls, computing Delaunay triangulations, and the planar Travelling Salesman Problem - using training examples alone. Ptr-Nets not only improve over sequence-to-sequence with input attention, but also allow us to generalize to variable size output dictionaries. We show that the learnt models generalize beyond the maximum lengths they were trained on. We hope our results on these tasks will encourage a broader exploration of neural learning for discrete problems."
    },
    {
        "paperId": "a5733ff08daff727af834345b9cfff1d0aa109ec",
        "url": "https://www.semanticscholar.org/paper/a5733ff08daff727af834345b9cfff1d0aa109ec",
        "title": "BinaryConnect: Training Deep Neural Networks with binary weights during propagations",
        "venue": "Neural Information Processing Systems",
        "year": 2015,
        "citationCount": 3129,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1511.00363, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2388466",
                "name": "Matthieu Courbariaux"
            },
            {
                "authorId": "1751762",
                "name": "Yoshua Bengio"
            },
            {
                "authorId": "145719986",
                "name": "J. David"
            }
        ],
        "abstract": "Deep Neural Networks (DNN) have achieved state-of-the-art results in a wide range of tasks, with the best results obtained with large training sets and large models. In the past, GPUs enabled these breakthroughs because of their greater computational speed. In the future, faster computation at both training and test time is likely to be crucial for further progress and for consumer applications on low-power devices. As a result, there is much interest in research and development of dedicated hardware for Deep Learning (DL). Binary weights, i.e., weights which are constrained to only two possible values (e.g. -1 or 1), would bring great benefits to specialized DL hardware by replacing many multiply-accumulate operations by simple accumulations, as multipliers are the most space and power-hungry components of the digital implementation of neural networks. We introduce BinaryConnect, a method which consists in training a DNN with binary weights during the forward and backward propagations, while retaining precision of the stored weights in which gradients are accumulated. Like other dropout schemes, we show that BinaryConnect acts as regularizer and we obtain near state-of-the-art results with BinaryConnect on the permutation-invariant MNIST, CIFAR-10 and SVHN."
    },
    {
        "paperId": "b624504240fa52ab76167acfe3156150ca01cf3b",
        "url": "https://www.semanticscholar.org/paper/b624504240fa52ab76167acfe3156150ca01cf3b",
        "title": "Attention-Based Models for Speech Recognition",
        "venue": "Neural Information Processing Systems",
        "year": 2015,
        "citationCount": 2708,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1506.07503, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2292403",
                "name": "J. Chorowski"
            },
            {
                "authorId": "3335364",
                "name": "Dzmitry Bahdanau"
            },
            {
                "authorId": "1862138",
                "name": "Dmitriy Serdyuk"
            },
            {
                "authorId": "1979489",
                "name": "Kyunghyun Cho"
            },
            {
                "authorId": "1751762",
                "name": "Yoshua Bengio"
            }
        ],
        "abstract": "Recurrent sequence generators conditioned on input data through an attention mechanism have recently shown very good performance on a range of tasks including machine translation, handwriting synthesis [1,2] and image caption generation [3]. We extend the attention-mechanism with features needed for speech recognition. We show that while an adaptation of the model used for machine translation in [2] reaches a competitive 18.7% phoneme error rate (PER) on the TIMET phoneme recognition task, it can only be applied to utterances which are roughly as long as the ones it was trained on. We offer a qualitative explanation of this failure and propose a novel and generic method of adding location-awareness to the attention mechanism to alleviate this issue. The new method yields a model that is robust to long inputs and achieves 18% PER in single utterances and 20% in 10-times longer (repeated) utterances. Finally, we propose a change to the attention mechanism that prevents it from concentrating too much on single frames, which further reduces PER to 17.6% level."
    },
    {
        "paperId": "b92aa7024b87f50737b372e5df31ef091ab54e62",
        "url": "https://www.semanticscholar.org/paper/b92aa7024b87f50737b372e5df31ef091ab54e62",
        "title": "Training Very Deep Networks",
        "venue": "Neural Information Processing Systems",
        "year": 2015,
        "citationCount": 1726,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1507.06228, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2100612",
                "name": "R. Srivastava"
            },
            {
                "authorId": "3035541",
                "name": "Klaus Greff"
            },
            {
                "authorId": "145341374",
                "name": "J. Schmidhuber"
            }
        ],
        "abstract": "Theoretical and empirical evidence indicates that the depth of neural networks is crucial for their success. However, training becomes more difficult as depth increases, and training of very deep networks remains an open problem. Here we introduce a new architecture designed to overcome this. Our so-called highway networks allow unimpeded information flow across many layers on information highways. They are inspired by Long Short-Term Memory recurrent networks and use adaptive gating units to regulate the information flow. Even with hundreds of layers, highway networks can be trained directly through simple gradient descent. This enables the study of extremely deep and efficient architectures."
    },
    {
        "paperId": "cddf8a10c7f48df67a797808a615be0d4acf9a8e",
        "url": "https://www.semanticscholar.org/paper/cddf8a10c7f48df67a797808a615be0d4acf9a8e",
        "title": "Semi-supervised Learning with Ladder Networks",
        "venue": "Neural Information Processing Systems",
        "year": 2015,
        "citationCount": 1410,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1507.02672, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2019807",
                "name": "Antti Rasmus"
            },
            {
                "authorId": "2438071",
                "name": "Mathias Berglund"
            },
            {
                "authorId": "2263102",
                "name": "M. Honkala"
            },
            {
                "authorId": "2132516",
                "name": "Harri Valpola"
            },
            {
                "authorId": "2785022",
                "name": "T. Raiko"
            }
        ],
        "abstract": "We combine supervised learning with unsupervised learning in deep neural networks. The proposed model is trained to simultaneously minimize the sum of supervised and unsupervised cost functions by backpropagation, avoiding the need for layer-wise pre-training. Our work builds on top of the Ladder network proposed by Valpola [1] which we extend by combining the model with supervision. We show that the resulting model reaches state-of-the-art performance in semi-supervised MNIST and CIFAR-10 classification in addition to permutation-invariant MNIST classification with all labels."
    },
    {
        "paperId": "d1505c6123c102e53eb19dff312cb25cea840b72",
        "url": "https://www.semanticscholar.org/paper/d1505c6123c102e53eb19dff312cb25cea840b72",
        "title": "Teaching Machines to Read and Comprehend",
        "venue": "Neural Information Processing Systems",
        "year": 2015,
        "citationCount": 3729,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1506.03340, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2910877",
                "name": "Karl Moritz Hermann"
            },
            {
                "authorId": "2367821",
                "name": "Toms Kocisk"
            },
            {
                "authorId": "1864353",
                "name": "Edward Grefenstette"
            },
            {
                "authorId": "2311318",
                "name": "L. Espeholt"
            },
            {
                "authorId": "2062879616",
                "name": "W. Kay"
            },
            {
                "authorId": "2573615",
                "name": "Mustafa Suleyman"
            },
            {
                "authorId": "1685771",
                "name": "Phil Blunsom"
            }
        ],
        "abstract": "Teaching machines to read natural language documents remains an elusive challenge. Machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen, but until now large scale training and test datasets have been missing for this type of evaluation. In this work we define a new methodology that resolves this bottleneck and provides large scale supervised reading comprehension data. This allows us to develop a class of attention based deep neural networks that learn to read real documents and answer complex questions with minimal prior knowledge of language structure."
    },
    {
        "paperId": "dbb6ded623159c867fbeca0772db7b2eb9489523",
        "url": "https://www.semanticscholar.org/paper/dbb6ded623159c867fbeca0772db7b2eb9489523",
        "title": "Spatial Transformer Networks",
        "venue": "Neural Information Processing Systems",
        "year": 2015,
        "citationCount": 7831,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1506.02025, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3093886",
                "name": "Max Jaderberg"
            },
            {
                "authorId": "34838386",
                "name": "K. Simonyan"
            },
            {
                "authorId": "1688869",
                "name": "Andrew Zisserman"
            },
            {
                "authorId": "2645384",
                "name": "K. Kavukcuoglu"
            }
        ],
        "abstract": "Convolutional Neural Networks define an exceptionally powerful class of models, but are still limited by the lack of ability to be spatially invariant to the input data in a computationally and parameter efficient manner. In this work we introduce a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network. This differentiable module can be inserted into existing convolutional architectures, giving neural networks the ability to actively spatially transform feature maps, conditional on the feature map itself, without any extra training supervision or modification to the optimisation process. We show that the use of spatial transformers results in models which learn invariance to translation, scale, rotation and more generic warping, resulting in state-of-the-art performance on several benchmarks, and for a number of classes of transformations."
    },
    {
        "paperId": "df137487e20ba7c6e1e2b9a1e749f2a578b5ad99",
        "url": "https://www.semanticscholar.org/paper/df137487e20ba7c6e1e2b9a1e749f2a578b5ad99",
        "title": "Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks",
        "venue": "Neural Information Processing Systems",
        "year": 2015,
        "citationCount": 2183,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1506.03099, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1751569",
                "name": "Samy Bengio"
            },
            {
                "authorId": "1689108",
                "name": "O. Vinyals"
            },
            {
                "authorId": "3111912",
                "name": "N. Jaitly"
            },
            {
                "authorId": "1846258",
                "name": "Noam Shazeer"
            }
        ],
        "abstract": "Recurrent Neural Networks can be trained to produce sequences of tokens given some input, as exemplified by recent results in machine translation and image captioning. The current approach to training them consists of maximizing the likelihood of each token in the sequence given the current (recurrent) state and the previous token. At inference, the unknown previous token is then replaced by a token generated by the model itself. This discrepancy between training and inference can yield errors that can accumulate quickly along the generated sequence. We propose a curriculum learning strategy to gently change the training process from a fully guided scheme using the true previous token, towards a less guided scheme which mostly uses the generated token instead. Experiments on several sequence prediction tasks show that this approach yields significant improvements. Moreover, it was used succesfully in our winning entry to the MSCOCO image captioning challenge, 2015."
    },
    {
        "paperId": "f9c990b1b5724e50e5632b94fdb7484ece8a6ce7",
        "url": "https://www.semanticscholar.org/paper/f9c990b1b5724e50e5632b94fdb7484ece8a6ce7",
        "title": "Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting",
        "venue": "Neural Information Processing Systems",
        "year": 2015,
        "citationCount": 9000,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1506.04214, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3008587",
                "name": "Xingjian Shi"
            },
            {
                "authorId": "2192200",
                "name": "Zhourong Chen"
            },
            {
                "authorId": "49528584",
                "name": "Hao Wang"
            },
            {
                "authorId": "1739816",
                "name": "D. Yeung"
            },
            {
                "authorId": "145771919",
                "name": "W. Wong"
            },
            {
                "authorId": "2183294",
                "name": "W. Woo"
            }
        ],
        "abstract": "The goal of precipitation nowcasting is to predict the future rainfall intensity in a local region over a relatively short period of time. Very few previous studies have examined this crucial and challenging weather forecasting problem from the machine learning perspective. In this paper, we formulate precipitation nowcasting as a spatiotemporal sequence forecasting problem in which both the input and the prediction target are spatiotemporal sequences. By extending the fully connected LSTM (FC-LSTM) to have convolutional structures in both the input-to-state and state-to-state transitions, we propose the convolutional LSTM (ConvLSTM) and use it to build an end-to-end trainable model for the precipitation nowcasting problem. Experiments show that our ConvLSTM network captures spatiotemporal correlations better and consistently outperforms FC-LSTM and the state-of-the-art operational ROVER algorithm for precipitation nowcasting."
    },
    {
        "paperId": "fa98d609eb14ce25dd73cd8713a5e284948b4ff4",
        "url": "https://www.semanticscholar.org/paper/fa98d609eb14ce25dd73cd8713a5e284948b4ff4",
        "title": "Deep Knowledge Tracing",
        "venue": "Neural Information Processing Systems",
        "year": 2015,
        "citationCount": 1391,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1506.05908, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2012749",
                "name": "C. Piech"
            },
            {
                "authorId": "3202393",
                "name": "J. Bassen"
            },
            {
                "authorId": "2136435893",
                "name": "Jonathan Huang"
            },
            {
                "authorId": "25769960",
                "name": "S. Ganguli"
            },
            {
                "authorId": "1764547",
                "name": "M. Sahami"
            },
            {
                "authorId": "2231847421",
                "name": "L. Guibas"
            },
            {
                "authorId": "1407546424",
                "name": "Jascha Narain Sohl-Dickstein"
            }
        ],
        "abstract": "Knowledge tracingwhere a machine models the knowledge of a student as they interact with courseworkis a well established problem in computer supported education. Though effectively modeling student knowledge would have high educational impact, the task has many inherent challenges. In this paper we explore the utility of using Recurrent Neural Networks (RNNs) to model student learning. The RNN family of models have important advantages over previous methods in that they do not require the explicit encoding of human domain knowledge, and can capture more complex representations of student knowledge. Using neural networks results in substantial improvements in prediction performance on a range of knowledge tracing datasets. Moreover the learned model can be used for intelligent curriculum design and allows straightforward interpretation and discovery of structure in student tasks. These results suggest a promising new line of research for knowledge tracing and an exemplary application task for RNNs."
    },
    {
        "paperId": "01a4f33da8ad94ced3cf58548b28dbbb44148571",
        "url": "https://www.semanticscholar.org/paper/01a4f33da8ad94ced3cf58548b28dbbb44148571",
        "title": "Understanding the Effective Receptive Field in Deep Convolutional Neural Networks",
        "venue": "Neural Information Processing Systems",
        "year": 2016,
        "citationCount": 2041,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1701.04128, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "49756115",
                "name": "Wenjie Luo"
            },
            {
                "authorId": "47002813",
                "name": "Yujia Li"
            },
            {
                "authorId": "2422559",
                "name": "R. Urtasun"
            },
            {
                "authorId": "1804104",
                "name": "R. Zemel"
            }
        ],
        "abstract": "We study characteristics of receptive fields of units in deep convolutional networks. The receptive field size is a crucial issue in many visual tasks, as the output must respond to large enough areas in the image to capture information about large objects. We introduce the notion of an effective receptive field size, and show that it both has a Gaussian distribution and only occupies a fraction of the full theoretical receptive field size. We analyze the effective receptive field in several architecture designs, and the effect of sub-sampling, skip connections, dropout and nonlinear activations on it. This leads to suggestions for ways to address its tendency to be too small."
    },
    {
        "paperId": "01cb4071a0a43aeef63e5d568ad5afe1fb8b2411",
        "url": "https://www.semanticscholar.org/paper/01cb4071a0a43aeef63e5d568ad5afe1fb8b2411",
        "title": "Domain Separation Networks",
        "venue": "Neural Information Processing Systems",
        "year": 2016,
        "citationCount": 1539,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1608.06019, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2732737",
                "name": "Konstantinos Bousmalis"
            },
            {
                "authorId": "2814229",
                "name": "George Trigeorgis"
            },
            {
                "authorId": "2286640",
                "name": "N. Silberman"
            },
            {
                "authorId": "1707347",
                "name": "Dilip Krishnan"
            },
            {
                "authorId": "1761978",
                "name": "D. Erhan"
            }
        ],
        "abstract": "The cost of large scale data collection and annotation often makes the application of machine learning algorithms to new tasks or datasets prohibitively expensive. One approach circumventing this cost is training models on synthetic data where annotations are provided automatically. Despite their appeal, such models often fail to generalize from synthetic to real images, necessitating domain adaptation algorithms to manipulate these models before they can be successfully applied. Existing approaches focus either on mapping representations from one domain to the other, or on learning to extract features that are invariant to the domain from which they were extracted. However, by focusing only on creating a mapping or shared representation between the two domains, they ignore the individual characteristics of each domain. We hypothesize that explicitly modeling what is unique to each domain can improve a model's ability to extract domain-invariant features. Inspired by work on private-shared component analysis, we explicitly learn to extract image representations that are partitioned into two subspaces: one component which is private to each domain and one which is shared across domains. Our model is trained to not only perform the task we care about in the source domain, but also to use the partitioned representation to reconstruct the images from both domains. Our novel architecture results in a model that outperforms the state-of-the-art on a range of unsupervised domain adaptation scenarios and additionally produces visualizations of the private and shared representations enabling interpretation of the domain adaptation process."
    },
    {
        "paperId": "0772905d40b9afa3dc087a88184f09f3b3e1464f",
        "url": "https://www.semanticscholar.org/paper/0772905d40b9afa3dc087a88184f09f3b3e1464f",
        "title": "Learning to Communicate with Deep Multi-Agent Reinforcement Learning",
        "venue": "Neural Information Processing Systems",
        "year": 2016,
        "citationCount": 1744,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1605.06676, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "145356667",
                "name": "Jakob N. Foerster"
            },
            {
                "authorId": "3365565",
                "name": "Yannis Assael"
            },
            {
                "authorId": "1737568",
                "name": "Nando de Freitas"
            },
            {
                "authorId": "1766767",
                "name": "Shimon Whiteson"
            }
        ],
        "abstract": "We consider the problem of multiple agents sensing and acting in environments with the goal of maximising their shared utility. In these environments, agents must learn communication protocols in order to share information that is needed to solve the tasks. By embracing deep neural networks, we are able to demonstrate end-to-end learning of protocols in complex environments inspired by communication riddles and multi-agent computer vision problems with partial observability. We propose two approaches for learning in these domains: Reinforced Inter-Agent Learning (RIAL) and Differentiable Inter-Agent Learning (DIAL). The former uses deep Q-learning, while the latter exploits the fact that, during learning, agents can backpropagate error derivatives through (noisy) communication channels. Hence, this approach uses centralised learning but decentralised execution. Our experiments introduce new environments for studying the learning of communication protocols and present a set of engineering innovations that are essential for success in these domains."
    },
    {
        "paperId": "0936352b78a52bc5d2b5e3f04233efc56664af51",
        "url": "https://www.semanticscholar.org/paper/0936352b78a52bc5d2b5e3f04233efc56664af51",
        "title": "Conditional Image Generation with PixelCNN Decoders",
        "venue": "Neural Information Processing Systems",
        "year": 2016,
        "citationCount": 2666,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1606.05328, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3422336",
                "name": "Aron van den Oord"
            },
            {
                "authorId": "2583391",
                "name": "Nal Kalchbrenner"
            },
            {
                "authorId": "2311318",
                "name": "L. Espeholt"
            },
            {
                "authorId": "2645384",
                "name": "K. Kavukcuoglu"
            },
            {
                "authorId": "1689108",
                "name": "O. Vinyals"
            },
            {
                "authorId": "1753223",
                "name": "Alex Graves"
            }
        ],
        "abstract": "This work explores conditional image generation with a new image density model based on the PixelCNN architecture. The model can be conditioned on any vector, including descriptive labels or tags, or latent embeddings created by other networks. When conditioned on class labels from the ImageNet database, the model is able to generate diverse, realistic scenes representing distinct animals, objects, landscapes and structures. When conditioned on an embedding produced by a convolutional network given a single image of an unseen face, it generates a variety of new portraits of the same person with different facial expressions, poses and lighting conditions. We also show that conditional PixelCNN can serve as a powerful decoder in an image autoencoder. Additionally, the gated convolutional layers in the proposed model improve the log-likelihood of PixelCNN to match the state-of-the-art performance of PixelRNN on ImageNet, with greatly reduced computational cost."
    },
    {
        "paperId": "18168aea48a22f6fe2fe407c0ff70083cba225a7",
        "url": "https://www.semanticscholar.org/paper/18168aea48a22f6fe2fe407c0ff70083cba225a7",
        "title": "Image Restoration Using Very Deep Convolutional Encoder-Decoder Networks with Symmetric Skip Connections",
        "venue": "Neural Information Processing Systems",
        "year": 2016,
        "citationCount": 1650,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "2598948",
                "name": "Xiao-Jiao Mao"
            },
            {
                "authorId": "1780381",
                "name": "Chunhua Shen"
            },
            {
                "authorId": "2020008",
                "name": "Yubin Yang"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "2a24b68ef180c0c8742bd494a55fb6f68864efed",
        "url": "https://www.semanticscholar.org/paper/2a24b68ef180c0c8742bd494a55fb6f68864efed",
        "title": "Residual Networks Behave Like Ensembles of Relatively Shallow Networks",
        "venue": "Neural Information Processing Systems",
        "year": 2016,
        "citationCount": 1088,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "2799898",
                "name": "Andreas Veit"
            },
            {
                "authorId": "3035230",
                "name": "Michael J. Wilber"
            },
            {
                "authorId": "50172592",
                "name": "Serge J. Belongie"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "3435491a6ccfd5cab1f3378c10636e9130610b9e",
        "url": "https://www.semanticscholar.org/paper/3435491a6ccfd5cab1f3378c10636e9130610b9e",
        "title": "Deep ADMM-Net for Compressive Sensing MRI",
        "venue": "Neural Information Processing Systems",
        "year": 2016,
        "citationCount": 1191,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "2108850237",
                "name": "Yan Yang"
            },
            {
                "authorId": null,
                "name": "Jian Sun"
            },
            {
                "authorId": "1680740",
                "name": "Huibin Li"
            },
            {
                "authorId": "98220533",
                "name": "Zongben Xu"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "372bc106c61e7eb004835e85bbfee997409f176a",
        "url": "https://www.semanticscholar.org/paper/372bc106c61e7eb004835e85bbfee997409f176a",
        "title": "Coupled Generative Adversarial Networks",
        "venue": "Neural Information Processing Systems",
        "year": 2016,
        "citationCount": 1694,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1606.07536, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "39793900",
                "name": "Ming-Yu Liu"
            },
            {
                "authorId": "2577513",
                "name": "Oncel Tuzel"
            }
        ],
        "abstract": "We propose coupled generative adversarial network (CoGAN) for learning a joint distribution of multi-domain images. In contrast to the existing approaches, which require tuples of corresponding images in different domains in the training set, CoGAN can learn a joint distribution without any tuple of corresponding images. It can learn a joint distribution with just samples drawn from the marginal distributions. This is achieved by enforcing a weight-sharing constraint that limits the network capacity and favors a joint distribution solution over a product of marginal distributions one. We apply CoGAN to several joint distribution learning tasks, including learning a joint distribution of color and depth images, and learning a joint distribution of face images with different attributes. For each task it successfully learns the joint distribution without any tuple of corresponding images. We also demonstrate its applications to domain adaptation and image transformation."
    },
    {
        "paperId": "3d2c6941a9b4608ba52b328369a3352db2092ae0",
        "url": "https://www.semanticscholar.org/paper/3d2c6941a9b4608ba52b328369a3352db2092ae0",
        "title": "Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks",
        "venue": "Neural Information Processing Systems",
        "year": 2016,
        "citationCount": 2038,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1602.07868, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2887364",
                "name": "Tim Salimans"
            },
            {
                "authorId": "1726807",
                "name": "Diederik P. Kingma"
            }
        ],
        "abstract": "We present weight normalization: a reparameterization of the weight vectors in a neural network that decouples the length of those weight vectors from their direction. By reparameterizing the weights in this way we improve the conditioning of the optimization problem and we speed up convergence of stochastic gradient descent. Our reparameterization is inspired by batch normalization but does not introduce any dependencies between the examples in a minibatch. This means that our method can also be applied successfully to recurrent models such as LSTMs and to noise-sensitive applications such as deep reinforcement learning or generative models, for which batch normalization is less well suited. Although our method is much simpler, it still provides much of the speed-up of full batch normalization. In addition, the computational overhead of our method is lower, permitting more optimization steps to be taken in the same amount of time. We demonstrate the usefulness of our method on applications in supervised image recognition, generative modelling, and deep reinforcement learning."
    },
    {
        "paperId": "4ab53de69372ec2cd2d90c126b6a100165dc8ed1",
        "url": "https://www.semanticscholar.org/paper/4ab53de69372ec2cd2d90c126b6a100165dc8ed1",
        "title": "Generative Adversarial Imitation Learning",
        "venue": "Neural Information Processing Systems",
        "year": 2016,
        "citationCount": 3449,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1606.03476, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2126278",
                "name": "Jonathan Ho"
            },
            {
                "authorId": "2490652",
                "name": "Stefano Ermon"
            }
        ],
        "abstract": "Consider learning a policy from example expert behavior, without interaction with the expert or access to reinforcement signal. One approach is to recover the expert's cost function with inverse reinforcement learning, then extract a policy from that cost function with reinforcement learning. This approach is indirect and can be slow. We propose a new general framework for directly extracting a policy from data, as if it were obtained by reinforcement learning following inverse reinforcement learning. We show that a certain instantiation of our framework draws an analogy between imitation learning and generative adversarial networks, from which we derive a model-free imitation learning algorithm that obtains significant performance gains over existing model-free methods in imitating complex behaviors in large, high-dimensional environments."
    },
    {
        "paperId": "4b63e34276aa98d5345efa7fe09bb06d8a9d8f52",
        "url": "https://www.semanticscholar.org/paper/4b63e34276aa98d5345efa7fe09bb06d8a9d8f52",
        "title": "Deep Exploration via Bootstrapped DQN",
        "venue": "Neural Information Processing Systems",
        "year": 2016,
        "citationCount": 1421,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1602.04621, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2561924",
                "name": "Ian Osband"
            },
            {
                "authorId": "1723876",
                "name": "C. Blundell"
            },
            {
                "authorId": "1863250",
                "name": "A. Pritzel"
            },
            {
                "authorId": "1731282",
                "name": "Benjamin Van Roy"
            }
        ],
        "abstract": "Efficient exploration in complex environments remains a major challenge for reinforcement learning. We propose bootstrapped DQN, a simple algorithm that explores in a computationally and statistically efficient manner through use of randomized value functions. Unlike dithering strategies such as epsilon-greedy exploration, bootstrapped DQN carries out temporally-extended (or deep) exploration; this can lead to exponentially faster learning. We demonstrate these benefits in complex stochastic MDPs and in the large-scale Arcade Learning Environment. Bootstrapped DQN substantially improves learning times and performance across most Atari games."
    },
    {
        "paperId": "4c20e7f95448ca3c1042a6d7fa5fa15ec27e9aeb",
        "url": "https://www.semanticscholar.org/paper/4c20e7f95448ca3c1042a6d7fa5fa15ec27e9aeb",
        "title": "Regularization With Stochastic Transformations and Perturbations for Deep Semi-Supervised Learning",
        "venue": "Neural Information Processing Systems",
        "year": 2016,
        "citationCount": 1206,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1606.04586, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2283034",
                "name": "Mehdi S. M. Sajjadi"
            },
            {
                "authorId": "3407490",
                "name": "Mehran Javanmardi"
            },
            {
                "authorId": "3198175",
                "name": "T. Tasdizen"
            }
        ],
        "abstract": "Effective convolutional neural networks are trained on large sets of labeled data. However, creating large labeled datasets is a very costly and time-consuming task. Semi-supervised learning uses unlabeled data to train a model with higher accuracy when there is a limited set of labeled data available. In this paper, we consider the problem of semi-supervised learning with convolutional neural networks. Techniques such as randomized data augmentation, dropout and random max-pooling provide better generalization and stability for classifiers that are trained using gradient descent. Multiple passes of an individual sample through the network might lead to different predictions due to the non-deterministic behavior of these techniques. We propose an unsupervised loss function that takes advantage of the stochastic nature of these methods and minimizes the difference between the predictions of multiple passes of a training sample through the network. We evaluate the proposed method on several benchmark datasets."
    },
    {
        "paperId": "50295c19e177480ba3599300de1ab837cc62b08c",
        "url": "https://www.semanticscholar.org/paper/50295c19e177480ba3599300de1ab837cc62b08c",
        "title": "Learning Multiagent Communication with Backpropagation",
        "venue": "Neural Information Processing Systems",
        "year": 2016,
        "citationCount": 1244,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1605.07736, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2265067",
                "name": "Sainbayar Sukhbaatar"
            },
            {
                "authorId": "3149531",
                "name": "Arthur Szlam"
            },
            {
                "authorId": "2276554",
                "name": "R. Fergus"
            }
        ],
        "abstract": "Many tasks in AI require the collaboration of multiple agents. Typically, the communication protocol between agents is manually specified and not altered during training. In this paper we explore a simple neural model, called CommNet, that uses continuous communication for fully cooperative tasks. The model consists of multiple agents and the communication between them is learned alongside their policy. We apply this model to a diverse set of tasks, demonstrating the ability of the agents to learn to communicate amongst themselves, yielding improved performance over non-communicative agents and baselines. In some cases, it is possible to interpret the language devised by the agents, revealing simple but effective strategies for solving the task at hand."
    },
    {
        "paperId": "571b0750085ae3d939525e62af510ee2cee9d5ea",
        "url": "https://www.semanticscholar.org/paper/571b0750085ae3d939525e62af510ee2cee9d5ea",
        "title": "Improved Techniques for Training GANs",
        "venue": "Neural Information Processing Systems",
        "year": 2016,
        "citationCount": 9809,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1606.03498, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2887364",
                "name": "Tim Salimans"
            },
            {
                "authorId": "153440022",
                "name": "I. Goodfellow"
            },
            {
                "authorId": "2563432",
                "name": "Wojciech Zaremba"
            },
            {
                "authorId": "34415167",
                "name": "Vicki Cheung"
            },
            {
                "authorId": "38909097",
                "name": "Alec Radford"
            },
            {
                "authorId": "41192764",
                "name": "Xi Chen"
            }
        ],
        "abstract": "We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. We focus on two applications of GANs: semi-supervised learning, and the generation of images that humans find visually realistic. Unlike most work on generative models, our primary goal is not to train a model that assigns high likelihood to test data, nor do we require the model to be able to learn well without using any labels. Using our new techniques, we achieve state-of-the-art results in semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated images are of high quality as confirmed by a visual Turing test: our model generates MNIST samples that humans cannot distinguish from real data, and CIFAR-10 samples that yield a human error rate of 21.3%. We also present ImageNet samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of ImageNet classes."
    },
    {
        "paperId": "6a97d2668187965743d1b825b306defccbabbb4c",
        "url": "https://www.semanticscholar.org/paper/6a97d2668187965743d1b825b306defccbabbb4c",
        "title": "Improved Variational Inference with Inverse Autoregressive Flow",
        "venue": "Neural Information Processing Systems",
        "year": 2016,
        "citationCount": 1932,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1606.04934, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1726807",
                "name": "Diederik P. Kingma"
            },
            {
                "authorId": "2887364",
                "name": "Tim Salimans"
            },
            {
                "authorId": "1678311",
                "name": "M. Welling"
            }
        ],
        "abstract": "The framework of normalizing flows provides a general strategy for flexible variational inference of posteriors over latent variables. We propose a new type of normalizing flow, inverse autoregressive flow (IAF), that, in contrast to earlier published flows, scales well to high-dimensional latent spaces. The proposed flow consists of a chain of invertible transformations, where each transformation is based on an autoregressive neural network. In experiments, we show that IAF significantly improves upon diagonal Gaussian approximate posteriors. In addition, we demonstrate that a novel type of variational autoencoder, coupled with IAF, is competitive with neural autoregressive models in terms of attained log-likelihood on natural images, while allowing significantly faster synthesis."
    },
    {
        "paperId": "6e90fd78e8a3b98af3954aae5209703aa966603e",
        "url": "https://www.semanticscholar.org/paper/6e90fd78e8a3b98af3954aae5209703aa966603e",
        "title": "Unifying Count-Based Exploration and Intrinsic Motivation",
        "venue": "Neural Information Processing Systems",
        "year": 2016,
        "citationCount": 1582,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1606.01868, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1792298",
                "name": "Marc G. Bellemare"
            },
            {
                "authorId": "40615554",
                "name": "S. Srinivasan"
            },
            {
                "authorId": "2273072",
                "name": "Georg Ostrovski"
            },
            {
                "authorId": "1725157",
                "name": "T. Schaul"
            },
            {
                "authorId": "143810408",
                "name": "D. Saxton"
            },
            {
                "authorId": "1708654",
                "name": "R. Munos"
            }
        ],
        "abstract": "We consider an agent's uncertainty about its environment and the problem of generalizing this uncertainty across observations. Specifically, we focus on the problem of exploration in non-tabular reinforcement learning. Drawing inspiration from the intrinsic motivation literature, we use density models to measure uncertainty, and propose a novel algorithm for deriving a pseudo-count from an arbitrary density model. This technique enables us to generalize count-based exploration algorithms to the non-tabular case. We apply our ideas to Atari 2600 games, providing sensible pseudo-counts from raw pixels. We transform these pseudo-counts into intrinsic rewards and obtain significantly improved exploration in a number of hard games, including the infamously difficult Montezuma's Revenge."
    },
    {
        "paperId": "6eecc808d4c74e7d0d7ef6b8a4112c985ced104d",
        "url": "https://www.semanticscholar.org/paper/6eecc808d4c74e7d0d7ef6b8a4112c985ced104d",
        "title": "Binarized Neural Networks",
        "venue": "Neural Information Processing Systems",
        "year": 2016,
        "citationCount": 1345,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1602.02830, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2388466",
                "name": "Matthieu Courbariaux"
            },
            {
                "authorId": "2477463",
                "name": "Itay Hubara"
            },
            {
                "authorId": "1912398",
                "name": "Daniel Soudry"
            },
            {
                "authorId": "1387872181",
                "name": "Ran El-Yaniv"
            },
            {
                "authorId": "1751762",
                "name": "Yoshua Bengio"
            }
        ],
        "abstract": "We introduce a method to train Binarized Neural Networks (BNNs) - neural networks with binary weights and activations at run-time. At train-time the binary weights and activations are used for computing the parameter gradients. During the forward pass, BNNs drastically reduce memory size and accesses, and replace most arithmetic operations with bit-wise operations, which is expected to substantially improve power-efficiency. To validate the effectiveness of BNNs, we conducted two sets of experiments on the Torch7 and Theano frameworks. On both, BNNs achieved nearly state-of-the-art results over the MNIST, CIFAR-10 and SVHN datasets. We also report our preliminary results on the challenging ImageNet dataset. Last but not least, we wrote a binary matrix multiplication GPU kernel with which it is possible to run our MNIST BNN 7 times faster than with an unoptimized GPU kernel, without suffering any loss in classification accuracy. The code for training and running our BNNs is available on-line."
    },
    {
        "paperId": "71683e224ab91617950956b5005ed0439a733a71",
        "url": "https://www.semanticscholar.org/paper/71683e224ab91617950956b5005ed0439a733a71",
        "title": "Learning to learn by gradient descent by gradient descent",
        "venue": "Neural Information Processing Systems",
        "year": 2016,
        "citationCount": 2132,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1606.04474, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2206490",
                "name": "Marcin Andrychowicz"
            },
            {
                "authorId": "1715051",
                "name": "Misha Denil"
            },
            {
                "authorId": "2016840",
                "name": "Sergio Gomez Colmenarejo"
            },
            {
                "authorId": "3243579",
                "name": "Matthew W. Hoffman"
            },
            {
                "authorId": "144846367",
                "name": "David Pfau"
            },
            {
                "authorId": "1725157",
                "name": "T. Schaul"
            },
            {
                "authorId": "1737568",
                "name": "Nando de Freitas"
            }
        ],
        "abstract": "The move from hand-designed features to learned features in machine learning has been wildly successful. In spite of this, optimization algorithms are still designed by hand. In this paper we show how the design of an optimization algorithm can be cast as a learning problem, allowing the algorithm to learn to exploit structure in the problems of interest in an automatic way. Our learned algorithms, implemented by LSTMs, outperform generic, hand-designed competitors on the tasks for which they are trained, and also generalize well to new tasks with similar structure. We demonstrate this on a number of tasks, including simple convex problems, training neural networks, and styling images with neural art."
    },
    {
        "paperId": "7601b995303f953955004db7b9b8b206c0e02ff8",
        "url": "https://www.semanticscholar.org/paper/7601b995303f953955004db7b9b8b206c0e02ff8",
        "title": "Learning Structured Sparsity in Deep Neural Networks",
        "venue": "Neural Information Processing Systems",
        "year": 2016,
        "citationCount": 2449,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1608.03665, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "145225262",
                "name": "W. Wen"
            },
            {
                "authorId": "3207491",
                "name": "Chunpeng Wu"
            },
            {
                "authorId": "2108738288",
                "name": "Yandan Wang"
            },
            {
                "authorId": "5442167",
                "name": "Yiran Chen"
            },
            {
                "authorId": "40348219",
                "name": "Hai Helen Li"
            }
        ],
        "abstract": "High demand for computation resources severely hinders deployment of large-scale Deep Neural Networks (DNN) in resource constrained devices. In this work, we propose a Structured Sparsity Learning (SSL) method to regularize the structures (i.e., filters, channels, filter shapes, and layer depth) of DNNs. SSL can: (1) learn a compact structure from a bigger DNN to reduce computation cost; (2) obtain a hardware-friendly structured sparsity of DNN to efficiently accelerate the DNNs evaluation. Experimental results show that SSL achieves on average 5.1x and 3.1x speedups of convolutional layer computation of AlexNet against CPU and GPU, respectively, with off-the-shelf libraries. These speedups are about twice speedups of non-structured sparsity; (3) regularize the DNN structure to improve classification accuracy. The results show that for CIFAR-10, regularization on layer depth can reduce 20 layers of a Deep Residual Network (ResNet) to 18 layers while improve the accuracy from 91.25% to 92.60%, which is still slightly higher than that of original ResNet with 32 layers. For AlexNet, structure regularization by SSL also reduces the error by around ~1%. Open source code is in this https URL"
    },
    {
        "paperId": "768f7353718c6d95f2d63f954f2236369a409135",
        "url": "https://www.semanticscholar.org/paper/768f7353718c6d95f2d63f954f2236369a409135",
        "title": "Stein Variational Gradient Descent: A General Purpose Bayesian Inference Algorithm",
        "venue": "Neural Information Processing Systems",
        "year": 2016,
        "citationCount": 1172,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1608.04471, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "47362268",
                "name": "Qiang Liu"
            },
            {
                "authorId": "2848320",
                "name": "Dilin Wang"
            }
        ],
        "abstract": "We propose a general purpose variational inference algorithm that forms a natural counterpart of gradient descent for optimization. Our method iteratively transports a set of particles to match the target distribution, by applying a form of functional gradient descent that minimizes the KL divergence. Empirical studies are performed on various real world models and datasets, on which our method is competitive with existing state-of-the-art methods. The derivation of our method is based on a new theoretical result that connects the derivative of KL divergence under smooth transforms with Stein's identity and a recently proposed kernelized Stein discrepancy, which is of independent interest."
    },
    {
        "paperId": "76cee11c6a9f1424f03571378a966c1417ff2935",
        "url": "https://www.semanticscholar.org/paper/76cee11c6a9f1424f03571378a966c1417ff2935",
        "title": "Learning a Probabilistic Latent Space of Object Shapes via 3D Generative-Adversarial Modeling",
        "venue": "Neural Information Processing Systems",
        "year": 2016,
        "citationCount": 2073,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1610.07584, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3045089",
                "name": "Jiajun Wu"
            },
            {
                "authorId": "2196255508",
                "name": "Chengkai Zhang"
            },
            {
                "authorId": "3222730",
                "name": "Tianfan Xue"
            },
            {
                "authorId": "36668046",
                "name": "Bill Freeman"
            },
            {
                "authorId": "1763295",
                "name": "J. Tenenbaum"
            }
        ],
        "abstract": "We study the problem of 3D object generation. We propose a novel framework, namely 3D Generative Adversarial Network (3D-GAN), which generates 3D objects from a probabilistic space by leveraging recent advances in volumetric convolutional networks and generative adversarial nets. The benefits of our model are three-fold: first, the use of an adversarial criterion, instead of traditional heuristic criteria, enables the generator to capture object structure implicitly and to synthesize high-quality 3D objects; second, the generator establishes a mapping from a low-dimensional probabilistic space to the space of 3D objects, so that we can sample objects without a reference image or CAD models, and explore the 3D object manifold; third, the adversarial discriminator provides a powerful 3D shape descriptor which, learned without supervision, has wide applications in 3D object recognition. Experiments demonstrate that our method generates high-quality 3D objects, and our unsupervisedly learned features achieve impressive performance on 3D object recognition, comparable with those of supervised learning methods."
    },
    {
        "paperId": "78a11b7d2d7e1b19d92d2afd51bd3624eca86c3c",
        "url": "https://www.semanticscholar.org/paper/78a11b7d2d7e1b19d92d2afd51bd3624eca86c3c",
        "title": "Improved Deep Metric Learning with Multi-class N-pair Loss Objective",
        "venue": "Neural Information Processing Systems",
        "year": 2016,
        "citationCount": 2360,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "1729571",
                "name": "Kihyuk Sohn"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "7ab8d3af6f78f9c9f64a2f2d38471401ad0988a9",
        "url": "https://www.semanticscholar.org/paper/7ab8d3af6f78f9c9f64a2f2d38471401ad0988a9",
        "title": "SoundNet: Learning Sound Representations from Unlabeled Video",
        "venue": "Neural Information Processing Systems",
        "year": 2016,
        "citationCount": 1086,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1610.09001, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3152281",
                "name": "Y. Aytar"
            },
            {
                "authorId": "1856025",
                "name": "Carl Vondrick"
            },
            {
                "authorId": "143805211",
                "name": "A. Torralba"
            }
        ],
        "abstract": "We learn rich natural sound representations by capitalizing on large amounts of unlabeled sound data collected in the wild. We leverage the natural synchronization between vision and sound to learn an acoustic representation using two-million unlabeled videos. Unlabeled video has the advantage that it can be economically acquired at massive scales, yet contains useful signals about natural sound. We propose a student-teacher training procedure which transfers discriminative visual knowledge from well established visual recognition models into the sound modality using unlabeled video as a bridge. Our sound representation yields significant performance improvements over the state-of-the-art results on standard benchmarks for acoustic scene/object classification. Visualizations suggest some high-level semantics automatically emerge in the sound network, even though it is trained without ground truth labels."
    },
    {
        "paperId": "802168a81571dde28f5ddb94d84677bc007afa7b",
        "url": "https://www.semanticscholar.org/paper/802168a81571dde28f5ddb94d84677bc007afa7b",
        "title": "Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles",
        "venue": "Neural Information Processing Systems",
        "year": 2016,
        "citationCount": 6709,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1612.01474, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "40627523",
                "name": "Balaji Lakshminarayanan"
            },
            {
                "authorId": "1863250",
                "name": "A. Pritzel"
            },
            {
                "authorId": "1723876",
                "name": "C. Blundell"
            }
        ],
        "abstract": "Deep neural networks (NNs) are powerful black box predictors that have recently achieved impressive performance on a wide spectrum of tasks. Quantifying predictive uncertainty in NNs is a challenging and yet unsolved problem. Bayesian NNs, which learn a distribution over weights, are currently the state-of-the-art for estimating predictive uncertainty; however these require significant modifications to the training procedure and are computationally expensive compared to standard (non-Bayesian) NNs. We propose an alternative to Bayesian NNs that is simple to implement, readily parallelizable, requires very little hyperparameter tuning, and yields high quality predictive uncertainty estimates. Through a series of experiments on classification and regression benchmarks, we demonstrate that our method produces well-calibrated uncertainty estimates which are as good or better than approximate Bayesian NNs. To assess robustness to dataset shift, we evaluate the predictive uncertainty on test examples from known and unknown distributions, and show that our method is able to express higher uncertainty on out-of-distribution examples. We demonstrate the scalability of our method by evaluating predictive uncertainty estimates on ImageNet."
    },
    {
        "paperId": "9179e740dad4ca4c183f7677b854e5b15f9a122f",
        "url": "https://www.semanticscholar.org/paper/9179e740dad4ca4c183f7677b854e5b15f9a122f",
        "title": "Generating Images with Perceptual Similarity Metrics based on Deep Networks",
        "venue": "Neural Information Processing Systems",
        "year": 2016,
        "citationCount": 1176,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1602.02644, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2841331",
                "name": "Alexey Dosovitskiy"
            },
            {
                "authorId": "1710872",
                "name": "T. Brox"
            }
        ],
        "abstract": "Image-generating machine learning models are typically trained with loss functions based on distance in the image space. This often leads to over-smoothed results. We propose a class of loss functions, which we call deep perceptual similarity metrics (DeePSiM), that mitigate this problem. Instead of computing distances in the image space, we compute distances between image features extracted by deep neural networks. This metric better reflects perceptually similarity of images and thus leads to better results. We show three applications: autoencoder training, a modification of a variational autoencoder, and inversion of deep convolutional networks. In all cases, the generated images look sharp and resemble natural images."
    },
    {
        "paperId": "aba48504f4f9563eafa44e0cfb22e1345d767c80",
        "url": "https://www.semanticscholar.org/paper/aba48504f4f9563eafa44e0cfb22e1345d767c80",
        "title": "Dynamic Filter Networks",
        "venue": "Neural Information Processing Systems",
        "year": 2016,
        "citationCount": 1055,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1605.09673, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1641711590",
                "name": "Xu Jia"
            },
            {
                "authorId": "3384995",
                "name": "Bert De Brabandere"
            },
            {
                "authorId": "1704728",
                "name": "T. Tuytelaars"
            },
            {
                "authorId": "1681236",
                "name": "L. Gool"
            }
        ],
        "abstract": "In a traditional convolutional layer, the learned filters stay fixed after training. In contrast, we introduce a new framework, the Dynamic Filter Network, where filters are generated dynamically conditioned on an input. We show that this architecture is a powerful one, with increased flexibility thanks to its adaptive nature, yet without an excessive increase in the number of model parameters. A wide variety of filtering operation can be learned this way, including local spatial transformations, but also others like selective (de)blurring or adaptive feature extraction. Moreover, multiple such layers can be combined, e.g. in a recurrent architecture. We demonstrate the effectiveness of the dynamic filter network on the tasks of video and stereo prediction, and reach state-of-the-art performance on the moving MNIST dataset with a much smaller model. By visualizing the learned filters, we illustrate that the network has picked up flow information by only looking at unlabelled training data. This suggests that the network can be used to pretrain networks for various supervised tasks in an unsupervised way, like optical flow and depth estimation."
    },
    {
        "paperId": "ae42c0cff384495683192b06bd985cdd7a54632a",
        "url": "https://www.semanticscholar.org/paper/ae42c0cff384495683192b06bd985cdd7a54632a",
        "title": "Interaction Networks for Learning about Objects, Relations and Physics",
        "venue": "Neural Information Processing Systems",
        "year": 2016,
        "citationCount": 1480,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1612.00222, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2019153",
                "name": "P. Battaglia"
            },
            {
                "authorId": "1996134",
                "name": "Razvan Pascanu"
            },
            {
                "authorId": "40227832",
                "name": "Matthew Lai"
            },
            {
                "authorId": "1748523",
                "name": "Danilo Jimenez Rezende"
            },
            {
                "authorId": "2645384",
                "name": "K. Kavukcuoglu"
            }
        ],
        "abstract": "Reasoning about objects, relations, and physics is central to human intelligence, and a key goal of artificial intelligence. Here we introduce the interaction network, a model which can reason about how objects in complex systems interact, supporting dynamical predictions, as well as inferences about the abstract properties of the system. Our model takes graphs as input, performs object- and relation-centric reasoning in a way that is analogous to a simulation, and is implemented using deep neural networks. We evaluate its ability to reason about several challenging physical domains: n-body problems, rigid-body collision, and non-rigid dynamics. Our results show it can be trained to accurately simulate the physical trajectories of dozens of objects over thousands of time steps, estimate abstract quantities such as energy, and generalize automatically to systems with different numbers and configurations of objects and relations. Our interaction network implementation is the first general-purpose, learnable physics engine, and a powerful general framework for reasoning about object and relations in a wide variety of complex real-world domains."
    },
    {
        "paperId": "b4929ef46d86b92753362bdd1aa7b6e3e03e6214",
        "url": "https://www.semanticscholar.org/paper/b4929ef46d86b92753362bdd1aa7b6e3e03e6214",
        "title": "Unsupervised Domain Adaptation with Residual Transfer Networks",
        "venue": "Neural Information Processing Systems",
        "year": 2016,
        "citationCount": 1564,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1602.04433, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "35776445",
                "name": "Mingsheng Long"
            },
            {
                "authorId": "2115315619",
                "name": "Hanjing Zhu"
            },
            {
                "authorId": "2144499343",
                "name": "Jianmin Wang"
            },
            {
                "authorId": "1694621",
                "name": "Michael I. Jordan"
            }
        ],
        "abstract": "The recent success of deep neural networks relies on massive amounts of labeled data. For a target task where labeled data is unavailable, domain adaptation can transfer a learner from a different source domain. In this paper, we propose a new approach to domain adaptation in deep networks that can jointly learn adaptive classifiers and transferable features from labeled data in the source domain and unlabeled data in the target domain. We relax a shared-classifier assumption made by previous methods and assume that the source classifier and target classifier differ by a residual function. We enable classifier adaptation by plugging several layers into deep network to explicitly learn the residual function with reference to the target classifier. We fuse features of multiple layers with tensor product and embed them into reproducing kernel Hilbert spaces to match distributions for feature adaptation. The adaptation can be achieved in most feed-forward models by extending them with new residual layers and loss functions, which can be trained efficiently via back-propagation. Empirical evidence shows that the new approach outperforms state of the art methods on standard domain adaptation benchmarks."
    },
    {
        "paperId": "b724c3f7ff395235b62537203ddeb710f0eb27bb",
        "url": "https://www.semanticscholar.org/paper/b724c3f7ff395235b62537203ddeb710f0eb27bb",
        "title": "R-FCN: Object Detection via Region-based Fully Convolutional Networks",
        "venue": "Neural Information Processing Systems",
        "year": 2016,
        "citationCount": 5919,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1605.06409, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3304536",
                "name": "Jifeng Dai"
            },
            {
                "authorId": "2153682629",
                "name": "Yi Li"
            },
            {
                "authorId": "39353098",
                "name": "Kaiming He"
            },
            {
                "authorId": null,
                "name": "Jian Sun"
            }
        ],
        "abstract": "We present region-based, fully convolutional networks for accurate and efficient object detection. In contrast to previous region-based detectors such as Fast/Faster R-CNN that apply a costly per-region subnetwork hundreds of times, our region-based detector is fully convolutional with almost all computation shared on the entire image. To achieve this goal, we propose position-sensitive score maps to address a dilemma between translation-invariance in image classification and translation-variance in object detection. Our method can thus naturally adopt fully convolutional image classifier backbones, such as the latest Residual Networks (ResNets), for object detection. We show competitive results on the PASCAL VOC datasets (e.g., 83.6% mAP on the 2007 set) with the 101-layer ResNet. Meanwhile, our result is achieved at a test-time speed of 170ms per image, 2.5-20 times faster than the Faster R-CNN counterpart. Code is made publicly available at: https://github.com/daijifeng001/r-fcn."
    },
    {
        "paperId": "be1bb4e4aa1fcf70281b4bd24d8cd31c04864bb6",
        "url": "https://www.semanticscholar.org/paper/be1bb4e4aa1fcf70281b4bd24d8cd31c04864bb6",
        "title": "Matching Networks for One Shot Learning",
        "venue": "Neural Information Processing Systems",
        "year": 2016,
        "citationCount": 7996,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1606.04080, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1689108",
                "name": "O. Vinyals"
            },
            {
                "authorId": "1723876",
                "name": "C. Blundell"
            },
            {
                "authorId": "2542999",
                "name": "T. Lillicrap"
            },
            {
                "authorId": "2645384",
                "name": "K. Kavukcuoglu"
            },
            {
                "authorId": "1688276",
                "name": "Daan Wierstra"
            }
        ],
        "abstract": "Learning from a few examples remains a key challenge in machine learning. Despite recent advances in important domains such as vision and language, the standard supervised deep learning paradigm does not offer a satisfactory solution for learning new concepts rapidly from little data. In this work, we employ ideas from metric learning based on deep neural features and from recent advances that augment neural networks with external memories. Our framework learns a network that maps a small labelled support set and an unlabelled example to its label, obviating the need for fine-tuning to adapt to new class types. We then define one-shot learning problems on vision (using Omniglot, ImageNet) and language tasks. Our algorithm improves one-shot accuracy on ImageNet from 87.6% to 93.2% and from 88.0% to 93.8% on Omniglot compared to competing approaches. We also demonstrate the usefulness of the same model on language modeling by introducing a one-shot task on the Penn Treebank."
    },
    {
        "paperId": "c220cdbcec6f92e4bc0f58c5fa6c1183105be1f9",
        "url": "https://www.semanticscholar.org/paper/c220cdbcec6f92e4bc0f58c5fa6c1183105be1f9",
        "title": "Dynamic Network Surgery for Efficient DNNs",
        "venue": "Neural Information Processing Systems",
        "year": 2016,
        "citationCount": 1120,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1608.04493, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2527106",
                "name": "Yiwen Guo"
            },
            {
                "authorId": "2021251",
                "name": "Anbang Yao"
            },
            {
                "authorId": "2109184871",
                "name": "Yurong Chen"
            }
        ],
        "abstract": "Deep learning has become a ubiquitous technology to improve machine intelligence. However, most of the existing deep models are structurally very complex, making them difficult to be deployed on the mobile platforms with limited computational power. In this paper, we propose a novel network compression method called dynamic network surgery, which can remarkably reduce the network complexity by making on-the-fly connection pruning. Unlike the previous methods which accomplish this task in a greedy way, we properly incorporate connection splicing into the whole process to avoid incorrect pruning and make it as a continual network maintenance. The effectiveness of our method is proved with experiments. Without any accuracy loss, our method can efficiently compress the number of parameters in LeNet-5 and AlexNet by a factor of $\\bm{108}\\times$ and $\\bm{17.7}\\times$ respectively, proving that it outperforms the recent pruning method by considerable margins. Code and some models are available at this https URL."
    },
    {
        "paperId": "c41eb895616e453dcba1a70c9b942c5063cc656c",
        "url": "https://www.semanticscholar.org/paper/c41eb895616e453dcba1a70c9b942c5063cc656c",
        "title": "Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering",
        "venue": "Neural Information Processing Systems",
        "year": 2016,
        "citationCount": 8225,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1606.09375, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3422350",
                "name": "Michal Defferrard"
            },
            {
                "authorId": "2549032",
                "name": "X. Bresson"
            },
            {
                "authorId": "1697397",
                "name": "P. Vandergheynst"
            }
        ],
        "abstract": "In this work, we are interested in generalizing convolutional neural networks (CNNs) from low-dimensional regular grids, where image, video and speech are represented, to high-dimensional irregular domains, such as social networks, brain connectomes or words' embedding, represented by graphs. We present a formulation of CNNs in the context of spectral graph theory, which provides the necessary mathematical background and efficient numerical schemes to design fast localized convolutional filters on graphs. Importantly, the proposed technique offers the same linear computational complexity and constant learning complexity as classical CNNs, while being universal to any graph structure. Experiments on MNIST and 20NEWS demonstrate the ability of this novel deep learning system to learn local, stationary, and compositional features on graphs."
    },
    {
        "paperId": "c9d64aaa2007b60ef7814acc895dd90f15578a20",
        "url": "https://www.semanticscholar.org/paper/c9d64aaa2007b60ef7814acc895dd90f15578a20",
        "title": "QSGD: Communication-Efficient SGD via Gradient Quantization and Encoding",
        "venue": "Neural Information Processing Systems",
        "year": 2016,
        "citationCount": 1502,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "3311387",
                "name": "Dan Alistarh"
            },
            {
                "authorId": "29916095",
                "name": "Demjan Grubic"
            },
            {
                "authorId": "2257378511",
                "name": "Jerry Li"
            },
            {
                "authorId": "2257289886",
                "name": "Ryota Tomioka"
            },
            {
                "authorId": "2257231171",
                "name": "Milan Vojnovic"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "ccf6a69a7f33bcf052aa7def176d3b9de495beb7",
        "url": "https://www.semanticscholar.org/paper/ccf6a69a7f33bcf052aa7def176d3b9de495beb7",
        "title": "Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings",
        "venue": "Neural Information Processing Systems",
        "year": 2016,
        "citationCount": 3474,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1607.06520, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2843215",
                "name": "Tolga Bolukbasi"
            },
            {
                "authorId": "2782886",
                "name": "Kai-Wei Chang"
            },
            {
                "authorId": "145085305",
                "name": "James Y. Zou"
            },
            {
                "authorId": "1699322",
                "name": "Venkatesh Saligrama"
            },
            {
                "authorId": "2186481",
                "name": "A. Kalai"
            }
        ],
        "abstract": "The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between the words receptionist and female, while maintaining desired associations such as between the words queen and female. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias."
    },
    {
        "paperId": "d37620e6f8fe678a43e12930743281cd8cca6a66",
        "url": "https://www.semanticscholar.org/paper/d37620e6f8fe678a43e12930743281cd8cca6a66",
        "title": "Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation",
        "venue": "Neural Information Processing Systems",
        "year": 2016,
        "citationCount": 1222,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1604.06057, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1954876",
                "name": "Tejas D. Kulkarni"
            },
            {
                "authorId": "144958935",
                "name": "Karthik Narasimhan"
            },
            {
                "authorId": "3231182",
                "name": "A. Saeedi"
            },
            {
                "authorId": "1763295",
                "name": "J. Tenenbaum"
            }
        ],
        "abstract": "Learning goal-directed behavior in environments with sparse feedback is a major challenge for reinforcement learning algorithms. The primary difficulty arises due to insufficient exploration, resulting in an agent being unable to learn robust value functions. Intrinsically motivated agents can explore new behavior for its own sake rather than to directly solve problems. Such intrinsic behaviors could eventually help the agent solve tasks posed by the environment. We present hierarchical-DQN (h-DQN), a framework to integrate hierarchical value functions, operating at different temporal scales, with intrinsically motivated deep reinforcement learning. A top-level value function learns a policy over intrinsic goals, and a lower-level function learns a policy over atomic actions to satisfy the given goals. h-DQN allows for flexible goal specifications, such as functions over entities and relations. This provides an efficient space for exploration in complicated environments. We demonstrate the strength of our approach on two problems with very sparse, delayed feedback: (1) a complex discrete stochastic decision process, and (2) the classic ATARI game `Montezuma's Revenge'."
    },
    {
        "paperId": "d42b11ce90c9c69a20ed015b73dc33e0e4100a7b",
        "url": "https://www.semanticscholar.org/paper/d42b11ce90c9c69a20ed015b73dc33e0e4100a7b",
        "title": "Equality of Opportunity in Supervised Learning",
        "venue": "Neural Information Processing Systems",
        "year": 2016,
        "citationCount": 4779,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1610.02413, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1775622",
                "name": "Moritz Hardt"
            },
            {
                "authorId": "4989538",
                "name": "Eric Price"
            },
            {
                "authorId": "1706280",
                "name": "N. Srebro"
            }
        ],
        "abstract": "We propose a criterion for discrimination against a specified sensitive attribute in supervised learning, where the goal is to predict some target based on available features. Assuming data about the predictor, target, and membership in the protected group are available, we show how to optimally adjust any learned predictor so as to remove discrimination according to our definition. Our framework also improves incentives by shifting the cost of poor classification from disadvantaged groups to the decision maker, who can respond by improving the classification accuracy. We enourage readers to consult the more complete manuscript on the arXiv."
    },
    {
        "paperId": "e8e9125704edbcf73999f2f452fe4a701163d6b6",
        "url": "https://www.semanticscholar.org/paper/e8e9125704edbcf73999f2f452fe4a701163d6b6",
        "title": "RETAIN: An Interpretable Predictive Model for Healthcare using Reverse Time Attention Mechanism",
        "venue": "Neural Information Processing Systems",
        "year": 2016,
        "citationCount": 1381,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1608.05745, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3242613",
                "name": "E. Choi"
            },
            {
                "authorId": "2342604",
                "name": "M. T. Bahadori"
            },
            {
                "authorId": "1738536",
                "name": "Jimeng Sun"
            },
            {
                "authorId": "3959063",
                "name": "Joshua A. Kulas"
            },
            {
                "authorId": "20079790",
                "name": "A. Schuetz"
            },
            {
                "authorId": "49523072",
                "name": "W. Stewart"
            }
        ],
        "abstract": "Accuracy and interpretability are two dominant features of successful predictive models. Typically, a choice must be made in favor of complex black box models such as recurrent neural networks (RNN) for accuracy versus less accurate but more interpretable traditional models such as logistic regression. This tradeoff poses challenges in medicine where both accuracy and interpretability are important. We addressed this challenge by developing the REverse Time AttentIoN model (RETAIN) for application to Electronic Health Records (EHR) data. RETAIN achieves high accuracy while remaining clinically interpretable and is based on a two-level neural attention model that detects influential past visits and significant clinical variables within those visits (e.g. key diagnoses). RETAIN mimics physician practice by attending the EHR data in a reverse time order so that recent clinical visits are likely to receive higher attention. RETAIN was tested on a large health system EHR dataset with 14 million visits completed by 263K patients over an 8 year period and demonstrated predictive accuracy and computational scalability comparable to state-of-the-art methods such as RNN, and ease of interpretability comparable to traditional models."
    },
    {
        "paperId": "eb7ee0bc355652654990bcf9f92f124688fde493",
        "url": "https://www.semanticscholar.org/paper/eb7ee0bc355652654990bcf9f92f124688fde493",
        "title": "InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets",
        "venue": "Neural Information Processing Systems",
        "year": 2016,
        "citationCount": 4408,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1606.03657, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "41192764",
                "name": "Xi Chen"
            },
            {
                "authorId": "144581158",
                "name": "Yan Duan"
            },
            {
                "authorId": "3127100",
                "name": "Rein Houthooft"
            },
            {
                "authorId": "47971768",
                "name": "John Schulman"
            },
            {
                "authorId": "1701686",
                "name": "I. Sutskever"
            },
            {
                "authorId": "1689992",
                "name": "P. Abbeel"
            }
        ],
        "abstract": "This paper describes InfoGAN, an information-theoretic extension to the Generative Adversarial Network that is able to learn disentangled representations in a completely unsupervised manner. InfoGAN is a generative adversarial network that also maximizes the mutual information between a small subset of the latent variables and the observation. We derive a lower bound to the mutual information objective that can be optimized efficiently, and show that our training procedure can be interpreted as a variation of the Wake-Sleep algorithm. Specifically, InfoGAN successfully disentangles writing styles from digit shapes on the MNIST dataset, pose from lighting of 3D rendered images, and background digits from the central digit on the SVHN dataset. It also discovers visual concepts that include hair styles, presence/absence of eyeglasses, and emotions on the CelebA face dataset. Experiments show that InfoGAN learns interpretable representations that are competitive with representations learned by existing fully supervised methods."
    },
    {
        "paperId": "ee091ccf24c4f053c5c3dfbefe4a7975ed3447c1",
        "url": "https://www.semanticscholar.org/paper/ee091ccf24c4f053c5c3dfbefe4a7975ed3447c1",
        "title": "Generating Videos with Scene Dynamics",
        "venue": "Neural Information Processing Systems",
        "year": 2016,
        "citationCount": 1547,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1609.02612, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1856025",
                "name": "Carl Vondrick"
            },
            {
                "authorId": "2367683",
                "name": "H. Pirsiavash"
            },
            {
                "authorId": "143805211",
                "name": "A. Torralba"
            }
        ],
        "abstract": "We capitalize on large amounts of unlabeled video in order to learn a model of scene dynamics for both video recognition tasks (e.g. action classification) and video generation tasks (e.g. future prediction). We propose a generative adversarial network for video with a spatio-temporal convolutional architecture that untangles the scene's foreground from the background. Experiments suggest this model can generate tiny videos up to a second at full frame rate better than simple baselines, and we show its utility at predicting plausible futures of static images. Moreover, experiments and visualizations show the model internally learns useful features for recognizing actions with minimal supervision, suggesting scene dynamics are a promising signal for representation learning. We believe generative video models can impact many applications in video understanding and simulation."
    },
    {
        "paperId": "f110cfdbe9ded7a384bcf5c0d56e536bd275a7eb",
        "url": "https://www.semanticscholar.org/paper/f110cfdbe9ded7a384bcf5c0d56e536bd275a7eb",
        "title": "Unsupervised Learning for Physical Interaction through Video Prediction",
        "venue": "Neural Information Processing Systems",
        "year": 2016,
        "citationCount": 1083,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1605.07157, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "46881670",
                "name": "Chelsea Finn"
            },
            {
                "authorId": "153440022",
                "name": "I. Goodfellow"
            },
            {
                "authorId": "1736651",
                "name": "S. Levine"
            }
        ],
        "abstract": "A core challenge for an agent learning to interact with the world is to predict how its actions affect objects in its environment. Many existing methods for learning the dynamics of physical interactions require labeled object information. However, to scale real-world interaction learning to a variety of scenes and objects, acquiring labeled data becomes increasingly impractical. To learn about physical object motion without labels, we develop an action-conditioned video prediction model that explicitly models pixel motion, by predicting a distribution over pixel motion from previous frames. Because our model explicitly predicts motion, it is partially invariant to object appearance, enabling it to generalize to previously unseen objects. To explore video prediction for real-world interactive agents, we also introduce a dataset of 59,000 robot interactions involving pushing motions, including a test set with novel objects. In this dataset, accurate prediction of videos conditioned on the robot's future actions amounts to learning a \"visual imagination\" of different futures based on different courses of action. Our experiments show that our proposed method produces more accurate video predictions both quantitatively and qualitatively, when compared to prior methods."
    },
    {
        "paperId": "fb9d253258d6b3beceb9d6cd7bba6e0a29ab875b",
        "url": "https://www.semanticscholar.org/paper/fb9d253258d6b3beceb9d6cd7bba6e0a29ab875b",
        "title": "Hierarchical Question-Image Co-Attention for Visual Question Answering",
        "venue": "Neural Information Processing Systems",
        "year": 2016,
        "citationCount": 1696,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1606.00061, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "8553015",
                "name": "Jiasen Lu"
            },
            {
                "authorId": "145743311",
                "name": "Jianwei Yang"
            },
            {
                "authorId": "1746610",
                "name": "Dhruv Batra"
            },
            {
                "authorId": "153432684",
                "name": "Devi Parikh"
            }
        ],
        "abstract": "A number of recent works have proposed attention models for Visual Question Answering (VQA) that generate spatial maps highlighting image regions relevant to answering the question. In this paper, we argue that in addition to modeling \"where to look\" or visual attention, it is equally important to model \"what words to listen to\" or question attention. We present a novel co-attention model for VQA that jointly reasons about image and question attention. In addition, our model reasons about the question (and consequently the image via the co-attention mechanism) in a hierarchical fashion via a novel 1-dimensional convolution neural networks (CNN). Our model improves the state-of-the-art on the VQA dataset from 60.3% to 60.5%, and from 61.6% to 63.3% on the COCO-QA dataset. By using ResNet, the performance is further improved to 62.1% for VQA and 65.4% for COCO-QA."
    },
    {
        "paperId": "ffdcad14d2f6a12f607b59f88da4a939f4821691",
        "url": "https://www.semanticscholar.org/paper/ffdcad14d2f6a12f607b59f88da4a939f4821691",
        "title": "f-GAN: Training Generative Neural Samplers using Variational Divergence Minimization",
        "venue": "Neural Information Processing Systems",
        "year": 2016,
        "citationCount": 1750,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1606.00709, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2388416",
                "name": "Sebastian Nowozin"
            },
            {
                "authorId": "2084925",
                "name": "Botond Cseke"
            },
            {
                "authorId": "2870603",
                "name": "Ryota Tomioka"
            }
        ],
        "abstract": "Generative neural samplers are probabilistic models that implement sampling using feedforward neural networks: they take a random input vector and produce a sample from a probability distribution defined by the network weights. These models are expressive and allow efficient computation of samples and derivatives, but cannot be used for computing likelihoods or for marginalization. The generative-adversarial training method allows to train such models through the use of an auxiliary discriminative neural network. We show that the generative-adversarial approach is a special case of an existing more general variational divergence estimation approach. We show that any f-divergence can be used for training generative neural samplers. We discuss the benefits of various choices of divergence functions on training complexity and the quality of the obtained generative models."
    },
    {
        "paperId": "007112213ece771be72cbecfd59f048209facabd",
        "url": "https://www.semanticscholar.org/paper/007112213ece771be72cbecfd59f048209facabd",
        "title": "A simple neural network module for relational reasoning",
        "venue": "Neural Information Processing Systems",
        "year": 2017,
        "citationCount": 1664,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1706.01427, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "35030998",
                "name": "Adam Santoro"
            },
            {
                "authorId": "143724694",
                "name": "David Raposo"
            },
            {
                "authorId": "50181861",
                "name": "D. Barrett"
            },
            {
                "authorId": "145478807",
                "name": "Mateusz Malinowski"
            },
            {
                "authorId": "1996134",
                "name": "Razvan Pascanu"
            },
            {
                "authorId": "2019153",
                "name": "P. Battaglia"
            },
            {
                "authorId": "2542999",
                "name": "T. Lillicrap"
            }
        ],
        "abstract": "Relational reasoning is a central component of generally intelligent behavior, but has proven difficult for neural networks to learn. In this paper we describe how to use Relation Networks (RNs) as a simple plug-and-play module to solve problems that fundamentally hinge on relational reasoning. We tested RN-augmented networks on three tasks: visual question answering using a challenging dataset called CLEVR, on which we achieve state-of-the-art, super-human performance; text-based question answering using the bAbI suite of tasks; and complex reasoning about dynamic physical systems. Then, using a curated dataset called Sort-of-CLEVR we show that powerful convolutional networks do not have a general capacity to solve relational questions, but can gain this capacity when augmented with RNs. Our work shows how a deep learning architecture equipped with an RN module can implicitly discover and learn to reason about entities and their relations."
    },
    {
        "paperId": "043f084e379a44608c470059c2aa174a323e9774",
        "url": "https://www.semanticscholar.org/paper/043f084e379a44608c470059c2aa174a323e9774",
        "title": "Counterfactual Fairness",
        "venue": "Neural Information Processing Systems",
        "year": 2017,
        "citationCount": 1741,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1703.06856, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1940272",
                "name": "Matt J. Kusner"
            },
            {
                "authorId": "48678411",
                "name": "Joshua R. Loftus"
            },
            {
                "authorId": "2052380526",
                "name": "Chris Russell"
            },
            {
                "authorId": "2187716",
                "name": "Ricardo Silva"
            }
        ],
        "abstract": "Machine learning can impact people with legal or ethical consequences when it is used to automate decisions in areas such as insurance, lending, hiring, and predictive policing. In many of these scenarios, previous decisions have been made that are unfairly biased against certain subpopulations, for example those of a particular race, gender, or sexual orientation. Since this past data may be biased, machine learning predictors must account for this to avoid perpetuating or creating discriminatory practices. In this paper, we develop a framework for modeling fairness using tools from causal inference. Our definition of counterfactual fairness captures the intuition that a decision is fair towards an individual if it the same in (a) the actual world and (b) a counterfactual world where the individual belonged to a different demographic group. We demonstrate our framework on a real-world problem of fair prediction of success in law school."
    },
    {
        "paperId": "118fae4b4d07453561f1eded88654f812c7c61ec",
        "url": "https://www.semanticscholar.org/paper/118fae4b4d07453561f1eded88654f812c7c61ec",
        "title": "Gradient Episodic Memory for Continual Learning",
        "venue": "Neural Information Processing Systems",
        "year": 2017,
        "citationCount": 3108,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1706.08840, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1401804750",
                "name": "David Lopez-Paz"
            },
            {
                "authorId": "1706809",
                "name": "Marc'Aurelio Ranzato"
            }
        ],
        "abstract": "One major obstacle towards AI is the poor ability of models to solve new problems quicker, and without forgetting previously acquired knowledge. To better understand this issue, we study the problem of continual learning, where the model observes, once and one by one, examples concerning a sequence of tasks. First, we propose a set of metrics to evaluate models learning over a continuum of data. These metrics characterize models not only by their test accuracy, but also in terms of their ability to transfer knowledge across tasks. Second, we propose a model for continual learning, called Gradient Episodic Memory (GEM) that alleviates forgetting, while allowing beneficial transfer of knowledge to previous tasks. Our experiments on variants of the MNIST and CIFAR-100 datasets demonstrate the strong performance of GEM when compared to the state-of-the-art."
    },
    {
        "paperId": "1590bd1bca945fc6ff50b8cdf2da14ea2061c79a",
        "url": "https://www.semanticscholar.org/paper/1590bd1bca945fc6ff50b8cdf2da14ea2061c79a",
        "title": "Poincar Embeddings for Learning Hierarchical Representations",
        "venue": "Neural Information Processing Systems",
        "year": 2017,
        "citationCount": 1476,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1705.08039, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1729762",
                "name": "Maximilian Nickel"
            },
            {
                "authorId": "1743722",
                "name": "Douwe Kiela"
            }
        ],
        "abstract": "Representation learning has become an invaluable approach for learning from symbolic data such as text and graphs. However, while complex symbolic datasets often exhibit a latent hierarchical structure, state-of-the-art methods typically learn embeddings in Euclidean vector spaces, which do not account for this property. For this purpose, we introduce a new approach for learning hierarchical representations of symbolic data by embedding them into hyperbolic space -- or more precisely into an n-dimensional Poincare ball. Due to the underlying hyperbolic geometry, this allows us to learn parsimonious representations of symbolic data by simultaneously capturing hierarchy and similarity. We introduce an efficient algorithm to learn the embeddings based on Riemannian optimization and show experimentally that Poincare embeddings outperform Euclidean embeddings significantly on data with latent hierarchies, both in terms of representation capacity and in terms of generalization ability."
    },
    {
        "paperId": "1e819f533ef2bf5ca50a6b2008d96eaea2a2706e",
        "url": "https://www.semanticscholar.org/paper/1e819f533ef2bf5ca50a6b2008d96eaea2a2706e",
        "title": "Learning Combinatorial Optimization Algorithms over Graphs",
        "venue": "Neural Information Processing Systems",
        "year": 2017,
        "citationCount": 1616,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1704.01665, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "35252180",
                "name": "Elias Boutros Khalil"
            },
            {
                "authorId": "2791430",
                "name": "H. Dai"
            },
            {
                "authorId": "2108307075",
                "name": "Yuyu Zhang"
            },
            {
                "authorId": "1796375",
                "name": "B. Dilkina"
            },
            {
                "authorId": "1779453",
                "name": "Le Song"
            }
        ],
        "abstract": "The design of good heuristics or approximation algorithms for NP-hard combinatorial optimization problems often requires significant specialized knowledge and trial-and-error. Can we automate this challenging, tedious process, and learn the algorithms instead? In many real-world applications, it is typically the case that the same optimization problem is solved again and again on a regular basis, maintaining the same problem structure but differing in the data. This provides an opportunity for learning heuristic algorithms that exploit the structure of such recurring problems. In this paper, we propose a unique combination of reinforcement learning and graph embedding to address this challenge. The learned greedy policy behaves like a meta-algorithm that incrementally constructs a solution, and the action is determined by the output of a graph embedding network capturing the current state of the solution. We show that our framework can be applied to a diverse range of optimization problems over graphs, and learns effective algorithms for the Minimum Vertex Cover, Maximum Cut and Traveling Salesman problems."
    },
    {
        "paperId": "1ecc2bd0bc6ffa0a2f466a058589c20593e3e57c",
        "url": "https://www.semanticscholar.org/paper/1ecc2bd0bc6ffa0a2f466a058589c20593e3e57c",
        "title": "The Marginal Value of Adaptive Gradient Methods in Machine Learning",
        "venue": "Neural Information Processing Systems",
        "year": 2017,
        "citationCount": 1103,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1705.08292, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "144102853",
                "name": "Ashia C. Wilson"
            },
            {
                "authorId": "40458654",
                "name": "R. Roelofs"
            },
            {
                "authorId": "144872294",
                "name": "Mitchell Stern"
            },
            {
                "authorId": "1706280",
                "name": "N. Srebro"
            },
            {
                "authorId": "9229182",
                "name": "B. Recht"
            }
        ],
        "abstract": "Adaptive optimization methods, which perform local optimization with a metric constructed from the history of iterates, are becoming increasingly popular for training deep neural networks. Examples include AdaGrad, RMSProp, and Adam. We show that for simple overparameterized problems, adaptive methods often find drastically different solutions than gradient descent (GD) or stochastic gradient descent (SGD). We construct an illustrative binary classification problem where the data is linearly separable, GD and SGD achieve zero test error, and AdaGrad, Adam, and RMSProp attain test errors arbitrarily close to half. We additionally study the empirical generalization capability of adaptive methods on several state-of-the-art deep learning models. We observe that the solutions found by adaptive methods generalize worse (often significantly worse) than SGD, even when these solutions have better training performance. These results suggest that practitioners should reconsider the use of adaptive methods to train neural networks."
    },
    {
        "paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
        "url": "https://www.semanticscholar.org/paper/204e3073870fae3d05bcbc2f6a8e263d9b72e776",
        "title": "Attention is All you Need",
        "venue": "Neural Information Processing Systems",
        "year": 2017,
        "citationCount": 159909,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1706.03762, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "40348417",
                "name": "Ashish Vaswani"
            },
            {
                "authorId": "1846258",
                "name": "Noam Shazeer"
            },
            {
                "authorId": "3877127",
                "name": "Niki Parmar"
            },
            {
                "authorId": "39328010",
                "name": "Jakob Uszkoreit"
            },
            {
                "authorId": "145024664",
                "name": "Llion Jones"
            },
            {
                "authorId": "19177000",
                "name": "Aidan N. Gomez"
            },
            {
                "authorId": "40527594",
                "name": "Lukasz Kaiser"
            },
            {
                "authorId": "3443442",
                "name": "I. Polosukhin"
            }
        ],
        "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data."
    },
    {
        "paperId": "231af7dc01a166cac3b5b01ca05778238f796e41",
        "url": "https://www.semanticscholar.org/paper/231af7dc01a166cac3b5b01ca05778238f796e41",
        "title": "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium",
        "venue": "Neural Information Processing Systems",
        "year": 2017,
        "citationCount": 16327,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "2445103",
                "name": "M. Heusel"
            },
            {
                "authorId": "19219270",
                "name": "Hubert Ramsauer"
            },
            {
                "authorId": "2465270",
                "name": "Thomas Unterthiner"
            },
            {
                "authorId": "37082831",
                "name": "Bernhard Nessler"
            },
            {
                "authorId": "3308557",
                "name": "Sepp Hochreiter"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "276194e96ebd620b5cff35a9168bdda39a0be57b",
        "url": "https://www.semanticscholar.org/paper/276194e96ebd620b5cff35a9168bdda39a0be57b",
        "title": "Federated Multi-Task Learning",
        "venue": "Neural Information Processing Systems",
        "year": 2017,
        "citationCount": 2003,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1705.10467, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "145260024",
                "name": "Virginia Smith"
            },
            {
                "authorId": "2054892",
                "name": "Chao-Kai Chiang"
            },
            {
                "authorId": "2095979",
                "name": "Maziar Sanjabi"
            },
            {
                "authorId": "145532827",
                "name": "Ameet Talwalkar"
            }
        ],
        "abstract": "Federated learning poses new statistical and systems challenges in training machine learning models over distributed networks of devices. In this work, we show that multi-task learning is naturally suited to handle the statistical challenges of this setting, and propose a novel systems-aware optimization method, MOCHA, that is robust to practical systems issues. Our method and theory for the first time consider issues of high communication cost, stragglers, and fault tolerance for distributed multi-task learning. The resulting method achieves significant speedups compared to alternatives in the federated setting, as we demonstrate through simulations on real-world federated datasets."
    },
    {
        "paperId": "3f1ab8b484f7881a68c8562ff908390742e4ba90",
        "url": "https://www.semanticscholar.org/paper/3f1ab8b484f7881a68c8562ff908390742e4ba90",
        "title": "Can Decentralized Algorithms Outperform Centralized Algorithms? A Case Study for Decentralized Parallel Stochastic Gradient Descent",
        "venue": "Neural Information Processing Systems",
        "year": 2017,
        "citationCount": 1353,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1705.09056, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2922996",
                "name": "Xiangru Lian"
            },
            {
                "authorId": "1776014",
                "name": "Ce Zhang"
            },
            {
                "authorId": "49723481",
                "name": "Huan Zhang"
            },
            {
                "authorId": "1793529",
                "name": "Cho-Jui Hsieh"
            },
            {
                "authorId": null,
                "name": "Wei Zhang"
            },
            {
                "authorId": "40478933",
                "name": "Ji Liu"
            }
        ],
        "abstract": "Most distributed machine learning systems nowadays, including TensorFlow and CNTK, are built in a centralized fashion. One bottleneck of centralized algorithms lies on high communication cost on the central node. Motivated by this, we ask, can decentralized algorithms be faster than its centralized counterpart? \nAlthough decentralized PSGD (D-PSGD) algorithms have been studied by the control community, existing analysis and theory do not show any advantage over centralized PSGD (C-PSGD) algorithms, simply assuming the application scenario where only the decentralized network is available. In this paper, we study a D-PSGD algorithm and provide the first theoretical analysis that indicates a regime in which decentralized algorithms might outperform centralized algorithms for distributed stochastic gradient descent. This is because D-PSGD has comparable total computational complexities to C-PSGD but requires much less communication cost on the busiest node. We further conduct an empirical study to validate our theoretical analysis across multiple frameworks (CNTK and Torch), different network configurations, and computation platforms up to 112 GPUs. On network configurations with low bandwidth or high latency, D-PSGD can be up to one order of magnitude faster than its well-optimized centralized counterparts."
    },
    {
        "paperId": "4070bf3a68b70ab52de35a076a17941543001be2",
        "url": "https://www.semanticscholar.org/paper/4070bf3a68b70ab52de35a076a17941543001be2",
        "title": "Toward Multimodal Image-to-Image Translation",
        "venue": "Neural Information Processing Systems",
        "year": 2017,
        "citationCount": 1413,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1711.11586, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2436356",
                "name": "Jun-Yan Zhu"
            },
            {
                "authorId": "2844849",
                "name": "Richard Zhang"
            },
            {
                "authorId": "38236002",
                "name": "Deepak Pathak"
            },
            {
                "authorId": "1753210",
                "name": "Trevor Darrell"
            },
            {
                "authorId": "1763086",
                "name": "Alexei A. Efros"
            },
            {
                "authorId": "39231399",
                "name": "Oliver Wang"
            },
            {
                "authorId": "2177801",
                "name": "Eli Shechtman"
            }
        ],
        "abstract": "Many image-to-image translation problems are ambiguous, as a single input image may correspond to multiple possible outputs. In this work, we aim to model a \\emph{distribution} of possible outputs in a conditional generative modeling setting. The ambiguity of the mapping is distilled in a low-dimensional latent vector, which can be randomly sampled at test time. A generator learns to map the given input, combined with this latent code, to the output. We explicitly encourage the connection between output and the latent code to be invertible. This helps prevent a many-to-one mapping from the latent code to the output during training, also known as the problem of mode collapse, and produces more diverse results. We explore several variants of this approach by employing different training objectives, network architectures, and methods of injecting the latent code. Our proposed method encourages bijective consistency between the latent encoding and output modes. We present a systematic comparison of our method and other variants on both perceptual realism and diversity."
    },
    {
        "paperId": "424a6e62084d919bfc2e39a507c263e5991ebdad",
        "url": "https://www.semanticscholar.org/paper/424a6e62084d919bfc2e39a507c263e5991ebdad",
        "title": "Self-Normalizing Neural Networks",
        "venue": "Neural Information Processing Systems",
        "year": 2017,
        "citationCount": 2734,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1706.02515, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1994964",
                "name": "G. Klambauer"
            },
            {
                "authorId": "2465270",
                "name": "Thomas Unterthiner"
            },
            {
                "authorId": "144831680",
                "name": "Andreas Mayr"
            },
            {
                "authorId": "3308557",
                "name": "Sepp Hochreiter"
            }
        ],
        "abstract": "Deep Learning has revolutionized vision via convolutional neural networks (CNNs) and natural language processing via recurrent neural networks (RNNs). However, success stories of Deep Learning with standard feed-forward neural networks (FNNs) are rare. FNNs that perform well are typically shallow and, therefore cannot exploit many levels of abstract representations. We introduce self-normalizing neural networks (SNNs) to enable high-level abstract representations. While batch normalization requires explicit normalization, neuron activations of SNNs automatically converge towards zero mean and unit variance. The activation function of SNNs are \"scaled exponential linear units\" (SELUs), which induce self-normalizing properties. Using the Banach fixed-point theorem, we prove that activations close to zero mean and unit variance that are propagated through many network layers will converge towards zero mean and unit variance -- even under the presence of noise and perturbations. This convergence property of SNNs allows to (1) train deep networks with many layers, (2) employ strong regularization, and (3) to make learning highly robust. Furthermore, for activations not close to unit variance, we prove an upper and lower bound on the variance, thus, vanishing and exploding gradients are impossible. We compared SNNs on (a) 121 tasks from the UCI machine learning repository, on (b) drug discovery benchmarks, and on (c) astronomy tasks with standard FNNs and other machine learning methods such as random forests and support vector machines. SNNs significantly outperformed all competing FNN methods at 121 UCI tasks, outperformed all competing methods at the Tox21 dataset, and set a new record at an astronomy data set. The winning SNN architectures are often very deep. Implementations are available at: this http URL."
    },
    {
        "paperId": "429ed4c9845d0abd1f8204e1d7705919559bc2a2",
        "url": "https://www.semanticscholar.org/paper/429ed4c9845d0abd1f8204e1d7705919559bc2a2",
        "title": "Hindsight Experience Replay",
        "venue": "Neural Information Processing Systems",
        "year": 2017,
        "citationCount": 2546,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1707.01495, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2206490",
                "name": "Marcin Andrychowicz"
            },
            {
                "authorId": "150074096",
                "name": "Dwight Crow"
            },
            {
                "authorId": "2064770039",
                "name": "Alex Ray"
            },
            {
                "authorId": "2113526509",
                "name": "Jonas Schneider"
            },
            {
                "authorId": "2062025076",
                "name": "Rachel Fong"
            },
            {
                "authorId": "2930640",
                "name": "Peter Welinder"
            },
            {
                "authorId": "39593364",
                "name": "Bob McGrew"
            },
            {
                "authorId": "2052880384",
                "name": "Joshua Tobin"
            },
            {
                "authorId": "1689992",
                "name": "P. Abbeel"
            },
            {
                "authorId": "2563432",
                "name": "Wojciech Zaremba"
            }
        ],
        "abstract": "Dealing with sparse rewards is one of the biggest challenges in Reinforcement Learning (RL). We present a novel technique called Hindsight Experience Replay which allows sample-efficient learning from rewards which are sparse and binary and therefore avoid the need for complicated reward engineering. It can be combined with an arbitrary off-policy RL algorithm and may be seen as a form of implicit curriculum. \nWe demonstrate our approach on the task of manipulating objects with a robotic arm. In particular, we run experiments on three different tasks: pushing, sliding, and pick-and-place, in each case using only binary rewards indicating whether or not the task is completed. Our ablation studies show that Hindsight Experience Replay is a crucial ingredient which makes training possible in these challenging environments. We show that our policies trained on a physics simulation can be deployed on a physical robot and successfully complete the task."
    },
    {
        "paperId": "442e10a3c6640ded9408622005e3c2a8906ce4c2",
        "url": "https://www.semanticscholar.org/paper/442e10a3c6640ded9408622005e3c2a8906ce4c2",
        "title": "A Unified Approach to Interpreting Model Predictions",
        "venue": "Neural Information Processing Systems",
        "year": 2017,
        "citationCount": 28982,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1705.07874, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "23451726",
                "name": "Scott M. Lundberg"
            },
            {
                "authorId": "2180463",
                "name": "Su-In Lee"
            }
        ],
        "abstract": "Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches."
    },
    {
        "paperId": "497e4b08279d69513e4d2313a7fd9a55dfb73273",
        "url": "https://www.semanticscholar.org/paper/497e4b08279d69513e4d2313a7fd9a55dfb73273",
        "title": "LightGBM: A Highly Efficient Gradient Boosting Decision Tree",
        "venue": "Neural Information Processing Systems",
        "year": 2017,
        "citationCount": 12813,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "35286545",
                "name": "Guolin Ke"
            },
            {
                "authorId": "47580728",
                "name": "Qi Meng"
            },
            {
                "authorId": "50256971",
                "name": "Thomas Finley"
            },
            {
                "authorId": null,
                "name": "Taifeng Wang"
            },
            {
                "authorId": null,
                "name": "Wei Chen"
            },
            {
                "authorId": "3029546",
                "name": "Weidong Ma"
            },
            {
                "authorId": "3006308",
                "name": "Qiwei Ye"
            },
            {
                "authorId": "2110264337",
                "name": "Tie-Yan Liu"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "4bdb91a6e47385292ab7a18e8901a6a25f50cc6b",
        "url": "https://www.semanticscholar.org/paper/4bdb91a6e47385292ab7a18e8901a6a25f50cc6b",
        "title": "TernGrad: Ternary Gradients to Reduce Communication in Distributed Deep Learning",
        "venue": "Neural Information Processing Systems",
        "year": 2017,
        "citationCount": 1035,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1705.07878, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "145225262",
                "name": "W. Wen"
            },
            {
                "authorId": "2110086707",
                "name": "Cong Xu"
            },
            {
                "authorId": "145552742",
                "name": "Feng Yan"
            },
            {
                "authorId": "3207491",
                "name": "Chunpeng Wu"
            },
            {
                "authorId": "2108738288",
                "name": "Yandan Wang"
            },
            {
                "authorId": "5442167",
                "name": "Yiran Chen"
            },
            {
                "authorId": "40348219",
                "name": "Hai Helen Li"
            }
        ],
        "abstract": "High network communication cost for synchronizing gradients and parameters is the well-known bottleneck of distributed training. In this work, we propose TernGrad that uses ternary gradients to accelerate distributed deep learning in data parallelism. Our approach requires only three numerical levels {-1,0,1}, which can aggressively reduce the communication time. We mathematically prove the convergence of TernGrad under the assumption of a bound on gradients. Guided by the bound, we propose layer-wise ternarizing and gradient clipping to improve its convergence. Our experiments show that applying TernGrad on AlexNet does not incur any accuracy loss and can even improve accuracy. The accuracy loss of GoogLeNet induced by TernGrad is less than 2% on average. Finally, a performance model is proposed to study the scalability of TernGrad. Experiments show significant speed gains for various deep neural networks. Our source code is available."
    },
    {
        "paperId": "585bf7bea8fa5267738bc465611d6f197e0f87dd",
        "url": "https://www.semanticscholar.org/paper/585bf7bea8fa5267738bc465611d6f197e0f87dd",
        "title": "Masked Autoregressive Flow for Density Estimation",
        "venue": "Neural Information Processing Systems",
        "year": 2017,
        "citationCount": 1487,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1705.07057, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3065681",
                "name": "G. Papamakarios"
            },
            {
                "authorId": "145797336",
                "name": "Iain Murray"
            },
            {
                "authorId": "2728190",
                "name": "Theo Pavlakou"
            }
        ],
        "abstract": "Autoregressive models are among the best performing neural density estimators. We describe an approach for increasing the flexibility of an autoregressive model, based on modelling the random numbers that the model uses internally when generating data. By constructing a stack of autoregressive models, each modelling the random numbers of the next model in the stack, we obtain a type of normalizing flow suitable for density estimation, which we call Masked Autoregressive Flow. This type of flow is closely related to Inverse Autoregressive Flow and is a generalization of Real NVP. Masked Autoregressive Flow achieves state-of-the-art performance in a range of general-purpose density estimation tasks."
    },
    {
        "paperId": "59a922212153d3407e658109f36c11a34ee7d283",
        "url": "https://www.semanticscholar.org/paper/59a922212153d3407e658109f36c11a34ee7d283",
        "title": "Continual Learning with Deep Generative Replay",
        "venue": "Neural Information Processing Systems",
        "year": 2017,
        "citationCount": 2329,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1705.08690, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "13574658",
                "name": "Hanul Shin"
            },
            {
                "authorId": "2119170990",
                "name": "Jung Kwon Lee"
            },
            {
                "authorId": "2116671349",
                "name": "Jaehong Kim"
            },
            {
                "authorId": "3968500",
                "name": "Jiwon Kim"
            }
        ],
        "abstract": "Attempts to train a comprehensive artificial intelligence capable of solving multiple tasks have been impeded by a chronic problem called catastrophic forgetting. Although simply replaying all previous data alleviates the problem, it requires large memory and even worse, often infeasible in real world applications where the access to past data is limited. Inspired by the generative nature of hippocampus as a short-term memory system in primate brain, we propose the Deep Generative Replay, a novel framework with a cooperative dual model architecture consisting of a deep generative model (\"generator\") and a task solving model (\"solver\"). With only these two models, training data for previous tasks can easily be sampled and interleaved with those for a new task. We test our methods in several sequential learning settings involving image classification tasks."
    },
    {
        "paperId": "5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd",
        "url": "https://www.semanticscholar.org/paper/5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd",
        "title": "Deep Reinforcement Learning from Human Preferences",
        "venue": "Neural Information Processing Systems",
        "year": 2017,
        "citationCount": 4333,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1706.03741, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "145791315",
                "name": "P. Christiano"
            },
            {
                "authorId": "2990741",
                "name": "Jan Leike"
            },
            {
                "authorId": "31035595",
                "name": "Tom B. Brown"
            },
            {
                "authorId": "26890260",
                "name": "Miljan Martic"
            },
            {
                "authorId": "34313265",
                "name": "S. Legg"
            },
            {
                "authorId": "2330246606",
                "name": "Dario Amodei"
            }
        ],
        "abstract": "For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals defined in terms of (non-expert) human preferences between pairs of trajectory segments. We show that this approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on less than one percent of our agent's interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. To demonstrate the flexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any that have been previously learned from human feedback."
    },
    {
        "paperId": "5bf31dc4bd54b623008c13f8bc8954dc7c9a2d80",
        "url": "https://www.semanticscholar.org/paper/5bf31dc4bd54b623008c13f8bc8954dc7c9a2d80",
        "title": "SchNet: A continuous-filter convolutional neural network for modeling quantum interactions",
        "venue": "Neural Information Processing Systems",
        "year": 2017,
        "citationCount": 1258,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1706.08566, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "31961144",
                "name": "Kristof Schtt"
            },
            {
                "authorId": "2113697",
                "name": "Pieter-Jan Kindermans"
            },
            {
                "authorId": "29800712",
                "name": "Huziel Enoc Sauceda Felix"
            },
            {
                "authorId": "7631063",
                "name": "Stefan Chmiela"
            },
            {
                "authorId": "2462983",
                "name": "A. Tkatchenko"
            },
            {
                "authorId": "145034054",
                "name": "K. Mller"
            }
        ],
        "abstract": "Deep learning has the potential to revolutionize quantum chemistry as it is ideally suited to learn representations for structured data and speed up the exploration of chemical space. While convolutional neural networks have proven to be the first choice for images, audio and video data, the atoms in molecules are not restricted to a grid. Instead, their precise locations contain essential physical information, that would get lost if discretized. Thus, we propose to use continuous-filter convolutional layers to be able to model local correlations without requiring the data to lie on a grid. We apply those layers in SchNet: a novel deep learning architecture modeling quantum interactions in molecules. We obtain a joint model for the total energy and interatomic forces that follows fundamental quantum-chemical principles. This includes rotationally invariant energy predictions and a smooth, differentiable potential energy surface. Our architecture achieves state-of-the-art performance for benchmarks of equilibrium molecules and molecular dynamics trajectories. Finally, we introduce a more challenging benchmark with chemical and structural variations that suggests the path for further work."
    },
    {
        "paperId": "6b7d6e6416343b2a122f8416e69059ce919026ef",
        "url": "https://www.semanticscholar.org/paper/6b7d6e6416343b2a122f8416e69059ce919026ef",
        "title": "Inductive Representation Learning on Large Graphs",
        "venue": "Neural Information Processing Systems",
        "year": 2017,
        "citationCount": 17944,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1706.02216, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "49437682",
                "name": "William L. Hamilton"
            },
            {
                "authorId": "4058003",
                "name": "Z. Ying"
            },
            {
                "authorId": "1702139",
                "name": "J. Leskovec"
            }
        ],
        "abstract": "Low-dimensional embeddings of nodes in large graphs have proved extremely useful in a variety of prediction tasks, from content recommendation to identifying protein functions. However, most existing approaches require that all nodes in the graph are present during training of the embeddings; these previous approaches are inherently transductive and do not naturally generalize to unseen nodes. Here we present GraphSAGE, a general, inductive framework that leverages node feature information (e.g., text attributes) to efficiently generate node embeddings for previously unseen data. Instead of training individual embeddings for each node, we learn a function that generates embeddings by sampling and aggregating features from a node's local neighborhood. Our algorithm outperforms strong baselines on three inductive node-classification benchmarks: we classify the category of unseen nodes in evolving information graphs based on citation and Reddit post data, and we show that our algorithm generalizes to completely unseen graphs using a multi-graph dataset of protein-protein interactions."
    },
    {
        "paperId": "6baca6351dc55baac44f0416e74a7e0ba2bfd03e",
        "url": "https://www.semanticscholar.org/paper/6baca6351dc55baac44f0416e74a7e0ba2bfd03e",
        "title": "Visualizing the Loss Landscape of Neural Nets",
        "venue": "Neural Information Processing Systems",
        "year": 2017,
        "citationCount": 2128,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1712.09913, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2152297881",
                "name": "Hao Li"
            },
            {
                "authorId": "144897102",
                "name": "Zheng Xu"
            },
            {
                "authorId": "2189083",
                "name": "Gavin Taylor"
            },
            {
                "authorId": "1962083",
                "name": "T. Goldstein"
            }
        ],
        "abstract": "Neural network training relies on our ability to find \"good\" minimizers of highly non-convex loss functions. It is well-known that certain network architecture designs (e.g., skip connections) produce loss functions that train easier, and well-chosen training parameters (batch size, learning rate, optimizer) produce minimizers that generalize better. However, the reasons for these differences, and their effects on the underlying loss landscape, are not well understood. In this paper, we explore the structure of neural loss functions, and the effect of loss landscapes on generalization, using a range of visualization methods. First, we introduce a simple \"filter normalization\" method that helps us visualize loss function curvature and make meaningful side-by-side comparisons between loss functions. Then, using a variety of visualizations, we explore how network architecture affects the loss landscape, and how training parameters affect the shape of minimizers."
    },
    {
        "paperId": "73b5932b72c780408a29b38ce19641a4c411dc53",
        "url": "https://www.semanticscholar.org/paper/73b5932b72c780408a29b38ce19641a4c411dc53",
        "title": "Learning Efficient Object Detection Models with Knowledge Distillation",
        "venue": "Neural Information Processing Systems",
        "year": 2017,
        "citationCount": 1077,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "2155230013",
                "name": "Guobin Chen"
            },
            {
                "authorId": "17132791",
                "name": "Wongun Choi"
            },
            {
                "authorId": "15644381",
                "name": "Xiang Yu"
            },
            {
                "authorId": "3244463",
                "name": "T. Han"
            },
            {
                "authorId": "2099305",
                "name": "Manmohan Chandraker"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "7493389667058116dbc7e808987f129325ee60d7",
        "url": "https://www.semanticscholar.org/paper/7493389667058116dbc7e808987f129325ee60d7",
        "title": "Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results",
        "venue": "Neural Information Processing Systems",
        "year": 2017,
        "citationCount": 4902,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "9942966",
                "name": "Antti Tarvainen"
            },
            {
                "authorId": "2132516",
                "name": "Harri Valpola"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "7c3ece1ba41c415d7e81cfa5ca33a8de66efd434",
        "url": "https://www.semanticscholar.org/paper/7c3ece1ba41c415d7e81cfa5ca33a8de66efd434",
        "title": "Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments",
        "venue": "Neural Information Processing Systems",
        "year": 2017,
        "citationCount": 5246,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1706.02275, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2054294",
                "name": "Ryan Lowe"
            },
            {
                "authorId": "31613801",
                "name": "Yi Wu"
            },
            {
                "authorId": "3025260",
                "name": "Aviv Tamar"
            },
            {
                "authorId": "40638357",
                "name": "J. Harb"
            },
            {
                "authorId": "1689992",
                "name": "P. Abbeel"
            },
            {
                "authorId": "2080746",
                "name": "Igor Mordatch"
            }
        ],
        "abstract": "We explore deep reinforcement learning methods for multi-agent domains. We begin by analyzing the difficulty of traditional algorithms in the multi-agent case: Q-learning is challenged by an inherent non-stationarity of the environment, while policy gradient suffers from a variance that increases as the number of agents grows. We then present an adaptation of actor-critic methods that considers action policies of other agents and is able to successfully learn policies that require complex multi-agent coordination. Additionally, we introduce a training regimen utilizing an ensemble of policies for each agent that leads to more robust multi-agent policies. We show the strength of our approach compared to existing methods in cooperative as well as competitive scenarios, where agent populations are able to discover various physical and informational coordination strategies."
    },
    {
        "paperId": "83083c5760bd1b58e5f827e57415e5ed676ef3bc",
        "url": "https://www.semanticscholar.org/paper/83083c5760bd1b58e5f827e57415e5ed676ef3bc",
        "title": "Universal Style Transfer via Feature Transforms",
        "venue": "Neural Information Processing Systems",
        "year": 2017,
        "citationCount": 1076,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1705.08086, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "152998391",
                "name": "Yijun Li"
            },
            {
                "authorId": "144823841",
                "name": "Chen Fang"
            },
            {
                "authorId": "1768964",
                "name": "Jimei Yang"
            },
            {
                "authorId": "8056043",
                "name": "Zhaowen Wang"
            },
            {
                "authorId": "145574672",
                "name": "Xin Lu"
            },
            {
                "authorId": "1715634",
                "name": "Ming-Hsuan Yang"
            }
        ],
        "abstract": "Universal style transfer aims to transfer arbitrary visual styles to content images. Existing feed-forward based methods, while enjoying the inference efficiency, are mainly limited by inability of generalizing to unseen styles or compromised visual quality. In this paper, we present a simple yet effective method that tackles these limitations without training on any pre-defined styles. The key ingredient of our method is a pair of feature transforms, whitening and coloring, that are embedded to an image reconstruction network. The whitening and coloring transforms reflect a direct matching of feature covariance of the content image to a given style image, which shares similar spirits with the optimization of Gram matrix based cost in neural style transfer. We demonstrate the effectiveness of our algorithm by generating high-quality stylized images with comparisons to a number of recent methods. We also analyze our method by visualizing the whitened features and synthesizing textures via simple feature coloring."
    },
    {
        "paperId": "8674494bd7a076286b905912d26d47f7501c4046",
        "url": "https://www.semanticscholar.org/paper/8674494bd7a076286b905912d26d47f7501c4046",
        "title": "PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space",
        "venue": "Neural Information Processing Systems",
        "year": 2017,
        "citationCount": 12891,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1706.02413, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "144329939",
                "name": "C. Qi"
            },
            {
                "authorId": "47782132",
                "name": "L. Yi"
            },
            {
                "authorId": "144914140",
                "name": "Hao Su"
            },
            {
                "authorId": "51352814",
                "name": "L. Guibas"
            }
        ],
        "abstract": "Few prior works study deep learning on point sets. PointNet by Qi et al. is a pioneer in this direction. However, by design PointNet does not capture local structures induced by the metric space points live in, limiting its ability to recognize fine-grained patterns and generalizability to complex scenes. In this work, we introduce a hierarchical neural network that applies PointNet recursively on a nested partitioning of the input point set. By exploiting metric space distances, our network is able to learn local features with increasing contextual scales. With further observation that point sets are usually sampled with varying densities, which results in greatly decreased performance for networks trained on uniform densities, we propose novel set learning layers to adaptively combine features from multiple scales. Experiments show that our network called PointNet++ is able to learn deep point set features efficiently and robustly. In particular, results significantly better than state-of-the-art have been obtained on challenging benchmarks of 3D point clouds."
    },
    {
        "paperId": "9171e83fb98299e14cbb3673437a0495a213767a",
        "url": "https://www.semanticscholar.org/paper/9171e83fb98299e14cbb3673437a0495a213767a",
        "title": "Conditional Adversarial Domain Adaptation",
        "venue": "Neural Information Processing Systems",
        "year": 2017,
        "citationCount": 2613,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "35776445",
                "name": "Mingsheng Long"
            },
            {
                "authorId": "3451430",
                "name": "Zhangjie Cao"
            },
            {
                "authorId": "2144499343",
                "name": "Jianmin Wang"
            },
            {
                "authorId": "1694621",
                "name": "Michael I. Jordan"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "9583ac53a19cdf0db81fef6eb0b63e66adbe2324",
        "url": "https://www.semanticscholar.org/paper/9583ac53a19cdf0db81fef6eb0b63e66adbe2324",
        "title": "Machine Learning with Adversaries: Byzantine Tolerant Gradient Descent",
        "venue": "Neural Information Processing Systems",
        "year": 2017,
        "citationCount": 2252,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "3094352",
                "name": "Peva Blanchard"
            },
            {
                "authorId": "9623412",
                "name": "El Mahdi El Mhamdi"
            },
            {
                "authorId": "1727558",
                "name": "R. Guerraoui"
            },
            {
                "authorId": "1718150",
                "name": "J. Stainer"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "9753967a3af8e1db1e2da52a9bb3255bd1ce5c51",
        "url": "https://www.semanticscholar.org/paper/9753967a3af8e1db1e2da52a9bb3255bd1ce5c51",
        "title": "Spectrally-normalized margin bounds for neural networks",
        "venue": "Neural Information Processing Systems",
        "year": 2017,
        "citationCount": 1329,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1706.08498, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1745169",
                "name": "P. Bartlett"
            },
            {
                "authorId": "26198391",
                "name": "Dylan J. Foster"
            },
            {
                "authorId": "1750943",
                "name": "Matus Telgarsky"
            }
        ],
        "abstract": "This paper presents a margin-based multiclass generalization bound for neural networks that scales with their margin-normalized \"spectral complexity\": their Lipschitz constant, meaning the product of the spectral norms of the weight matrices, times a certain correction factor. This bound is empirically investigated for a standard AlexNet network trained with SGD on the mnist and cifar10 datasets, with both original and random labels; the bound, the Lipschitz constants, and the excess risks are all in direct correlation, suggesting both that SGD selects predictors whose complexity scales with the difficulty of the learning task, and secondly that the presented bound is sensitive to this complexity."
    },
    {
        "paperId": "b69badabc3fddc9710faa44c530473397303b0b9",
        "url": "https://www.semanticscholar.org/paper/b69badabc3fddc9710faa44c530473397303b0b9",
        "title": "Unsupervised Image-to-Image Translation Networks",
        "venue": "Neural Information Processing Systems",
        "year": 2017,
        "citationCount": 2883,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1703.00848, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "39793900",
                "name": "Ming-Yu Liu"
            },
            {
                "authorId": "1733858",
                "name": "T. Breuel"
            },
            {
                "authorId": "2376331450",
                "name": "Jan Kautz"
            }
        ],
        "abstract": "Unsupervised image-to-image translation aims at learning a joint distribution of images in different domains by using images from the marginal distributions in individual domains. Since there exists an infinite set of joint distributions that can arrive the given marginal distributions, one could infer nothing about the joint distribution from the marginal distributions without additional assumptions. To address the problem, we make a shared-latent space assumption and propose an unsupervised image-to-image translation framework based on Coupled GANs. We compare the proposed framework with competing approaches and present high quality image translation results on various challenging unsupervised image translation tasks, including street scene image translation, animal image translation, and face image translation. We also apply the proposed framework to domain adaptation and achieve state-of-the-art performance on benchmark datasets. Code and additional results are available in this https URL ."
    },
    {
        "paperId": "c269858a7bb34e8350f2442ccf37797856ae9bca",
        "url": "https://www.semanticscholar.org/paper/c269858a7bb34e8350f2442ccf37797856ae9bca",
        "title": "Prototypical Networks for Few-shot Learning",
        "venue": "Neural Information Processing Systems",
        "year": 2017,
        "citationCount": 9224,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1703.05175, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "39770136",
                "name": "Jake Snell"
            },
            {
                "authorId": "1754860",
                "name": "Kevin Swersky"
            },
            {
                "authorId": "1804104",
                "name": "R. Zemel"
            }
        ],
        "abstract": "We propose Prototypical Networks for the problem of few-shot classification, where a classifier must generalize to new classes not seen in the training set, given only a small number of examples of each new class. Prototypical Networks learn a metric space in which classification can be performed by computing distances to prototype representations of each class. Compared to recent approaches for few-shot learning, they reflect a simpler inductive bias that is beneficial in this limited-data regime, and achieve excellent results. We provide an analysis showing that some simple design decisions can yield substantial improvements over recent approaches involving complicated architectural choices and meta-learning. We further extend Prototypical Networks to zero-shot learning and achieve state-of-the-art results on the CU-Birds dataset."
    },
    {
        "paperId": "c4c06578f4870e4b126e6837907929f3c900b99f",
        "url": "https://www.semanticscholar.org/paper/c4c06578f4870e4b126e6837907929f3c900b99f",
        "title": "Dynamic Routing Between Capsules",
        "venue": "Neural Information Processing Systems",
        "year": 2017,
        "citationCount": 4937,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1710.09829, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "143752292",
                "name": "S. Sabour"
            },
            {
                "authorId": "27737461",
                "name": "Nicholas Frosst"
            },
            {
                "authorId": "1695689",
                "name": "Geoffrey E. Hinton"
            }
        ],
        "abstract": "A capsule is a group of neurons whose activity vector represents the instantiation parameters of a specific type of entity such as an object or an object part. We use the length of the activity vector to represent the probability that the entity exists and its orientation to represent the instantiation parameters. Active capsules at one level make predictions, via transformation matrices, for the instantiation parameters of higher-level capsules. When multiple predictions agree, a higher level capsule becomes active. We show that a discrimininatively trained, multi-layer capsule system achieves state-of-the-art performance on MNIST and is considerably better than a convolutional net at recognizing highly overlapping digits. To achieve these results we use an iterative routing-by-agreement mechanism: A lower-level capsule prefers to send its output to higher level capsules whose activity vectors have a big scalar product with the prediction coming from the lower-level capsule."
    },
    {
        "paperId": "c88e8d85fd5160b0793598bda037f977366acf7a",
        "url": "https://www.semanticscholar.org/paper/c88e8d85fd5160b0793598bda037f977366acf7a",
        "title": "Are GANs Created Equal? A Large-Scale Study",
        "venue": "Neural Information Processing Systems",
        "year": 2017,
        "citationCount": 1067,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1711.10337, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "34302129",
                "name": "Mario Lucic"
            },
            {
                "authorId": "2006889",
                "name": "Karol Kurach"
            },
            {
                "authorId": "145605490",
                "name": "Marcin Michalski"
            },
            {
                "authorId": "1802148",
                "name": "S. Gelly"
            },
            {
                "authorId": "1698617",
                "name": "O. Bousquet"
            }
        ],
        "abstract": "Generative adversarial networks (GAN) are a powerful subclass of generative models. Despite a very rich research activity leading to numerous interesting GAN algorithms, it is still very hard to assess which algorithm(s) perform better than others. We conduct a neutral, multi-faceted large-scale empirical study on state-of-the art models and evaluation measures. We find that most models can reach similar scores with enough hyperparameter optimization and random restarts. This suggests that improvements can arise from a higher computational budget and tuning more than fundamental algorithmic changes. To overcome some limitations of the current metrics, we also propose several data sets on which precision and recall can be computed. Our experimental results suggest that future GAN research should be based on more systematic and objective evaluation procedures. Finally, we did not find evidence that any of the tested algorithms consistently outperforms the non-saturating GAN introduced in \\cite{goodfellow2014generative}."
    },
    {
        "paperId": "d53fb3feeeab07a0d70bf466dd473ec6052ecc07",
        "url": "https://www.semanticscholar.org/paper/d53fb3feeeab07a0d70bf466dd473ec6052ecc07",
        "title": "Exploring Generalization in Deep Learning",
        "venue": "Neural Information Processing Systems",
        "year": 2017,
        "citationCount": 1350,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1706.08947, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3007442",
                "name": "Behnam Neyshabur"
            },
            {
                "authorId": "1798880",
                "name": "Srinadh Bhojanapalli"
            },
            {
                "authorId": "46948352",
                "name": "D. McAllester"
            },
            {
                "authorId": "1706280",
                "name": "N. Srebro"
            }
        ],
        "abstract": "With a goal of understanding what drives generalization in deep networks, we consider several recently suggested explanations, including norm-based control, sharpness and robustness. We study how these measures can ensure generalization, highlighting the importance of scale normalization, and making a connection between sharpness and PAC-Bayes theory. We then investigate how well the measures explain different observed phenomena."
    },
    {
        "paperId": "d89ee98810039d2061ed42ee8026da49c503d16b",
        "url": "https://www.semanticscholar.org/paper/d89ee98810039d2061ed42ee8026da49c503d16b",
        "title": "Learning multiple visual domains with residual adapters",
        "venue": "Neural Information Processing Systems",
        "year": 2017,
        "citationCount": 1018,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1705.08045, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "8478422",
                "name": "Sylvestre-Alvise Rebuffi"
            },
            {
                "authorId": "2518212",
                "name": "Hakan Bilen"
            },
            {
                "authorId": "1687524",
                "name": "A. Vedaldi"
            }
        ],
        "abstract": "There is a growing interest in learning data representations that work well for many different types of problems and data. In this paper, we look in particular at the task of learning a single visual representation that can be successfully utilized in the analysis of very different types of images, from dog breeds to stop signs and digits. Inspired by recent work on learning networks that predict the parameters of another, we develop a tunable deep network architecture that, by means of adapter residual modules, can be steered on the fly to diverse visual domains. Our method achieves a high degree of parameter sharing while maintaining or even improving the accuracy of domain-specific representations. We also introduce the Visual Decathlon Challenge, a benchmark that evaluates the ability of representations to capture simultaneously ten very different visual domains and measures their ability to recognize well uniformly."
    },
    {
        "paperId": "edf73ab12595c6709f646f542a0d2b33eb20a3f4",
        "url": "https://www.semanticscholar.org/paper/edf73ab12595c6709f646f542a0d2b33eb20a3f4",
        "title": "Improved Training of Wasserstein GANs",
        "venue": "Neural Information Processing Systems",
        "year": 2017,
        "citationCount": 10378,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1704.00028, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2708454",
                "name": "Ishaan Gulrajani"
            },
            {
                "authorId": "2054472270",
                "name": "Faruk Ahmed"
            },
            {
                "authorId": "2877311",
                "name": "Martn Arjovsky"
            },
            {
                "authorId": "3074927",
                "name": "Vincent Dumoulin"
            },
            {
                "authorId": "1760871",
                "name": "Aaron C. Courville"
            }
        ],
        "abstract": "Generative Adversarial Networks (GANs) are powerful generative models, but suffer from training instability. The recently proposed Wasserstein GAN (WGAN) makes progress toward stable training of GANs, but sometimes can still generate only low-quality samples or fail to converge. We find that these problems are often due to the use of weight clipping in WGAN to enforce a Lipschitz constraint on the critic, which can lead to undesired behavior. We propose an alternative to clipping weights: penalize the norm of gradient of the critic with respect to its input. Our proposed method performs better than standard WGAN and enables stable training of a wide variety of GAN architectures with almost no hyperparameter tuning, including 101-layer ResNets and language models over discrete data. We also achieve high quality generations on CIFAR-10 and LSUN bedrooms."
    },
    {
        "paperId": "ee0a0f04d45f86bf50b24d7258e884725fcaa621",
        "url": "https://www.semanticscholar.org/paper/ee0a0f04d45f86bf50b24d7258e884725fcaa621",
        "title": "CatBoost: unbiased boosting with categorical features",
        "venue": "Neural Information Processing Systems",
        "year": 2017,
        "citationCount": 4645,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "144944365",
                "name": "L. Ostroumova"
            },
            {
                "authorId": "145004420",
                "name": "Gleb Gusev"
            },
            {
                "authorId": "144050382",
                "name": "A. Vorobev"
            },
            {
                "authorId": "19256191",
                "name": "Anna Veronika Dorogush"
            },
            {
                "authorId": "31460507",
                "name": "Andrey Gulin"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "f466157848d1a7772fb6d02cdac9a7a5e7ef982e",
        "url": "https://www.semanticscholar.org/paper/f466157848d1a7772fb6d02cdac9a7a5e7ef982e",
        "title": "Neural Discrete Representation Learning",
        "venue": "Neural Information Processing Systems",
        "year": 2017,
        "citationCount": 6305,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1711.00937, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3422336",
                "name": "Aron van den Oord"
            },
            {
                "authorId": "1689108",
                "name": "O. Vinyals"
            },
            {
                "authorId": "2645384",
                "name": "K. Kavukcuoglu"
            }
        ],
        "abstract": "Learning useful representations without supervision remains a key challenge in machine learning. In this paper, we propose a simple yet powerful generative model that learns such discrete representations. Our model, the Vector Quantised-Variational AutoEncoder (VQ-VAE), differs from VAEs in two key ways: the encoder network outputs discrete, rather than continuous, codes; and the prior is learnt rather than static. In order to learn a discrete latent representation, we incorporate ideas from vector quantisation (VQ). Using the VQ method allows the model to circumvent issues of \"posterior collapse\" -- where the latents are ignored when they are paired with a powerful autoregressive decoder -- typically observed in the VAE framework. Pairing these representations with an autoregressive prior, the model can generate high quality images, videos, and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes, providing further evidence of the utility of the learnt representations."
    },
    {
        "paperId": "ff7bcaa4556cb13fc7bf03e477172493546172cd",
        "url": "https://www.semanticscholar.org/paper/ff7bcaa4556cb13fc7bf03e477172493546172cd",
        "title": "What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?",
        "venue": "Neural Information Processing Systems",
        "year": 2017,
        "citationCount": 5351,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1703.04977, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "47645184",
                "name": "Alex Kendall"
            },
            {
                "authorId": "2681954",
                "name": "Y. Gal"
            }
        ],
        "abstract": "There are two major types of uncertainty one can model. Aleatoric uncertainty captures noise inherent in the observations. On the other hand, epistemic uncertainty accounts for uncertainty in the model - uncertainty which can be explained away given enough data. Traditionally it has been difficult to model epistemic uncertainty in computer vision, but with new Bayesian deep learning tools this is now possible. We study the benefits of modeling epistemic vs. aleatoric uncertainty in Bayesian deep learning models for vision tasks. For this we present a Bayesian deep learning framework combining input-dependent aleatoric uncertainty together with epistemic uncertainty. We study models under the framework with per-pixel semantic segmentation and depth regression tasks. Further, our explicit uncertainty formulation leads to new loss functions for these tasks, which can be interpreted as learned attenuation. This makes the loss more robust to noisy data, also giving new state-of-the-art results on segmentation and depth regression benchmarks."
    },
    {
        "paperId": "0366b6396610708a77540564050a90a761a28937",
        "url": "https://www.semanticscholar.org/paper/0366b6396610708a77540564050a90a761a28937",
        "title": "Reinforcement Learning for Solving the Vehicle Routing Problem",
        "venue": "Neural Information Processing Systems",
        "year": 2018,
        "citationCount": 1035,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1802.04240, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "49900733",
                "name": "M. Nazari"
            },
            {
                "authorId": "35643794",
                "name": "Afshin Oroojlooy"
            },
            {
                "authorId": "2908169",
                "name": "L. Snyder"
            },
            {
                "authorId": "144696183",
                "name": "Martin Takc"
            }
        ],
        "abstract": "We present an end-to-end framework for solving the Vehicle Routing Problem (VRP) using reinforcement learning. In this approach, we train a single model that finds near-optimal solutions for problem instances sampled from a given distribution, only by observing the reward signals and following feasibility rules. Our model represents a parameterized stochastic policy, and by applying a policy gradient algorithm to optimize its parameters, the trained model produces the solution as a sequence of consecutive actions in real time, without the need to re-train for every new problem instance. On capacitated VRP, our approach outperforms classical heuristics and Google's OR-Tools on medium-sized instances in solution quality with comparable computation time (after training). We demonstrate how our approach can handle problems with split delivery and explore the effect of such deliveries on the solution quality. Our proposed framework can be applied to other variants of the VRP such as the stochastic VRP, and has the potential to be applied more generally to combinatorial optimization problems."
    },
    {
        "paperId": "0cf102da6dd4276115c63cbb6797f24ed450fea1",
        "url": "https://www.semanticscholar.org/paper/0cf102da6dd4276115c63cbb6797f24ed450fea1",
        "title": "Towards Robust Interpretability with Self-Explaining Neural Networks",
        "venue": "Neural Information Processing Systems",
        "year": 2018,
        "citationCount": 1055,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1806.07538, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1390096054",
                "name": "David Alvarez-Melis"
            },
            {
                "authorId": "35132120",
                "name": "T. Jaakkola"
            }
        ],
        "abstract": "Most recent work on interpretability of complex machine learning models has focused on estimating a posteriori explanations for previously trained models around specific predictions. Self-explaining models where interpretability plays a key role already during learning have received much less attention. We propose three desiderata for explanations in general  explicitness, faithfulness, and stability  and show that existing methods do not satisfy them. In response, we design self-explaining models in stages, progressively generalizing linear classifiers to complex yet architecturally explicit models. Faithfulness and stability are enforced via regularization specifically tailored to such models. Experimental results across various benchmark datasets show that our framework offers a promising direction for reconciling model complexity and interpretability."
    },
    {
        "paperId": "138fafc2d679bc6446ff74f55dfd316b0d5674b9",
        "url": "https://www.semanticscholar.org/paper/138fafc2d679bc6446ff74f55dfd316b0d5674b9",
        "title": "Evidential Deep Learning to Quantify Classification Uncertainty",
        "venue": "Neural Information Processing Systems",
        "year": 2018,
        "citationCount": 1249,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1806.01768, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1715430",
                "name": "M. Sensoy"
            },
            {
                "authorId": "11213556",
                "name": "M. Kandemir"
            },
            {
                "authorId": "1795727",
                "name": "Lance M. Kaplan"
            }
        ],
        "abstract": "Deterministic neural nets have been shown to learn effective predictors on a wide range of machine learning problems. However, as the standard approach is to train the network to minimize a prediction loss, the resultant model remains ignorant to its prediction confidence. Orthogonally to Bayesian neural nets that indirectly infer prediction uncertainty through weight uncertainties, we propose explicit modeling of the same using the theory of subjective logic. By placing a Dirichlet distribution on the class probabilities, we treat predictions of a neural net as subjective opinions and learn the function that collects the evidence leading to these opinions by a deterministic neural net from data. The resultant predictor for a multi-class classification problem is another Dirichlet distribution whose parameters are set by the continuous output of a neural net. We provide a preliminary analysis on how the peculiarities of our new loss function drive improved uncertainty estimation. We observe that our method achieves unprecedented success on detection of out-of-distribution queries and endurance against adversarial perturbations."
    },
    {
        "paperId": "1e1855ca80e8ac3de0e169871f320416902e9ad1",
        "url": "https://www.semanticscholar.org/paper/1e1855ca80e8ac3de0e169871f320416902e9ad1",
        "title": "Generalized Cross Entropy Loss for Training Deep Neural Networks with Noisy Labels",
        "venue": "Neural Information Processing Systems",
        "year": 2018,
        "citationCount": 2991,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1805.07836, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1491240545",
                "name": "Zhilu Zhang"
            },
            {
                "authorId": "2369409",
                "name": "M. Sabuncu"
            }
        ],
        "abstract": "Deep neural networks (DNNs) have achieved tremendous success in a variety of applications across many disciplines. Yet, their superior performance comes with the expensive cost of requiring correctly annotated large-scale datasets. Moreover, due to DNNs' rich capacity, errors in training labels can hamper performance. To combat this problem, mean absolute error (MAE) has recently been proposed as a noise-robust alternative to the commonly-used categorical cross entropy (CCE) loss. However, as we show in this paper, MAE can perform poorly with DNNs and challenging datasets. Here, we present a theoretically grounded set of noise-robust loss functions that can be seen as a generalization of MAE and CCE. Proposed loss functions can be readily applied with any existing DNN architecture and algorithm, while yielding good performance in a wide range of noisy label scenarios. We report results from experiments conducted with CIFAR-10, CIFAR-100 and FASHION-MNIST datasets and synthetically generated noisy labels."
    },
    {
        "paperId": "21b786b3f870fc7fa247c143aa41de88b1fc6141",
        "url": "https://www.semanticscholar.org/paper/21b786b3f870fc7fa247c143aa41de88b1fc6141",
        "title": "Glow: Generative Flow with Invertible 1x1 Convolutions",
        "venue": "Neural Information Processing Systems",
        "year": 2018,
        "citationCount": 3395,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1807.03039, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1726807",
                "name": "Diederik P. Kingma"
            },
            {
                "authorId": "6515819",
                "name": "Prafulla Dhariwal"
            }
        ],
        "abstract": "Flow-based generative models (Dinh et al., 2014) are conceptually attractive due to tractability of the exact log-likelihood, tractability of exact latent-variable inference, and parallelizability of both training and synthesis. In this paper we propose Glow, a simple type of generative flow using an invertible 1x1 convolution. Using our method we demonstrate a significant improvement in log-likelihood on standard benchmarks. Perhaps most strikingly, we demonstrate that a generative model optimized towards the plain log-likelihood objective is capable of efficient realistic-looking synthesis and manipulation of large images. The code for our model is available at this https URL"
    },
    {
        "paperId": "2b0d7e51efd004fe3847f54863540c79312f3546",
        "url": "https://www.semanticscholar.org/paper/2b0d7e51efd004fe3847f54863540c79312f3546",
        "title": "Multi-Task Learning as Multi-Objective Optimization",
        "venue": "Neural Information Processing Systems",
        "year": 2018,
        "citationCount": 1524,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1810.04650, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3114252",
                "name": "Ozan Sener"
            },
            {
                "authorId": "145231047",
                "name": "V. Koltun"
            }
        ],
        "abstract": "In multi-task learning, multiple tasks are solved jointly, sharing inductive bias between them. Multi-task learning is inherently a multi-objective problem because different tasks may conflict, necessitating a trade-off. A common compromise is to optimize a proxy objective that minimizes a weighted linear combination of per-task losses. However, this workaround is only valid when the tasks do not compete, which is rarely the case. In this paper, we explicitly cast multi-task learning as multi-objective optimization, with the overall objective of finding a Pareto optimal solution. To this end, we use algorithms developed in the gradient-based multi-objective optimization literature. These algorithms are not directly applicable to large-scale learning problems since they scale poorly with the dimensionality of the gradients and the number of tasks. We therefore propose an upper bound for the multi-objective loss and show that it can be optimized efficiently. We further prove that optimizing this upper bound yields a Pareto optimal solution under realistic assumptions. We apply our method to a variety of multi-task deep learning problems including digit classification, scene understanding (joint semantic segmentation, instance segmentation, and depth estimation), and multi-label classification. Our method produces higher-performing models than recent multi-task learning formulations or per-task training."
    },
    {
        "paperId": "41cca0b0a27ba363ca56e7033569aeb1922b0ac9",
        "url": "https://www.semanticscholar.org/paper/41cca0b0a27ba363ca56e7033569aeb1922b0ac9",
        "title": "Recurrent World Models Facilitate Policy Evolution",
        "venue": "Neural Information Processing Systems",
        "year": 2018,
        "citationCount": 1112,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1809.01999, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "39810222",
                "name": "David R Ha"
            },
            {
                "authorId": "145341374",
                "name": "J. Schmidhuber"
            }
        ],
        "abstract": "A generative recurrent neural network is quickly trained in an unsupervised manner to model popular reinforcement learning environments through compressed spatio-temporal representations. The world model's extracted features are fed into compact and simple policies trained by evolution, achieving state of the art results in various environments. We also train our agent entirely inside of an environment generated by its own internal world model, and transfer this policy back into the actual environment. Interactive version of this paper is available at https://worldmodels.github.io"
    },
    {
        "paperId": "449310e3538b08b43227d660227dfd2875c3c3c1",
        "url": "https://www.semanticscholar.org/paper/449310e3538b08b43227d660227dfd2875c3c3c1",
        "title": "Neural Ordinary Differential Equations",
        "venue": "Neural Information Processing Systems",
        "year": 2018,
        "citationCount": 6154,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1806.07366, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "11126631",
                "name": "T. Chen"
            },
            {
                "authorId": "40959192",
                "name": "Yulia Rubanova"
            },
            {
                "authorId": "51012893",
                "name": "J. Bettencourt"
            },
            {
                "authorId": "1704657",
                "name": "D. Duvenaud"
            }
        ],
        "abstract": "We introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The output of the network is computed using a black-box differential equation solver. These continuous-depth models have constant memory cost, adapt their evaluation strategy to each input, and can explicitly trade numerical precision for speed. We demonstrate these properties in continuous-depth residual networks and continuous-time latent variable models. We also construct continuous normalizing flows, a generative model that can train by maximum likelihood, without partitioning or ordering the data dimensions. For training, we show how to scalably backpropagate through any ODE solver, without access to its internal operations. This allows end-to-end training of ODEs within larger models."
    },
    {
        "paperId": "56136aa0b2c347cbcf3d50821f310c4253155026",
        "url": "https://www.semanticscholar.org/paper/56136aa0b2c347cbcf3d50821f310c4253155026",
        "title": "Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models",
        "venue": "Neural Information Processing Systems",
        "year": 2018,
        "citationCount": 1389,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1805.12114, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "46224156",
                "name": "Kurtland Chua"
            },
            {
                "authorId": "35159852",
                "name": "R. Calandra"
            },
            {
                "authorId": "49686609",
                "name": "R. McAllister"
            },
            {
                "authorId": "1736651",
                "name": "S. Levine"
            }
        ],
        "abstract": "Model-based reinforcement learning (RL) algorithms can attain excellent sample efficiency, but often lag behind the best model-free algorithms in terms of asymptotic performance. This is especially true with high-capacity parametric function approximators, such as deep networks. In this paper, we study how to bridge this gap, by employing uncertainty-aware dynamics models. We propose a new algorithm called probabilistic ensembles with trajectory sampling (PETS) that combines uncertainty-aware deep network dynamics models with sampling-based uncertainty propagation. Our comparison to state-of-the-art model-based and model-free deep RL algorithms shows that our approach matches the asymptotic performance of model-free algorithms on several challenging benchmark tasks, while requiring significantly fewer samples (e.g., 8 and 125 times fewer samples than Soft Actor Critic and Proximal Policy Optimization respectively on the half-cheetah task)."
    },
    {
        "paperId": "6400c36efdb8a66b401b6aef26c057227266fddd",
        "url": "https://www.semanticscholar.org/paper/6400c36efdb8a66b401b6aef26c057227266fddd",
        "title": "PointCNN: Convolution On X-Transformed Points",
        "venue": "Neural Information Processing Systems",
        "year": 2018,
        "citationCount": 2736,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1801.07791, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1920864",
                "name": "Yangyan Li"
            },
            {
                "authorId": "35526877",
                "name": "Rui Bu"
            },
            {
                "authorId": "71614496",
                "name": "Mingchao Sun"
            },
            {
                "authorId": "39533001",
                "name": "Wei Wu"
            },
            {
                "authorId": "7759488",
                "name": "Xinhan Di"
            },
            {
                "authorId": "2028246830",
                "name": "Baoquan Chen"
            }
        ],
        "abstract": "We present a simple and general framework for feature learning from point clouds. The key to the success of CNNs is the convolution operator that is capable of leveraging spatially-local correlation in data represented densely in grids (e.g. images). However, point clouds are irregular and unordered, thus directly convolving kernels against features associated with the points, will result in desertion of shape information and variance to point ordering. To address these problems, we propose to learn an $\\mathcal{X}$-transformation from the input points, to simultaneously promote two causes. The first is the weighting of the input features associated with the points, and the second is the permutation of the points into a latent and potentially canonical order. Element-wise product and sum operations of the typical convolution operator are subsequently applied on the $\\mathcal{X}$-transformed features. The proposed method is a generalization of typical CNNs to feature learning from point clouds, thus we call it PointCNN. Experiments show that PointCNN achieves on par or better performance than state-of-the-art methods on multiple challenging benchmark datasets and tasks."
    },
    {
        "paperId": "7a84a692327534fd227fa1e07fcb3816b633c591",
        "url": "https://www.semanticscholar.org/paper/7a84a692327534fd227fa1e07fcb3816b633c591",
        "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
        "venue": "Neural Information Processing Systems",
        "year": 2018,
        "citationCount": 3646,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1806.07572, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "51034104",
                "name": "Arthur Jacot"
            },
            {
                "authorId": "50810629",
                "name": "Franck Gabriel"
            },
            {
                "authorId": "100474520",
                "name": "Clment Hongler"
            }
        ],
        "abstract": "At initialization, artificial neural networks (ANNs) are equivalent to Gaussian processes in the infinite-width limit, thus connecting them to kernel methods. We prove that the evolution of an ANN during training can also be described by a kernel: during gradient descent on the parameters of an ANN, the network function (which maps input vectors to output vectors) follows the so-called kernel gradient associated with a new object, which we call the Neural Tangent Kernel (NTK). This kernel is central to describe the generalization features of ANNs. While the NTK is random at initialization and varies during training, in the infinite-width limit it converges to an explicit limiting kernel and stays constant during training. This makes it possible to study the training of ANNs in function space instead of parameter space. Convergence of the training can then be related to the positive-definiteness of the limiting NTK. We then focus on the setting of least-squares regression and show that in the infinite-width limit, the network function follows a linear differential equation during training. The convergence is fastest along the largest kernel principal components of the input data with respect to the NTK, hence suggesting a theoretical motivation for early stopping. Finally we study the NTK numerically, observe its behavior for wide networks, and compare it to the infinite-width limit."
    },
    {
        "paperId": "7cf1969d90062090953b8c7a85070328e968ece4",
        "url": "https://www.semanticscholar.org/paper/7cf1969d90062090953b8c7a85070328e968ece4",
        "title": "Joint Autoregressive and Hierarchical Priors for Learned Image Compression",
        "venue": "Neural Information Processing Systems",
        "year": 2018,
        "citationCount": 1459,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1809.02736, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3144223",
                "name": "David C. Minnen"
            },
            {
                "authorId": "144517934",
                "name": "J. Ball"
            },
            {
                "authorId": "1805076",
                "name": "G. Toderici"
            }
        ],
        "abstract": "Recent models for learned image compression are based on autoencoders that learn approximately invertible mappings from pixels to a quantized latent representation. The transforms are combined with an entropy model, which is a prior on the latent representation that can be used with standard arithmetic coding algorithms to generate a compressed bitstream. Recently, hierarchical entropy models were introduced as a way to exploit more structure in the latents than previous fully factorized priors, improving compression performance while maintaining end-to-end optimization. Inspired by the success of autoregressive priors in probabilistic generative models, we examine autoregressive, hierarchical, and combined priors as alternatives, weighing their costs and benefits in the context of image compression. While it is well known that autoregressive models can incur a significant computational penalty, we find that in terms of compression performance, autoregressive and hierarchical priors are complementary and can be combined to exploit the probabilistic structure in the latents better than all previous learned models. The combined model yields state-of-the-art ratedistortion performance and generates smaller files than existing methods: 15.8% rate reductions over the baseline hierarchical model and 59.8%, 35%, and 8.4% savings over JPEG, JPEG2000, and BPG, respectively. To the best of our knowledge, our model is the first learning-based method to outperform the top standard image codec (BPG) on both the PSNR and MS-SSIM distortion metrics."
    },
    {
        "paperId": "8cf29f44cd5f813d8352473460ea71ae31f975e4",
        "url": "https://www.semanticscholar.org/paper/8cf29f44cd5f813d8352473460ea71ae31f975e4",
        "title": "GPyTorch: Blackbox Matrix-Matrix Gaussian Process Inference with GPU Acceleration",
        "venue": "Neural Information Processing Systems",
        "year": 2018,
        "citationCount": 1282,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1809.11165, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "31693738",
                "name": "J. Gardner"
            },
            {
                "authorId": "10804137",
                "name": "Geoff Pleiss"
            },
            {
                "authorId": "1752264",
                "name": "D. Bindel"
            },
            {
                "authorId": "7446832",
                "name": "Kilian Q. Weinberger"
            },
            {
                "authorId": "145771261",
                "name": "A. Wilson"
            }
        ],
        "abstract": "Despite advances in scalable models, the inference tools used for Gaussian processes (GPs) have yet to fully capitalize on developments in computing hardware. We present an efficient and general approach to GP inference based on Blackbox Matrix-Matrix multiplication (BBMM). BBMM inference uses a modified batched version of the conjugate gradients algorithm to derive all terms for training and inference in a single call. BBMM reduces the asymptotic complexity of exact GP inference from $O(n^3)$ to $O(n^2)$. Adapting this algorithm to scalable approximations and complex GP models simply requires a routine for efficient matrix-matrix multiplication with the kernel and its derivative. In addition, BBMM uses a specialized preconditioner to substantially speed up convergence. In experiments we show that BBMM effectively uses GPU hardware to dramatically accelerate both exact GP inference and scalable approximations. Additionally, we provide GPyTorch, a software platform for scalable GP inference via BBMM, built on PyTorch."
    },
    {
        "paperId": "8dc8f3e0127adc6985d4695e9b69d04717b2fde8",
        "url": "https://www.semanticscholar.org/paper/8dc8f3e0127adc6985d4695e9b69d04717b2fde8",
        "title": "Sanity Checks for Saliency Maps",
        "venue": "Neural Information Processing Systems",
        "year": 2018,
        "citationCount": 2201,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1810.03292, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "145949622",
                "name": "Julius Adebayo"
            },
            {
                "authorId": "2058362",
                "name": "J. Gilmer"
            },
            {
                "authorId": "2065563823",
                "name": "Michael Muelly"
            },
            {
                "authorId": "153440022",
                "name": "I. Goodfellow"
            },
            {
                "authorId": "1775622",
                "name": "Moritz Hardt"
            },
            {
                "authorId": "3351164",
                "name": "Been Kim"
            }
        ],
        "abstract": "Saliency methods have emerged as a popular tool to highlight features in an input deemed relevant for the prediction of a learned model. Several saliency methods have been proposed, often guided by visual appeal on image data. In this work, we propose an actionable methodology to evaluate what kinds of explanations a given method can and cannot provide. We find that reliance, solely, on visual assessment can be misleading. Through extensive experiments we show that some existing saliency methods are independent both of the model and of the data generating process. Consequently, methods that fail the proposed tests are inadequate for tasks that are sensitive to either data or model, such as, finding outliers in the data, explaining the relationship between inputs and outputs that the model learned, and debugging the model. We interpret our findings through an analogy with edge detection in images, a technique that requires neither training data nor model. Theory in the case of a linear model and a single-layer convolutional neural network supports our experimental findings."
    },
    {
        "paperId": "8ef09697eca6a3201b1703a5a21722159cf64fcd",
        "url": "https://www.semanticscholar.org/paper/8ef09697eca6a3201b1703a5a21722159cf64fcd",
        "title": "DAGs with NO TEARS: Continuous Optimization for Structure Learning",
        "venue": "Neural Information Processing Systems",
        "year": 2018,
        "citationCount": 1147,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1803.01422, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2496226",
                "name": "Xun Zheng"
            },
            {
                "authorId": "2050920",
                "name": "Bryon Aragam"
            },
            {
                "authorId": "145969795",
                "name": "Pradeep Ravikumar"
            },
            {
                "authorId": "143977260",
                "name": "E. Xing"
            }
        ],
        "abstract": "Estimating the structure of directed acyclic graphs (DAGs, also known as Bayesian networks) is a challenging problem since the search space of DAGs is combinatorial and scales superexponentially with the number of nodes. Existing approaches rely on various local heuristics for enforcing the acyclicity constraint. In this paper, we introduce a fundamentally different strategy: we formulate the structure learning problem as a purely continuous optimization problem over real matrices that avoids this combinatorial constraint entirely. This is achieved by a novel characterization of acyclicity that is not only smooth but also exact. The resulting problem can be efficiently solved by standard numerical algorithms, which also makes implementation effortless. The proposed method outperforms existing ones, without imposing any structural assumptions on the graph such as bounded treewidth or in-degree."
    },
    {
        "paperId": "94be567c32ae76bdaadabd4975807a94181e39b3",
        "url": "https://www.semanticscholar.org/paper/94be567c32ae76bdaadabd4975807a94181e39b3",
        "title": "How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift)",
        "venue": "Neural Information Processing Systems",
        "year": 2018,
        "citationCount": 1666,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1805.11604, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2852106",
                "name": "Shibani Santurkar"
            },
            {
                "authorId": "2754804",
                "name": "Dimitris Tsipras"
            },
            {
                "authorId": "34562927",
                "name": "Andrew Ilyas"
            },
            {
                "authorId": "143826246",
                "name": "A. Madry"
            }
        ],
        "abstract": "Batch Normalization (BatchNorm) is a widely adopted technique that enables faster and more stable training of deep neural networks (DNNs). Despite its pervasiveness, the exact reasons for BatchNorm's effectiveness are still poorly understood. The popular belief is that this effectiveness stems from controlling the change of the layers' input distributions during training to reduce the so-called \"internal covariate shift\". In this work, we demonstrate that such distributional stability of layer inputs has little to do with the success of BatchNorm. Instead, we uncover a more fundamental impact of BatchNorm on the training process: it makes the optimization landscape significantly smoother. This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training. These findings bring us closer to a true understanding of our DNN training toolkit."
    },
    {
        "paperId": "9fcb88ec529aba79a5860e8fde76e7f4762a267d",
        "url": "https://www.semanticscholar.org/paper/9fcb88ec529aba79a5860e8fde76e7f4762a267d",
        "title": "Predictive Uncertainty Estimation via Prior Networks",
        "venue": "Neural Information Processing Systems",
        "year": 2018,
        "citationCount": 1033,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1802.10501, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "145275970",
                "name": "A. Malinin"
            },
            {
                "authorId": "1740397",
                "name": "M. Gales"
            }
        ],
        "abstract": "Estimating how uncertain an AI system is in its predictions is important to improve the safety of such systems. Uncertainty in predictive can result from uncertainty in model parameters, irreducible data uncertainty and uncertainty due to distributional mismatch between the test and training data distributions. Different actions might be taken depending on the source of the uncertainty so it is important to be able to distinguish between them. Recently, baseline tasks and metrics have been defined and several practical methods to estimate uncertainty developed. These methods, however, attempt to model uncertainty due to distributional mismatch either implicitly through model uncertainty or as data uncertainty. This work proposes a new framework for modeling predictive uncertainty called Prior Networks (PNs) which explicitly models distributional uncertainty. PNs do this by parameterizing a prior distribution over predictive distributions. This work focuses on uncertainty for classification and evaluates PNs on the tasks of identifying out-of-distribution (OOD) samples and detecting misclassification on the MNIST dataset, where they are found to outperform previous methods. Experiments on synthetic and MNIST and CIFAR-10 data show that unlike previous non-Bayesian methods PNs are able to distinguish between data and distributional uncertainty."
    },
    {
        "paperId": "c5b55f410365bb889c25a9f0354f2b86ec61c4f0",
        "url": "https://www.semanticscholar.org/paper/c5b55f410365bb889c25a9f0354f2b86ec61c4f0",
        "title": "Video-to-Video Synthesis",
        "venue": "Neural Information Processing Systems",
        "year": 2018,
        "citationCount": 1047,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1808.06601, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2195314",
                "name": "Ting-Chun Wang"
            },
            {
                "authorId": "39793900",
                "name": "Ming-Yu Liu"
            },
            {
                "authorId": "2436356",
                "name": "Jun-Yan Zhu"
            },
            {
                "authorId": "2457939",
                "name": "Guilin Liu"
            },
            {
                "authorId": "29955511",
                "name": "Andrew Tao"
            },
            {
                "authorId": "2376331450",
                "name": "Jan Kautz"
            },
            {
                "authorId": "2301680",
                "name": "Bryan Catanzaro"
            }
        ],
        "abstract": "We study the problem of video-to-video synthesis, whose goal is to learn a mapping function from an input source video (e.g., a sequence of semantic segmentation masks) to an output photorealistic video that precisely depicts the content of the source video. While its image counterpart, the image-to-image synthesis problem, is a popular topic, the video-to-video synthesis problem is less explored in the literature. Without understanding temporal dynamics, directly applying existing image synthesis approaches to an input video often results in temporally incoherent videos of low visual quality. In this paper, we propose a novel video-to-video synthesis approach under the generative adversarial learning framework. Through carefully-designed generator and discriminator architectures, coupled with a spatio-temporal adversarial objective, we achieve high-resolution, photorealistic, temporally coherent video results on a diverse set of input formats including segmentation masks, sketches, and poses. Experiments on multiple benchmarks show the advantage of our method compared to strong baselines. In particular, our model is capable of synthesizing 2K resolution videos of street scenes up to 30 seconds long, which significantly advances the state-of-the-art of video synthesis. Finally, we apply our approach to future video prediction, outperforming several state-of-the-art competing systems."
    },
    {
        "paperId": "c6d4ef1a98b1b9e5875b79efd3ef2a73403a0884",
        "url": "https://www.semanticscholar.org/paper/c6d4ef1a98b1b9e5875b79efd3ef2a73403a0884",
        "title": "Poison Frogs! Targeted Clean-Label Poisoning Attacks on Neural Networks",
        "venue": "Neural Information Processing Systems",
        "year": 2018,
        "citationCount": 1196,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1804.00792, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3246287",
                "name": "Ali Shafahi"
            },
            {
                "authorId": "2218231201",
                "name": "W. R. Huang"
            },
            {
                "authorId": "40465379",
                "name": "Mahyar Najibi"
            },
            {
                "authorId": "31765629",
                "name": "Octavian Suciu"
            },
            {
                "authorId": "1746575",
                "name": "Christoph Studer"
            },
            {
                "authorId": "3343194",
                "name": "Tudor Dumitras"
            },
            {
                "authorId": "1962083",
                "name": "T. Goldstein"
            }
        ],
        "abstract": "Data poisoning is an attack on machine learning models wherein the attacker adds examples to the training set to manipulate the behavior of the model at test time. This paper explores poisoning attacks on neural nets. The proposed attacks use \"clean-labels\"; they don't require the attacker to have any control over the labeling of training data. They are also targeted; they control the behavior of the classifier on a $\\textit{specific}$ test instance without degrading overall classifier performance. For example, an attacker could add a seemingly innocuous image (that is properly labeled) to a training set for a face recognition engine, and control the identity of a chosen person at test time. Because the attacker does not need to control the labeling function, poisons could be entered into the training set simply by leaving them on the web and waiting for them to be scraped by a data collection bot. \nWe present an optimization-based method for crafting poisons, and show that just one single poison image can control classifier behavior when transfer learning is used. For full end-to-end training, we present a \"watermarking\" strategy that makes poisoning reliable using multiple ($\\approx$50) poisoned training instances. We demonstrate our method by generating poisoned frog images from the CIFAR dataset and using them to manipulate image classifiers."
    },
    {
        "paperId": "cc145f046788029322835979a14459652da7247e",
        "url": "https://www.semanticscholar.org/paper/cc145f046788029322835979a14459652da7247e",
        "title": "This looks like that: deep learning for interpretable image recognition",
        "venue": "Neural Information Processing Systems",
        "year": 2018,
        "citationCount": 1375,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1806.10574, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": null,
                "name": "Chaofan Chen"
            },
            {
                "authorId": "33282601",
                "name": "Oscar Li"
            },
            {
                "authorId": "2052759437",
                "name": "A. Barnett"
            },
            {
                "authorId": "2148843160",
                "name": "Jonathan Su"
            },
            {
                "authorId": "48395540",
                "name": "C. Rudin"
            }
        ],
        "abstract": "When we are faced with challenging image classification tasks, we often explain our reasoning by dissecting the image, and pointing out prototypical aspects of one class or another. The mounting evidence for each of the classes helps us make our final decision. In this work, we introduce a deep network architecture -- prototypical part network (ProtoPNet), that reasons in a similar way: the network dissects the image by finding prototypical parts, and combines evidence from the prototypes to make a final classification. The model thus reasons in a way that is qualitatively similar to the way ornithologists, physicians, and others would explain to people on how to solve challenging image classification tasks. The network uses only image-level labels for training without any annotations for parts of images. We demonstrate our method on the CUB-200-2011 dataset and the Stanford Cars dataset. Our experiments show that ProtoPNet can achieve comparable accuracy with its analogous non-interpretable counterpart, and when several ProtoPNets are combined into a larger network, it can achieve an accuracy that is on par with some of the best-performing deep models. Moreover, ProtoPNet provides a level of interpretability that is absent in other interpretable deep models."
    },
    {
        "paperId": "d03ca175e2b2745126e792fdc31dfadae4c63afa",
        "url": "https://www.semanticscholar.org/paper/d03ca175e2b2745126e792fdc31dfadae4c63afa",
        "title": "A Simple Unified Framework for Detecting Out-of-Distribution Samples and Adversarial Attacks",
        "venue": "Neural Information Processing Systems",
        "year": 2018,
        "citationCount": 2348,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1807.03888, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3436470",
                "name": "Kimin Lee"
            },
            {
                "authorId": "2208511",
                "name": "Kibok Lee"
            },
            {
                "authorId": "1697141",
                "name": "Honglak Lee"
            },
            {
                "authorId": "143720148",
                "name": "Jinwoo Shin"
            }
        ],
        "abstract": "Detecting test samples drawn sufficiently far away from the training distribution statistically or adversarially is a fundamental requirement for deploying a good classifier in many real-world machine learning applications. However, deep neural networks with the softmax classifier are known to produce highly overconfident posterior distributions even for such abnormal samples. In this paper, we propose a simple yet effective method for detecting any abnormal samples, which is applicable to any pre-trained softmax neural classifier. We obtain the class conditional Gaussian distributions with respect to (low- and upper-level) features of the deep models under Gaussian discriminant analysis, which result in a confidence score based on the Mahalanobis distance. While most prior methods have been evaluated for detecting either out-of-distribution or adversarial samples, but not both, the proposed method achieves the state-of-the-art performances for both cases in our experiments. Moreover, we found that our proposed method is more robust in harsh cases, e.g., when the training dataset has noisy labels or small number of samples. Finally, we show that the proposed method enjoys broader usage by applying it to class-incremental learning: whenever out-of-distribution samples are detected, our classification rule can incorporate new classes well without further training deep models."
    },
    {
        "paperId": "d18b48f77eb5c517a6d2c1fa434d2952a1b0a825",
        "url": "https://www.semanticscholar.org/paper/d18b48f77eb5c517a6d2c1fa434d2952a1b0a825",
        "title": "Hierarchical Graph Representation Learning with Differentiable Pooling",
        "venue": "Neural Information Processing Systems",
        "year": 2018,
        "citationCount": 2345,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1806.08804, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "83539859",
                "name": "Rex Ying"
            },
            {
                "authorId": "145829303",
                "name": "Jiaxuan You"
            },
            {
                "authorId": "143622465",
                "name": "Christopher Morris"
            },
            {
                "authorId": "145201124",
                "name": "Xiang Ren"
            },
            {
                "authorId": "49437682",
                "name": "William L. Hamilton"
            },
            {
                "authorId": "1702139",
                "name": "J. Leskovec"
            }
        ],
        "abstract": "Recently, graph neural networks (GNNs) have revolutionized the field of graph representation learning through effectively learned node embeddings, and achieved state-of-the-art results in tasks such as node classification and link prediction. However, current GNN methods are inherently flat and do not learn hierarchical representations of graphs---a limitation that is especially problematic for the task of graph classification, where the goal is to predict the label associated with an entire graph. Here we propose DiffPool, a differentiable graph pooling module that can generate hierarchical representations of graphs and can be combined with various graph neural network architectures in an end-to-end fashion. DiffPool learns a differentiable soft cluster assignment for nodes at each layer of a deep GNN, mapping nodes to a set of clusters, which then form the coarsened input for the next GNN layer. Our experimental results show that combining existing GNN methods with DiffPool yields an average improvement of 5-10% accuracy on graph classification benchmarks, compared to all existing pooling approaches, achieving a new state-of-the-art on four out of five benchmark datasets."
    },
    {
        "paperId": "d79a26226393f687ddbc375e32055b40b8ad8d38",
        "url": "https://www.semanticscholar.org/paper/d79a26226393f687ddbc375e32055b40b8ad8d38",
        "title": "GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism",
        "venue": "Neural Information Processing Systems",
        "year": 2018,
        "citationCount": 1920,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "2145438541",
                "name": "Yanping Huang"
            },
            {
                "authorId": "50607661",
                "name": "Yonglong Cheng"
            },
            {
                "authorId": "7167328",
                "name": "Dehao Chen"
            },
            {
                "authorId": "34946720",
                "name": "HyoukJoong Lee"
            },
            {
                "authorId": "2020608",
                "name": "Jiquan Ngiam"
            },
            {
                "authorId": "2827616",
                "name": "Quoc V. Le"
            },
            {
                "authorId": "2545358",
                "name": "Z. Chen"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "d9ff7a9344dd5d6653bd7a02bfd704422bb29951",
        "url": "https://www.semanticscholar.org/paper/d9ff7a9344dd5d6653bd7a02bfd704422bb29951",
        "title": "Experience Replay for Continual Learning",
        "venue": "Neural Information Processing Systems",
        "year": 2018,
        "citationCount": 1400,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1811.11682, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2346588790",
                "name": "David Rolnick"
            },
            {
                "authorId": "37968006",
                "name": "Arun Ahuja"
            },
            {
                "authorId": "144735987",
                "name": "Jonathan Schwarz"
            },
            {
                "authorId": "2542999",
                "name": "T. Lillicrap"
            },
            {
                "authorId": "89504302",
                "name": "Greg Wayne"
            }
        ],
        "abstract": "Continual learning is the problem of learning new tasks or knowledge while protecting old knowledge and ideally generalizing from old experience to learn new tasks faster. Neural networks trained by stochastic gradient descent often degrade on old tasks when trained successively on new tasks with different data distributions. This phenomenon, referred to as catastrophic forgetting, is considered a major hurdle to learning with non-stationary data or sequences of new tasks, and prevents networks from continually accumulating knowledge and skills. We examine this issue in the context of reinforcement learning, in a setting where an agent is exposed to tasks in a sequence. Unlike most other work, we do not provide an explicit indication to the model of task boundaries, which is the most general circumstance for a learning agent exposed to continuous experience. While various methods to counteract catastrophic forgetting have recently been proposed, we explore a straightforward, general, and seemingly overlooked solution - that of using experience replay buffers for all past events - with a mixture of on- and off-policy learning, leveraging behavioral cloning. We show that this strategy can still learn new tasks quickly yet can substantially reduce catastrophic forgetting in both Atari and DMLab domains, even matching the performance of methods that require task identities. When buffer storage is constrained, we confirm that a simple mechanism for randomly discarding data allows a limited size buffer to perform almost as well as an unbounded one."
    },
    {
        "paperId": "e061d23b68e7d4aac5aece4794c044c80e638dca",
        "url": "https://www.semanticscholar.org/paper/e061d23b68e7d4aac5aece4794c044c80e638dca",
        "title": "Co-teaching: Robust training of deep neural networks with extremely noisy labels",
        "venue": "Neural Information Processing Systems",
        "year": 2018,
        "citationCount": 2379,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1804.06872, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2087238859",
                "name": "Bo Han"
            },
            {
                "authorId": "3259992",
                "name": "Quanming Yao"
            },
            {
                "authorId": "31487928",
                "name": "Xingrui Yu"
            },
            {
                "authorId": "47537639",
                "name": "Gang Niu"
            },
            {
                "authorId": "49235356",
                "name": "Miao Xu"
            },
            {
                "authorId": "48594758",
                "name": "Weihua Hu"
            },
            {
                "authorId": "1807998",
                "name": "I. Tsang"
            },
            {
                "authorId": "67154907",
                "name": "Masashi Sugiyama"
            }
        ],
        "abstract": "Deep learning with noisy labels is practically challenging, as the capacity of deep models is so high that they can totally memorize these noisy labels sooner or later during training. Nonetheless, recent studies on the memorization effects of deep neural networks show that they would first memorize training data of clean labels and then those of noisy labels. Therefore in this paper, we propose a new deep learning paradigm called Co-teaching for combating with noisy labels. Namely, we train two deep neural networks simultaneously, and let them teach each other given every mini-batch: firstly, each network feeds forward all data and selects some data of possibly clean labels; secondly, two networks communicate with each other what data in this mini-batch should be used for training; finally, each network back propagates the data selected by its peer network and updates itself. Empirical results on noisy versions of MNIST, CIFAR-10 and CIFAR-100 demonstrate that Co-teaching is much superior to the state-of-the-art methods in the robustness of trained deep models."
    },
    {
        "paperId": "e4715a13f6364b1c81e64f247651c3d9e80b6808",
        "url": "https://www.semanticscholar.org/paper/e4715a13f6364b1c81e64f247651c3d9e80b6808",
        "title": "Link Prediction Based on Graph Neural Networks",
        "venue": "Neural Information Processing Systems",
        "year": 2018,
        "citationCount": 2202,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1802.09691, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3098251",
                "name": "Muhan Zhang"
            },
            {
                "authorId": "9527255",
                "name": "Yixin Chen"
            }
        ],
        "abstract": "Link prediction is a key problem for network-structured data. Link prediction heuristics use some score functions, such as common neighbors and Katz index, to measure the likelihood of links. They have obtained wide practical uses due to their simplicity, interpretability, and for some of them, scalability. However, every heuristic has a strong assumption on when two nodes are likely to link, which limits their effectiveness on networks where these assumptions fail. In this regard, a more reasonable way should be learning a suitable heuristic from a given network instead of using predefined ones. By extracting a local subgraph around each target link, we aim to learn a function mapping the subgraph patterns to link existence, thus automatically learning a `heuristic' that suits the current network. In this paper, we study this heuristic learning paradigm for link prediction. First, we develop a novel $\\gamma$-decaying heuristic theory. The theory unifies a wide range of heuristics in a single framework, and proves that all these heuristics can be well approximated from local subgraphs. Our results show that local subgraphs reserve rich information related to link existence. Second, based on the $\\gamma$-decaying theory, we propose a new algorithm to learn heuristics from local subgraphs using a graph neural network (GNN). Its experimental results show unprecedented performance, working consistently well on a wide range of problems."
    },
    {
        "paperId": "e4b64a75d321311447e11c363b45cc07bb74acc2",
        "url": "https://www.semanticscholar.org/paper/e4b64a75d321311447e11c363b45cc07bb74acc2",
        "title": "DropBlock: A regularization method for convolutional networks",
        "venue": "Neural Information Processing Systems",
        "year": 2018,
        "citationCount": 1012,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1810.12890, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1898210",
                "name": "Golnaz Ghiasi"
            },
            {
                "authorId": "33493200",
                "name": "Tsung-Yi Lin"
            },
            {
                "authorId": "2827616",
                "name": "Quoc V. Le"
            }
        ],
        "abstract": "Deep neural networks often work well when they are over-parameterized and trained with a massive amount of noise and regularization, such as weight decay and dropout. Although dropout is widely used as a regularization technique for fully connected layers, it is often less effective for convolutional layers. This lack of success of dropout for convolutional layers is perhaps due to the fact that activation units in convolutional layers are spatially correlated so information can still flow through convolutional networks despite dropout. Thus a structured form of dropout is needed to regularize convolutional networks. In this paper, we introduce DropBlock, a form of structured dropout, where units in a contiguous region of a feature map are dropped together. We found that applying DropbBlock in skip connections in addition to the convolution layers increases the accuracy. Also, gradually increasing number of dropped units during training leads to better accuracy and more robust to hyperparameter choices. Extensive experiments show that DropBlock works better than dropout in regularizing convolutional networks. On ImageNet classification, ResNet-50 architecture with DropBlock achieves $78.13\\%$ accuracy, which is more than $1.6\\%$ improvement on the baseline. On COCO detection, DropBlock improves Average Precision of RetinaNet from $36.8\\%$ to $38.4\\%$."
    },
    {
        "paperId": "e6a83abec5cffb0bf1669f2f2c1efdf2b15cb171",
        "url": "https://www.semanticscholar.org/paper/e6a83abec5cffb0bf1669f2f2c1efdf2b15cb171",
        "title": "TADAM: Task dependent adaptive metric for improved few-shot learning",
        "venue": "Neural Information Processing Systems",
        "year": 2018,
        "citationCount": 1429,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1805.10123, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3313242",
                "name": "Boris N. Oreshkin"
            },
            {
                "authorId": "117849477",
                "name": "Pau Rodrguez Lpez"
            },
            {
                "authorId": "8651990",
                "name": "Alexandre Lacoste"
            }
        ],
        "abstract": "Few-shot learning has become essential for producing models that generalize from few examples. In this work, we identify that metric scaling and metric task conditioning are important to improve the performance of few-shot algorithms. Our analysis reveals that simple metric scaling completely changes the nature of few-shot algorithm parameter updates. Metric scaling provides improvements up to 14% in accuracy for certain metrics on the mini-Imagenet 5-way 5-shot classification task. We further propose a simple and effective way of conditioning a learner on the task sample set, resulting in learning a task-dependent metric space. Moreover, we propose and empirically test a practical end-to-end optimization procedure based on auxiliary task co-training to learn a task-dependent metric space. The resulting few-shot learning model based on the task-dependent scaled metric achieves state of the art on mini-Imagenet. We confirm these results on another few-shot dataset that we introduce in this paper based on CIFAR100. Our code is publicly available at this https URL."
    },
    {
        "paperId": "00358a3f17821476d93461192b9229fe7d92bb3f",
        "url": "https://www.semanticscholar.org/paper/00358a3f17821476d93461192b9229fe7d92bb3f",
        "title": "GNNExplainer: Generating Explanations for Graph Neural Networks",
        "venue": "Neural Information Processing Systems",
        "year": 2019,
        "citationCount": 1623,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1903.03894, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "83539859",
                "name": "Rex Ying"
            },
            {
                "authorId": "40974349",
                "name": "Dylan Bourgeois"
            },
            {
                "authorId": "145829303",
                "name": "Jiaxuan You"
            },
            {
                "authorId": "2095762",
                "name": "M. Zitnik"
            },
            {
                "authorId": "1702139",
                "name": "J. Leskovec"
            }
        ],
        "abstract": "Graph Neural Networks (GNNs) are a powerful tool for machine learning on graphs. GNNs combine node feature information with the graph structure by recursively passing neural messages along edges of the input graph. However, incorporating both graph structure and feature information leads to complex models and explaining predictions made by GNNs remains unsolved. Here we propose GnnExplainer, the first general, model-agnostic approach for providing interpretable explanations for predictions of any GNN-based model on any graph-based machine learning task. Given an instance, GnnExplainer identifies a compact subgraph structure and a small subset of node features that have a crucial role in GNN's prediction. Further, GnnExplainer can generate consistent and concise explanations for an entire class of instances. We formulate GnnExplainer as an optimization task that maximizes the mutual information between a GNN's prediction and distribution of possible subgraph structures. Experiments on synthetic and real-world graphs show that our approach can identify important graph structures as well as node features, and outperforms alternative baseline approaches by up to 43.0% in explanation accuracy. GnnExplainer provides a variety of benefits, from the ability to visualize semantically relevant structures to interpretability, to giving insights into errors of faulty GNNs."
    },
    {
        "paperId": "08d350a25720d865a52c00f3af6cb80e7af52d58",
        "url": "https://www.semanticscholar.org/paper/08d350a25720d865a52c00f3af6cb80e7af52d58",
        "title": "Time-series Generative Adversarial Networks",
        "venue": "Neural Information Processing Systems",
        "year": 2019,
        "citationCount": 1212,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "2144029",
                "name": "Jinsung Yoon"
            },
            {
                "authorId": "123723354",
                "name": "Daniel Jarrett"
            },
            {
                "authorId": "1729969",
                "name": "M. Schaar"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "0feea94f89d395436bf41bd10c797447eecbc128",
        "url": "https://www.semanticscholar.org/paper/0feea94f89d395436bf41bd10c797447eecbc128",
        "title": "Unsupervised Data Augmentation for Consistency Training",
        "venue": "Neural Information Processing Systems",
        "year": 2019,
        "citationCount": 2528,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1904.12848, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1912046",
                "name": "Qizhe Xie"
            },
            {
                "authorId": "3422912",
                "name": "Zihang Dai"
            },
            {
                "authorId": "144547315",
                "name": "E. Hovy"
            },
            {
                "authorId": "1707242",
                "name": "Minh-Thang Luong"
            },
            {
                "authorId": "2827616",
                "name": "Quoc V. Le"
            }
        ],
        "abstract": "Semi-supervised learning lately has shown much promise in improving deep learning models when labeled data is scarce. Common among recent approaches is the use of consistency training on a large amount of unlabeled data to constrain model predictions to be invariant to input noise. In this work, we present a new perspective on how to effectively noise unlabeled examples and argue that the quality of noising, specifically those produced by advanced data augmentation methods, plays a crucial role in semi-supervised learning. By substituting simple noising operations with advanced data augmentation methods such as RandAugment and back-translation, our method brings substantial improvements across six language and three vision tasks under the same consistency training framework. On the IMDb text classification dataset, with only 20 labeled examples, our method achieves an error rate of 4.20, outperforming the state-of-the-art model trained on 25,000 labeled examples. On a standard semi-supervised learning benchmark, CIFAR-10, our method outperforms all previous approaches and achieves an error rate of 5.43 with only 250 examples. Our method also combines well with transfer learning, e.g., when finetuning from BERT, and yields improvements in high-data regime, such as ImageNet, whether when there is only 10% labeled data or when a full labeled set with 1.3M extra unlabeled examples is used. Code is available at this https URL."
    },
    {
        "paperId": "10eda4521c032adabaa8e70d6569e17370b29dcd",
        "url": "https://www.semanticscholar.org/paper/10eda4521c032adabaa8e70d6569e17370b29dcd",
        "title": "Root Mean Square Layer Normalization",
        "venue": "Neural Information Processing Systems",
        "year": 2019,
        "citationCount": 1178,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1910.07467, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "48335426",
                "name": "Biao Zhang"
            },
            {
                "authorId": "2082372",
                "name": "Rico Sennrich"
            }
        ],
        "abstract": "Layer normalization (LayerNorm) has been successfully applied to various deep neural networks to help stabilize training and boost model convergence because of its capability in handling re-centering and re-scaling of both inputs and weight matrix. However, the computational overhead introduced by LayerNorm makes these improvements expensive and significantly slows the underlying network, e.g. RNN in particular. In this paper, we hypothesize that re-centering invariance in LayerNorm is dispensable and propose root mean square layer normalization, or RMSNorm. RMSNorm regularizes the summed inputs to a neuron in one layer according to root mean square (RMS), giving the model re-scaling invariance property and implicit learning rate adaptation ability. RMSNorm is computationally simpler and thus more efficient than LayerNorm. We also present partial RMSNorm, or pRMSNorm where the RMS is estimated from p% of the summed inputs without breaking the above properties. Extensive experiments on several tasks using diverse network architectures show that RMSNorm achieves comparable performance against LayerNorm but reduces the running time by 7%~64% on different models. Source code is available at https://github.com/bzhangGo/rmsnorm."
    },
    {
        "paperId": "1c71771c701aadfd72c5866170a9f5d71464bb88",
        "url": "https://www.semanticscholar.org/paper/1c71771c701aadfd72c5866170a9f5d71464bb88",
        "title": "Unified Language Model Pre-training for Natural Language Understanding and Generation",
        "venue": "Neural Information Processing Systems",
        "year": 2019,
        "citationCount": 1630,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1905.03197, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "145307652",
                "name": "Li Dong"
            },
            {
                "authorId": "144610884",
                "name": "Nan Yang"
            },
            {
                "authorId": "51456429",
                "name": "Wenhui Wang"
            },
            {
                "authorId": "49807919",
                "name": "Furu Wei"
            },
            {
                "authorId": "46522098",
                "name": "Xiaodong Liu"
            },
            {
                "authorId": "72682749",
                "name": "Yu Wang"
            },
            {
                "authorId": "1800422",
                "name": "Jianfeng Gao"
            },
            {
                "authorId": "143849609",
                "name": "M. Zhou"
            },
            {
                "authorId": "145058181",
                "name": "H. Hon"
            }
        ],
        "abstract": "This paper presents a new Unified pre-trained Language Model (UniLM) that can be fine-tuned for both natural language understanding and generation tasks. The model is pre-trained using three types of language modeling tasks: unidirectional, bidirectional, and sequence-to-sequence prediction. The unified modeling is achieved by employing a shared Transformer network and utilizing specific self-attention masks to control what context the prediction conditions on. UniLM compares favorably with BERT on the GLUE benchmark, and the SQuAD 2.0 and CoQA question answering tasks. Moreover, UniLM achieves new state-of-the-art results on five natural language generation datasets, including improving the CNN/DailyMail abstractive summarization ROUGE-L to 40.51 (2.04 absolute improvement), the Gigaword abstractive summarization ROUGE-L to 35.75 (0.86 absolute improvement), the CoQA generative question answering F1 score to 82.5 (37.1 absolute improvement), the SQuAD question generation BLEU-4 to 22.12 (3.75 absolute improvement), and the DSTC7 document-grounded dialog response generation NIST-4 to 2.67 (human performance is 2.65). The code and pre-trained models are available at this https URL."
    },
    {
        "paperId": "1eb7f46b1a0a7df823194d86543e5554aa21021a",
        "url": "https://www.semanticscholar.org/paper/1eb7f46b1a0a7df823194d86543e5554aa21021a",
        "title": "Can You Trust Your Model's Uncertainty? Evaluating Predictive Uncertainty Under Dataset Shift",
        "venue": "Neural Information Processing Systems",
        "year": 2019,
        "citationCount": 1909,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1906.02530, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2067138676",
                "name": "Yaniv Ovadia"
            },
            {
                "authorId": "35105647",
                "name": "Emily Fertig"
            },
            {
                "authorId": "92095972",
                "name": "Jie Jessie Ren"
            },
            {
                "authorId": "81408931",
                "name": "Zachary Nado"
            },
            {
                "authorId": "1733143",
                "name": "D. Sculley"
            },
            {
                "authorId": "2388416",
                "name": "Sebastian Nowozin"
            },
            {
                "authorId": "2403637",
                "name": "Joshua V. Dillon"
            },
            {
                "authorId": "40627523",
                "name": "Balaji Lakshminarayanan"
            },
            {
                "authorId": "144108062",
                "name": "Jasper Snoek"
            }
        ],
        "abstract": "Modern machine learning methods including deep learning have achieved great success in predictive accuracy for supervised learning tasks, but may still fall short in giving useful estimates of their predictive {\\em uncertainty}. Quantifying uncertainty is especially critical in real-world settings, which often involve input distributions that are shifted from the training distribution due to a variety of factors including sample bias and non-stationarity. In such settings, well calibrated uncertainty estimates convey information about when a model's output should (or should not) be trusted. Many probabilistic deep learning methods, including Bayesian-and non-Bayesian methods, have been proposed in the literature for quantifying predictive uncertainty, but to our knowledge there has not previously been a rigorous large-scale empirical comparison of these methods under dataset shift. We present a large-scale benchmark of existing state-of-the-art methods on classification problems and investigate the effect of dataset shift on accuracy and calibration. We find that traditional post-hoc calibration does indeed fall short, as do several other previous methods. However, some methods that marginalize over models give surprisingly strong results across a broad spectrum of tasks."
    },
    {
        "paperId": "1f4294d8e0b0c8559479fac569fc0ea91b4dc0bd",
        "url": "https://www.semanticscholar.org/paper/1f4294d8e0b0c8559479fac569fc0ea91b4dc0bd",
        "title": "Adversarial Examples Are Not Bugs, They Are Features",
        "venue": "Neural Information Processing Systems",
        "year": 2019,
        "citationCount": 2002,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1905.02175, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "34562927",
                "name": "Andrew Ilyas"
            },
            {
                "authorId": "2852106",
                "name": "Shibani Santurkar"
            },
            {
                "authorId": "2754804",
                "name": "Dimitris Tsipras"
            },
            {
                "authorId": "39468283",
                "name": "Logan Engstrom"
            },
            {
                "authorId": "2057910582",
                "name": "Brandon Tran"
            },
            {
                "authorId": "143826246",
                "name": "A. Madry"
            }
        ],
        "abstract": "Adversarial examples have attracted significant attention in machine learning, but the reasons for their existence and pervasiveness remain unclear. We demonstrate that adversarial examples can be directly attributed to the presence of non-robust features: features derived from patterns in the data distribution that are highly predictive, yet brittle and incomprehensible to humans. After capturing these features within a theoretical framework, we establish their widespread existence in standard datasets. Finally, we present a simple setting where we can rigorously tie the phenomena we observe in practice to a misalignment between the (human-specified) notion of robustness and the inherent geometry of the data."
    },
    {
        "paperId": "36e30516683032634975c53e60f3737b6e35ff80",
        "url": "https://www.semanticscholar.org/paper/36e30516683032634975c53e60f3737b6e35ff80",
        "title": "Enhancing the Locality and Breaking the Memory Bottleneck of Transformer on Time Series Forecasting",
        "venue": "Neural Information Processing Systems",
        "year": 2019,
        "citationCount": 1742,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1907.00235, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "50341591",
                "name": "SHIYANG LI"
            },
            {
                "authorId": "2149111126",
                "name": "Xiaoyong Jin"
            },
            {
                "authorId": "2067812849",
                "name": "Yao Xuan"
            },
            {
                "authorId": "3363593",
                "name": "Xiyou Zhou"
            },
            {
                "authorId": "2928777",
                "name": "Wenhu Chen"
            },
            {
                "authorId": "2040617",
                "name": "Yu-Xiang Wang"
            },
            {
                "authorId": "1740249",
                "name": "Xifeng Yan"
            }
        ],
        "abstract": "Time series forecasting is an important problem across many domains, including predictions of solar plant energy output, electricity consumption, and traffic jam situation. In this paper, we propose to tackle such forecasting problem with Transformer [1]. Although impressed by its performance in our preliminary study, we found its two major weaknesses: (1) locality-agnostics: the point-wise dot-product self-attention in canonical Transformer architecture is insensitive to local context, which can make the model prone to anomalies in time series; (2) memory bottleneck: space complexity of canonical Transformer grows quadratically with sequence length $L$, making directly modeling long time series infeasible. In order to solve these two issues, we first propose convolutional self-attention by producing queries and keys with causal convolution so that local context can be better incorporated into attention mechanism. Then, we propose LogSparse Transformer with only $O(L(\\log L)^{2})$ memory cost, improving forecasting accuracy for time series with fine granularity and strong long-term dependencies under constrained memory budget. Our experiments on both synthetic data and real-world datasets show that it compares favorably to the state-of-the-art."
    },
    {
        "paperId": "37e52ff4714c7a08900b518127e438a195b84611",
        "url": "https://www.semanticscholar.org/paper/37e52ff4714c7a08900b518127e438a195b84611",
        "title": "MelGAN: Generative Adversarial Networks for Conditional Waveform Synthesis",
        "venue": "Neural Information Processing Systems",
        "year": 2019,
        "citationCount": 1072,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1910.06711, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2110802467",
                "name": "Kundan Kumar"
            },
            {
                "authorId": "39458024",
                "name": "Rithesh Kumar"
            },
            {
                "authorId": "102540625",
                "name": "T. Boissiere"
            },
            {
                "authorId": "120180969",
                "name": "L. Gestin"
            },
            {
                "authorId": "1380272287",
                "name": "Wei Zhen Teoh"
            },
            {
                "authorId": "143778281",
                "name": "Jose M. R. Sotelo"
            },
            {
                "authorId": "2346028",
                "name": "A. D. Brbisson"
            },
            {
                "authorId": "1751762",
                "name": "Yoshua Bengio"
            },
            {
                "authorId": "1760871",
                "name": "Aaron C. Courville"
            }
        ],
        "abstract": "Previous works (Donahue et al., 2018a; Engel et al., 2019a) have found that generating coherent raw audio waveforms with GANs is challenging. In this paper, we show that it is possible to train GANs reliably to generate high quality coherent waveforms by introducing a set of architectural changes and simple training techniques. Subjective evaluation metric (Mean Opinion Score, or MOS) shows the effectiveness of the proposed approach for high quality mel-spectrogram inversion. To establish the generality of the proposed techniques, we show qualitative results of our model in speech synthesis, music domain translation and unconditional music synthesis. We evaluate the various components of the model through ablation studies and suggest a set of guidelines to design general purpose discriminators and generators for conditional sequence synthesis tasks. Our model is non-autoregressive, fully convolutional, with significantly fewer parameters than competing models and generalizes to unseen speakers for mel-spectrogram inversion. Our pytorch implementation runs at more than 100x faster than realtime on GTX 1080Ti GPU and more than 2x faster than real-time on CPU, without any hardware specific optimization tricks."
    },
    {
        "paperId": "3c8a456509e6c0805354bd40a35e3f2dbf8069b1",
        "url": "https://www.semanticscholar.org/paper/3c8a456509e6c0805354bd40a35e3f2dbf8069b1",
        "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library",
        "venue": "Neural Information Processing Systems",
        "year": 2019,
        "citationCount": 48432,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1912.01703, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3407277",
                "name": "Adam Paszke"
            },
            {
                "authorId": "39793298",
                "name": "Sam Gross"
            },
            {
                "authorId": "1403239967",
                "name": "Francisco Massa"
            },
            {
                "authorId": "1977806",
                "name": "Adam Lerer"
            },
            {
                "authorId": "2065251344",
                "name": "James Bradbury"
            },
            {
                "authorId": "114250963",
                "name": "Gregory Chanan"
            },
            {
                "authorId": "2059271276",
                "name": "Trevor Killeen"
            },
            {
                "authorId": "3370429",
                "name": "Zeming Lin"
            },
            {
                "authorId": "3365851",
                "name": "N. Gimelshein"
            },
            {
                "authorId": "3029482",
                "name": "L. Antiga"
            },
            {
                "authorId": "3050846",
                "name": "Alban Desmaison"
            },
            {
                "authorId": "1473151134",
                "name": "Andreas Kpf"
            },
            {
                "authorId": "2052812305",
                "name": "E. Yang"
            },
            {
                "authorId": "2253681376",
                "name": "Zachary DeVito"
            },
            {
                "authorId": "10707709",
                "name": "Martin Raison"
            },
            {
                "authorId": "41203992",
                "name": "Alykhan Tejani"
            },
            {
                "authorId": "22236100",
                "name": "Sasank Chilamkurthy"
            },
            {
                "authorId": "32163737",
                "name": "Benoit Steiner"
            },
            {
                "authorId": "152599430",
                "name": "Lu Fang"
            },
            {
                "authorId": "2113829116",
                "name": "Junjie Bai"
            },
            {
                "authorId": "2127604",
                "name": "Soumith Chintala"
            }
        ],
        "abstract": "Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it was designed from first principles to support an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several commonly used benchmarks."
    },
    {
        "paperId": "4ae0c4a511697e960c477ea3e37b3e11bf3e0e02",
        "url": "https://www.semanticscholar.org/paper/4ae0c4a511697e960c477ea3e37b3e11bf3e0e02",
        "title": "Learning Robust Global Representations by Penalizing Local Predictive Power",
        "venue": "Neural Information Processing Systems",
        "year": 2019,
        "citationCount": 1179,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1905.13549, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3669925",
                "name": "Haohan Wang"
            },
            {
                "authorId": "51499340",
                "name": "Songwei Ge"
            },
            {
                "authorId": "143977260",
                "name": "E. Xing"
            },
            {
                "authorId": "32219137",
                "name": "Zachary Chase Lipton"
            }
        ],
        "abstract": "Despite their renowned predictive power on i.i.d. data, convolutional neural networks are known to rely more on high-frequency patterns that humans deem superficial than on low-frequency patterns that agree better with intuitions about what constitutes category membership. This paper proposes a method for training robust convolutional networks by penalizing the predictive power of the local representations learned by earlier layers. Intuitively, our networks are forced to discard predictive signals such as color and texture that can be gleaned from local receptive fields and to rely instead on the global structures of the image. Across a battery of synthetic and benchmark domain adaptation tasks, our method confers improved generalization out of the domain. Also, to evaluate cross-domain transfer, we introduce ImageNet-Sketch, a new dataset consisting of sketch-like images, that matches the ImageNet classification validation set in categories and scale."
    },
    {
        "paperId": "5507d267bbf0b4cdb9f893c3c0960a45016f7010",
        "url": "https://www.semanticscholar.org/paper/5507d267bbf0b4cdb9f893c3c0960a45016f7010",
        "title": "Deep Leakage from Gradients",
        "venue": "Neural Information Processing Systems",
        "year": 2019,
        "citationCount": 2594,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1906.08935, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "20515689",
                "name": "Ligeng Zhu"
            },
            {
                "authorId": "47781592",
                "name": "Zhijian Liu"
            },
            {
                "authorId": "143840275",
                "name": "Song Han"
            }
        ],
        "abstract": "Exchanging gradients is a widely used method in modern multi-node machine learning system (e.g., distributed training, collaborative learning). For a long time, people believed that gradients are safe to share: i.e., the training data will not be leaked by gradient exchange. However, we show that it is possible to obtain the private training data from the publicly shared gradients. We name this leakage as Deep Leakage from Gradient and empirically validate the effectiveness on both computer vision and natural language processing tasks. Experimental results show that our attack is much stronger than previous approaches: the recovery is pixel-wise accurate for images and token-wise matching for texts. We want to raise people's awareness to rethink the gradient's safety. Finally, we discuss several possible strategies to prevent such deep leakage. The most effective defense method is gradient pruning."
    },
    {
        "paperId": "65a9c7b0800c86a196bc14e7621ff895cc6ab287",
        "url": "https://www.semanticscholar.org/paper/65a9c7b0800c86a196bc14e7621ff895cc6ab287",
        "title": "ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks",
        "venue": "Neural Information Processing Systems",
        "year": 2019,
        "citationCount": 4186,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1908.02265, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "8553015",
                "name": "Jiasen Lu"
            },
            {
                "authorId": "1746610",
                "name": "Dhruv Batra"
            },
            {
                "authorId": "153432684",
                "name": "Devi Parikh"
            },
            {
                "authorId": "2297229",
                "name": "Stefan Lee"
            }
        ],
        "abstract": "We present ViLBERT (short for Vision-and-Language BERT), a model for learning task-agnostic joint representations of image content and natural language. We extend the popular BERT architecture to a multi-modal two-stream model, pro-cessing both visual and textual inputs in separate streams that interact through co-attentional transformer layers. We pretrain our model through two proxy tasks on the large, automatically collected Conceptual Captions dataset and then transfer it to multiple established vision-and-language tasks -- visual question answering, visual commonsense reasoning, referring expressions, and caption-based image retrieval -- by making only minor additions to the base architecture. We observe significant improvements across tasks compared to existing task-specific models -- achieving state-of-the-art on all four tasks. Our work represents a shift away from learning groundings between vision and language only as part of task training and towards treating visual grounding as a pretrainable and transferable capability."
    },
    {
        "paperId": "6be216d93421bf19c1659e7721241ae73d483baf",
        "url": "https://www.semanticscholar.org/paper/6be216d93421bf19c1659e7721241ae73d483baf",
        "title": "Generating Diverse High-Fidelity Images with VQ-VAE-2",
        "venue": "Neural Information Processing Systems",
        "year": 2019,
        "citationCount": 2120,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1906.00446, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "143653164",
                "name": "Ali Razavi"
            },
            {
                "authorId": "3422336",
                "name": "Aron van den Oord"
            },
            {
                "authorId": "1689108",
                "name": "O. Vinyals"
            }
        ],
        "abstract": "We explore the use of Vector Quantized Variational AutoEncoder (VQ-VAE) models for large scale image generation. To this end, we scale and enhance the autoregressive priors used in VQ-VAE to generate synthetic samples of much higher coherence and fidelity than possible before. We use simple feed-forward encoder and decoder networks, making our model an attractive candidate for applications where the encoding and/or decoding speed is critical. Additionally, VQ-VAE requires sampling an autoregressive model only in the compressed latent space, which is an order of magnitude faster than sampling in the pixel space, especially for large images. We demonstrate that a multi-scale hierarchical organization of VQ-VAE, augmented with powerful priors over the latent codes, is able to generate samples with quality that rivals that of state of the art Generative Adversarial Networks on multifaceted datasets such as ImageNet, while not suffering from GAN's known shortcomings such as mode collapse and lack of diversity."
    },
    {
        "paperId": "82b4b03a4659d6e04bd7cbf51d6e08fde1348dbd",
        "url": "https://www.semanticscholar.org/paper/82b4b03a4659d6e04bd7cbf51d6e08fde1348dbd",
        "title": "Stabilizing Off-Policy Q-Learning via Bootstrapping Error Reduction",
        "venue": "Neural Information Processing Systems",
        "year": 2019,
        "citationCount": 1178,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1906.00949, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1488785534",
                "name": "Aviral Kumar"
            },
            {
                "authorId": "2550764",
                "name": "Justin Fu"
            },
            {
                "authorId": "145499435",
                "name": "G. Tucker"
            },
            {
                "authorId": "1736651",
                "name": "S. Levine"
            }
        ],
        "abstract": "Off-policy reinforcement learning aims to leverage experience collected from prior policies for sample-efficient learning. However, in practice, commonly used off-policy approximate dynamic programming methods based on Q-learning and actor-critic methods are highly sensitive to the data distribution, and can make only limited progress without collecting additional on-policy data. As a step towards more robust off-policy algorithms, we study the setting where the off-policy experience is fixed and there is no further interaction with the environment. We identify bootstrapping error as a key source of instability in current methods. Bootstrapping error is due to bootstrapping from actions that lie outside of the training data distribution, and it accumulates via the Bellman backup operator. We theoretically analyze bootstrapping error, and demonstrate how carefully constraining action selection in the backup can mitigate it. Based on our analysis, we propose a practical algorithm, bootstrapping error accumulation reduction (BEAR). We demonstrate that BEAR is able to learn robustly from different off-policy distributions, including random and suboptimal demonstrations, on a range of continuous control tasks."
    },
    {
        "paperId": "9001698e033524864d4d45f051a5ba362d4afd9e",
        "url": "https://www.semanticscholar.org/paper/9001698e033524864d4d45f051a5ba362d4afd9e",
        "title": "When to Trust Your Model: Model-Based Policy Optimization",
        "venue": "Neural Information Processing Systems",
        "year": 2019,
        "citationCount": 1075,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1906.08253, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "35163402",
                "name": "Michael Janner"
            },
            {
                "authorId": "2550764",
                "name": "Justin Fu"
            },
            {
                "authorId": "2634261",
                "name": "Marvin Zhang"
            },
            {
                "authorId": "1736651",
                "name": "S. Levine"
            }
        ],
        "abstract": "Designing effective model-based reinforcement learning algorithms is difficult because the ease of data generation must be weighed against the bias of model-generated data. In this paper, we study the role of model usage in policy optimization both theoretically and empirically. We first formulate and analyze a model-based reinforcement learning algorithm with a guarantee of monotonic improvement at each step. In practice, this analysis is overly pessimistic and suggests that real off-policy data is always preferable to model-generated on-policy data, but we show that an empirical estimate of model generalization can be incorporated into such analysis to justify model usage. Motivated by this analysis, we then demonstrate that a simple procedure of using short model-generated rollouts branched from real data has the benefits of more complicated model-based algorithms without the usual pitfalls. In particular, this approach surpasses the sample efficiency of prior model-based methods, matches the asymptotic performance of the best model-free algorithms, and scales to horizons that cause other model-based methods to fail entirely."
    },
    {
        "paperId": "965359b3008ab50dd04e171551220ec0e7f83aba",
        "url": "https://www.semanticscholar.org/paper/965359b3008ab50dd04e171551220ec0e7f83aba",
        "title": "Generative Modeling by Estimating Gradients of the Data Distribution",
        "venue": "Neural Information Processing Systems",
        "year": 2019,
        "citationCount": 4806,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1907.05600, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "115504645",
                "name": "Yang Song"
            },
            {
                "authorId": "2490652",
                "name": "Stefano Ermon"
            }
        ],
        "abstract": "We introduce a new generative model where samples are produced via Langevin dynamics using gradients of the data distribution estimated with score matching. Because gradients can be ill-defined and hard to estimate when the data resides on low-dimensional manifolds, we perturb the data with different levels of Gaussian noise, and jointly estimate the corresponding scores, i.e., the vector fields of gradients of the perturbed data distribution for all noise levels. For sampling, we propose an annealed Langevin dynamics where we use gradients corresponding to gradually decreasing noise levels as the sampling process gets closer to the data manifold. Our framework allows flexible model architectures, requires no sampling during training or the use of adversarial methods, and provides a learning objective that can be used for principled model comparisons. Our models produce samples comparable to GANs on MNIST, CelebA and CIFAR-10 datasets, achieving a new state-of-the-art inception score of 8.87 on CIFAR-10. Additionally, we demonstrate that our models learn effective representations via image inpainting experiments."
    },
    {
        "paperId": "9b09d296059909490096e34e9df2d95314787ad5",
        "url": "https://www.semanticscholar.org/paper/9b09d296059909490096e34e9df2d95314787ad5",
        "title": "Learning Representations by Maximizing Mutual Information Across Views",
        "venue": "Neural Information Processing Systems",
        "year": 2019,
        "citationCount": 1589,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1906.00910, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "143902541",
                "name": "Philip Bachman"
            },
            {
                "authorId": "40482726",
                "name": "R. Devon Hjelm"
            },
            {
                "authorId": "134859150",
                "name": "William Buchwalter"
            }
        ],
        "abstract": "We propose an approach to self-supervised representation learning based on maximizing mutual information between features extracted from multiple views of a shared context. For example, one could produce multiple views of a local spatio-temporal context by observing it from different locations (e.g., camera positions within a scene), and via different modalities (e.g., tactile, auditory, or visual). Or, an ImageNet image could provide a context from which one produces multiple views by repeatedly applying data augmentation. Maximizing mutual information between features extracted from these views requires capturing information about high-level factors whose influence spans multiple views -- e.g., presence of certain objects or occurrence of certain events. \nFollowing our proposed approach, we develop a model which learns image representations that significantly outperform prior methods on the tasks we consider. Most notably, using self-supervised learning, our model learns representations which achieve 68.1% accuracy on ImageNet using standard linear evaluation. This beats prior results by over 12% and concurrent results by 7%. When we extend our model to use mixture-based representations, segmentation behaviour emerges as a natural side-effect. Our code is available online: this https URL."
    },
    {
        "paperId": "9f9fc406c76255fec51a6196ce167c0ff1d1efc0",
        "url": "https://www.semanticscholar.org/paper/9f9fc406c76255fec51a6196ce167c0ff1d1efc0",
        "title": "Wide neural networks of any depth evolve as linear models under gradient descent",
        "venue": "Neural Information Processing Systems",
        "year": 2019,
        "citationCount": 1209,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1902.06720",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1902.06720, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "49685832",
                "name": "Jaehoon Lee"
            },
            {
                "authorId": "50819275",
                "name": "Lechao Xiao"
            },
            {
                "authorId": "2601641",
                "name": "S. Schoenholz"
            },
            {
                "authorId": "12383244",
                "name": "Yasaman Bahri"
            },
            {
                "authorId": "39068839",
                "name": "Roman Novak"
            },
            {
                "authorId": "1407546424",
                "name": "Jascha Narain Sohl-Dickstein"
            },
            {
                "authorId": "2056622832",
                "name": "Jeffrey Pennington"
            }
        ],
        "abstract": "A longstanding goal in deep learning research has been to precisely characterize training and generalization. However, the often complex loss landscapes of neural networks (NNs) have made a theory of learning dynamics elusive. In this work, we show that for wide NNs the learning dynamics simplify considerably and that, in the infinite width limit, they are governed by a linear model obtained from the first-order Taylor expansion of the network around its initial parameters. Furthermore, mirroring the correspondence between wide Bayesian NNs and Gaussian processes (GPs), gradient-based training of wide NNs with a squared loss produces test set predictions drawn from a GP with a particular compositional kernel. While these theoretical results are only exact in the infinite width limit, we nevertheless find excellent empirical agreement between the predictions of the original network and those of the linearized version even for finite practically-sized networks. This agreement is robust across different architectures, optimization methods, and loss functions."
    },
    {
        "paperId": "aa63ac11aa9dcaa9edd4c88db18bec87e0834328",
        "url": "https://www.semanticscholar.org/paper/aa63ac11aa9dcaa9edd4c88db18bec87e0834328",
        "title": "Graph Transformer Networks",
        "venue": "Neural Information Processing Systems",
        "year": 2019,
        "citationCount": 1196,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1911.06455, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3110007",
                "name": "Seongjun Yun"
            },
            {
                "authorId": "147498986",
                "name": "Minbyul Jeong"
            },
            {
                "authorId": "2054540583",
                "name": "Raehyun Kim"
            },
            {
                "authorId": "144323862",
                "name": "Jaewoo Kang"
            },
            {
                "authorId": "2108926667",
                "name": "Hyunwoo J. Kim"
            }
        ],
        "abstract": "Graph neural networks (GNNs) have been widely used in representation learning on graphs and achieved state-of-the-art performance in tasks such as node classification and link prediction. However, most existing GNNs are designed to learn node representations on the fixed and homogeneous graphs. The limitations especially become problematic when learning representations on a misspecified graph or a heterogeneous graph that consists of various types of nodes and edges. In this paper, we propose Graph Transformer Networks (GTNs) that are capable of generating new graph structures, which involve identifying useful connections between unconnected nodes on the original graph, while learning effective node representation on the new graphs in an end-to-end fashion. Graph Transformer layer, a core layer of GTNs, learns a soft selection of edge types and composite relations for generating useful multi-hop connections so-call meta-paths. Our experiments show that GTNs learn new graph structures, based on data and tasks without domain knowledge, and yield powerful node representation via convolution on the new graphs. Without domain-specific graph preprocessing, GTNs achieved the best performance in all three benchmark node classification tasks against the state-of-the-art methods that require pre-defined meta-paths from domain knowledge."
    },
    {
        "paperId": "ad7129af0644dbcafa9aa2f111cb76526ea444a1",
        "url": "https://www.semanticscholar.org/paper/ad7129af0644dbcafa9aa2f111cb76526ea444a1",
        "title": "Defending Against Neural Fake News",
        "venue": "Neural Information Processing Systems",
        "year": 2019,
        "citationCount": 1150,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1905.12616, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2545335",
                "name": "Rowan Zellers"
            },
            {
                "authorId": "14487640",
                "name": "Ari Holtzman"
            },
            {
                "authorId": "2516777",
                "name": "Hannah Rashkin"
            },
            {
                "authorId": "3312309",
                "name": "Yonatan Bisk"
            },
            {
                "authorId": "143787583",
                "name": "Ali Farhadi"
            },
            {
                "authorId": "3268360",
                "name": "Franziska Roesner"
            },
            {
                "authorId": "1699545",
                "name": "Yejin Choi"
            }
        ],
        "abstract": "Recent progress in natural language generation has raised dual-use concerns. While applications like summarization and translation are positive, the underlying technology also might enable adversaries to generate neural fake news: targeted propaganda that closely mimics the style of real news. \nModern computer security relies on careful threat modeling: identifying potential threats and vulnerabilities from an adversary's point of view, and exploring potential mitigations to these threats. Likewise, developing robust defenses against neural fake news requires us first to carefully investigate and characterize the risks of these models. We thus present a model for controllable text generation called Grover. Given a headline like `Link Found Between Vaccines and Autism,' Grover can generate the rest of the article; humans find these generations to be more trustworthy than human-written disinformation. \nDeveloping robust verification techniques against generators like Grover is critical. We find that best current discriminators can classify neural fake news from real, human-written, news with 73% accuracy, assuming access to a moderate level of training data. Counterintuitively, the best defense against Grover turns out to be Grover itself, with 92% accuracy, demonstrating the importance of public release of strong generators. We investigate these results further, showing that exposure bias -- and sampling strategies that alleviate its effects -- both leave artifacts that similar discriminators can pick up on. We conclude by discussing ethical issues regarding the technology, and plan to release Grover publicly, helping pave the way for better detection of neural fake news."
    },
    {
        "paperId": "b03c7ff961822183bab66b2e594415e585d3fd09",
        "url": "https://www.semanticscholar.org/paper/b03c7ff961822183bab66b2e594415e585d3fd09",
        "title": "Are Sixteen Heads Really Better than One?",
        "venue": "Neural Information Processing Systems",
        "year": 2019,
        "citationCount": 1217,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1905.10650, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "144397625",
                "name": "Paul Michel"
            },
            {
                "authorId": "39455775",
                "name": "Omer Levy"
            },
            {
                "authorId": "1700325",
                "name": "Graham Neubig"
            }
        ],
        "abstract": "Attention is a powerful and ubiquitous mechanism for allowing neural models to focus on particular salient pieces of information by taking their weighted average when making predictions. In particular, multi-headed attention is a driving force behind many recent state-of-the-art NLP models such as Transformer-based MT models and BERT. These models apply multiple attention mechanisms in parallel, with each attention \"head\" potentially focusing on different parts of the input, which makes it possible to express sophisticated functions beyond the simple weighted average. In this paper we make the surprising observation that even if models have been trained using multiple heads, in practice, a large percentage of attention heads can be removed at test time without significantly impacting performance. In fact, some layers can even be reduced to a single head. We further examine greedy algorithms for pruning down models, and the potential speed, memory efficiency, and accuracy improvements obtainable therefrom. Finally, we analyze the results with respect to which parts of the model are more reliant on having multiple heads, and provide precursory evidence that training dynamics play a role in the gains provided by multi-head attention."
    },
    {
        "paperId": "b9d4a1ac5e41570082828b405b289aa6959252a4",
        "url": "https://www.semanticscholar.org/paper/b9d4a1ac5e41570082828b405b289aa6959252a4",
        "title": "Scene Representation Networks: Continuous 3D-Structure-Aware Neural Scene Representations",
        "venue": "Neural Information Processing Systems",
        "year": 2019,
        "citationCount": 1350,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1906.01618, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "5021307",
                "name": "V. Sitzmann"
            },
            {
                "authorId": "2065899365",
                "name": "Michael Zollhoefer"
            },
            {
                "authorId": "1731170",
                "name": "Gordon Wetzstein"
            }
        ],
        "abstract": "Unsupervised learning with generative models has the potential of discovering rich representations of 3D scenes. While geometric deep learning has explored 3D-structure-aware representations of scene geometry, these models typically require explicit 3D supervision. Emerging neural scene representations can be trained only with posed 2D images, but existing methods ignore the three-dimensional structure of scenes. We propose Scene Representation Networks (SRNs), a continuous, 3D-structure-aware scene representation that encodes both geometry and appearance. SRNs represent scenes as continuous functions that map world coordinates to a feature representation of local scene properties. By formulating the image formation as a differentiable ray-marching algorithm, SRNs can be trained end-to-end from only 2D images and their camera poses, without access to depth or shape. This formulation naturally generalizes across scenes, learning powerful geometry and appearance priors in the process. We demonstrate the potential of SRNs by evaluating them for novel view synthesis, few-shot reconstruction, joint shape and appearance interpolation, and unsupervised discovery of a non-rigid face model."
    },
    {
        "paperId": "bb758228488bd5a67a8bc7369bf090b3d2bc4cc3",
        "url": "https://www.semanticscholar.org/paper/bb758228488bd5a67a8bc7369bf090b3d2bc4cc3",
        "title": "Hamiltonian Neural Networks",
        "venue": "Neural Information Processing Systems",
        "year": 2019,
        "citationCount": 1045,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1906.01563, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "14851288",
                "name": "S. Greydanus"
            },
            {
                "authorId": "3185224",
                "name": "Misko Dzamba"
            },
            {
                "authorId": "2965424",
                "name": "J. Yosinski"
            }
        ],
        "abstract": "Even though neural networks enjoy widespread use, they still struggle to learn the basic laws of physics. How might we endow them with better inductive biases? In this paper, we draw inspiration from Hamiltonian mechanics to train models that learn and respect exact conservation laws in an unsupervised manner. We evaluate our models on problems where conservation of energy is important, including the two-body problem and pixel observations of a pendulum. Our model trains faster and generalizes better than a regular neural network. An interesting side effect is that our model is perfectly reversible in time."
    },
    {
        "paperId": "bcfba69c2fadf2efea83be12fda2601f8d4681af",
        "url": "https://www.semanticscholar.org/paper/bcfba69c2fadf2efea83be12fda2601f8d4681af",
        "title": "Learning Imbalanced Datasets with Label-Distribution-Aware Margin Loss",
        "venue": "Neural Information Processing Systems",
        "year": 2019,
        "citationCount": 1873,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1906.07413, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "48865984",
                "name": "Kaidi Cao"
            },
            {
                "authorId": "3460405",
                "name": "Colin Wei"
            },
            {
                "authorId": "1799820",
                "name": "Adrien Gaidon"
            },
            {
                "authorId": "1802507",
                "name": "Nikos Archiga"
            },
            {
                "authorId": "1901958",
                "name": "Tengyu Ma"
            }
        ],
        "abstract": "Deep learning algorithms can fare poorly when the training dataset suffers from heavy class-imbalance but the testing criterion requires good generalization on less frequent classes. We design two novel methods to improve performance in such scenarios. First, we propose a theoretically-principled label-distribution-aware margin (LDAM) loss motivated by minimizing a margin-based generalization bound. This loss replaces the standard cross-entropy objective during training and can be applied with prior strategies for training with class-imbalance such as re-weighting or re-sampling. Second, we propose a simple, yet effective, training schedule that defers re-weighting until after the initial stage, allowing the model to learn an initial representation while avoiding some of the complications associated with re-weighting or re-sampling. We test our methods on several benchmark vision tasks including the real-world imbalanced dataset iNaturalist 2018. Our experiments show that either of these methods alone can already improve over existing techniques and their combination achieves even better performance gains."
    },
    {
        "paperId": "c42816f497d663c681df20d48a6e66a5632600d8",
        "url": "https://www.semanticscholar.org/paper/c42816f497d663c681df20d48a6e66a5632600d8",
        "title": "MixMatch: A Holistic Approach to Semi-Supervised Learning",
        "venue": "Neural Information Processing Systems",
        "year": 2019,
        "citationCount": 3337,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1905.02249, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "39835551",
                "name": "David Berthelot"
            },
            {
                "authorId": "2288725419",
                "name": "Nicholas Carlini"
            },
            {
                "authorId": "153440022",
                "name": "I. Goodfellow"
            },
            {
                "authorId": "2367796356",
                "name": "Nicolas Papernot"
            },
            {
                "authorId": "35679876",
                "name": "Avital Oliver"
            },
            {
                "authorId": "2402716",
                "name": "Colin Raffel"
            }
        ],
        "abstract": "Semi-supervised learning has proven to be a powerful paradigm for leveraging unlabeled data to mitigate the reliance on large labeled datasets. In this work, we unify the current dominant approaches for semi-supervised learning to produce a new algorithm, MixMatch, that works by guessing low-entropy labels for data-augmented unlabeled examples and mixing labeled and unlabeled data using MixUp. We show that MixMatch obtains state-of-the-art results by a large margin across many datasets and labeled data amounts. For example, on CIFAR-10 with 250 labels, we reduce error rate by a factor of 4 (from 38% to 11%) and by a factor of 2 on STL-10. We also demonstrate how MixMatch can help achieve a dramatically better accuracy-privacy trade-off for differential privacy. Finally, we perform an ablation study to tease apart which components of MixMatch are most important for its success."
    },
    {
        "paperId": "c92be891c5f8f0f60b6de206364f9a744612d1e8",
        "url": "https://www.semanticscholar.org/paper/c92be891c5f8f0f60b6de206364f9a744612d1e8",
        "title": "Adversarial Training for Free!",
        "venue": "Neural Information Processing Systems",
        "year": 2019,
        "citationCount": 1363,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1904.12843, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3246287",
                "name": "Ali Shafahi"
            },
            {
                "authorId": "40465379",
                "name": "Mahyar Najibi"
            },
            {
                "authorId": "115752784",
                "name": "Amin Ghiasi"
            },
            {
                "authorId": "144897102",
                "name": "Zheng Xu"
            },
            {
                "authorId": "1718974",
                "name": "John P. Dickerson"
            },
            {
                "authorId": "1746575",
                "name": "Christoph Studer"
            },
            {
                "authorId": "1693428",
                "name": "L. Davis"
            },
            {
                "authorId": "2189083",
                "name": "Gavin Taylor"
            },
            {
                "authorId": "1962083",
                "name": "T. Goldstein"
            }
        ],
        "abstract": "Adversarial training, in which a network is trained on adversarial examples, is one of the few defenses against adversarial attacks that withstands strong attacks. Unfortunately, the high cost of generating strong adversarial examples makes standard adversarial training impractical on large-scale problems like ImageNet. We present an algorithm that eliminates the overhead cost of generating adversarial examples by recycling the gradient information computed when updating model parameters. Our \"free\" adversarial training algorithm achieves comparable robustness to PGD adversarial training on the CIFAR-10 and CIFAR-100 datasets at negligible additional cost compared to natural training, and can be 7 to 30 times faster than other strong adversarial training methods. Using a single workstation with 4 P100 GPUs and 2 days of runtime, we can train a robust model for the large-scale ImageNet classification task that maintains 40% accuracy against PGD attacks. The code is available at this https URL."
    },
    {
        "paperId": "cff4cb74f4466bd0407977e40ef0be9f444c63ea",
        "url": "https://www.semanticscholar.org/paper/cff4cb74f4466bd0407977e40ef0be9f444c63ea",
        "title": "Transfusion: Understanding Transfer Learning for Medical Imaging",
        "venue": "Neural Information Processing Systems",
        "year": 2019,
        "citationCount": 1111,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1902.07208, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "40297238",
                "name": "M. Raghu"
            },
            {
                "authorId": "151505981",
                "name": "Chiyuan Zhang"
            },
            {
                "authorId": "3371403",
                "name": "J. Kleinberg"
            },
            {
                "authorId": "1751569",
                "name": "Samy Bengio"
            }
        ],
        "abstract": "Transfer learning from natural image datasets, particularly ImageNet, using standard large models and corresponding pretrained weights has become a de-facto method for deep learning applications to medical imaging. However, there are fundamental differences in data sizes, features and task specifications between natural image classification and the target medical tasks, and there is little understanding of the effects of transfer. In this paper, we explore properties of transfer learning for medical imaging. A performance evaluation on two large scale medical imaging tasks shows that surprisingly, transfer offers little benefit to performance, and simple, lightweight models can perform comparably to ImageNet architectures. Investigating the learned representations and features, we find that some of the differences from transfer learning are due to the over-parametrization of standard models rather than sophisticated feature reuse. We isolate where useful feature reuse occurs, and outline the implications for more efficient model exploration. We also explore feature independent benefits of transfer arising from weight scalings."
    },
    {
        "paperId": "d383dd8ced85d7898d8b1546c514a34fb626ea16",
        "url": "https://www.semanticscholar.org/paper/d383dd8ced85d7898d8b1546c514a34fb626ea16",
        "title": "Improved Precision and Recall Metric for Assessing Generative Models",
        "venue": "Neural Information Processing Systems",
        "year": 2019,
        "citationCount": 1058,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1904.06991, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "100563136",
                "name": "T. Kynknniemi"
            },
            {
                "authorId": "2976930",
                "name": "Tero Karras"
            },
            {
                "authorId": "36436218",
                "name": "S. Laine"
            },
            {
                "authorId": "49244945",
                "name": "J. Lehtinen"
            },
            {
                "authorId": "1761103",
                "name": "Timo Aila"
            }
        ],
        "abstract": "The ability to automatically estimate the quality and coverage of the samples produced by a generative model is a vital requirement for driving algorithm research. We present an evaluation metric that can separately and reliably measure both of these aspects in image generation tasks by forming explicit, non-parametric representations of the manifolds of real and generated data. We demonstrate the effectiveness of our metric in StyleGAN and BigGAN by providing several illustrative examples where existing metrics yield uninformative or contradictory results. Furthermore, we analyze multiple design variants of StyleGAN to better understand the relationships between the model architecture, training methods, and the properties of the resulting sample distribution. In the process, we identify new variants that improve the state-of-the-art. We also perform the first principled analysis of truncation methods and identify an improved method. Finally, we extend our metric to estimate the perceptual quality of individual samples, and use this to study latent space interpolations."
    },
    {
        "paperId": "d6dccb5d71fbb6f5765f89633ba3a8e6809a720d",
        "url": "https://www.semanticscholar.org/paper/d6dccb5d71fbb6f5765f89633ba3a8e6809a720d",
        "title": "Stand-Alone Self-Attention in Vision Models",
        "venue": "Neural Information Processing Systems",
        "year": 2019,
        "citationCount": 1319,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1906.05909, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3377142",
                "name": "Prajit Ramachandran"
            },
            {
                "authorId": "3877127",
                "name": "Niki Parmar"
            },
            {
                "authorId": "40348417",
                "name": "Ashish Vaswani"
            },
            {
                "authorId": "4689792",
                "name": "Irwan Bello"
            },
            {
                "authorId": "6639036",
                "name": "Anselm Levskaya"
            },
            {
                "authorId": "1789737",
                "name": "Jonathon Shlens"
            }
        ],
        "abstract": "Convolutions are a fundamental building block of modern computer vision systems. Recent approaches have argued for going beyond convolutions in order to capture long-range dependencies. These efforts focus on augmenting convolutional models with content-based interactions, such as self-attention and non-local means, to achieve gains on a number of vision tasks. The natural question that arises is whether attention can be a stand-alone primitive for vision models instead of serving as just an augmentation on top of convolutions. In developing and testing a pure self-attention vision model, we verify that self-attention can indeed be an effective stand-alone layer. A simple procedure of replacing all instances of spatial convolutions with a form of self-attention applied to ResNet model produces a fully self-attentional model that outperforms the baseline on ImageNet classification with 12% fewer FLOPS and 29% fewer parameters. On COCO object detection, a pure self-attention model matches the mAP of a baseline RetinaNet while having 39% fewer FLOPS and 34% fewer parameters. Detailed ablation studies demonstrate that self-attention is especially impactful when used in later layers. These results establish that stand-alone self-attention is an important addition to the vision practitioner's toolbox."
    },
    {
        "paperId": "d9f6ada77448664b71128bb19df15765336974a6",
        "url": "https://www.semanticscholar.org/paper/d9f6ada77448664b71128bb19df15765336974a6",
        "title": "SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems",
        "venue": "Neural Information Processing Systems",
        "year": 2019,
        "citationCount": 2597,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1905.00537, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "144906624",
                "name": "Alex Wang"
            },
            {
                "authorId": "100984698",
                "name": "Yada Pruksachatkun"
            },
            {
                "authorId": "10666396",
                "name": "Nikita Nangia"
            },
            {
                "authorId": "50286460",
                "name": "Amanpreet Singh"
            },
            {
                "authorId": "38614754",
                "name": "Julian Michael"
            },
            {
                "authorId": "145783676",
                "name": "Felix Hill"
            },
            {
                "authorId": "39455775",
                "name": "Omer Levy"
            },
            {
                "authorId": "3644767",
                "name": "Samuel R. Bowman"
            }
        ],
        "abstract": "In the last year, new models and methods for pretraining and transfer learning have driven striking performance improvements across a range of language understanding tasks. The GLUE benchmark, introduced a little over one year ago, offers a single-number metric that summarizes progress on a diverse set of such tasks, but performance on the benchmark has recently surpassed the level of non-expert humans, suggesting limited headroom for further research. In this paper we present SuperGLUE, a new benchmark styled after GLUE with a new set of more difficult language understanding tasks, a software toolkit, and a public leaderboard. SuperGLUE is available at this http URL."
    },
    {
        "paperId": "db787640c9b42416ff8d7015546e667e58267177",
        "url": "https://www.semanticscholar.org/paper/db787640c9b42416ff8d7015546e667e58267177",
        "title": "Using Self-Supervised Learning Can Improve Model Robustness and Uncertainty",
        "venue": "Neural Information Processing Systems",
        "year": 2019,
        "citationCount": 1023,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1906.12340, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3422872",
                "name": "Dan Hendrycks"
            },
            {
                "authorId": "16787428",
                "name": "Mantas Mazeika"
            },
            {
                "authorId": "148070327",
                "name": "Saurav Kadavath"
            },
            {
                "authorId": "143711382",
                "name": "D. Song"
            }
        ],
        "abstract": "Self-supervision provides effective representations for downstream tasks without requiring labels. However, existing approaches lag behind fully supervised training and are often not thought beneficial beyond obviating the need for annotations. We find that self-supervision can benefit robustness in a variety of ways, including robustness to adversarial examples, label corruption, and common input corruptions. Additionally, self-supervision greatly benefits out-of-distribution detection on difficult, near-distribution outliers, so much so that it exceeds the performance of fully supervised methods. These results demonstrate the promise of self-supervision for improving robustness and uncertainty estimation and establish these tasks as new axes of evaluation for future self-supervised learning research."
    },
    {
        "paperId": "e0c6abdbdecf04ffac65c440da77fb9d66bb474c",
        "url": "https://www.semanticscholar.org/paper/e0c6abdbdecf04ffac65c440da77fb9d66bb474c",
        "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding",
        "venue": "Neural Information Processing Systems",
        "year": 2019,
        "citationCount": 9059,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1906.08237, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2109512754",
                "name": "Zhilin Yang"
            },
            {
                "authorId": "3422912",
                "name": "Zihang Dai"
            },
            {
                "authorId": "35729970",
                "name": "Yiming Yang"
            },
            {
                "authorId": "143712374",
                "name": "J. Carbonell"
            },
            {
                "authorId": "145124475",
                "name": "R. Salakhutdinov"
            },
            {
                "authorId": "2827616",
                "name": "Quoc V. Le"
            }
        ],
        "abstract": "With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, under comparable experiment settings, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking."
    },
    {
        "paperId": "e7f3478fd8aac6940a4bf4f5eb60ac38f6b0b85b",
        "url": "https://www.semanticscholar.org/paper/e7f3478fd8aac6940a4bf4f5eb60ac38f6b0b85b",
        "title": "Modeling Tabular data using Conditional GAN",
        "venue": "Neural Information Processing Systems",
        "year": 2019,
        "citationCount": 1609,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1907.00503, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2112281287",
                "name": "Lei Xu"
            },
            {
                "authorId": "2349196",
                "name": "Maria Skoularidou"
            },
            {
                "authorId": "1402348047",
                "name": "Alfredo Cuesta-Infante"
            },
            {
                "authorId": "1803567",
                "name": "K. Veeramachaneni"
            }
        ],
        "abstract": "Modeling the probability distribution of rows in tabular data and generating realistic synthetic data is a non-trivial task. Tabular data usually contains a mix of discrete and continuous columns. Continuous columns may have multiple modes whereas discrete columns are sometimes imbalanced making the modeling difficult. Existing statistical and deep neural network models fail to properly model this type of data. We design TGAN, which uses a conditional generative adversarial network to address these challenges. To aid in a fair and thorough comparison, we design a benchmark with 7 simulated and 8 real datasets and several Bayesian network baselines. TGAN outperforms Bayesian methods on most of the real datasets whereas other deep learning methods could not."
    },
    {
        "paperId": "ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc",
        "url": "https://www.semanticscholar.org/paper/ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc",
        "title": "Cross-lingual Language Model Pretraining",
        "venue": "Neural Information Processing Systems",
        "year": 2019,
        "citationCount": 2892,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1901.07291, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1830914",
                "name": "Guillaume Lample"
            },
            {
                "authorId": "2480903",
                "name": "Alexis Conneau"
            }
        ],
        "abstract": "Recent studies have demonstrated the efficiency of generative pretraining for English natural language understanding. In this work, we extend this approach to multiple languages and show the effectiveness of cross-lingual pretraining. We propose two methods to learn cross-lingual language models (XLMs): one unsupervised that only relies on monolingual data, and one supervised that leverages parallel data with a new cross-lingual language model objective. We obtain state-of-the-art results on cross-lingual classification, unsupervised and supervised machine translation. On XNLI, our approach pushes the state of the art by an absolute gain of 4.9% accuracy. On unsupervised machine translation, we obtain 34.3 BLEU on WMT16 German-English, improving the previous state of the art by more than 9 BLEU. On supervised machine translation, we obtain a new state of the art of 38.5 BLEU on WMT16 Romanian-English, outperforming the previous best approach by more than 4 BLEU. Our code and pretrained models will be made publicly available."
    },
    {
        "paperId": "f8de25118af2abc4c48afb947d6ec298e05ef1e5",
        "url": "https://www.semanticscholar.org/paper/f8de25118af2abc4c48afb947d6ec298e05ef1e5",
        "title": "When Does Label Smoothing Help?",
        "venue": "Neural Information Processing Systems",
        "year": 2019,
        "citationCount": 2195,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1906.02629, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2114054259",
                "name": "Rafael Mller"
            },
            {
                "authorId": "40464924",
                "name": "Simon Kornblith"
            },
            {
                "authorId": "1695689",
                "name": "Geoffrey E. Hinton"
            }
        ],
        "abstract": "The generalization and learning speed of a multi-class neural network can often be significantly improved by using soft targets that are a weighted average of the hard targets and the uniform distribution over labels. Smoothing the labels in this way prevents the network from becoming over-confident and label smoothing has been used in many state-of-the-art models, including image classification, language translation and speech recognition. Despite its widespread use, label smoothing is still poorly understood. Here we show empirically that in addition to improving generalization, label smoothing improves model calibration which can significantly improve beam-search. However, we also observe that if a teacher network is trained with label smoothing, knowledge distillation into a student network is much less effective. To explain these observations, we visualize how label smoothing changes the representations learned by the penultimate layer of the network. We show that label smoothing encourages the representations of training examples from the same class to group in tight clusters. This results in loss of information in the logits about resemblances between instances of different classes, which is necessary for distillation, but does not hurt generalization or calibration of the model's predictions."
    },
    {
        "paperId": "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3",
        "url": "https://www.semanticscholar.org/paper/044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3",
        "title": "Big Bird: Transformers for Longer Sequences",
        "venue": "Neural Information Processing Systems",
        "year": 2020,
        "citationCount": 2498,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2007.14062, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1771307",
                "name": "M. Zaheer"
            },
            {
                "authorId": "1947314",
                "name": "Guru Guruganesh"
            },
            {
                "authorId": "89890133",
                "name": "Kumar Avinava Dubey"
            },
            {
                "authorId": "1643737606",
                "name": "J. Ainslie"
            },
            {
                "authorId": "114577307",
                "name": "Chris Alberti"
            },
            {
                "authorId": "1722671",
                "name": "Santiago Ontan"
            },
            {
                "authorId": "38552691",
                "name": "Philip Pham"
            },
            {
                "authorId": "101210026",
                "name": "Anirudh Ravula"
            },
            {
                "authorId": "2401629861",
                "name": "Qifan Wang"
            },
            {
                "authorId": "113906155",
                "name": "Li Yang"
            },
            {
                "authorId": "143629707",
                "name": "Amr Ahmed"
            }
        ],
        "abstract": "Transformers-based models, such as BERT, have been one of the most successful deep learning models for NLP. Unfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence length due to their full attention mechanism. To remedy this, we propose, BigBird, a sparse attention mechanism that reduces this quadratic dependency to linear. We show that BigBird is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, our theoretical analysis reveals some of the benefits of having $O(1)$ global tokens (such as CLS), that attend to the entire sequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to 8x of what was previously possible using similar hardware. As a consequence of the capability to handle longer context, BigBird drastically improves performance on various NLP tasks such as question answering and summarization. We also propose novel applications to genomics data."
    },
    {
        "paperId": "053b1d7b97eb2c91fc3921d589c160b0923c70b1",
        "url": "https://www.semanticscholar.org/paper/053b1d7b97eb2c91fc3921d589c160b0923c70b1",
        "title": "Learning to summarize from human feedback",
        "venue": "Neural Information Processing Systems",
        "year": 2020,
        "citationCount": 2706,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2009.01325, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1387983862",
                "name": "Nisan Stiennon"
            },
            {
                "authorId": "31793034",
                "name": "Long Ouyang"
            },
            {
                "authorId": "49387725",
                "name": "Jeff Wu"
            },
            {
                "authorId": "2052152920",
                "name": "Daniel M. Ziegler"
            },
            {
                "authorId": "49407415",
                "name": "Ryan J. Lowe"
            },
            {
                "authorId": "153387869",
                "name": "Chelsea Voss"
            },
            {
                "authorId": "38909097",
                "name": "Alec Radford"
            },
            {
                "authorId": "2698777",
                "name": "Dario Amodei"
            },
            {
                "authorId": "145370786",
                "name": "Paul Christiano"
            }
        ],
        "abstract": "As language models become more powerful, training and evaluation are increasingly bottlenecked by the data and metrics used for a particular task. For example, summarization models are often trained to predict human reference summaries and evaluated using ROUGE, but both of these metrics are rough proxies for what we really care about---summary quality. In this work, we show that it is possible to significantly improve summary quality by training a model to optimize for human preferences. We collect a large, high-quality dataset of human comparisons between summaries, train a model to predict the human-preferred summary, and use that model as a reward function to fine-tune a summarization policy using reinforcement learning. We apply our method to a version of the TL;DR dataset of Reddit posts and find that our models significantly outperform both human reference summaries and much larger models fine-tuned with supervised learning alone. Our models also transfer to CNN/DM news articles, producing summaries nearly as good as the human reference without any news-specific fine-tuning. We conduct extensive analyses to understand our human feedback dataset and fine-tuned models We establish that our reward model generalizes to new datasets, and that optimizing our reward model results in better summaries than optimizing ROUGE according to humans. We hope the evidence from our paper motivates machine learning researchers to pay closer attention to how their training loss affects the model behavior they actually want."
    },
    {
        "paperId": "053f4d6715a4dba6f8103456fc1bb5fd6a5266c4",
        "url": "https://www.semanticscholar.org/paper/053f4d6715a4dba6f8103456fc1bb5fd6a5266c4",
        "title": "Ensemble Distillation for Robust Model Fusion in Federated Learning",
        "venue": "Neural Information Processing Systems",
        "year": 2020,
        "citationCount": 1290,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2006.07242, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "145724662",
                "name": "Tao Lin"
            },
            {
                "authorId": "2069275317",
                "name": "Lingjing Kong"
            },
            {
                "authorId": "2127057",
                "name": "Sebastian U. Stich"
            },
            {
                "authorId": "2456863",
                "name": "Martin Jaggi"
            }
        ],
        "abstract": "Federated Learning (FL) is a machine learning setting where many devices collaboratively train a machine learning model while keeping the training data decentralized. In most of the current training schemes the central model is refined by averaging the parameters of the server model and the updated parameters from the client side. However, directly averaging model parameters is only possible if all models have the same structure and size, which could be a restrictive constraint in many scenarios. \nIn this work we investigate more powerful and more flexible aggregation schemes for FL. Specifically, we propose ensemble distillation for model fusion, i.e. training the central classifier through unlabeled data on the outputs of the models from the clients. This knowledge distillation technique mitigates privacy risk and cost to the same extent as the baseline FL algorithms, but allows flexible aggregation over heterogeneous client models that can differ e.g. in size, numerical precision or structure. We show in extensive empirical experiments on various CV/NLP datasets (CIFAR-10/100, ImageNet, AG News, SST2) and settings (heterogeneous models/data) that the server model can be trained much faster, requiring fewer communication rounds than any existing FL technique so far."
    },
    {
        "paperId": "1156e277fa7ec195b043161d3c5c97715da17658",
        "url": "https://www.semanticscholar.org/paper/1156e277fa7ec195b043161d3c5c97715da17658",
        "title": "Improved Techniques for Training Score-Based Generative Models",
        "venue": "Neural Information Processing Systems",
        "year": 2020,
        "citationCount": 1351,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2006.09011, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "115504645",
                "name": "Yang Song"
            },
            {
                "authorId": "2490652",
                "name": "Stefano Ermon"
            }
        ],
        "abstract": "Score-based generative models can produce high quality image samples comparable to GANs, without requiring adversarial optimization. However, existing training procedures are limited to images of low resolution (typically below 32x32), and can be unstable under some settings. We provide a new theoretical analysis of learning and sampling from score models in high dimensional spaces, explaining existing failure modes and motivating new solutions that generalize across datasets. To enhance stability, we also propose to maintain an exponential moving average of model weights. With these improvements, we can effortlessly scale score-based generative models to images with unprecedented resolutions ranging from 64x64 to 256x256. Our score-based models can generate high-fidelity samples that rival best-in-class GANs on various image datasets, including CelebA, FFHQ, and multiple LSUN categories."
    },
    {
        "paperId": "17d7767a6ea87f4ab24d9cfaa5039160af9cad76",
        "url": "https://www.semanticscholar.org/paper/17d7767a6ea87f4ab24d9cfaa5039160af9cad76",
        "title": "Neural Sparse Voxel Fields",
        "venue": "Neural Information Processing Systems",
        "year": 2020,
        "citationCount": 1413,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2007.11571, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "46458089",
                "name": "Lingjie Liu"
            },
            {
                "authorId": "3016273",
                "name": "Jiatao Gu"
            },
            {
                "authorId": "38943657",
                "name": "Kyaw Zaw Lin"
            },
            {
                "authorId": "144078686",
                "name": "Tat-Seng Chua"
            },
            {
                "authorId": "1680185",
                "name": "C. Theobalt"
            }
        ],
        "abstract": "Photo-realistic free-viewpoint rendering of real-world scenes using classical computer graphics techniques is challenging, because it requires the difficult step of capturing detailed appearance and geometry models. Recent studies have demonstrated promising results by learning scene representations that implicitly encode both geometry and appearance without 3D supervision. However, existing approaches in practice often show blurry renderings caused by the limited network capacity or the difficulty in finding accurate intersections of camera rays with the scene geometry. Synthesizing high-resolution imagery from these representations often requires time-consuming optical ray marching. In this work, we introduce Neural Sparse Voxel Fields (NSVF), a new neural scene representation for fast and high-quality free-viewpoint rendering. NSVF defines a set of voxel-bounded implicit fields organized in a sparse voxel octree to model local properties in each cell. We progressively learn the underlying voxel structures with a diffentiable ray-marching operation from only a set of posed RGB images. With the sparse voxel octree structure, rendering novel views can be accelerated by skipping the voxels containing no relevant scene content. Our method is over 10 times faster than the state-of-the-art (namely, NeRF) at inference time while achieving higher quality results. Furthermore, by utilizing an explicit sparse voxel representation, our method can easily be applied to scene editing and scene composition. We also demonstrate several challenging tasks, including multi-scene learning, free-viewpoint rendering of a moving human, and large-scale scene rendering."
    },
    {
        "paperId": "1e1e10d75c4ebabdbfb7912ca4cc06a27ffa85af",
        "url": "https://www.semanticscholar.org/paper/1e1e10d75c4ebabdbfb7912ca4cc06a27ffa85af",
        "title": "Unsupervised Learning of Visual Features by Contrasting Cluster Assignments",
        "venue": "Neural Information Processing Systems",
        "year": 2020,
        "citationCount": 4639,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2006.09882, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2062862676",
                "name": "Mathilde Caron"
            },
            {
                "authorId": "1806773",
                "name": "Ishan Misra"
            },
            {
                "authorId": "2599292",
                "name": "J. Mairal"
            },
            {
                "authorId": "47316088",
                "name": "Priya Goyal"
            },
            {
                "authorId": "2329288",
                "name": "Piotr Bojanowski"
            },
            {
                "authorId": "2319608",
                "name": "Armand Joulin"
            }
        ],
        "abstract": "Unsupervised image representations have significantly reduced the gap with supervised pretraining, notably with the recent achievements of contrastive learning methods. These contrastive methods typically work online and rely on a large number of explicit pairwise feature comparisons, which is computationally challenging. In this paper, we propose an online algorithm, SwAV, that takes advantage of contrastive methods without requiring to compute pairwise comparisons. Specifically, our method simultaneously clusters the data while enforcing consistency between cluster assignments produced for different augmentations (or views) of the same image, instead of comparing features directly as in contrastive learning. Simply put, we use a swapped prediction mechanism where we predict the cluster assignment of a view from the representation of another view. Our method can be trained with large and small batches and can scale to unlimited amounts of data. Compared to previous contrastive methods, our method is more memory efficient since it does not require a large memory bank or a special momentum network. In addition, we also propose a new data augmentation strategy, multi-crop, that uses a mix of views with different resolutions in place of two full-resolution views, without increasing the memory or compute requirements much. We validate our findings by achieving 75.3% top-1 accuracy on ImageNet with ResNet-50, as well as surpassing supervised pretraining on all the considered transfer tasks."
    },
    {
        "paperId": "21e33bd0ad95ee1f79d8b778e693fd316cbb72d4",
        "url": "https://www.semanticscholar.org/paper/21e33bd0ad95ee1f79d8b778e693fd316cbb72d4",
        "title": "Beyond Homophily in Graph Neural Networks: Current Limitations and Effective Designs",
        "venue": "Neural Information Processing Systems",
        "year": 2020,
        "citationCount": 1201,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "50077183",
                "name": "Jiong Zhu"
            },
            {
                "authorId": "7957569",
                "name": "Yujun Yan"
            },
            {
                "authorId": "21613538",
                "name": "Lingxiao Zhao"
            },
            {
                "authorId": "35505461",
                "name": "Mark Heimann"
            },
            {
                "authorId": "3255268",
                "name": "L. Akoglu"
            },
            {
                "authorId": "2479152",
                "name": "Danai Koutra"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "270f3bea8ca801870a6cc56b4d36f7f2019c9ed0",
        "url": "https://www.semanticscholar.org/paper/270f3bea8ca801870a6cc56b4d36f7f2019c9ed0",
        "title": "MPNet: Masked and Permuted Pre-training for Language Understanding",
        "venue": "Neural Information Processing Systems",
        "year": 2020,
        "citationCount": 1446,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2004.09297, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "50982078",
                "name": "Kaitao Song"
            },
            {
                "authorId": "2112780268",
                "name": "Xu Tan"
            },
            {
                "authorId": "143826491",
                "name": "Tao Qin"
            },
            {
                "authorId": "2152960206",
                "name": "Jianfeng Lu"
            },
            {
                "authorId": "2110264337",
                "name": "Tie-Yan Liu"
            }
        ],
        "abstract": "BERT adopts masked language modeling (MLM) for pre-training and is one of the most successful pre-training models. Since BERT neglects dependency among predicted tokens, XLNet introduces permuted language modeling (PLM) for pre-training to address this problem. We argue that XLNet does not leverage the full position information of a sentence and thus suffers from position discrepancy between pre-training and fine-tuning. In this paper, we propose MPNet, a novel pre-training method that inherits the advantages of BERT and XLNet and avoids their limitations. MPNet leverages the dependency among predicted tokens through permuted language modeling (vs. MLM in BERT), and takes auxiliary position information as input to make the model see a full sentence and thus reducing the position discrepancy (vs. PLM in XLNet). We pre-train MPNet on a large-scale dataset (over 160GB text corpora) and fine-tune on a variety of down-streaming tasks (GLUE, SQuAD, etc). Experimental results show that MPNet outperforms MLM and PLM by a large margin, and achieves better results on these tasks compared with previous state-of-the-art pre-trained methods (e.g., BERT, XLNet, RoBERTa) under the same model setting. We release the code and pre-trained model in GitHub\\footnote{\\url{this https URL}}."
    },
    {
        "paperId": "28db20a81eec74a50204686c3cf796c42a020d2e",
        "url": "https://www.semanticscholar.org/paper/28db20a81eec74a50204686c3cf796c42a020d2e",
        "title": "Conservative Q-Learning for Offline Reinforcement Learning",
        "venue": "Neural Information Processing Systems",
        "year": 2020,
        "citationCount": 2186,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2006.04779, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1488785534",
                "name": "Aviral Kumar"
            },
            {
                "authorId": "35499972",
                "name": "Aurick Zhou"
            },
            {
                "authorId": "145499435",
                "name": "G. Tucker"
            },
            {
                "authorId": "1736651",
                "name": "S. Levine"
            }
        ],
        "abstract": "Effectively leveraging large, previously collected datasets in reinforcement learning (RL) is a key challenge for large-scale real-world applications. Offline RL algorithms promise to learn effective policies from previously-collected, static datasets without further interaction. However, in practice, offline RL presents a major challenge, and standard off-policy RL methods can fail due to overestimation of values induced by the distributional shift between the dataset and the learned policy, especially when training on complex and multi-modal data distributions. In this paper, we propose conservative Q-learning (CQL), which aims to address these limitations by learning a conservative Q-function such that the expected value of a policy under this Q-function lower-bounds its true value. We theoretically show that CQL produces a lower bound on the value of the current policy and that it can be incorporated into a policy learning procedure with theoretical improvement guarantees. In practice, CQL augments the standard Bellman error objective with a simple Q-value regularizer which is straightforward to implement on top of existing deep Q-learning and actor-critic implementations. On both discrete and continuous control domains, we show that CQL substantially outperforms existing offline RL methods, often learning policies that attain 2-5 times higher final return, especially when learning from complex and multi-modal data distributions."
    },
    {
        "paperId": "29858b40a15704398aecdca6bd2820f2fcc99891",
        "url": "https://www.semanticscholar.org/paper/29858b40a15704398aecdca6bd2820f2fcc99891",
        "title": "Training Generative Adversarial Networks with Limited Data",
        "venue": "Neural Information Processing Systems",
        "year": 2020,
        "citationCount": 2109,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2006.06676, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2976930",
                "name": "Tero Karras"
            },
            {
                "authorId": "1907688",
                "name": "M. Aittala"
            },
            {
                "authorId": "1454226629",
                "name": "Janne Hellsten"
            },
            {
                "authorId": "36436218",
                "name": "S. Laine"
            },
            {
                "authorId": "49244945",
                "name": "J. Lehtinen"
            },
            {
                "authorId": "1761103",
                "name": "Timo Aila"
            }
        ],
        "abstract": "Training generative adversarial networks (GAN) using too little data typically leads to discriminator overfitting, causing training to diverge. We propose an adaptive discriminator augmentation mechanism that significantly stabilizes training in limited data regimes. The approach does not require changes to loss functions or network architectures, and is applicable both when training from scratch and when fine-tuning an existing GAN on another dataset. We demonstrate, on several datasets, that good results are now possible using only a few thousand training images, often matching StyleGAN2 results with an order of magnitude fewer images. We expect this to open up new application domains for GANs. We also find that the widely used CIFAR-10 is, in fact, a limited data benchmark, and improve the record FID from 5.59 to 2.42."
    },
    {
        "paperId": "299847adf3ee558a760475ffa364facac3ebbb16",
        "url": "https://www.semanticscholar.org/paper/299847adf3ee558a760475ffa364facac3ebbb16",
        "title": "FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence",
        "venue": "Neural Information Processing Systems",
        "year": 2020,
        "citationCount": 4226,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2001.07685, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1729571",
                "name": "Kihyuk Sohn"
            },
            {
                "authorId": "39835551",
                "name": "David Berthelot"
            },
            {
                "authorId": "2116729195",
                "name": "Chun-Liang Li"
            },
            {
                "authorId": "2476328",
                "name": "Zizhao Zhang"
            },
            {
                "authorId": "2288725419",
                "name": "Nicholas Carlini"
            },
            {
                "authorId": "8132903",
                "name": "E. D. Cubuk"
            },
            {
                "authorId": "145714153",
                "name": "Alexey Kurakin"
            },
            {
                "authorId": null,
                "name": "Han Zhang"
            },
            {
                "authorId": "2402716",
                "name": "Colin Raffel"
            }
        ],
        "abstract": "Semi-supervised learning (SSL) provides an effective means of leveraging unlabeled data to improve a model's performance. In this paper, we demonstrate the power of a simple combination of two common SSL methods: consistency regularization and pseudo-labeling. Our algorithm, FixMatch, first generates pseudo-labels using the model's predictions on weakly-augmented unlabeled images. For a given image, the pseudo-label is only retained if the model produces a high-confidence prediction. The model is then trained to predict the pseudo-label when fed a strongly-augmented version of the same image. Despite its simplicity, we show that FixMatch achieves state-of-the-art performance across a variety of standard semi-supervised learning benchmarks, including 94.93% accuracy on CIFAR-10 with 250 labels and 88.61% accuracy with 40 -- just 4 labels per class. Since FixMatch bears many similarities to existing SSL methods that achieve worse performance, we carry out an extensive ablation study to tease apart the experimental factors that are most important to FixMatch's success. We make our code available at this https URL."
    },
    {
        "paperId": "35b966347dae2f0d496ea713edf03a68211838a5",
        "url": "https://www.semanticscholar.org/paper/35b966347dae2f0d496ea713edf03a68211838a5",
        "title": "Energy-based Out-of-distribution Detection",
        "venue": "Neural Information Processing Systems",
        "year": 2020,
        "citationCount": 1655,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2010.03759, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "46641825",
                "name": "Weitang Liu"
            },
            {
                "authorId": "2118776393",
                "name": "Xiaoyun Wang"
            },
            {
                "authorId": "1758404",
                "name": "John Douglas Owens"
            },
            {
                "authorId": "1527103472",
                "name": "Yixuan Li"
            }
        ],
        "abstract": "Determining whether inputs are out-of-distribution (OOD) is an essential building block for safely deploying machine learning models in the open world. However, previous methods relying on the softmax confidence score suffer from overconfident posterior distributions for OOD data. We propose a unified framework for OOD detection that uses an energy score. We show that energy scores better distinguish in- and out-of-distribution samples than the traditional approach using the softmax scores. Unlike softmax confidence scores, energy scores are theoretically aligned with the probability density of the inputs and are less susceptible to the overconfidence issue. Within this framework, energy can be flexibly used as a scoring function for any pre-trained neural classifier as well as a trainable cost function to shape the energy surface explicitly for OOD detection. On a CIFAR-10 pre-trained WideResNet, using the energy score reduces the average FPR (at TPR 95%) by 18.03% compared to the softmax confidence score. With energy-based training, our method outperforms the state-of-the-art on common benchmarks."
    },
    {
        "paperId": "38643c2926b10f6f74f122a7037e2cd20d77c0f1",
        "url": "https://www.semanticscholar.org/paper/38643c2926b10f6f74f122a7037e2cd20d77c0f1",
        "title": "Supervised Contrastive Learning",
        "venue": "Neural Information Processing Systems",
        "year": 2020,
        "citationCount": 5492,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2004.11362, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "8980423",
                "name": "Prannay Khosla"
            },
            {
                "authorId": "1388407541",
                "name": "Piotr Teterwak"
            },
            {
                "authorId": "2146562893",
                "name": "Chen Wang"
            },
            {
                "authorId": "8707513",
                "name": "Aaron Sarna"
            },
            {
                "authorId": "2476765",
                "name": "Yonglong Tian"
            },
            {
                "authorId": "2094770",
                "name": "Phillip Isola"
            },
            {
                "authorId": "2064102741",
                "name": "Aaron Maschinot"
            },
            {
                "authorId": "1681442",
                "name": "Ce Liu"
            },
            {
                "authorId": "1707347",
                "name": "Dilip Krishnan"
            }
        ],
        "abstract": "Cross entropy is the most widely used loss function for supervised training of image classification models. In this paper, we propose a novel training methodology that consistently outperforms cross entropy on supervised learning tasks across different architectures and data augmentations. We modify the batch contrastive loss, which has recently been shown to be very effective at learning powerful representations in the self-supervised setting. We are thus able to leverage label information more effectively than cross entropy. Clusters of points belonging to the same class are pulled together in embedding space, while simultaneously pushing apart clusters of samples from different classes. In addition to this, we leverage key ingredients such as large batch sizes and normalized embeddings, which have been shown to benefit self-supervised learning. On both ResNet-50 and ResNet-200, we outperform cross entropy by over 1%, setting a new state of the art number of 78.8% among methods that use AutoAugment data augmentation. The loss also shows clear benefits for robustness to natural corruptions on standard benchmarks on both calibration and accuracy. Compared to cross entropy, our supervised contrastive loss is more stable to hyperparameter settings such as optimizers or data augmentations."
    },
    {
        "paperId": "38f93092ece8eee9771e61c1edaf11b1293cae1b",
        "url": "https://www.semanticscholar.org/paper/38f93092ece8eee9771e61c1edaf11b1293cae1b",
        "title": "Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning",
        "venue": "Neural Information Processing Systems",
        "year": 2020,
        "citationCount": 7856,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2006.07733, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "145840757",
                "name": "Jean-Bastien Grill"
            },
            {
                "authorId": "3367628",
                "name": "Florian Strub"
            },
            {
                "authorId": "2064347514",
                "name": "Florent Altch'e"
            },
            {
                "authorId": "31803582",
                "name": "Corentin Tallec"
            },
            {
                "authorId": "16326904",
                "name": "Pierre H. Richemond"
            },
            {
                "authorId": "118801223",
                "name": "Elena Buchatskaya"
            },
            {
                "authorId": "2786693",
                "name": "Carl Doersch"
            },
            {
                "authorId": "3429927",
                "name": "B. '. Pires"
            },
            {
                "authorId": "3407143",
                "name": "Z. Guo"
            },
            {
                "authorId": "37666967",
                "name": "M. G. Azar"
            },
            {
                "authorId": "1808897",
                "name": "Bilal Piot"
            },
            {
                "authorId": "2645384",
                "name": "K. Kavukcuoglu"
            },
            {
                "authorId": "1708654",
                "name": "R. Munos"
            },
            {
                "authorId": "1806291",
                "name": "Michal Valko"
            }
        ],
        "abstract": "We introduce Bootstrap Your Own Latent (BYOL), a new approach to self-supervised image representation learning. BYOL relies on two neural networks, referred to as online and target networks, that interact and learn from each other. From an augmented view of an image, we train the online network to predict the target network representation of the same image under a different augmented view. At the same time, we update the target network with a slow-moving average of the online network. While state-of-the art methods rely on negative pairs, BYOL achieves a new state of the art without them. BYOL reaches $74.3\\%$ top-1 classification accuracy on ImageNet using a linear evaluation with a ResNet-50 architecture and $79.6\\%$ with a larger ResNet. We show that BYOL performs on par or better than the current state of the art on both transfer and semi-supervised benchmarks. Our implementation and pretrained models are given on GitHub."
    },
    {
        "paperId": "43b1e34451f783fed053c1d539d7560dc4ec16a9",
        "url": "https://www.semanticscholar.org/paper/43b1e34451f783fed053c1d539d7560dc4ec16a9",
        "title": "Implicit Neural Representations with Periodic Activation Functions",
        "venue": "Neural Information Processing Systems",
        "year": 2020,
        "citationCount": 3113,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2006.09661, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "5021307",
                "name": "V. Sitzmann"
            },
            {
                "authorId": "144247981",
                "name": "Julien N. P. Martel"
            },
            {
                "authorId": "83485035",
                "name": "Alexander W. Bergman"
            },
            {
                "authorId": "2202838",
                "name": "David B. Lindell"
            },
            {
                "authorId": "1731170",
                "name": "Gordon Wetzstein"
            }
        ],
        "abstract": "Implicitly defined, continuous, differentiable signal representations parameterized by neural networks have emerged as a powerful paradigm, offering many possible benefits over conventional representations. However, current network architectures for such implicit neural representations are incapable of modeling signals with fine detail, and fail to represent a signal's spatial and temporal derivatives, despite the fact that these are essential to many physical signals defined implicitly as the solution to partial differential equations. We propose to leverage periodic activation functions for implicit neural representations and demonstrate that these networks, dubbed sinusoidal representation networks or Sirens, are ideally suited for representing complex natural signals and their derivatives. We analyze Siren activation statistics to propose a principled initialization scheme and demonstrate the representation of images, wavefields, video, sound, and their derivatives. Further, we show how Sirens can be leveraged to solve challenging boundary value problems, such as particular Eikonal equations (yielding signed distance functions), the Poisson equation, and the Helmholtz and wave equations. Lastly, we combine Sirens with hypernetworks to learn priors over the space of Siren functions."
    },
    {
        "paperId": "449c5660d637741f7aa7ff42549c32b43c9968bf",
        "url": "https://www.semanticscholar.org/paper/449c5660d637741f7aa7ff42549c32b43c9968bf",
        "title": "Gradient Surgery for Multi-Task Learning",
        "venue": "Neural Information Processing Systems",
        "year": 2020,
        "citationCount": 1506,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2001.06782, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "10909315",
                "name": "Tianhe Yu"
            },
            {
                "authorId": "2121434953",
                "name": "Saurabh Kumar"
            },
            {
                "authorId": "2129458064",
                "name": "Abhishek Gupta"
            },
            {
                "authorId": "1736651",
                "name": "S. Levine"
            },
            {
                "authorId": "1944801",
                "name": "Karol Hausman"
            },
            {
                "authorId": "46881670",
                "name": "Chelsea Finn"
            }
        ],
        "abstract": "While deep learning and deep reinforcement learning (RL) systems have demonstrated impressive results in domains such as image classification, game playing, and robotic control, data efficiency remains a major challenge. Multi-task learning has emerged as a promising approach for sharing structure across multiple tasks to enable more efficient learning. However, the multi-task setting presents a number of optimization challenges, making it difficult to realize large efficiency gains compared to learning tasks independently. The reasons why multi-task learning is so challenging compared to single-task learning are not fully understood. In this work, we identify a set of three conditions of the multi-task optimization landscape that cause detrimental gradient interference, and develop a simple yet general approach for avoiding such interference between task gradients. We propose a form of gradient surgery that projects a task's gradient onto the normal plane of the gradient of any other task that has a conflicting gradient. On a series of challenging multi-task supervised and multi-task RL problems, this approach leads to substantial gains in efficiency and performance. Further, it is model-agnostic and can be combined with previously-proposed multi-task architectures for enhanced performance."
    },
    {
        "paperId": "49a049dc85e2380dde80501a984878341dd8efdf",
        "url": "https://www.semanticscholar.org/paper/49a049dc85e2380dde80501a984878341dd8efdf",
        "title": "wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations",
        "venue": "Neural Information Processing Systems",
        "year": 2020,
        "citationCount": 7324,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2006.11477, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "51428394",
                "name": "Alexei Baevski"
            },
            {
                "authorId": "2110147709",
                "name": "Henry Zhou"
            },
            {
                "authorId": "40360972",
                "name": "Abdel-rahman Mohamed"
            },
            {
                "authorId": "2325985",
                "name": "Michael Auli"
            }
        ],
        "abstract": "We show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on transcribed speech can outperform the best semi-supervised methods while being conceptually simpler. wav2vec 2.0 masks the speech input in the latent space and solves a contrastive task defined over a quantization of the latent representations which are jointly learned. Experiments using all labeled data of Librispeech achieve 1.8/3.3 WER on the clean/other test sets. When lowering the amount of labeled data to one hour, wav2vec 2.0 outperforms the previous state of the art on the 100 hour subset while using 100 times less labeled data. Using just ten minutes of labeled data and pre-training on 53k hours of unlabeled data still achieves 4.8/8.2 WER. This demonstrates the feasibility of speech recognition with limited amounts of labeled data."
    },
    {
        "paperId": "4b6661347d5b58250130b89145dbd34ce310f2a0",
        "url": "https://www.semanticscholar.org/paper/4b6661347d5b58250130b89145dbd34ce310f2a0",
        "title": "Tackling the Objective Inconsistency Problem in Heterogeneous Federated Optimization",
        "venue": "Neural Information Processing Systems",
        "year": 2020,
        "citationCount": 1701,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2007.07481, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "30880777",
                "name": "Jianyu Wang"
            },
            {
                "authorId": "2116656839",
                "name": "Qinghua Liu"
            },
            {
                "authorId": "2111208854",
                "name": "Hao Liang"
            },
            {
                "authorId": "144225970",
                "name": "Gauri Joshi"
            },
            {
                "authorId": "145967056",
                "name": "H. Poor"
            }
        ],
        "abstract": "In federated optimization, heterogeneity in the clients' local datasets and computation speeds results in large variations in the number of local updates performed by each client in each communication round. Naive weighted aggregation of such models causes objective inconsistency, that is, the global model converges to a stationary point of a mismatched objective function which can be arbitrarily different from the true objective. This paper provides a general framework to analyze the convergence of federated heterogeneous optimization algorithms. It subsumes previously proposed methods such as FedAvg and FedProx and provides the first principled understanding of the solution bias and the convergence slowdown due to objective inconsistency. Using insights from this analysis, we propose FedNova, a normalized averaging method that eliminates objective inconsistency while preserving fast error convergence."
    },
    {
        "paperId": "4e468d3da1797d791db8d514d695b183acb027ee",
        "url": "https://www.semanticscholar.org/paper/4e468d3da1797d791db8d514d695b183acb027ee",
        "title": "HiFi-GAN: Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis",
        "venue": "Neural Information Processing Systems",
        "year": 2020,
        "citationCount": 2422,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2010.05646, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1712238254",
                "name": "Jungil Kong"
            },
            {
                "authorId": "94862572",
                "name": "Jaehyeon Kim"
            },
            {
                "authorId": "1703883959",
                "name": "Jaekyoung Bae"
            }
        ],
        "abstract": "Several recent work on speech synthesis have employed generative adversarial networks (GANs) to produce raw waveforms. Although such methods improve the sampling efficiency and memory usage, their sample quality has not yet reached that of autoregressive and flow-based generative models. In this work, we propose HiFi-GAN, which achieves both efficient and high-fidelity speech synthesis. As speech audio consists of sinusoidal signals with various periods, we demonstrate that modeling periodic patterns of an audio is crucial for enhancing sample quality. A subjective human evaluation (mean opinion score, MOS) of a single speaker dataset indicates that our proposed method demonstrates similarity to human quality while generating 22.05 kHz high-fidelity audio 167.9 times faster than real-time on a single V100 GPU. We further show the generality of HiFi-GAN to the mel-spectrogram inversion of unseen speakers and end-to-end speech synthesis. Finally, a small footprint version of HiFi-GAN generates samples 13.4 times faster than real-time on CPU with comparable quality to an autoregressive counterpart."
    },
    {
        "paperId": "597bd2e45427563cdf025e53a3239006aa364cfc",
        "url": "https://www.semanticscholar.org/paper/597bd2e45427563cdf025e53a3239006aa364cfc",
        "title": "Open Graph Benchmark: Datasets for Machine Learning on Graphs",
        "venue": "Neural Information Processing Systems",
        "year": 2020,
        "citationCount": 3209,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2005.00687, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "48594758",
                "name": "Weihua Hu"
            },
            {
                "authorId": "3410500",
                "name": "Matthias Fey"
            },
            {
                "authorId": "2095762",
                "name": "M. Zitnik"
            },
            {
                "authorId": "2047998",
                "name": "Yuxiao Dong"
            },
            {
                "authorId": "40046694",
                "name": "Hongyu Ren"
            },
            {
                "authorId": "2156641189",
                "name": "Bowen Liu"
            },
            {
                "authorId": "1754926",
                "name": "Michele Catasta"
            },
            {
                "authorId": "1702139",
                "name": "J. Leskovec"
            }
        ],
        "abstract": "We present the Open Graph Benchmark (OGB), a diverse set of challenging and realistic benchmark datasets to facilitate scalable, robust, and reproducible graph machine learning (ML) research. OGB datasets are large-scale (up to 100+ million nodes and 1+ billion edges), encompass multiple important graph ML tasks, and cover a diverse range of domains, ranging from social and information networks to biological networks, molecular graphs, source code ASTs, and knowledge graphs. For each dataset, we provide a unified evaluation protocol using meaningful application-specific data splits and evaluation metrics. In addition to building the datasets, we also perform extensive benchmark experiments for each dataset. Our experiments suggest that OGB datasets present significant challenges of scalability to large-scale graphs and out-of-distribution generalization under realistic data splits, indicating fruitful opportunities for future research. Finally, OGB provides an automated end-to-end graph ML pipeline that simplifies and standardizes the process of graph data loading, experimental setup, and model evaluation. OGB will be regularly updated and welcomes inputs from the community. OGB datasets as well as data loaders, evaluation scripts, baseline code, and leaderboards are publicly available at this https URL ."
    },
    {
        "paperId": "5c126ae3421f05768d8edd97ecd44b1364e2c99a",
        "url": "https://www.semanticscholar.org/paper/5c126ae3421f05768d8edd97ecd44b1364e2c99a",
        "title": "Denoising Diffusion Probabilistic Models",
        "venue": "Neural Information Processing Systems",
        "year": 2020,
        "citationCount": 25438,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2006.11239, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2126278",
                "name": "Jonathan Ho"
            },
            {
                "authorId": "1623995772",
                "name": "Ajay Jain"
            },
            {
                "authorId": "1689992",
                "name": "P. Abbeel"
            }
        ],
        "abstract": "We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at this https URL"
    },
    {
        "paperId": "659bf9ce7175e1ec266ff54359e2bd76e0b7ff31",
        "url": "https://www.semanticscholar.org/paper/659bf9ce7175e1ec266ff54359e2bd76e0b7ff31",
        "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks",
        "venue": "Neural Information Processing Systems",
        "year": 2020,
        "citationCount": 10167,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2005.11401, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "145222654",
                "name": "Patrick Lewis"
            },
            {
                "authorId": "3439053",
                "name": "Ethan Perez"
            },
            {
                "authorId": "1716179427",
                "name": "Aleksandara Piktus"
            },
            {
                "authorId": "40052301",
                "name": "F. Petroni"
            },
            {
                "authorId": "2067091563",
                "name": "Vladimir Karpukhin"
            },
            {
                "authorId": "39589154",
                "name": "Naman Goyal"
            },
            {
                "authorId": "103131985",
                "name": "Heinrich Kuttler"
            },
            {
                "authorId": "35084211",
                "name": "M. Lewis"
            },
            {
                "authorId": "144105277",
                "name": "Wen-tau Yih"
            },
            {
                "authorId": "2620211",
                "name": "Tim Rocktschel"
            },
            {
                "authorId": "48662861",
                "name": "Sebastian Riedel"
            },
            {
                "authorId": "1743722",
                "name": "Douwe Kiela"
            }
        ],
        "abstract": "Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline."
    },
    {
        "paperId": "698ab1cc02a79596a87f92d5a0882ab1a7aee266",
        "url": "https://www.semanticscholar.org/paper/698ab1cc02a79596a87f92d5a0882ab1a7aee266",
        "title": "Inverting Gradients - How easy is it to break privacy in federated learning?",
        "venue": "Neural Information Processing Systems",
        "year": 2020,
        "citationCount": 1468,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2003.14053, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "8284185",
                "name": "Jonas Geiping"
            },
            {
                "authorId": "1602291733",
                "name": "Hartmut Bauermeister"
            },
            {
                "authorId": "1606356241",
                "name": "Hannah Drge"
            },
            {
                "authorId": "49863090",
                "name": "Michael Moeller"
            }
        ],
        "abstract": "The idea of federated learning is to collaboratively train a neural network on a server. Each user receives the current weights of the network and in turns sends parameter updates (gradients) based on local data. This protocol has been designed not only to train neural networks data-efficiently, but also to provide privacy benefits for users, as their input data remains on device and only parameter gradients are shared. But how secure is sharing parameter gradients? Previous attacks have provided a false sense of security, by succeeding only in contrived settings - even for a single image. However, by exploiting a magnitude-invariant loss along with optimization strategies based on adversarial attacks, we show that is is actually possible to faithfully reconstruct images at high resolution from the knowledge of their parameter gradients, and demonstrate that such a break of privacy is possible even for trained deep networks. We analyze the effects of architecture as well as parameters on the difficulty of reconstructing an input image and prove that any input to a fully connected layer can be reconstructed analytically independent of the remaining architecture. Finally we discuss settings encountered in practice and show that even averaging gradients over several iterations or several images does not protect the user's privacy in federated learning applications in computer vision."
    },
    {
        "paperId": "70f1b279f96a9ba8e15f599635ba0e3ec449ef5f",
        "url": "https://www.semanticscholar.org/paper/70f1b279f96a9ba8e15f599635ba0e3ec449ef5f",
        "title": "Personalized Federated Learning with Moreau Envelopes",
        "venue": "Neural Information Processing Systems",
        "year": 2020,
        "citationCount": 1221,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2006.08848, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "35784038",
                "name": "Canh T. Dinh"
            },
            {
                "authorId": "145359941",
                "name": "N. H. Tran"
            },
            {
                "authorId": "2116225572",
                "name": "Tuan Dung Nguyen"
            }
        ],
        "abstract": "Federated learning (FL) is a decentralized and privacy-preserving machine learning technique in which a group of clients collaborate with a server to learn a global model without sharing clients' data. One challenge associated with FL is statistical diversity among clients, which restricts the global model from delivering good performance on each client's task. To address this, we propose an algorithm for personalized FL (pFedMe) using Moreau envelopes as clients' regularized loss functions, which help decouple personalized model optimization from the global model learning in a bi-level problem stylized for personalized FL. Theoretically, we show that pFedMe's convergence rate is state-of-the-art: achieving quadratic speedup for strongly convex and sublinear speedup of order 2/3 for smooth nonconvex objectives. Experimentally, we verify that pFedMe excels at empirical performance compared with the vanilla FedAvg and Per-FedAvg, a meta-learning based personalized FL algorithm."
    },
    {
        "paperId": "76c124786ccf4263e6403a15a8e350ac28be4e65",
        "url": "https://www.semanticscholar.org/paper/76c124786ccf4263e6403a15a8e350ac28be4e65",
        "title": "Graph Contrastive Learning with Augmentations",
        "venue": "Neural Information Processing Systems",
        "year": 2020,
        "citationCount": 2493,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2010.13902, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "89197162",
                "name": "Yuning You"
            },
            {
                "authorId": "2648459",
                "name": "Tianlong Chen"
            },
            {
                "authorId": "2003767516",
                "name": "Yongduo Sui"
            },
            {
                "authorId": "145358498",
                "name": "Ting Chen"
            },
            {
                "authorId": "2969311",
                "name": "Zhangyang Wang"
            },
            {
                "authorId": "1705610299",
                "name": "Yang Shen"
            }
        ],
        "abstract": "Generalizable, transferrable, and robust representation learning on graph-structured data remains a challenge for current graph neural networks (GNNs). Unlike what has been developed for convolutional neural networks (CNNs) for image data, self-supervised learning and pre-training are less explored for GNNs. In this paper, we propose a graph contrastive learning (GraphCL) framework for learning unsupervised representations of graph data. We first design four types of graph augmentations to incorporate various priors. We then systematically study the impact of various combinations of graph augmentations on multiple datasets, in four different settings: semi-supervised, unsupervised, and transfer learning as well as adversarial attacks. The results show that, even without tuning augmentation extents nor using sophisticated GNN architectures, our GraphCL framework can produce graph representations of similar or better generalizability, transferrability, and robustness compared to state-of-the-art methods. We also investigate the impact of parameterized graph augmentation extents and patterns, and observe further performance gains in preliminary experiments. Our codes are available at https://github.com/Shen-Lab/GraphCL."
    },
    {
        "paperId": "797389ca052efd160ed759d7ef7adf9c30a917d6",
        "url": "https://www.semanticscholar.org/paper/797389ca052efd160ed759d7ef7adf9c30a917d6",
        "title": "First Order Motion Model for Image Animation",
        "venue": "Neural Information Processing Systems",
        "year": 2020,
        "citationCount": 1104,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2003.00196, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "10753214",
                "name": "Aliaksandr Siarohin"
            },
            {
                "authorId": "3099587",
                "name": "Stphane Lathuilire"
            },
            {
                "authorId": "145582202",
                "name": "S. Tulyakov"
            },
            {
                "authorId": "40811261",
                "name": "E. Ricci"
            },
            {
                "authorId": "1703601",
                "name": "N. Sebe"
            }
        ],
        "abstract": "Image animation consists of generating a video sequence so that an object in a source image is animated according to the motion of a driving video. Our framework addresses this problem without using any annotation or prior information about the specific object to animate. Once trained on a set of videos depicting objects of the same category (e.g. faces, human bodies), our method can be applied to any object of this class. To achieve this, we decouple appearance and motion information using a self-supervised formulation. To support complex motions, we use a representation consisting of a set of learned keypoints along with their local affine transformations. A generator network models occlusions arising during target motions and combines the appearance extracted from the source image and the motion derived from the driving video. Our framework scores best on diverse benchmarks and on a variety of object categories."
    },
    {
        "paperId": "7f768fa192a76ab097ccfda0a68523bc36425423",
        "url": "https://www.semanticscholar.org/paper/7f768fa192a76ab097ccfda0a68523bc36425423",
        "title": "What makes for good views for contrastive learning",
        "venue": "Neural Information Processing Systems",
        "year": 2020,
        "citationCount": 1483,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2005.10243, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2476765",
                "name": "Yonglong Tian"
            },
            {
                "authorId": "1491624845",
                "name": "Chen Sun"
            },
            {
                "authorId": "16443937",
                "name": "Ben Poole"
            },
            {
                "authorId": "1707347",
                "name": "Dilip Krishnan"
            },
            {
                "authorId": "2462253",
                "name": "C. Schmid"
            },
            {
                "authorId": "2094770",
                "name": "Phillip Isola"
            }
        ],
        "abstract": "Contrastive learning between multiple views of the data has recently achieved state of the art performance in the field of self-supervised representation learning. Despite its success, the influence of different view choices has been less studied. In this paper, we use empirical analysis to better understand the importance of view selection, and argue that we should reduce the mutual information (MI) between views while keeping task-relevant information intact. To verify this hypothesis, we devise unsupervised and semi-supervised frameworks that learn effective views by aiming to reduce their MI. We also consider data augmentation as a way to reduce MI, and show that increasing data augmentation indeed leads to decreasing MI and improves downstream classification accuracy. As a by-product, we also achieve a new state-of-the-art accuracy on unsupervised pre-training for ImageNet classification ($73\\%$ top-1 linear readoff with a ResNet-50). In addition, transferring our models to PASCAL VOC object detection and COCO instance segmentation consistently outperforms supervised pre-training. Code:this http URL"
    },
    {
        "paperId": "8b163b75a6b833911c4e958f8bd52124205382ec",
        "url": "https://www.semanticscholar.org/paper/8b163b75a6b833911c4e958f8bd52124205382ec",
        "title": "Adaptive Graph Convolutional Recurrent Network for Traffic Forecasting",
        "venue": "Neural Information Processing Systems",
        "year": 2020,
        "citationCount": 1646,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2007.02842, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "50010487",
                "name": "Lei Bai"
            },
            {
                "authorId": "2082966",
                "name": "Lina Yao"
            },
            {
                "authorId": "2118036284",
                "name": "Can Li"
            },
            {
                "authorId": "2877263",
                "name": "Xianzhi Wang"
            },
            {
                "authorId": "2117934428",
                "name": "Can Wang"
            }
        ],
        "abstract": "Modeling complex spatial and temporal correlations in the correlated time series data is indispensable for understanding the traffic dynamics and predicting the future status of an evolving traffic system. Recent works focus on designing complicated graph neural network architectures to capture shared patterns with the help of pre-defined graphs. In this paper, we argue that learning node-specific patterns is essential for traffic forecasting while the pre-defined graph is avoidable. To this end, we propose two adaptive modules for enhancing Graph Convolutional Network (GCN) with new capabilities: 1) a Node Adaptive Parameter Learning (NAPL) module to capture node-specific patterns; 2) a Data Adaptive Graph Generation (DAGG) module to infer the inter-dependencies among different traffic series automatically. We further propose an Adaptive Graph Convolutional Recurrent Network (AGCRN) to capture fine-grained spatial and temporal correlations in traffic series automatically based on the two modules and recurrent networks. Our experiments on two real-world traffic datasets show AGCRN outperforms state-of-the-art by a significant margin without pre-defined graphs about spatial connections."
    },
    {
        "paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0",
        "url": "https://www.semanticscholar.org/paper/90abbc2cf38462b954ae1b772fac9532e2ccd8b0",
        "title": "Language Models are Few-Shot Learners",
        "venue": "Neural Information Processing Systems",
        "year": 2020,
        "citationCount": 51925,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2005.14165, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "31035595",
                "name": "Tom B. Brown"
            },
            {
                "authorId": "2056658938",
                "name": "Benjamin Mann"
            },
            {
                "authorId": "39849748",
                "name": "Nick Ryder"
            },
            {
                "authorId": "2065894334",
                "name": "Melanie Subbiah"
            },
            {
                "authorId": "152724169",
                "name": "J. Kaplan"
            },
            {
                "authorId": "6515819",
                "name": "Prafulla Dhariwal"
            },
            {
                "authorId": "2072676",
                "name": "Arvind Neelakantan"
            },
            {
                "authorId": "67311962",
                "name": "Pranav Shyam"
            },
            {
                "authorId": "144864359",
                "name": "Girish Sastry"
            },
            {
                "authorId": "119609682",
                "name": "Amanda Askell"
            },
            {
                "authorId": "144517868",
                "name": "Sandhini Agarwal"
            },
            {
                "authorId": "1404060687",
                "name": "Ariel Herbert-Voss"
            },
            {
                "authorId": "2064404342",
                "name": "Gretchen Krueger"
            },
            {
                "authorId": "103143311",
                "name": "T. Henighan"
            },
            {
                "authorId": "48422824",
                "name": "R. Child"
            },
            {
                "authorId": "1992922591",
                "name": "A. Ramesh"
            },
            {
                "authorId": "2052152920",
                "name": "Daniel M. Ziegler"
            },
            {
                "authorId": "49387725",
                "name": "Jeff Wu"
            },
            {
                "authorId": "2059411355",
                "name": "Clemens Winter"
            },
            {
                "authorId": "144239765",
                "name": "Christopher Hesse"
            },
            {
                "authorId": "2108828435",
                "name": "Mark Chen"
            },
            {
                "authorId": "2064673055",
                "name": "Eric Sigler"
            },
            {
                "authorId": "1380985420",
                "name": "Ma-teusz Litwin"
            },
            {
                "authorId": "145565184",
                "name": "Scott Gray"
            },
            {
                "authorId": "1490681878",
                "name": "Benjamin Chess"
            },
            {
                "authorId": "2115193883",
                "name": "Jack Clark"
            },
            {
                "authorId": "133740015",
                "name": "Christopher Berner"
            },
            {
                "authorId": "52238703",
                "name": "Sam McCandlish"
            },
            {
                "authorId": "38909097",
                "name": "Alec Radford"
            },
            {
                "authorId": "1701686",
                "name": "I. Sutskever"
            },
            {
                "authorId": "2698777",
                "name": "Dario Amodei"
            }
        ],
        "abstract": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general."
    },
    {
        "paperId": "9a75cb455b4e70c66f3b72e6bb1498d8cab72fb2",
        "url": "https://www.semanticscholar.org/paper/9a75cb455b4e70c66f3b72e6bb1498d8cab72fb2",
        "title": "Big Self-Supervised Models are Strong Semi-Supervised Learners",
        "venue": "Neural Information Processing Systems",
        "year": 2020,
        "citationCount": 2431,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2006.10029, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "145358498",
                "name": "Ting Chen"
            },
            {
                "authorId": "40464924",
                "name": "Simon Kornblith"
            },
            {
                "authorId": "2329092656",
                "name": "Kevin Swersky"
            },
            {
                "authorId": "144739074",
                "name": "Mohammad Norouzi"
            },
            {
                "authorId": "1695689",
                "name": "Geoffrey E. Hinton"
            }
        ],
        "abstract": "One paradigm for learning from few labeled examples while making best use of a large amount of unlabeled data is unsupervised pretraining followed by supervised fine-tuning. Although this paradigm uses unlabeled data in a task-agnostic way, in contrast to most previous approaches to semi-supervised learning for computer vision, we show that it is surprisingly effective for semi-supervised learning on ImageNet. A key ingredient of our approach is the use of a big (deep and wide) network during pretraining and fine-tuning. We find that, the fewer the labels, the more this approach (task-agnostic use of unlabeled data) benefits from a bigger network. After fine-tuning, the big network can be further improved and distilled into a much smaller one with little loss in classification accuracy by using the unlabeled examples for a second time, but in a task-specific way. The proposed semi-supervised learning algorithm can be summarized in three steps: unsupervised pretraining of a big ResNet model using SimCLRv2 (a modification of SimCLR), supervised fine-tuning on a few labeled examples, and distillation with unlabeled examples for refining and transferring the task-specific knowledge. This procedure achieves 73.9\\% ImageNet top-1 accuracy with just 1\\% of the labels ($\\le$13 labeled images per class) using ResNet-50, a $10\\times$ improvement in label efficiency over the previous state-of-the-art. With 10\\% of labels, ResNet-50 trained with our method achieves 77.5\\% top-1 accuracy, outperforming standard supervised training with all of the labels."
    },
    {
        "paperId": "a0dc3135c40e150f0271002a96b7c9680b6cac40",
        "url": "https://www.semanticscholar.org/paper/a0dc3135c40e150f0271002a96b7c9680b6cac40",
        "title": "Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains",
        "venue": "Neural Information Processing Systems",
        "year": 2020,
        "citationCount": 2997,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2006.10739, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "7638730",
                "name": "Matthew Tancik"
            },
            {
                "authorId": "2179732",
                "name": "Pratul P. Srinivasan"
            },
            {
                "authorId": "2577533",
                "name": "B. Mildenhall"
            },
            {
                "authorId": "1405260934",
                "name": "Sara Fridovich-Keil"
            },
            {
                "authorId": "1752583931",
                "name": "Nithin Raghavan"
            },
            {
                "authorId": "34886103",
                "name": "Utkarsh Singhal"
            },
            {
                "authorId": "1752236",
                "name": "R. Ramamoorthi"
            },
            {
                "authorId": "50329510",
                "name": "J. Barron"
            },
            {
                "authorId": "47383180",
                "name": "Ren Ng"
            }
        ],
        "abstract": "We show that passing input points through a simple Fourier feature mapping enables a multilayer perceptron (MLP) to learn high-frequency functions in low-dimensional problem domains. These results shed light on recent advances in computer vision and graphics that achieve state-of-the-art results by using MLPs to represent complex 3D objects and scenes. Using tools from the neural tangent kernel (NTK) literature, we show that a standard MLP fails to learn high frequencies both in theory and in practice. To overcome this spectral bias, we use a Fourier feature mapping to transform the effective NTK into a stationary kernel with a tunable bandwidth. We suggest an approach for selecting problem-specific Fourier features that greatly improves the performance of MLPs for low-dimensional regression tasks relevant to the computer vision and graphics communities."
    },
    {
        "paperId": "c6c734e16f66fbfcefac7625cc64599e83292c1e",
        "url": "https://www.semanticscholar.org/paper/c6c734e16f66fbfcefac7625cc64599e83292c1e",
        "title": "MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers",
        "venue": "Neural Information Processing Systems",
        "year": 2020,
        "citationCount": 1731,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2002.10957, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "51456429",
                "name": "Wenhui Wang"
            },
            {
                "authorId": "49807919",
                "name": "Furu Wei"
            },
            {
                "authorId": "145307652",
                "name": "Li Dong"
            },
            {
                "authorId": "10699417",
                "name": "Hangbo Bao"
            },
            {
                "authorId": "144610884",
                "name": "Nan Yang"
            },
            {
                "authorId": "92660691",
                "name": "Ming Zhou"
            }
        ],
        "abstract": "Pre-trained language models (e.g., BERT (Devlin et al., 2018) and its variants) have achieved remarkable success in varieties of NLP tasks. However, these models usually consist of hundreds of millions of parameters which brings challenges for fine-tuning and online serving in real-life applications due to latency and capacity constraints. In this work, we present a simple and effective approach to compress large Transformer (Vaswani et al., 2017) based pre-trained models, termed as deep self-attention distillation. The small model (student) is trained by deeply mimicking the self-attention module, which plays a vital role in Transformer networks, of the large model (teacher). Specifically, we propose distilling the self-attention module of the last Transformer layer of the teacher, which is effective and flexible for the student. Furthermore, we introduce the scaled dot-product between values in the self-attention module as the new deep self-attention knowledge, in addition to the attention distributions (i.e., the scaled dot-product of queries and keys) that have been used in existing works. Moreover, we show that introducing a teacher assistant (Mirzadeh et al., 2019) also helps the distillation of large pre-trained Transformer models. Experimental results demonstrate that our monolingual model outperforms state-of-the-art baselines in different parameter size of student models. In particular, it retains more than 99% accuracy on SQuAD 2.0 and several GLUE benchmark tasks using 50% of the Transformer parameters and computations of the teacher model. We also obtain competitive results in applying deep self-attention distillation to multilingual pre-trained models."
    },
    {
        "paperId": "d73795d03114e3ff80c1b42e9f7a1bb95872bea9",
        "url": "https://www.semanticscholar.org/paper/d73795d03114e3ff80c1b42e9f7a1bb95872bea9",
        "title": "SOLOv2: Dynamic and Fast Instance Segmentation",
        "venue": "Neural Information Processing Systems",
        "year": 2020,
        "citationCount": 1007,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "1875266191",
                "name": "Xinlong Wang"
            },
            {
                "authorId": "1807746211",
                "name": "Rufeng Zhang"
            },
            {
                "authorId": "145868988",
                "name": "Tao Kong"
            },
            {
                "authorId": "143900005",
                "name": "Lei Li"
            },
            {
                "authorId": "12459603",
                "name": "Chunhua Shen"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "da60e046aac895b5775ed34bde45beb86aad0fe8",
        "url": "https://www.semanticscholar.org/paper/da60e046aac895b5775ed34bde45beb86aad0fe8",
        "title": "Generalized Focal Loss: Learning Qualified and Distributed Bounding Boxes for Dense Object Detection",
        "venue": "Neural Information Processing Systems",
        "year": 2020,
        "citationCount": 1562,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2006.04388, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2144439048",
                "name": "Xiang Li"
            },
            {
                "authorId": "71074736",
                "name": "Wenhai Wang"
            },
            {
                "authorId": "47767791",
                "name": "Lijun Wu"
            },
            {
                "authorId": "15841516",
                "name": "Shuo Chen"
            },
            {
                "authorId": "2109753669",
                "name": "Xiaolin Hu"
            },
            {
                "authorId": "2152748444",
                "name": "Jun Li"
            },
            {
                "authorId": "8053308",
                "name": "Jinhui Tang"
            },
            {
                "authorId": "2146236917",
                "name": "Jian Yang"
            }
        ],
        "abstract": "One-stage detector basically formulates object detection as dense classification and localization. The classification is usually optimized by Focal Loss and the box location is commonly learned under Dirac delta distribution. A recent trend for one-stage detectors is to introduce an individual prediction branch to estimate the quality of localization, where the predicted quality facilitates the classification to improve detection performance. This paper delves into the representations of the above three fundamental elements: quality estimation, classification and localization. Two problems are discovered in existing practices, including (1) the inconsistent usage of the quality estimation and classification between training and inference and (2) the inflexible Dirac delta distribution for localization when there is ambiguity and uncertainty in complex scenes. To address the problems, we design new representations for these elements. Specifically, we merge the quality estimation into the class prediction vector to form a joint representation of localization quality and classification, and use a vector to represent arbitrary distribution of box locations. The improved representations eliminate the inconsistency risk and accurately depict the flexible distribution in real data, but contain continuous labels, which is beyond the scope of Focal Loss. We then propose Generalized Focal Loss (GFL) that generalizes Focal Loss from its discrete form to the continuous version for successful optimization. On COCO test-dev, GFL achieves 45.0\\% AP using ResNet-101 backbone, surpassing state-of-the-art SAPD (43.5\\%) and ATSS (43.6\\%) with higher or comparable inference speed, under the same backbone and training settings. Notably, our best model can achieve a single-model single-scale AP of 48.2\\%, at 10 FPS on a single 2080Ti GPU. Code and models are available at this https URL."
    },
    {
        "paperId": "e10b7cb072737b1fbdcba7948e573268b3573ae9",
        "url": "https://www.semanticscholar.org/paper/e10b7cb072737b1fbdcba7948e573268b3573ae9",
        "title": "Dark Experience for General Continual Learning: a Strong, Simple Baseline",
        "venue": "Neural Information Processing Systems",
        "year": 2020,
        "citationCount": 1122,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2004.07211, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1429191945",
                "name": "Pietro Buzzega"
            },
            {
                "authorId": "51096265",
                "name": "Matteo Boschini"
            },
            {
                "authorId": "51119730",
                "name": "Angelo Porrello"
            },
            {
                "authorId": "3309130",
                "name": "Davide Abati"
            },
            {
                "authorId": "2175529",
                "name": "S. Calderara"
            }
        ],
        "abstract": "Neural networks struggle to learn continuously, as they forget the old knowledge catastrophically whenever the data distribution changes over time. Recently, Continual Learning has inspired a plethora of approaches and evaluation settings; however, the majority of them overlooks the properties of a practical scenario, where the data stream cannot be shaped as a sequence of tasks and offline training is not viable. We work towards General Continual Learning (GCL), where task boundaries blur and the domain and class distributions shift either gradually or suddenly. We address it through Dark Experience Replay, namely matching the network's logits sampled throughout the optimization trajectory, thus promoting consistency with its past. By conducting an extensive analysis on top of standard benchmarks, we show that such a seemingly simple baseline outperforms consolidated approaches and leverages limited resources. To provide a better understanding, we further introduce MNIST-360, a novel GCL evaluation setting."
    },
    {
        "paperId": "f56ae74c8c0842064654aeecb93d42bb3456b5b6",
        "url": "https://www.semanticscholar.org/paper/f56ae74c8c0842064654aeecb93d42bb3456b5b6",
        "title": "Personalized Federated Learning with Theoretical Guarantees: A Model-Agnostic Meta-Learning Approach",
        "venue": "Neural Information Processing Systems",
        "year": 2020,
        "citationCount": 1081,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "2141226716",
                "name": "Alireza Fallah"
            },
            {
                "authorId": "2706423",
                "name": "Aryan Mokhtari"
            },
            {
                "authorId": "145744184",
                "name": "A. Ozdaglar"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "f6d32ed0eee5fb3f6ac518f3aebc8ceff2aae397",
        "url": "https://www.semanticscholar.org/paper/f6d32ed0eee5fb3f6ac518f3aebc8ceff2aae397",
        "title": "NVAE: A Deep Hierarchical Variational Autoencoder",
        "venue": "Neural Information Processing Systems",
        "year": 2020,
        "citationCount": 1047,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2007.03898, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3214848",
                "name": "Arash Vahdat"
            },
            {
                "authorId": "2376331464",
                "name": "Jan Kautz"
            }
        ],
        "abstract": "Normalizing flows, autoregressive models, variational autoencoders (VAEs), and deep energy-based models are among competing likelihood-based frameworks for deep generative learning. Among them, VAEs have the advantage of fast and tractable sampling and easy-to-access encoding networks. However, they are currently outperformed by other models such as normalizing flows and autoregressive models. While the majority of the research in VAEs is focused on the statistical challenges, we explore the orthogonal direction of carefully designing neural architectures for hierarchical VAEs. We propose Nouveau VAE (NVAE), a deep hierarchical VAE built for image generation using depth-wise separable convolutions and batch normalization. NVAE is equipped with a residual parameterization of Normal distributions and its training is stabilized by spectral regularization. We show that NVAE achieves state-of-the-art results among non-autoregressive likelihood-based models on the MNIST, CIFAR-10, CelebA 64, and CelebA HQ datasets and it provides a strong baseline on FFHQ. For example, on CIFAR-10, NVAE pushes the state-of-the-art from 2.98 to 2.91 bits per dimension, and it produces high-quality images on CelebA HQ. To the best of our knowledge, NVAE is the first successful VAE applied to natural images as large as 256$\\times$256 pixels. The source code is available at this https URL ."
    },
    {
        "paperId": "0ae67202f0584afccefa770865d14a46655d2975",
        "url": "https://www.semanticscholar.org/paper/0ae67202f0584afccefa770865d14a46655d2975",
        "title": "Transformer in Transformer",
        "venue": "Neural Information Processing Systems",
        "year": 2021,
        "citationCount": 1967,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2103.00112, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3826388",
                "name": "Kai Han"
            },
            {
                "authorId": "1569696821",
                "name": "An Xiao"
            },
            {
                "authorId": "145344139",
                "name": "E. Wu"
            },
            {
                "authorId": "2148899357",
                "name": "Jianyuan Guo"
            },
            {
                "authorId": "1691522",
                "name": "Chunjing Xu"
            },
            {
                "authorId": "2108702980",
                "name": "Yunhe Wang"
            }
        ],
        "abstract": "Transformer is a new kind of neural architecture which encodes the input data as powerful features via the attention mechanism. Basically, the visual transformers first divide the input images into several local patches and then calculate both representations and their relationship. Since natural images are of high complexity with abundant detail and color information, the granularity of the patch dividing is not fine enough for excavating features of objects in different scales and locations. In this paper, we point out that the attention inside these local patches are also essential for building visual transformers with high performance and we explore a new architecture, namely, Transformer iN Transformer (TNT). Specifically, we regard the local patches (e.g., 16$\\times$16) as\"visual sentences\"and present to further divide them into smaller patches (e.g., 4$\\times$4) as\"visual words\". The attention of each word will be calculated with other words in the given visual sentence with negligible computational costs. Features of both words and sentences will be aggregated to enhance the representation ability. Experiments on several benchmarks demonstrate the effectiveness of the proposed TNT architecture, e.g., we achieve an 81.5% top-1 accuracy on the ImageNet, which is about 1.7% higher than that of the state-of-the-art visual transformer with similar computational cost. The PyTorch code is available at https://github.com/huawei-noah/CV-Backbones, and the MindSpore code is available at https://gitee.com/mindspore/models/tree/master/research/cv/TNT."
    },
    {
        "paperId": "260ad39a1dac4b451019e2bf17925f4df8e3b69a",
        "url": "https://www.semanticscholar.org/paper/260ad39a1dac4b451019e2bf17925f4df8e3b69a",
        "title": "Per-Pixel Classification is Not All You Need for Semantic Segmentation",
        "venue": "Neural Information Processing Systems",
        "year": 2021,
        "citationCount": 1844,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2107.06278, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "50563570",
                "name": "Bowen Cheng"
            },
            {
                "authorId": "2068227",
                "name": "A. Schwing"
            },
            {
                "authorId": "144843400",
                "name": "Alexander Kirillov"
            }
        ],
        "abstract": "Modern approaches typically formulate semantic segmentation as a per-pixel classification task, while instance-level segmentation is handled with an alternative mask classification. Our key insight: mask classification is sufficiently general to solve both semantic- and instance-level segmentation tasks in a unified manner using the exact same model, loss, and training procedure. Following this observation, we propose MaskFormer, a simple mask classification model which predicts a set of binary masks, each associated with a single global class label prediction. Overall, the proposed mask classification-based method simplifies the landscape of effective approaches to semantic and panoptic segmentation tasks and shows excellent empirical results. In particular, we observe that MaskFormer outperforms per-pixel classification baselines when the number of classes is large. Our mask classification-based method outperforms both current state-of-the-art semantic (55.6 mIoU on ADE20K) and panoptic segmentation (52.7 PQ on COCO) models."
    },
    {
        "paperId": "39b492db00faead70bc3f4fb4b0364d94398ffdb",
        "url": "https://www.semanticscholar.org/paper/39b492db00faead70bc3f4fb4b0364d94398ffdb",
        "title": "Do Vision Transformers See Like Convolutional Neural Networks?",
        "venue": "Neural Information Processing Systems",
        "year": 2021,
        "citationCount": 1195,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2108.08810, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "40297238",
                "name": "M. Raghu"
            },
            {
                "authorId": "2465270",
                "name": "Thomas Unterthiner"
            },
            {
                "authorId": "40464924",
                "name": "Simon Kornblith"
            },
            {
                "authorId": "2145179541",
                "name": "Chiyuan Zhang"
            },
            {
                "authorId": "2841331",
                "name": "Alexey Dosovitskiy"
            }
        ],
        "abstract": "Convolutional neural networks (CNNs) have so far been the de-facto model for visual data. Recent work has shown that (Vision) Transformer models (ViT) can achieve comparable or even superior performance on image classification tasks. This raises a central question: how are Vision Transformers solving these tasks? Are they acting like convolutional networks, or learning entirely different visual representations? Analyzing the internal representation structure of ViTs and CNNs on image classification benchmarks, we find striking differences between the two architectures, such as ViT having more uniform representations across all layers. We explore how these differences arise, finding crucial roles played by self-attention, which enables early aggregation of global information, and ViT residual connections, which strongly propagate features from lower to higher layers. We study the ramifications for spatial localization, demonstrating ViTs successfully preserve input spatial information, with noticeable effects from different classification methods. Finally, we study the effect of (pretraining) dataset scale on intermediate features and transfer learning, and conclude with a discussion on connections to new architectures such as the MLP-Mixer."
    },
    {
        "paperId": "3a315c81a98851f0614c09fef6a14c30d6a1e63c",
        "url": "https://www.semanticscholar.org/paper/3a315c81a98851f0614c09fef6a14c30d6a1e63c",
        "title": "The Surprising Effectiveness of PPO in Cooperative Multi-Agent Games",
        "venue": "Neural Information Processing Systems",
        "year": 2021,
        "citationCount": 1790,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2103.01955, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2110962386",
                "name": "Chao Yu"
            },
            {
                "authorId": "1999664196",
                "name": "Akash Velu"
            },
            {
                "authorId": "13430868",
                "name": "Eugene Vinitsky"
            },
            {
                "authorId": "2153607473",
                "name": "Yu Wang"
            },
            {
                "authorId": "1705102",
                "name": "A. Bayen"
            },
            {
                "authorId": "2108052525",
                "name": "Yi Wu"
            }
        ],
        "abstract": "Proximal Policy Optimization (PPO) is a ubiquitous on-policy reinforcement learning algorithm but is significantly less utilized than off-policy learning algorithms in multi-agent settings. This is often due to the belief that PPO is significantly less sample efficient than off-policy methods in multi-agent systems. In this work, we carefully study the performance of PPO in cooperative multi-agent settings. We show that PPO-based multi-agent algorithms achieve surprisingly strong performance in four popular multi-agent testbeds: the particle-world environments, the StarCraft multi-agent challenge, Google Research Football, and the Hanabi challenge, with minimal hyperparameter tuning and without any domain-specific algorithmic modifications or architectures. Importantly, compared to competitive off-policy methods, PPO often achieves competitive or superior results in both final returns and sample efficiency. Finally, through ablation studies, we analyze implementation and hyperparameter factors that are critical to PPO's empirical performance, and give concrete practical suggestions regarding these factors. Our results show that when using these practices, simple PPO-based methods can be a strong baseline in cooperative multi-agent reinforcement learning. Source code is released at \\url{https://github.com/marlbenchmark/on-policy}."
    },
    {
        "paperId": "4b842ba29f6d244b9f456056a2d7efab9e4903a5",
        "url": "https://www.semanticscholar.org/paper/4b842ba29f6d244b9f456056a2d7efab9e4903a5",
        "title": "FlexMatch: Boosting Semi-Supervised Learning with Curriculum Pseudo Labeling",
        "venue": "Neural Information Processing Systems",
        "year": 2021,
        "citationCount": 1128,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2110.08263, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2141912823",
                "name": "Bowen Zhang"
            },
            {
                "authorId": "2108024279",
                "name": "Yidong Wang"
            },
            {
                "authorId": "32216189",
                "name": "Wenxin Hou"
            },
            {
                "authorId": "1664776313",
                "name": "Hao Wu"
            },
            {
                "authorId": "1519290245",
                "name": "Jindong Wang"
            },
            {
                "authorId": "144859189",
                "name": "M. Okumura"
            },
            {
                "authorId": "1732454",
                "name": "T. Shinozaki"
            }
        ],
        "abstract": "The recently proposed FixMatch achieved state-of-the-art results on most semi-supervised learning (SSL) benchmarks. However, like other modern SSL algorithms, FixMatch uses a pre-defined constant threshold for all classes to select unlabeled data that contribute to the training, thus failing to consider different learning status and learning difficulties of different classes. To address this issue, we propose Curriculum Pseudo Labeling (CPL), a curriculum learning approach to leverage unlabeled data according to the model's learning status. The core of CPL is to flexibly adjust thresholds for different classes at each time step to let pass informative unlabeled data and their pseudo labels. CPL does not introduce additional parameters or computations (forward or backward propagation). We apply CPL to FixMatch and call our improved algorithm FlexMatch. FlexMatch achieves state-of-the-art performance on a variety of SSL benchmarks, with especially strong performances when the labeled data are extremely limited or when the task is challenging. For example, FlexMatch achieves 13.96% and 18.96% error rate reduction over FixMatch on CIFAR-100 and STL-10 datasets respectively, when there are only 4 labels per class. CPL also significantly boosts the convergence speed, e.g., FlexMatch can use only 1/5 training time of FixMatch to achieve even better performance. Furthermore, we show that CPL can be easily adapted to other SSL algorithms and remarkably improve their performances. We open-source our code at https://github.com/TorchSSL/TorchSSL."
    },
    {
        "paperId": "5fa06d856ba6ae9cd1366888f8134d7fd0db75b9",
        "url": "https://www.semanticscholar.org/paper/5fa06d856ba6ae9cd1366888f8134d7fd0db75b9",
        "title": "Revisiting Deep Learning Models for Tabular Data",
        "venue": "Neural Information Processing Systems",
        "year": 2021,
        "citationCount": 1047,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2106.11959, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "102481418",
                "name": "Yu. V. Gorishniy"
            },
            {
                "authorId": "2114431113",
                "name": "Ivan Rubachev"
            },
            {
                "authorId": "10662951",
                "name": "Valentin Khrulkov"
            },
            {
                "authorId": "143743802",
                "name": "Artem Babenko"
            }
        ],
        "abstract": "The existing literature on deep learning for tabular data proposes a wide range of novel architectures and reports competitive results on various datasets. However, the proposed models are usually not properly compared to each other and existing works often use different benchmarks and experiment protocols. As a result, it is unclear for both researchers and practitioners what models perform best. Additionally, the field still lacks effective baselines, that is, the easy-to-use models that provide competitive performance across different problems. In this work, we perform an overview of the main families of DL architectures for tabular data and raise the bar of baselines in tabular DL by identifying two simple and powerful deep architectures. The first one is a ResNet-like architecture which turns out to be a strong baseline that is often missing in prior works. The second model is our simple adaptation of the Transformer architecture for tabular data, which outperforms other solutions on most tasks. Both models are compared to many existing architectures on a diverse set of tasks under the same training and tuning protocols. We also compare the best DL models with Gradient Boosted Decision Trees and conclude that there is still no universally superior solution."
    },
    {
        "paperId": "64ea8f180d0682e6c18d1eb688afdb2027c02794",
        "url": "https://www.semanticscholar.org/paper/64ea8f180d0682e6c18d1eb688afdb2027c02794",
        "title": "Diffusion Models Beat GANs on Image Synthesis",
        "venue": "Neural Information Processing Systems",
        "year": 2021,
        "citationCount": 10194,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2105.05233, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "6515819",
                "name": "Prafulla Dhariwal"
            },
            {
                "authorId": "38967461",
                "name": "Alex Nichol"
            }
        ],
        "abstract": "We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128$\\times$128, 4.59 on ImageNet 256$\\times$256, and 7.72 on ImageNet 512$\\times$512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256$\\times$256 and 3.85 on ImageNet 512$\\times$512. We release our code at https://github.com/openai/guided-diffusion"
    },
    {
        "paperId": "6709d5583f658f589ae6a2184805933aceb18849",
        "url": "https://www.semanticscholar.org/paper/6709d5583f658f589ae6a2184805933aceb18849",
        "title": "Twins: Revisiting the Design of Spatial Attention in Vision Transformers",
        "venue": "Neural Information Processing Systems",
        "year": 2021,
        "citationCount": 1211,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2104.13840, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "27628828",
                "name": "Xiangxiang Chu"
            },
            {
                "authorId": "2069520672",
                "name": "Zhi Tian"
            },
            {
                "authorId": "2108035065",
                "name": "Yuqing Wang"
            },
            {
                "authorId": null,
                "name": "Bo Zhang"
            },
            {
                "authorId": "1558541225",
                "name": "Haibing Ren"
            },
            {
                "authorId": "49141839",
                "name": "Xiaolin Wei"
            },
            {
                "authorId": "2065044626",
                "name": "Huaxia Xia"
            },
            {
                "authorId": "12459603",
                "name": "Chunhua Shen"
            }
        ],
        "abstract": "Very recently, a variety of vision transformer architectures for dense prediction tasks have been proposed and they show that the design of spatial attention is critical to their success in these tasks. In this work, we revisit the design of the spatial attention and demonstrate that a carefully-devised yet simple spatial attention mechanism performs favourably against the state-of-the-art schemes. As a result, we propose two vision transformer architectures, namely, Twins-PCPVT and Twins-SVT. Our proposed architectures are highly-efficient and easy to implement, only involving matrix multiplications that are highly optimized in modern deep learning frameworks. More importantly, the proposed architectures achieve excellent performance on a wide range of visual tasks, including image level classification as well as dense detection and segmentation. The simplicity and strong performance suggest that our proposed architectures may serve as stronger backbones for many vision tasks. Our code is released at https://github.com/Meituan-AutoML/Twins ."
    },
    {
        "paperId": "67571d29190faea9fbd104acd16274f8c4edf254",
        "url": "https://www.semanticscholar.org/paper/67571d29190faea9fbd104acd16274f8c4edf254",
        "title": "MLP-Mixer: An all-MLP Architecture for Vision",
        "venue": "Neural Information Processing Systems",
        "year": 2021,
        "citationCount": 3255,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2105.01601, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3121264",
                "name": "I. Tolstikhin"
            },
            {
                "authorId": "2815290",
                "name": "N. Houlsby"
            },
            {
                "authorId": "144629422",
                "name": "Alexander Kolesnikov"
            },
            {
                "authorId": "39611591",
                "name": "Lucas Beyer"
            },
            {
                "authorId": "2743563",
                "name": "Xiaohua Zhai"
            },
            {
                "authorId": "2465270",
                "name": "Thomas Unterthiner"
            },
            {
                "authorId": "1470947219",
                "name": "Jessica Yung"
            },
            {
                "authorId": "51027911",
                "name": "Daniel Keysers"
            },
            {
                "authorId": "39328010",
                "name": "Jakob Uszkoreit"
            },
            {
                "authorId": "34302129",
                "name": "Mario Lucic"
            },
            {
                "authorId": "2841331",
                "name": "Alexey Dosovitskiy"
            }
        ],
        "abstract": "Convolutional Neural Networks (CNNs) are the go-to model for computer vision. Recently, attention-based networks, such as the Vision Transformer, have also become popular. In this paper we show that while convolutions and attention are both sufficient for good performance, neither of them are necessary. We present MLP-Mixer, an architecture based exclusively on multi-layer perceptrons (MLPs). MLP-Mixer contains two types of layers: one with MLPs applied independently to image patches (i.e.\"mixing\"the per-location features), and one with MLPs applied across patches (i.e.\"mixing\"spatial information). When trained on large datasets, or with modern regularization schemes, MLP-Mixer attains competitive scores on image classification benchmarks, with pre-training and inference cost comparable to state-of-the-art models. We hope that these results spark further research beyond the realms of well established CNNs and Transformers."
    },
    {
        "paperId": "91b32fc0a23f0af53229fceaae9cce43a0406d2e",
        "url": "https://www.semanticscholar.org/paper/91b32fc0a23f0af53229fceaae9cce43a0406d2e",
        "title": "Structured Denoising Diffusion Models in Discrete State-Spaces",
        "venue": "Neural Information Processing Systems",
        "year": 2021,
        "citationCount": 1338,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2107.03006, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2058365883",
                "name": "Jacob Austin"
            },
            {
                "authorId": "2150444707",
                "name": "Daniel D. Johnson"
            },
            {
                "authorId": "2126278",
                "name": "Jonathan Ho"
            },
            {
                "authorId": "1725299",
                "name": "Daniel Tarlow"
            },
            {
                "authorId": "9965217",
                "name": "Rianne van den Berg"
            }
        ],
        "abstract": "Denoising diffusion probabilistic models (DDPMs) (Ho et al. 2020) have shown impressive results on image and waveform generation in continuous state spaces. Here, we introduce Discrete Denoising Diffusion Probabilistic Models (D3PMs), diffusion-like generative models for discrete data that generalize the multinomial diffusion model of Hoogeboom et al. 2021, by going beyond corruption processes with uniform transition probabilities. This includes corruption with transition matrices that mimic Gaussian kernels in continuous space, matrices based on nearest neighbors in embedding space, and matrices that introduce absorbing states. The third allows us to draw a connection between diffusion models and autoregressive and mask-based generative models. We show that the choice of transition matrix is an important design decision that leads to improved results in image and text domains. We also introduce a new loss function that combines the variational lower bound with an auxiliary cross entropy loss. For text, this model class achieves strong results on character-level text generation while scaling to large vocabularies on LM1B. On the image dataset CIFAR-10, our models approach the sample quality and exceed the log-likelihood of the continuous-space DDPM model."
    },
    {
        "paperId": "9f4b69762ffb1ba42b573fd4ced996f3153e21c0",
        "url": "https://www.semanticscholar.org/paper/9f4b69762ffb1ba42b573fd4ced996f3153e21c0",
        "title": "CoAtNet: Marrying Convolution and Attention for All Data Sizes",
        "venue": "Neural Information Processing Systems",
        "year": 2021,
        "citationCount": 1449,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2106.04803, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3422912",
                "name": "Zihang Dai"
            },
            {
                "authorId": "2391802",
                "name": "Hanxiao Liu"
            },
            {
                "authorId": "2827616",
                "name": "Quoc V. Le"
            },
            {
                "authorId": "120805419",
                "name": "Mingxing Tan"
            }
        ],
        "abstract": "Transformers have attracted increasing interests in computer vision, but they still fall behind state-of-the-art convolutional networks. In this work, we show that while Transformers tend to have larger model capacity, their generalization can be worse than convolutional networks due to the lack of the right inductive bias. To effectively combine the strengths from both architectures, we present CoAtNets(pronounced\"coat\"nets), a family of hybrid models built from two key insights: (1) depthwise Convolution and self-Attention can be naturally unified via simple relative attention; (2) vertically stacking convolution layers and attention layers in a principled way is surprisingly effective in improving generalization, capacity and efficiency. Experiments show that our CoAtNets achieve state-of-the-art performance under different resource constraints across various datasets: Without extra data, CoAtNet achieves 86.0% ImageNet top-1 accuracy; When pre-trained with 13M images from ImageNet-21K, our CoAtNet achieves 88.56% top-1 accuracy, matching ViT-huge pre-trained with 300M images from JFT-300M while using 23x less data; Notably, when we further scale up CoAtNet with JFT-3B, it achieves 90.88% top-1 accuracy on ImageNet, establishing a new state-of-the-art result."
    },
    {
        "paperId": "acf87283fa8ae426f1a4987b345b401bf2913f61",
        "url": "https://www.semanticscholar.org/paper/acf87283fa8ae426f1a4987b345b401bf2913f61",
        "title": "Do Transformers Really Perform Badly for Graph Representation?",
        "venue": "Neural Information Processing Systems",
        "year": 2021,
        "citationCount": 1169,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "2051552141",
                "name": "Chengxuan Ying"
            },
            {
                "authorId": "123970124",
                "name": "Tianle Cai"
            },
            {
                "authorId": "2108801920",
                "name": "Shengjie Luo"
            },
            {
                "authorId": "150311931",
                "name": "Shuxin Zheng"
            },
            {
                "authorId": "35286545",
                "name": "Guolin Ke"
            },
            {
                "authorId": "2266036459",
                "name": "Di He"
            },
            {
                "authorId": "2266126249",
                "name": "Yanming Shen"
            },
            {
                "authorId": "2266182896",
                "name": "Tie-Yan Liu"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "b82c5f9efdb2ae56baa084ca41aeddd8a665c1d1",
        "url": "https://www.semanticscholar.org/paper/b82c5f9efdb2ae56baa084ca41aeddd8a665c1d1",
        "title": "Align before Fuse: Vision and Language Representation Learning with Momentum Distillation",
        "venue": "Neural Information Processing Systems",
        "year": 2021,
        "citationCount": 2442,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2107.07651, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "49299019",
                "name": "Junnan Li"
            },
            {
                "authorId": "35100058",
                "name": "Ramprasaath R. Selvaraju"
            },
            {
                "authorId": "144049726",
                "name": "Akhilesh Deepak Gotmare"
            },
            {
                "authorId": "2708940",
                "name": "Shafiq R. Joty"
            },
            {
                "authorId": "2054594326",
                "name": "Caiming Xiong"
            },
            {
                "authorId": "1741126",
                "name": "S. Hoi"
            }
        ],
        "abstract": "Large-scale vision and language representation learning has shown promising improvements on various vision-language tasks. Most existing methods employ a transformer-based multimodal encoder to jointly model visual tokens (region-based image features) and word tokens. Because the visual tokens and word tokens are unaligned, it is challenging for the multimodal encoder to learn image-text interactions. In this paper, we introduce a contrastive loss to ALign the image and text representations BEfore Fusing (ALBEF) them through cross-modal attention, which enables more grounded vision and language representation learning. Unlike most existing methods, our method does not require bounding box annotations nor high-resolution images. In order to improve learning from noisy web data, we propose momentum distillation, a self-training method which learns from pseudo-targets produced by a momentum model. We provide a theoretical analysis of ALBEF from a mutual information maximization perspective, showing that different training tasks can be interpreted as different ways to generate views for an image-text pair. ALBEF achieves state-of-the-art performance on multiple downstream vision-language tasks. On image-text retrieval, ALBEF outperforms methods that are pre-trained on orders of magnitude larger datasets. On VQA and NLVR$^2$, ALBEF achieves absolute improvements of 2.37% and 3.84% compared to the state-of-the-art, while enjoying faster inference speed. Code and pre-trained models are available at https://github.com/salesforce/ALBEF/."
    },
    {
        "paperId": "c1ad5f9b32d80f1c65d67894e5b8c2fdf0ae4500",
        "url": "https://www.semanticscholar.org/paper/c1ad5f9b32d80f1c65d67894e5b8c2fdf0ae4500",
        "title": "Decision Transformer: Reinforcement Learning via Sequence Modeling",
        "venue": "Neural Information Processing Systems",
        "year": 2021,
        "citationCount": 1980,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2106.01345, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2108435457",
                "name": "Lili Chen"
            },
            {
                "authorId": "2070275468",
                "name": "Kevin Lu"
            },
            {
                "authorId": "19275599",
                "name": "A. Rajeswaran"
            },
            {
                "authorId": "3436470",
                "name": "Kimin Lee"
            },
            {
                "authorId": "1954250",
                "name": "Aditya Grover"
            },
            {
                "authorId": "51093256",
                "name": "M. Laskin"
            },
            {
                "authorId": "1689992",
                "name": "P. Abbeel"
            },
            {
                "authorId": "41207614",
                "name": "A. Srinivas"
            },
            {
                "authorId": "2080746",
                "name": "Igor Mordatch"
            }
        ],
        "abstract": "We introduce a framework that abstracts Reinforcement Learning (RL) as a sequence modeling problem. This allows us to draw upon the simplicity and scalability of the Transformer architecture, and associated advances in language modeling such as GPT-x and BERT. In particular, we present Decision Transformer, an architecture that casts the problem of RL as conditional sequence modeling. Unlike prior approaches to RL that fit value functions or compute policy gradients, Decision Transformer simply outputs the optimal actions by leveraging a causally masked Transformer. By conditioning an autoregressive model on the desired return (reward), past states, and actions, our Decision Transformer model can generate future actions that achieve the desired return. Despite its simplicity, Decision Transformer matches or exceeds the performance of state-of-the-art model-free offline RL baselines on Atari, OpenAI Gym, and Key-to-Door tasks."
    },
    {
        "paperId": "c1ff08b59f00c44f34dfdde55cd53370733a2c19",
        "url": "https://www.semanticscholar.org/paper/c1ff08b59f00c44f34dfdde55cd53370733a2c19",
        "title": "Alias-Free Generative Adversarial Networks",
        "venue": "Neural Information Processing Systems",
        "year": 2021,
        "citationCount": 1856,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2106.12423, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2976930",
                "name": "Tero Karras"
            },
            {
                "authorId": "1907688",
                "name": "M. Aittala"
            },
            {
                "authorId": "36436218",
                "name": "S. Laine"
            },
            {
                "authorId": "103642338",
                "name": "Erik Hrknen"
            },
            {
                "authorId": "1454226629",
                "name": "Janne Hellsten"
            },
            {
                "authorId": "49244945",
                "name": "J. Lehtinen"
            },
            {
                "authorId": "1761103",
                "name": "Timo Aila"
            }
        ],
        "abstract": "We observe that despite their hierarchical convolutional nature, the synthesis process of typical generative adversarial networks depends on absolute pixel coordinates in an unhealthy manner. This manifests itself as, e.g., detail appearing to be glued to image coordinates instead of the surfaces of depicted objects. We trace the root cause to careless signal processing that causes aliasing in the generator network. Interpreting all signals in the network as continuous, we derive generally applicable, small architectural changes that guarantee that unwanted information cannot leak into the hierarchical synthesis process. The resulting networks match the FID of StyleGAN2 but differ dramatically in their internal representations, and they are fully equivariant to translation and rotation even at subpixel scales. Our results pave the way for generative models better suited for video and animation."
    },
    {
        "paperId": "cf5647cb2613f5f697729eab567383006dcd4913",
        "url": "https://www.semanticscholar.org/paper/cf5647cb2613f5f697729eab567383006dcd4913",
        "title": "NeuS: Learning Neural Implicit Surfaces by Volume Rendering for Multi-view Reconstruction",
        "venue": "Neural Information Processing Systems",
        "year": 2021,
        "citationCount": 2109,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2106.10689, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2155300848",
                "name": "Peng Wang"
            },
            {
                "authorId": "46458089",
                "name": "Lingjie Liu"
            },
            {
                "authorId": "2143861796",
                "name": "Yuan Liu"
            },
            {
                "authorId": "1680185",
                "name": "C. Theobalt"
            },
            {
                "authorId": "2254293",
                "name": "T. Komura"
            },
            {
                "authorId": "2108349601",
                "name": "Wenping Wang"
            }
        ],
        "abstract": "We present a novel neural surface reconstruction method, called NeuS, for reconstructing objects and scenes with high fidelity from 2D image inputs. Existing neural surface reconstruction approaches, such as DVR and IDR, require foreground mask as supervision, easily get trapped in local minima, and therefore struggle with the reconstruction of objects with severe self-occlusion or thin structures. Meanwhile, recent neural methods for novel view synthesis, such as NeRF and its variants, use volume rendering to produce a neural scene representation with robustness of optimization, even for highly complex objects. However, extracting high-quality surfaces from this learned implicit representation is difficult because there are not sufficient surface constraints in the representation. In NeuS, we propose to represent a surface as the zero-level set of a signed distance function (SDF) and develop a new volume rendering method to train a neural SDF representation. We observe that the conventional volume rendering method causes inherent geometric errors (i.e. bias) for surface reconstruction, and therefore propose a new formulation that is free of bias in the first order of approximation, thus leading to more accurate surface reconstruction even without the mask supervision. Experiments on the DTU dataset and the BlendedMVS dataset show that NeuS outperforms the state-of-the-arts in high-quality surface reconstruction, especially for objects and scenes with complex structures and self-occlusion."
    },
    {
        "paperId": "e3d7778a47c6cab4ea1ef3ee9d19ec1510c15c60",
        "url": "https://www.semanticscholar.org/paper/e3d7778a47c6cab4ea1ef3ee9d19ec1510c15c60",
        "title": "SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers",
        "venue": "Neural Information Processing Systems",
        "year": 2021,
        "citationCount": 6933,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2105.15203, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "41020000",
                "name": "Enze Xie"
            },
            {
                "authorId": "71074736",
                "name": "Wenhai Wang"
            },
            {
                "authorId": "1751019",
                "name": "Zhiding Yu"
            },
            {
                "authorId": "2047844",
                "name": "Anima Anandkumar"
            },
            {
                "authorId": "2974008",
                "name": "J. lvarez"
            },
            {
                "authorId": "47571885",
                "name": "Ping Luo"
            }
        ],
        "abstract": "We present SegFormer, a simple, efficient yet powerful semantic segmentation framework which unifies Transformers with lightweight multilayer perception (MLP) decoders. SegFormer has two appealing features: 1) SegFormer comprises a novel hierarchically structured Transformer encoder which outputs multiscale features. It does not need positional encoding, thereby avoiding the interpolation of positional codes which leads to decreased performance when the testing resolution differs from training. 2) SegFormer avoids complex decoders. The proposed MLP decoder aggregates information from different layers, and thus combining both local attention and global attention to render powerful representations. We show that this simple and lightweight design is the key to efficient segmentation on Transformers. We scale our approach up to obtain a series of models from SegFormer-B0 to SegFormer-B5, reaching significantly better performance and efficiency than previous counterparts. For example, SegFormer-B4 achieves 50.3% mIoU on ADE20K with 64M parameters, being 5x smaller and 2.2% better than the previous best method. Our best model, SegFormer-B5, achieves 84.0% mIoU on Cityscapes validation set and shows excellent zero-shot robustness on Cityscapes-C. Code will be released at: github.com/NVlabs/SegFormer."
    },
    {
        "paperId": "eded1f3acaba853499fd5a6b3de63fa9d5e0cef2",
        "url": "https://www.semanticscholar.org/paper/eded1f3acaba853499fd5a6b3de63fa9d5e0cef2",
        "title": "Volume Rendering of Neural Implicit Surfaces",
        "venue": "Neural Information Processing Systems",
        "year": 2021,
        "citationCount": 1236,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2106.12052, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "121960008",
                "name": "Lior Yariv"
            },
            {
                "authorId": "3016273",
                "name": "Jiatao Gu"
            },
            {
                "authorId": "3437811",
                "name": "Yoni Kasten"
            },
            {
                "authorId": "3232072",
                "name": "Y. Lipman"
            }
        ],
        "abstract": "Neural volume rendering became increasingly popular recently due to its success in synthesizing novel views of a scene from a sparse set of input images. So far, the geometry learned by neural volume rendering techniques was modeled using a generic density function. Furthermore, the geometry itself was extracted using an arbitrary level set of the density function leading to a noisy, often low fidelity reconstruction. The goal of this paper is to improve geometry representation and reconstruction in neural volume rendering. We achieve that by modeling the volume density as a function of the geometry. This is in contrast to previous work modeling the geometry as a function of the volume density. In more detail, we define the volume density function as Laplace's cumulative distribution function (CDF) applied to a signed distance function (SDF) representation. This simple density representation has three benefits: (i) it provides a useful inductive bias to the geometry learned in the neural volume rendering process; (ii) it facilitates a bound on the opacity approximation error, leading to an accurate sampling of the viewing ray. Accurate sampling is important to provide a precise coupling of geometry and radiance; and (iii) it allows efficient unsupervised disentanglement of shape and appearance in volume rendering. Applying this new density representation to challenging scene multiview datasets produced high quality geometry reconstructions, outperforming relevant baselines. Furthermore, switching shape and appearance between scenes is possible due to the disentanglement of the two."
    },
    {
        "paperId": "fc46ccb83dc121c33de7ab6bdedab7d970780b2f",
        "url": "https://www.semanticscholar.org/paper/fc46ccb83dc121c33de7ab6bdedab7d970780b2f",
        "title": "Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting",
        "venue": "Neural Information Processing Systems",
        "year": 2021,
        "citationCount": 3602,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2106.13008, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2051867856",
                "name": "Haixu Wu"
            },
            {
                "authorId": "2111064536",
                "name": "Jiehui Xu"
            },
            {
                "authorId": "2144499343",
                "name": "Jianmin Wang"
            },
            {
                "authorId": "2054275000",
                "name": "Mingsheng Long"
            }
        ],
        "abstract": "Extending the forecasting time is a critical demand for real applications, such as extreme weather early warning and long-term energy consumption planning. This paper studies the long-term forecasting problem of time series. Prior Transformer-based models adopt various self-attention mechanisms to discover the long-range dependencies. However, intricate temporal patterns of the long-term future prohibit the model from finding reliable dependencies. Also, Transformers have to adopt the sparse versions of point-wise self-attentions for long series efficiency, resulting in the information utilization bottleneck. Going beyond Transformers, we design Autoformer as a novel decomposition architecture with an Auto-Correlation mechanism. We break with the pre-processing convention of series decomposition and renovate it as a basic inner block of deep models. This design empowers Autoformer with progressive decomposition capacities for complex time series. Further, inspired by the stochastic process theory, we design the Auto-Correlation mechanism based on the series periodicity, which conducts the dependencies discovery and representation aggregation at the sub-series level. Auto-Correlation outperforms self-attention in both efficiency and accuracy. In long-term forecasting, Autoformer yields state-of-the-art accuracy, with a 38% relative improvement on six benchmarks, covering five practical applications: energy, traffic, economics, weather and disease. Code is available at this repository: \\url{https://github.com/thuml/Autoformer}."
    },
    {
        "paperId": "1386b8a11929cf02da291c56aca353e33bbc22ed",
        "url": "https://www.semanticscholar.org/paper/1386b8a11929cf02da291c56aca353e33bbc22ed",
        "title": "Diffusion-LM Improves Controllable Text Generation",
        "venue": "Neural Information Processing Systems",
        "year": 2022,
        "citationCount": 1085,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2205.14217",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2205.14217, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "32551341",
                "name": "Xiang Lisa Li"
            },
            {
                "authorId": "50343904",
                "name": "John Thickstun"
            },
            {
                "authorId": "2708454",
                "name": "Ishaan Gulrajani"
            },
            {
                "authorId": "145419642",
                "name": "Percy Liang"
            },
            {
                "authorId": "2117567142",
                "name": "Tatsunori Hashimoto"
            }
        ],
        "abstract": "Controlling the behavior of language models (LMs) without re-training is a major open problem in natural language generation. While recent works have demonstrated successes on controlling simple sentence attributes (e.g., sentiment), there has been little progress on complex, fine-grained controls (e.g., syntactic structure). To address this challenge, we develop a new non-autoregressive language model based on continuous diffusions that we call Diffusion-LM. Building upon the recent successes of diffusion models in continuous domains, Diffusion-LM iteratively denoises a sequence of Gaussian vectors into word vectors, yielding a sequence of intermediate latent variables. The continuous, hierarchical nature of these intermediate variables enables a simple gradient-based algorithm to perform complex, controllable generation tasks. We demonstrate successful control of Diffusion-LM for six challenging fine-grained control tasks, significantly outperforming prior work."
    },
    {
        "paperId": "1b6e810ce0afd0dd093f789d2b2742d047e316d5",
        "url": "https://www.semanticscholar.org/paper/1b6e810ce0afd0dd093f789d2b2742d047e316d5",
        "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models",
        "venue": "Neural Information Processing Systems",
        "year": 2022,
        "citationCount": 14321,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2201.11903, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "119640649",
                "name": "Jason Wei"
            },
            {
                "authorId": "2275277634",
                "name": "Xuezhi Wang"
            },
            {
                "authorId": "1714772",
                "name": "Dale Schuurmans"
            },
            {
                "authorId": "40377863",
                "name": "Maarten Bosma"
            },
            {
                "authorId": "2226805",
                "name": "Ed H. Chi"
            },
            {
                "authorId": "144956443",
                "name": "F. Xia"
            },
            {
                "authorId": "1998340269",
                "name": "Quoc Le"
            },
            {
                "authorId": "65855107",
                "name": "Denny Zhou"
            }
        ],
        "abstract": "We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier."
    },
    {
        "paperId": "26218bdcc3945c7edae7aa2adbfba4cd820a2df3",
        "url": "https://www.semanticscholar.org/paper/26218bdcc3945c7edae7aa2adbfba4cd820a2df3",
        "title": "Flamingo: a Visual Language Model for Few-Shot Learning",
        "venue": "Neural Information Processing Systems",
        "year": 2022,
        "citationCount": 4776,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2204.14198, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2285263",
                "name": "Jean-Baptiste Alayrac"
            },
            {
                "authorId": "7408951",
                "name": "Jeff Donahue"
            },
            {
                "authorId": "152831141",
                "name": "Pauline Luc"
            },
            {
                "authorId": "19200186",
                "name": "Antoine Miech"
            },
            {
                "authorId": "2159207795",
                "name": "Iain Barr"
            },
            {
                "authorId": "66535271",
                "name": "Yana Hasson"
            },
            {
                "authorId": "3257286",
                "name": "Karel Lenc"
            },
            {
                "authorId": "1697879",
                "name": "A. Mensch"
            },
            {
                "authorId": "2143434227",
                "name": "Katie Millican"
            },
            {
                "authorId": "47447264",
                "name": "Malcolm Reynolds"
            },
            {
                "authorId": "81387328",
                "name": "Roman Ring"
            },
            {
                "authorId": "2143538252",
                "name": "Eliza Rutherford"
            },
            {
                "authorId": "12159303",
                "name": "Serkan Cabi"
            },
            {
                "authorId": "22237490",
                "name": "Tengda Han"
            },
            {
                "authorId": "48398849",
                "name": "Zhitao Gong"
            },
            {
                "authorId": "2412073",
                "name": "Sina Samangooei"
            },
            {
                "authorId": "49601928",
                "name": "Marianne Monteiro"
            },
            {
                "authorId": "10698483",
                "name": "Jacob Menick"
            },
            {
                "authorId": "148016269",
                "name": "Sebastian Borgeaud"
            },
            {
                "authorId": "2065040422",
                "name": "Andy Brock"
            },
            {
                "authorId": "3208081",
                "name": "Aida Nematzadeh"
            },
            {
                "authorId": "7782886",
                "name": "Sahand Sharifzadeh"
            },
            {
                "authorId": "9961753",
                "name": "Mikolaj Binkowski"
            },
            {
                "authorId": "2026369796",
                "name": "Ricardo Barreira"
            },
            {
                "authorId": "1689108",
                "name": "O. Vinyals"
            },
            {
                "authorId": "1688869",
                "name": "Andrew Zisserman"
            },
            {
                "authorId": "34838386",
                "name": "K. Simonyan"
            }
        ],
        "abstract": "Building models that can be rapidly adapted to novel tasks using only a handful of annotated examples is an open challenge for multimodal machine learning research. We introduce Flamingo, a family of Visual Language Models (VLM) with this ability. We propose key architectural innovations to: (i) bridge powerful pretrained vision-only and language-only models, (ii) handle sequences of arbitrarily interleaved visual and textual data, and (iii) seamlessly ingest images or videos as inputs. Thanks to their flexibility, Flamingo models can be trained on large-scale multimodal web corpora containing arbitrarily interleaved text and images, which is key to endow them with in-context few-shot learning capabilities. We perform a thorough evaluation of our models, exploring and measuring their ability to rapidly adapt to a variety of image and video tasks. These include open-ended tasks such as visual question-answering, where the model is prompted with a question which it has to answer; captioning tasks, which evaluate the ability to describe a scene or an event; and close-ended tasks such as multiple-choice visual question-answering. For tasks lying anywhere on this spectrum, a single Flamingo model can achieve a new state of the art with few-shot learning, simply by prompting the model with task-specific examples. On numerous benchmarks, Flamingo outperforms models fine-tuned on thousands of times more task-specific data."
    },
    {
        "paperId": "2f4c451922e227cbbd4f090b74298445bbd900d0",
        "url": "https://www.semanticscholar.org/paper/2f4c451922e227cbbd4f090b74298445bbd900d0",
        "title": "Elucidating the Design Space of Diffusion-Based Generative Models",
        "venue": "Neural Information Processing Systems",
        "year": 2022,
        "citationCount": 2720,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2206.00364",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2206.00364, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2976930",
                "name": "Tero Karras"
            },
            {
                "authorId": "1907688",
                "name": "M. Aittala"
            },
            {
                "authorId": "1761103",
                "name": "Timo Aila"
            },
            {
                "authorId": "36436218",
                "name": "S. Laine"
            }
        ],
        "abstract": "We argue that the theory and practice of diffusion-based generative models are currently unnecessarily convoluted and seek to remedy the situation by presenting a design space that clearly separates the concrete design choices. This lets us identify several changes to both the sampling and training processes, as well as preconditioning of the score networks. Together, our improvements yield new state-of-the-art FID of 1.79 for CIFAR-10 in a class-conditional setting and 1.97 in an unconditional setting, with much faster sampling (35 network evaluations per image) than prior designs. To further demonstrate their modular nature, we show that our design changes dramatically improve both the efficiency and quality obtainable with pre-trained score networks from previous work, including improving the FID of a previously trained ImageNet-64 model from 2.07 to near-SOTA 1.55, and after re-training with our proposed improvements to a new SOTA of 1.36."
    },
    {
        "paperId": "3b2a675bb617ae1a920e8e29d535cdf27826e999",
        "url": "https://www.semanticscholar.org/paper/3b2a675bb617ae1a920e8e29d535cdf27826e999",
        "title": "Video Diffusion Models",
        "venue": "Neural Information Processing Systems",
        "year": 2022,
        "citationCount": 2182,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2204.03458",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2204.03458, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2112615253",
                "name": "Jonathan Ho"
            },
            {
                "authorId": "2117261512",
                "name": "Tim Salimans"
            },
            {
                "authorId": "2161660107",
                "name": "Alexey Gritsenko"
            },
            {
                "authorId": "2150198218",
                "name": "William Chan"
            },
            {
                "authorId": "2138892698",
                "name": "Mohammad Norouzi"
            },
            {
                "authorId": "1793739",
                "name": "David J. Fleet"
            }
        ],
        "abstract": "Generating temporally coherent high fidelity video is an important milestone in generative modeling research. We make progress towards this milestone by proposing a diffusion model for video generation that shows very promising initial results. Our model is a natural extension of the standard image diffusion architecture, and it enables jointly training from image and video data, which we find to reduce the variance of minibatch gradients and speed up optimization. To generate long and higher resolution videos we introduce a new conditional sampling technique for spatial and temporal video extension that performs better than previously proposed methods. We present the first results on a large text-conditioned video generation task, as well as state-of-the-art results on established benchmarks for video prediction and unconditional video generation. Supplementary material is available at https://video-diffusion.github.io/"
    },
    {
        "paperId": "3d3c5fcbc40aadccceda58d3d9c5cd00588ea0b7",
        "url": "https://www.semanticscholar.org/paper/3d3c5fcbc40aadccceda58d3d9c5cd00588ea0b7",
        "title": "Denoising Diffusion Restoration Models",
        "venue": "Neural Information Processing Systems",
        "year": 2022,
        "citationCount": 1080,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2201.11793, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2047309422",
                "name": "Bahjat Kawar"
            },
            {
                "authorId": "1753908",
                "name": "Michael Elad"
            },
            {
                "authorId": "2490652",
                "name": "Stefano Ermon"
            },
            {
                "authorId": "51453887",
                "name": "Jiaming Song"
            }
        ],
        "abstract": "Many interesting tasks in image restoration can be cast as linear inverse problems. A recent family of approaches for solving these problems uses stochastic algorithms that sample from the posterior distribution of natural images given the measurements. However, efficient solutions often require problem-specific supervised training to model the posterior, whereas unsupervised methods that are not problem-specific typically rely on inefficient iterative methods. This work addresses these issues by introducing Denoising Diffusion Restoration Models (DDRM), an efficient, unsupervised posterior sampling method. Motivated by variational inference, DDRM takes advantage of a pre-trained denoising diffusion generative model for solving any linear inverse problem. We demonstrate DDRM's versatility on several image datasets for super-resolution, deblurring, inpainting, and colorization under various amounts of measurement noise. DDRM outperforms the current leading unsupervised methods on the diverse ImageNet dataset in reconstruction quality, perceptual quality, and runtime, being 5x faster than the nearest competitor. DDRM also generalizes well for natural images out of the distribution of the observed ImageNet training set."
    },
    {
        "paperId": "4530c25da949bb2185c50663158ef19d52e3c6b5",
        "url": "https://www.semanticscholar.org/paper/4530c25da949bb2185c50663158ef19d52e3c6b5",
        "title": "DPM-Solver: A Fast ODE Solver for Diffusion Probabilistic Model Sampling in Around 10 Steps",
        "venue": "Neural Information Processing Systems",
        "year": 2022,
        "citationCount": 1937,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2206.00927",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2206.00927, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "102517285",
                "name": "Cheng Lu"
            },
            {
                "authorId": "2120327644",
                "name": "Yuhao Zhou"
            },
            {
                "authorId": "2071898125",
                "name": "Fan Bao"
            },
            {
                "authorId": "2276707",
                "name": "Jianfei Chen"
            },
            {
                "authorId": "2399563",
                "name": "Chongxuan Li"
            },
            {
                "authorId": "2155220672",
                "name": "Jun Zhu"
            }
        ],
        "abstract": "Diffusion probabilistic models (DPMs) are emerging powerful generative models. Despite their high-quality generation performance, DPMs still suffer from their slow sampling as they generally need hundreds or thousands of sequential function evaluations (steps) of large neural networks to draw a sample. Sampling from DPMs can be viewed alternatively as solving the corresponding diffusion ordinary differential equations (ODEs). In this work, we propose an exact formulation of the solution of diffusion ODEs. The formulation analytically computes the linear part of the solution, rather than leaving all terms to black-box ODE solvers as adopted in previous works. By applying change-of-variable, the solution can be equivalently simplified to an exponentially weighted integral of the neural network. Based on our formulation, we propose DPM-Solver, a fast dedicated high-order solver for diffusion ODEs with the convergence order guarantee. DPM-Solver is suitable for both discrete-time and continuous-time DPMs without any further training. Experimental results show that DPM-Solver can generate high-quality samples in only 10 to 20 function evaluations on various datasets. We achieve 4.70 FID in 10 function evaluations and 2.87 FID in 20 function evaluations on the CIFAR10 dataset, and a $4\\sim 16\\times$ speedup compared with previous state-of-the-art training-free samplers on various datasets."
    },
    {
        "paperId": "4990f7542f0600e0501a7e7a931b32eb7cb804d5",
        "url": "https://www.semanticscholar.org/paper/4990f7542f0600e0501a7e7a931b32eb7cb804d5",
        "title": "VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training",
        "venue": "Neural Information Processing Systems",
        "year": 2022,
        "citationCount": 1605,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2203.12602",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2203.12602, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2143681543",
                "name": "Zhan Tong"
            },
            {
                "authorId": "2255687",
                "name": "Yibing Song"
            },
            {
                "authorId": "2144536737",
                "name": "Jue Wang"
            },
            {
                "authorId": "2109120086",
                "name": "Limin Wang"
            }
        ],
        "abstract": "Pre-training video transformers on extra large-scale datasets is generally required to achieve premier performance on relatively small datasets. In this paper, we show that video masked autoencoders (VideoMAE) are data-efficient learners for self-supervised video pre-training (SSVP). We are inspired by the recent ImageMAE and propose customized video tube masking with an extremely high ratio. This simple design makes video reconstruction a more challenging self-supervision task, thus encouraging extracting more effective video representations during this pre-training process. We obtain three important findings on SSVP: (1) An extremely high proportion of masking ratio (i.e., 90% to 95%) still yields favorable performance of VideoMAE. The temporally redundant video content enables a higher masking ratio than that of images. (2) VideoMAE achieves impressive results on very small datasets (i.e., around 3k-4k videos) without using any extra data. (3) VideoMAE shows that data quality is more important than data quantity for SSVP. Domain shift between pre-training and target datasets is an important issue. Notably, our VideoMAE with the vanilla ViT can achieve 87.4% on Kinetics-400, 75.4% on Something-Something V2, 91.3% on UCF101, and 62.6% on HMDB51, without using any extra data. Code is available at https://github.com/MCG-NJU/VideoMAE."
    },
    {
        "paperId": "5a00b32876f7d4869bce980500d4ccc978389315",
        "url": "https://www.semanticscholar.org/paper/5a00b32876f7d4869bce980500d4ccc978389315",
        "title": "Why do tree-based models still outperform deep learning on typical tabular data?",
        "venue": "Neural Information Processing Systems",
        "year": 2022,
        "citationCount": 1120,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "1509420956",
                "name": "Lo Grinsztajn"
            },
            {
                "authorId": "3306593",
                "name": "Edouard Oyallon"
            },
            {
                "authorId": "3025780",
                "name": "G. Varoquaux"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "7cdaa08890895e1ad92afb5fad429690ad7b1dac",
        "url": "https://www.semanticscholar.org/paper/7cdaa08890895e1ad92afb5fad429690ad7b1dac",
        "title": "Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning",
        "venue": "Neural Information Processing Systems",
        "year": 2022,
        "citationCount": 1148,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2205.05638",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2205.05638, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "48447436",
                "name": "Haokun Liu"
            },
            {
                "authorId": "1390031652",
                "name": "Derek Tam"
            },
            {
                "authorId": "1582888954",
                "name": "Mohammed Muqeeth"
            },
            {
                "authorId": "2390409554",
                "name": "Jay Mohta"
            },
            {
                "authorId": "2110510944",
                "name": "Tenghao Huang"
            },
            {
                "authorId": "143977268",
                "name": "Mohit Bansal"
            },
            {
                "authorId": "2402716",
                "name": "Colin Raffel"
            }
        ],
        "abstract": "Few-shot in-context learning (ICL) enables pre-trained language models to perform a previously-unseen task without any gradient-based training by feeding a small number of training examples as part of the input. ICL incurs substantial computational, memory, and storage costs because it involves processing all of the training examples every time a prediction is made. Parameter-efficient fine-tuning (PEFT) (e.g. adapter modules, prompt tuning, sparse update methods, etc.) offers an alternative paradigm where a small set of parameters are trained to enable a model to perform the new task. In this paper, we rigorously compare few-shot ICL and PEFT and demonstrate that the latter offers better accuracy as well as dramatically lower computational costs. Along the way, we introduce a new PEFT method called (IA)$^3$ that scales activations by learned vectors, attaining stronger performance while only introducing a relatively tiny amount of new parameters. We also propose a simple recipe based on the T0 model called T-Few that can be applied to new tasks without task-specific tuning or modifications. We validate the effectiveness of T-Few on completely unseen tasks by applying it to the RAFT benchmark, attaining super-human performance for the first time and outperforming the state-of-the-art by 6% absolute. All of the code used in our experiments is publicly available."
    },
    {
        "paperId": "87c5b281fa43e6f27191b20a8dd694eda1126336",
        "url": "https://www.semanticscholar.org/paper/87c5b281fa43e6f27191b20a8dd694eda1126336",
        "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness",
        "venue": "Neural Information Processing Systems",
        "year": 2022,
        "citationCount": 3287,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2205.14135, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "24593911",
                "name": "Tri Dao"
            },
            {
                "authorId": "49577833",
                "name": "Daniel Y. Fu"
            },
            {
                "authorId": "2490652",
                "name": "Stefano Ermon"
            },
            {
                "authorId": "1755572",
                "name": "A. Rudra"
            },
            {
                "authorId": "2061444681",
                "name": "Christopher R'e"
            }
        ],
        "abstract": "Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy)."
    },
    {
        "paperId": "9695824d7a01fad57ba9c01d7d76a519d78d65e7",
        "url": "https://www.semanticscholar.org/paper/9695824d7a01fad57ba9c01d7d76a519d78d65e7",
        "title": "Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding",
        "venue": "Neural Information Processing Systems",
        "year": 2022,
        "citationCount": 7421,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2205.11487",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2205.11487, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "51497543",
                "name": "Chitwan Saharia"
            },
            {
                "authorId": "144333684",
                "name": "William Chan"
            },
            {
                "authorId": "2054003577",
                "name": "Saurabh Saxena"
            },
            {
                "authorId": "2111917831",
                "name": "Lala Li"
            },
            {
                "authorId": "21040156",
                "name": "Jay Whang"
            },
            {
                "authorId": "40081727",
                "name": "Emily L. Denton"
            },
            {
                "authorId": "81419386",
                "name": "Seyed Kamyar Seyed Ghasemipour"
            },
            {
                "authorId": "143990191",
                "name": "Burcu Karagol Ayan"
            },
            {
                "authorId": "1982213",
                "name": "S. S. Mahdavi"
            },
            {
                "authorId": "143826364",
                "name": "Raphael Gontijo Lopes"
            },
            {
                "authorId": "2887364",
                "name": "Tim Salimans"
            },
            {
                "authorId": "2126278",
                "name": "Jonathan Ho"
            },
            {
                "authorId": "1793739",
                "name": "David J. Fleet"
            },
            {
                "authorId": "144739074",
                "name": "Mohammad Norouzi"
            }
        ],
        "abstract": "We present Imagen, a text-to-image diffusion model with an unprecedented degree of photorealism and a deep level of language understanding. Imagen builds on the power of large transformer language models in understanding text and hinges on the strength of diffusion models in high-fidelity image generation. Our key discovery is that generic large language models (e.g. T5), pretrained on text-only corpora, are surprisingly effective at encoding text for image synthesis: increasing the size of the language model in Imagen boosts both sample fidelity and image-text alignment much more than increasing the size of the image diffusion model. Imagen achieves a new state-of-the-art FID score of 7.27 on the COCO dataset, without ever training on COCO, and human raters find Imagen samples to be on par with the COCO data itself in image-text alignment. To assess text-to-image models in greater depth, we introduce DrawBench, a comprehensive and challenging benchmark for text-to-image models. With DrawBench, we compare Imagen with recent methods including VQ-GAN+CLIP, Latent Diffusion Models, and DALL-E 2, and find that human raters prefer Imagen over other models in side-by-side comparisons, both in terms of sample quality and image-text alignment. See https://imagen.research.google/ for an overview of the results."
    },
    {
        "paperId": "996445d847f06e99b0bd259345408a0cf1bce87e",
        "url": "https://www.semanticscholar.org/paper/996445d847f06e99b0bd259345408a0cf1bce87e",
        "title": "Locating and Editing Factual Associations in GPT",
        "venue": "Neural Information Processing Systems",
        "year": 2022,
        "citationCount": 1903,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2202.05262, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "153615419",
                "name": "Kevin Meng"
            },
            {
                "authorId": "144159726",
                "name": "David Bau"
            },
            {
                "authorId": "50112310",
                "name": "A. Andonian"
            },
            {
                "authorId": "2083259",
                "name": "Yonatan Belinkov"
            }
        ],
        "abstract": "We analyze the storage and recall of factual associations in autoregressive transformer language models, finding evidence that these associations correspond to localized, directly-editable computations. We first develop a causal intervention for identifying neuron activations that are decisive in a model's factual predictions. This reveals a distinct set of steps in middle-layer feed-forward modules that mediate factual predictions while processing subject tokens. To test our hypothesis that these computations correspond to factual association recall, we modify feed-forward weights to update specific factual associations using Rank-One Model Editing (ROME). We find that ROME is effective on a standard zero-shot relation extraction (zsRE) model-editing task, comparable to existing methods. To perform a more sensitive evaluation, we also evaluate ROME on a new dataset of counterfactual assertions, on which it simultaneously maintains both specificity and generalization, whereas other methods sacrifice one or another. Our results confirm an important role for mid-layer feed-forward modules in storing factual associations and suggest that direct manipulation of computational mechanisms may be a feasible approach for model editing. The code, dataset, visualizations, and an interactive demo notebook are available at https://rome.baulab.info/"
    },
    {
        "paperId": "ab0e3d3e4d42369de5933a3b4c237780b41c0d77",
        "url": "https://www.semanticscholar.org/paper/ab0e3d3e4d42369de5933a3b4c237780b41c0d77",
        "title": "Solving Quantitative Reasoning Problems with Language Models",
        "venue": "Neural Information Processing Systems",
        "year": 2022,
        "citationCount": 1282,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2206.14858",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2206.14858, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "102549875",
                "name": "Aitor Lewkowycz"
            },
            {
                "authorId": "39552848",
                "name": "Anders Andreassen"
            },
            {
                "authorId": "35363891",
                "name": "David Dohan"
            },
            {
                "authorId": "52136425",
                "name": "Ethan Dyer"
            },
            {
                "authorId": "47407464",
                "name": "H. Michalewski"
            },
            {
                "authorId": "96641652",
                "name": "V. Ramasesh"
            },
            {
                "authorId": "133666998",
                "name": "Ambrose Slone"
            },
            {
                "authorId": "48314480",
                "name": "Cem Anil"
            },
            {
                "authorId": "35328044",
                "name": "Imanol Schlag"
            },
            {
                "authorId": "2174177407",
                "name": "Theo Gutman-Solo"
            },
            {
                "authorId": "3374063",
                "name": "Yuhuai Wu"
            },
            {
                "authorId": "3007442",
                "name": "Behnam Neyshabur"
            },
            {
                "authorId": "2284681044",
                "name": "Guy Gur-Ari"
            },
            {
                "authorId": "40055795",
                "name": "Vedant Misra"
            }
        ],
        "abstract": "Language models have achieved remarkable performance on a wide range of tasks that require natural language understanding. Nevertheless, state-of-the-art models have generally struggled with tasks that require quantitative reasoning, such as solving mathematics, science, and engineering problems at the college level. To help close this gap, we introduce Minerva, a large language model pretrained on general natural language data and further trained on technical content. The model achieves state-of-the-art performance on technical benchmarks without the use of external tools. We also evaluate our model on over two hundred undergraduate-level problems in physics, biology, chemistry, economics, and other sciences that require quantitative reasoning, and find that the model can correctly answer nearly a third of them."
    },
    {
        "paperId": "d3135733aa39dec20ce72aa138589dda27c8406d",
        "url": "https://www.semanticscholar.org/paper/d3135733aa39dec20ce72aa138589dda27c8406d",
        "title": "Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering",
        "venue": "Neural Information Processing Systems",
        "year": 2022,
        "citationCount": 1837,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2209.09513",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2209.09513, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2887562",
                "name": "Pan Lu"
            },
            {
                "authorId": "1817207",
                "name": "Swaroop Mishra"
            },
            {
                "authorId": "2143749775",
                "name": "Tony Xia"
            },
            {
                "authorId": "47659905",
                "name": "Liang Qiu"
            },
            {
                "authorId": "2782886",
                "name": "Kai-Wei Chang"
            },
            {
                "authorId": "145380991",
                "name": "Song-Chun Zhu"
            },
            {
                "authorId": "3385516",
                "name": "Oyvind Tafjord"
            },
            {
                "authorId": "48323507",
                "name": "Peter Clark"
            },
            {
                "authorId": "51043791",
                "name": "A. Kalyan"
            }
        ],
        "abstract": "When answering a question, humans utilize the information available across different modalities to synthesize a consistent and complete chain of thought (CoT). This process is normally a black box in the case of deep learning models like large-scale language models. Recently, science question benchmarks have been used to diagnose the multi-hop reasoning ability and interpretability of an AI system. However, existing datasets fail to provide annotations for the answers, or are restricted to the textual-only modality, small scales, and limited domain diversity. To this end, we present Science Question Answering (ScienceQA), a new benchmark that consists of ~21k multimodal multiple choice questions with a diverse set of science topics and annotations of their answers with corresponding lectures and explanations. We further design language models to learn to generate lectures and explanations as the chain of thought (CoT) to mimic the multi-hop reasoning process when answering ScienceQA questions. ScienceQA demonstrates the utility of CoT in language models, as CoT improves the question answering performance by 1.20% in few-shot GPT-3 and 3.99% in fine-tuned UnifiedQA. We also explore the upper bound for models to leverage explanations by feeding those in the input; we observe that it improves the few-shot performance of GPT-3 by 18.96%. Our analysis further shows that language models, similar to humans, benefit from explanations to learn from fewer data and achieve the same performance with just 40% of the data. The data and code are available at https://scienceqa.github.io."
    },
    {
        "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "url": "https://www.semanticscholar.org/paper/d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback",
        "venue": "Neural Information Processing Systems",
        "year": 2022,
        "citationCount": 17257,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2203.02155, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "31793034",
                "name": "Long Ouyang"
            },
            {
                "authorId": "49387725",
                "name": "Jeff Wu"
            },
            {
                "authorId": "2115903168",
                "name": "Xu Jiang"
            },
            {
                "authorId": "2061137049",
                "name": "Diogo Almeida"
            },
            {
                "authorId": "2064084601",
                "name": "Carroll L. Wainwright"
            },
            {
                "authorId": "2051714782",
                "name": "Pamela Mishkin"
            },
            {
                "authorId": null,
                "name": "Chong Zhang"
            },
            {
                "authorId": "144517868",
                "name": "Sandhini Agarwal"
            },
            {
                "authorId": "2117680841",
                "name": "Katarina Slama"
            },
            {
                "authorId": "2064770039",
                "name": "Alex Ray"
            },
            {
                "authorId": "47971768",
                "name": "John Schulman"
            },
            {
                "authorId": "2052366271",
                "name": "Jacob Hilton"
            },
            {
                "authorId": "2151735262",
                "name": "Fraser Kelton"
            },
            {
                "authorId": "2142365973",
                "name": "Luke E. Miller"
            },
            {
                "authorId": "2151735251",
                "name": "Maddie Simens"
            },
            {
                "authorId": "119609682",
                "name": "Amanda Askell"
            },
            {
                "authorId": "2930640",
                "name": "Peter Welinder"
            },
            {
                "authorId": "145791315",
                "name": "P. Christiano"
            },
            {
                "authorId": "2990741",
                "name": "Jan Leike"
            },
            {
                "authorId": "49407415",
                "name": "Ryan J. Lowe"
            }
        ],
        "abstract": "Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent."
    },
    {
        "paperId": "e5c8960eb2ec034ffbd353ef39fd1cb541d3c7c9",
        "url": "https://www.semanticscholar.org/paper/e5c8960eb2ec034ffbd353ef39fd1cb541d3c7c9",
        "title": "LAION-5B: An open large-scale dataset for training next generation image-text models",
        "venue": "Neural Information Processing Systems",
        "year": 2022,
        "citationCount": 4486,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2210.08402",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2210.08402, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2137341362",
                "name": "Christoph Schuhmann"
            },
            {
                "authorId": "2125377840",
                "name": "R. Beaumont"
            },
            {
                "authorId": "2137306046",
                "name": "R. Vencu"
            },
            {
                "authorId": "2007745319",
                "name": "Cade Gordon"
            },
            {
                "authorId": "2113839396",
                "name": "Ross Wightman"
            },
            {
                "authorId": "40063601",
                "name": "Mehdi Cherti"
            },
            {
                "authorId": "2137419050",
                "name": "Theo Coombes"
            },
            {
                "authorId": "2137380426",
                "name": "Aarush Katta"
            },
            {
                "authorId": "2137306237",
                "name": "Clayton Mullis"
            },
            {
                "authorId": "52193502",
                "name": "Mitchell Wortsman"
            },
            {
                "authorId": "40896023",
                "name": "P. Schramowski"
            },
            {
                "authorId": "2165304339",
                "name": "Srivatsa Kundurthy"
            },
            {
                "authorId": "104998244",
                "name": "Katherine Crowson"
            },
            {
                "authorId": "152772922",
                "name": "Ludwig Schmidt"
            },
            {
                "authorId": "8095484",
                "name": "R. Kaczmarczyk"
            },
            {
                "authorId": "2191688",
                "name": "J. Jitsev"
            }
        ],
        "abstract": "Groundbreaking language-vision architectures like CLIP and DALL-E proved the utility of training on large amounts of noisy image-text data, without relying on expensive accurate labels used in standard vision unimodal supervised learning. The resulting models showed capabilities of strong text-guided image generation and transfer to downstream tasks, while performing remarkably at zero-shot classification with noteworthy out-of-distribution robustness. Since then, large-scale language-vision models like ALIGN, BASIC, GLIDE, Flamingo and Imagen made further improvements. Studying the training and capabilities of such models requires datasets containing billions of image-text pairs. Until now, no datasets of this size have been made openly available for the broader research community. To address this problem and democratize research on large-scale multi-modal models, we present LAION-5B - a dataset consisting of 5.85 billion CLIP-filtered image-text pairs, of which 2.32B contain English language. We show successful replication and fine-tuning of foundational models like CLIP, GLIDE and Stable Diffusion using the dataset, and discuss further experiments enabled with an openly available dataset of this scale. Additionally we provide several nearest neighbor indices, an improved web-interface for dataset exploration and subset generation, and detection scores for watermark, NSFW, and toxic content detection. Announcement page https://laion.ai/laion-5b-a-new-era-of-open-large-scale-multi-modal-datasets/"
    },
    {
        "paperId": "e7ad08848d5d7c5c47673ffe0da06af443643bda",
        "url": "https://www.semanticscholar.org/paper/e7ad08848d5d7c5c47673ffe0da06af443643bda",
        "title": "Large Language Models are Zero-Shot Reasoners",
        "venue": "Neural Information Processing Systems",
        "year": 2022,
        "citationCount": 6008,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2205.11916, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2081836120",
                "name": "Takeshi Kojima"
            },
            {
                "authorId": "2046135",
                "name": "S. Gu"
            },
            {
                "authorId": "1557386977",
                "name": "Machel Reid"
            },
            {
                "authorId": "2153732825",
                "name": "Yutaka Matsuo"
            },
            {
                "authorId": "1715282",
                "name": "Yusuke Iwasawa"
            }
        ],
        "abstract": "Pretrained large language models (LLMs) are widely used in many sub-fields of natural language processing (NLP) and generally known as excellent few-shot learners with task-specific exemplars. Notably, chain of thought (CoT) prompting, a recent technique for eliciting complex multi-step reasoning through step-by-step answer examples, achieved the state-of-the-art performances in arithmetics and symbolic reasoning, difficult system-2 tasks that do not follow the standard scaling laws for LLMs. While these successes are often attributed to LLMs' ability for few-shot learning, we show that LLMs are decent zero-shot reasoners by simply adding\"Let's think step by step\"before each answer. Experimental results demonstrate that our Zero-shot-CoT, using the same single prompt template, significantly outperforms zero-shot LLM performances on diverse benchmark reasoning tasks including arithmetics (MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning (Last Letter, Coin Flip), and other logical reasoning tasks (Date Understanding, Tracking Shuffled Objects), without any hand-crafted few-shot examples, e.g. increasing the accuracy on MultiArith from 17.7% to 78.7% and GSM8K from 10.4% to 40.7% with large InstructGPT model (text-davinci-002), as well as similar magnitudes of improvements with another off-the-shelf large model, 540B parameter PaLM. The versatility of this single prompt across very diverse reasoning tasks hints at untapped and understudied fundamental zero-shot capabilities of LLMs, suggesting high-level, multi-task broad cognitive capabilities may be extracted by simple prompting. We hope our work not only serves as the minimal strongest zero-shot baseline for the challenging reasoning benchmarks, but also highlights the importance of carefully exploring and analyzing the enormous zero-shot knowledge hidden inside LLMs before crafting finetuning datasets or few-shot exemplars."
    },
    {
        "paperId": "0671fd553dd670a4e820553a974bc48040ba0819",
        "url": "https://www.semanticscholar.org/paper/0671fd553dd670a4e820553a974bc48040ba0819",
        "title": "Reflexion: language agents with verbal reinforcement learning",
        "venue": "Neural Information Processing Systems",
        "year": 2023,
        "citationCount": 2200,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2303.11366, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2212367248",
                "name": "Noah Shinn"
            },
            {
                "authorId": "2197063831",
                "name": "Federico Cassano"
            },
            {
                "authorId": "2212367414",
                "name": "Beck Labash"
            },
            {
                "authorId": "2162047785",
                "name": "A. Gopinath"
            },
            {
                "authorId": "144958935",
                "name": "Karthik Narasimhan"
            },
            {
                "authorId": "47188964",
                "name": "Shunyu Yao"
            }
        ],
        "abstract": "Large language models (LLMs) have been increasingly used to interact with external environments (e.g., games, compilers, APIs) as goal-driven agents. However, it remains challenging for these language agents to quickly and efficiently learn from trial-and-error as traditional reinforcement learning methods require extensive training samples and expensive model fine-tuning. We propose Reflexion, a novel framework to reinforce language agents not by updating weights, but instead through linguistic feedback. Concretely, Reflexion agents verbally reflect on task feedback signals, then maintain their own reflective text in an episodic memory buffer to induce better decision-making in subsequent trials. Reflexion is flexible enough to incorporate various types (scalar values or free-form language) and sources (external or internally simulated) of feedback signals, and obtains significant improvements over a baseline agent across diverse tasks (sequential decision-making, coding, language reasoning). For example, Reflexion achieves a 91% pass@1 accuracy on the HumanEval coding benchmark, surpassing the previous state-of-the-art GPT-4 that achieves 80%. We also conduct ablation and analysis studies using different feedback signals, feedback incorporation methods, and agent types, and provide insights into how they affect performance."
    },
    {
        "paperId": "0d1c76d45afa012ded7ab741194baf142117c495",
        "url": "https://www.semanticscholar.org/paper/0d1c76d45afa012ded7ab741194baf142117c495",
        "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model",
        "venue": "Neural Information Processing Systems",
        "year": 2023,
        "citationCount": 6550,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.18290, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "102801230",
                "name": "Rafael Rafailov"
            },
            {
                "authorId": "50465276",
                "name": "Archit Sharma"
            },
            {
                "authorId": "49688913",
                "name": "E. Mitchell"
            },
            {
                "authorId": "2490652",
                "name": "Stefano Ermon"
            },
            {
                "authorId": "144783904",
                "name": "Christopher D. Manning"
            },
            {
                "authorId": "46881670",
                "name": "Chelsea Finn"
            }
        ],
        "abstract": "While large-scale unsupervised language models (LMs) learn broad world knowledge and some reasoning skills, achieving precise control of their behavior is difficult due to the completely unsupervised nature of their training. Existing methods for gaining such steerability collect human labels of the relative quality of model generations and fine-tune the unsupervised LM to align with these preferences, often with reinforcement learning from human feedback (RLHF). However, RLHF is a complex and often unstable procedure, first fitting a reward model that reflects the human preferences, and then fine-tuning the large unsupervised LM using reinforcement learning to maximize this estimated reward without drifting too far from the original model. In this paper we introduce a new parameterization of the reward model in RLHF that enables extraction of the corresponding optimal policy in closed form, allowing us to solve the standard RLHF problem with only a simple classification loss. The resulting algorithm, which we call Direct Preference Optimization (DPO), is stable, performant, and computationally lightweight, eliminating the need for sampling from the LM during fine-tuning or performing significant hyperparameter tuning. Our experiments show that DPO can fine-tune LMs to align with human preferences as well as or better than existing methods. Notably, fine-tuning with DPO exceeds PPO-based RLHF in ability to control sentiment of generations, and matches or improves response quality in summarization and single-turn dialogue while being substantially simpler to implement and train."
    },
    {
        "paperId": "2f3822eb380b5e753a6d579f31dfc3ec4c4a0820",
        "url": "https://www.semanticscholar.org/paper/2f3822eb380b5e753a6d579f31dfc3ec4c4a0820",
        "title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models",
        "venue": "Neural Information Processing Systems",
        "year": 2023,
        "citationCount": 3054,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2305.10601",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.10601, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2093302161",
                "name": "Shunyu Yao"
            },
            {
                "authorId": "150978762",
                "name": "Dian Yu"
            },
            {
                "authorId": "2144551262",
                "name": "Jeffrey Zhao"
            },
            {
                "authorId": "1697494",
                "name": "Izhak Shafran"
            },
            {
                "authorId": "1799860",
                "name": "T. Griffiths"
            },
            {
                "authorId": "145144022",
                "name": "Yuan Cao"
            },
            {
                "authorId": "144958935",
                "name": "Karthik Narasimhan"
            }
        ],
        "abstract": "Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm."
    },
    {
        "paperId": "32ac52069e562d4f900afee70bdca63f53461481",
        "url": "https://www.semanticscholar.org/paper/32ac52069e562d4f900afee70bdca63f53461481",
        "title": "QLoRA: Efficient Finetuning of Quantized LLMs",
        "venue": "Neural Information Processing Systems",
        "year": 2023,
        "citationCount": 3617,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2305.14314",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.14314, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3239480",
                "name": "Tim Dettmers"
            },
            {
                "authorId": "51152502",
                "name": "Artidoro Pagnoni"
            },
            {
                "authorId": "14487640",
                "name": "Ari Holtzman"
            },
            {
                "authorId": "1982950",
                "name": "Luke Zettlemoyer"
            }
        ],
        "abstract": "We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters~(LoRA). Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3% of the performance level of ChatGPT while only requiring 24 hours of finetuning on a single GPU. QLoRA introduces a number of innovations to save memory without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is information theoretically optimal for normally distributed weights (b) double quantization to reduce the average memory footprint by quantizing the quantization constants, and (c) paged optimziers to manage memory spikes. We use QLoRA to finetune more than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across 8 instruction datasets, multiple model types (LLaMA, T5), and model scales that would be infeasible to run with regular finetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous SoTA. We provide a detailed analysis of chatbot performance based on both human and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Furthermore, we find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to ChatGPT. We release all of our models and code, including CUDA kernels for 4-bit training."
    },
    {
        "paperId": "3aaf6a2cbad5850ad81ab5c163599cb3d523436f",
        "url": "https://www.semanticscholar.org/paper/3aaf6a2cbad5850ad81ab5c163599cb3d523436f",
        "title": "Self-Refine: Iterative Refinement with Self-Feedback",
        "venue": "Neural Information Processing Systems",
        "year": 2023,
        "citationCount": 2548,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2303.17651",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2303.17651, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "21626987",
                "name": "Aman Madaan"
            },
            {
                "authorId": "1721168",
                "name": "Niket Tandon"
            },
            {
                "authorId": "1491232062",
                "name": "Prakhar Gupta"
            },
            {
                "authorId": "1474550731",
                "name": "Skyler Hallinan"
            },
            {
                "authorId": "49715441",
                "name": "Luyu Gao"
            },
            {
                "authorId": "35823986",
                "name": "Sarah Wiegreffe"
            },
            {
                "authorId": "47051926",
                "name": "Uri Alon"
            },
            {
                "authorId": "46217681",
                "name": "Nouha Dziri"
            },
            {
                "authorId": "9358910",
                "name": "Shrimai Prabhumoye"
            },
            {
                "authorId": "46286308",
                "name": "Yiming Yang"
            },
            {
                "authorId": "2129663",
                "name": "S. Welleck"
            },
            {
                "authorId": "3165738",
                "name": "Bodhisattwa Prasad Majumder"
            },
            {
                "authorId": "2152953535",
                "name": "Shashank Gupta"
            },
            {
                "authorId": "2112229",
                "name": "A. Yazdanbakhsh"
            },
            {
                "authorId": "48323507",
                "name": "Peter Clark"
            }
        ],
        "abstract": "Like humans, large language models (LLMs) do not always generate the best output on their first try. Motivated by how humans refine their written text, we introduce Self-Refine, an approach for improving initial outputs from LLMs through iterative feedback and refinement. The main idea is to generate an initial output using an LLMs; then, the same LLMs provides feedback for its output and uses it to refine itself, iteratively. Self-Refine does not require any supervised training data, additional training, or reinforcement learning, and instead uses a single LLM as the generator, refiner, and feedback provider. We evaluate Self-Refine across 7 diverse tasks, ranging from dialog response generation to mathematical reasoning, using state-of-the-art (GPT-3.5, ChatGPT, and GPT-4) LLMs. Across all evaluated tasks, outputs generated with Self-Refine are preferred by humans and automatic metrics over those generated with the same LLM using conventional one-step generation, improving by ~20% absolute on average in task performance. Our work demonstrates that even state-of-the-art LLMs like GPT-4 can be further improved at test time using our simple, standalone approach."
    },
    {
        "paperId": "53d128ea815bcc0526856eb5a9c42cc977cb36a7",
        "url": "https://www.semanticscholar.org/paper/53d128ea815bcc0526856eb5a9c42cc977cb36a7",
        "title": "Toolformer: Language Models Can Teach Themselves to Use Tools",
        "venue": "Neural Information Processing Systems",
        "year": 2023,
        "citationCount": 2586,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2302.04761",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2302.04761, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "32246932",
                "name": "Timo Schick"
            },
            {
                "authorId": "2173509991",
                "name": "Jane Dwivedi-Yu"
            },
            {
                "authorId": "145188261",
                "name": "Roberto Dess"
            },
            {
                "authorId": "48647153",
                "name": "R. Raileanu"
            },
            {
                "authorId": "3376175",
                "name": "M. Lomeli"
            },
            {
                "authorId": "1982950",
                "name": "Luke Zettlemoyer"
            },
            {
                "authorId": "2313189469",
                "name": "Nicola Cancedda"
            },
            {
                "authorId": "90745780",
                "name": "Thomas Scialom"
            }
        ],
        "abstract": "Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q\\&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities."
    },
    {
        "paperId": "546d0624adfc6e18fb87d8cc77e7705bb9ea7445",
        "url": "https://www.semanticscholar.org/paper/546d0624adfc6e18fb87d8cc77e7705bb9ea7445",
        "title": "LIMA: Less Is More for Alignment",
        "venue": "Neural Information Processing Systems",
        "year": 2023,
        "citationCount": 1114,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.11206, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2384711",
                "name": "Chunting Zhou"
            },
            {
                "authorId": "144118452",
                "name": "Pengfei Liu"
            },
            {
                "authorId": "2214843767",
                "name": "Puxin Xu"
            },
            {
                "authorId": "1900163",
                "name": "Srini Iyer"
            },
            {
                "authorId": "145478138",
                "name": "Jiao Sun"
            },
            {
                "authorId": "3375249",
                "name": "Yuning Mao"
            },
            {
                "authorId": "2378954",
                "name": "Xuezhe Ma"
            },
            {
                "authorId": "1388010852",
                "name": "Avia Efrat"
            },
            {
                "authorId": "2114104308",
                "name": "Ping Yu"
            },
            {
                "authorId": "49297123",
                "name": "L. Yu"
            },
            {
                "authorId": "2108244542",
                "name": "Susan Zhang"
            },
            {
                "authorId": "134007132",
                "name": "Gargi Ghosh"
            },
            {
                "authorId": "35084211",
                "name": "M. Lewis"
            },
            {
                "authorId": "1982950",
                "name": "Luke Zettlemoyer"
            },
            {
                "authorId": "39455775",
                "name": "Omer Levy"
            }
        ],
        "abstract": "Large language models are trained in two stages: (1) unsupervised pretraining from raw text, to learn general-purpose representations, and (2) large scale instruction tuning and reinforcement learning, to better align to end tasks and user preferences. We measure the relative importance of these two stages by training LIMA, a 65B parameter LLaMa language model fine-tuned with the standard supervised loss on only 1,000 carefully curated prompts and responses, without any reinforcement learning or human preference modeling. LIMA demonstrates remarkably strong performance, learning to follow specific response formats from only a handful of examples in the training data, including complex queries that range from planning trip itineraries to speculating about alternate history. Moreover, the model tends to generalize well to unseen tasks that did not appear in the training data. In a controlled human study, responses from LIMA are either equivalent or strictly preferred to GPT-4 in 43% of cases; this statistic is as high as 58% when compared to Bard and 65% versus DaVinci003, which was trained with human feedback. Taken together, these results strongly suggest that almost all knowledge in large language models is learned during pretraining, and only limited instruction tuning data is necessary to teach models to produce high quality output."
    },
    {
        "paperId": "8bd6a2a89503be083176f2cc26fabedb79238cbd",
        "url": "https://www.semanticscholar.org/paper/8bd6a2a89503be083176f2cc26fabedb79238cbd",
        "title": "InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning",
        "venue": "Neural Information Processing Systems",
        "year": 2023,
        "citationCount": 2851,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2305.06500",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.06500, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "47653392",
                "name": "Wenliang Dai"
            },
            {
                "authorId": "49299019",
                "name": "Junnan Li"
            },
            {
                "authorId": "2981509",
                "name": "Dongxu Li"
            },
            {
                "authorId": "73137089",
                "name": "A. Tiong"
            },
            {
                "authorId": "7818312",
                "name": "Junqi Zhao"
            },
            {
                "authorId": "2108527937",
                "name": "Weisheng Wang"
            },
            {
                "authorId": "1728712",
                "name": "Boyang Albert Li"
            },
            {
                "authorId": "2057151752",
                "name": "Pascale Fung"
            },
            {
                "authorId": "2184854289",
                "name": "Steven C. H. Hoi"
            }
        ],
        "abstract": "Large-scale pre-training and instruction tuning have been successful at creating general-purpose language models with broad competence. However, building general-purpose vision-language models is challenging due to the rich input distributions and task diversity resulting from the additional visual input. Although vision-language pretraining has been widely studied, vision-language instruction tuning remains under-explored. In this paper, we conduct a systematic and comprehensive study on vision-language instruction tuning based on the pretrained BLIP-2 models. We gather 26 publicly available datasets, covering a wide variety of tasks and capabilities, and transform them into instruction tuning format. Additionally, we introduce an instruction-aware Query Transformer, which extracts informative features tailored to the given instruction. Trained on 13 held-in datasets, InstructBLIP attains state-of-the-art zero-shot performance across all 13 held-out datasets, substantially outperforming BLIP-2 and larger Flamingo models. Our models also lead to state-of-the-art performance when finetuned on individual downstream tasks (e.g., 90.7% accuracy on ScienceQA questions with image contexts). Furthermore, we qualitatively demonstrate the advantages of InstructBLIP over concurrent multimodal models. All InstructBLIP models are open-sourced at https://github.com/salesforce/LAVIS/tree/main/projects/instructblip."
    },
    {
        "paperId": "929305892d4ddae575a0fc23227a8139f7681632",
        "url": "https://www.semanticscholar.org/paper/929305892d4ddae575a0fc23227a8139f7681632",
        "title": "Jailbroken: How Does LLM Safety Training Fail?",
        "venue": "Neural Information Processing Systems",
        "year": 2023,
        "citationCount": 1378,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2307.02483",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2307.02483, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "143797846",
                "name": "Alexander Wei"
            },
            {
                "authorId": "3033269",
                "name": "Nika Haghtalab"
            },
            {
                "authorId": "5164568",
                "name": "J. Steinhardt"
            }
        ],
        "abstract": "Large language models trained for safety and harmlessness remain susceptible to adversarial misuse, as evidenced by the prevalence of\"jailbreak\"attacks on early releases of ChatGPT that elicit undesired behavior. Going beyond recognition of the issue, we investigate why such attacks succeed and how they can be created. We hypothesize two failure modes of safety training: competing objectives and mismatched generalization. Competing objectives arise when a model's capabilities and safety goals conflict, while mismatched generalization occurs when safety training fails to generalize to a domain for which capabilities exist. We use these failure modes to guide jailbreak design and then evaluate state-of-the-art models, including OpenAI's GPT-4 and Anthropic's Claude v1.3, against both existing and newly designed attacks. We find that vulnerabilities persist despite the extensive red-teaming and safety-training efforts behind these models. Notably, new attacks utilizing our failure modes succeed on every prompt in a collection of unsafe requests from the models' red-teaming evaluation sets and outperform existing ad hoc jailbreaks. Our analysis emphasizes the need for safety-capability parity -- that safety mechanisms should be as sophisticated as the underlying model -- and argues against the idea that scaling alone can resolve these safety failure modes."
    },
    {
        "paperId": "a0a79dad89857a96f8f71b14238e5237cbfc4787",
        "url": "https://www.semanticscholar.org/paper/a0a79dad89857a96f8f71b14238e5237cbfc4787",
        "title": "Judging LLM-as-a-judge with MT-Bench and Chatbot Arena",
        "venue": "Neural Information Processing Systems",
        "year": 2023,
        "citationCount": 6447,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2306.05685, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2149970173",
                "name": "Lianmin Zheng"
            },
            {
                "authorId": "2537924",
                "name": "Wei-Lin Chiang"
            },
            {
                "authorId": "2209360681",
                "name": "Ying Sheng"
            },
            {
                "authorId": "92721493",
                "name": "Siyuan Zhuang"
            },
            {
                "authorId": "1390573666",
                "name": "Zhanghao Wu"
            },
            {
                "authorId": "2152482391",
                "name": "Yonghao Zhuang"
            },
            {
                "authorId": "143872641",
                "name": "Zi Lin"
            },
            {
                "authorId": "2141335450",
                "name": "Zhuohan Li"
            },
            {
                "authorId": "2117961435",
                "name": "Dacheng Li"
            },
            {
                "authorId": "143977260",
                "name": "E. Xing"
            },
            {
                "authorId": "145140331",
                "name": "Haotong Zhang"
            },
            {
                "authorId": "49988044",
                "name": "Joseph E. Gonzalez"
            },
            {
                "authorId": "2055174324",
                "name": "Ion Stoica"
            }
        ],
        "abstract": "Evaluating large language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences. To address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions. We examine the usage and limitations of LLM-as-a-judge, including position, verbosity, and self-enhancement biases, as well as limited reasoning ability, and propose solutions to mitigate some of them. We then verify the agreement between LLM judges and human preferences by introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80% agreement, the same level of agreement between humans. Hence, LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain. Additionally, we show our benchmark and traditional benchmarks complement each other by evaluating several variants of LLaMA and Vicuna. The MT-bench questions, 3K expert votes, and 30K conversations with human preferences are publicly available at https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge."
    },
    {
        "paperId": "a5036f31f0e629dc661f120b8c3b1f374d479ab8",
        "url": "https://www.semanticscholar.org/paper/a5036f31f0e629dc661f120b8c3b1f374d479ab8",
        "title": "Visual Instruction Tuning",
        "venue": "Neural Information Processing Systems",
        "year": 2023,
        "citationCount": 7305,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2304.08485",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2304.08485, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2143856368",
                "name": "Haotian Liu"
            },
            {
                "authorId": "2109737569",
                "name": "Chunyuan Li"
            },
            {
                "authorId": "31060482",
                "name": "Qingyang Wu"
            },
            {
                "authorId": "144756076",
                "name": "Yong Jae Lee"
            }
        ],
        "abstract": "Instruction tuning large language models (LLMs) using machine-generated instruction-following data has improved zero-shot capabilities on new tasks, but the idea is less explored in the multimodal field. In this paper, we present the first attempt to use language-only GPT-4 to generate multimodal language-image instruction-following data. By instruction tuning on such generated data, we introduce LLaVA: Large Language and Vision Assistant, an end-to-end trained large multimodal model that connects a vision encoder and LLM for general-purpose visual and language understanding.Our early experiments show that LLaVA demonstrates impressive multimodel chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and yields a 85.1% relative score compared with GPT-4 on a synthetic multimodal instruction-following dataset. When fine-tuned on Science QA, the synergy of LLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53%. We make GPT-4 generated visual instruction tuning data, our model and code base publicly available."
    },
    {
        "paperId": "b45ec1cb2ba6b2d1ac24723fa836aee06a3db97a",
        "url": "https://www.semanticscholar.org/paper/b45ec1cb2ba6b2d1ac24723fa836aee06a3db97a",
        "title": "Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation",
        "venue": "Neural Information Processing Systems",
        "year": 2023,
        "citationCount": 1375,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.01210, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2124960443",
                "name": "Jiawei Liu"
            },
            {
                "authorId": "145349987",
                "name": "Chun Xia"
            },
            {
                "authorId": "2215893835",
                "name": "Yuyao Wang"
            },
            {
                "authorId": "2145398332",
                "name": "Lingming Zhang"
            }
        ],
        "abstract": "Program synthesis has been long studied with recent approaches focused on directly using the power of Large Language Models (LLMs) to generate code. Programming benchmarks, with curated synthesis problems and test-cases, are used to measure the performance of various LLMs on code synthesis. However, these test-cases can be limited in both quantity and quality for fully assessing the functional correctness of the generated code. Such limitation in the existing benchmarks begs the following question: In the era of LLMs, is the code generated really correct? To answer this, we propose EvalPlus -- a code synthesis evaluation framework to rigorously benchmark the functional correctness of LLM-synthesized code. EvalPlus augments a given evaluation dataset with large amounts of test-cases newly produced by an automatic test input generator, powered by both LLM- and mutation-based strategies. While EvalPlus is general, we extend the test-cases of the popular HumanEval benchmark by 80x to build HumanEval+. Our extensive evaluation across 26 popular LLMs (e.g., GPT-4 and ChatGPT) demonstrates that HumanEval+ is able to catch significant amounts of previously undetected wrong code synthesized by LLMs, reducing the pass@k by up-to 19.3-28.9%. We also surprisingly found that test insufficiency can lead to mis-ranking. For example, both WizardCoder-CodeLlama and Phind-CodeLlama now outperform ChatGPT on HumanEval+, while none of them could on HumanEval. Our work not only indicates that prior popular code synthesis evaluation results do not accurately reflect the true performance of LLMs for code synthesis, but also opens up a new direction to improve such programming benchmarks through automated testing. We have open-sourced our tools, enhanced datasets as well as all LLM-generated code at https://github.com/evalplus/evalplus to facilitate and accelerate future LLM-for-code research."
    },
    {
        "paperId": "c5e9fd131cde68c218d0ea69cd617a67c7f35d42",
        "url": "https://www.semanticscholar.org/paper/c5e9fd131cde68c218d0ea69cd617a67c7f35d42",
        "title": "ProlificDreamer: High-Fidelity and Diverse Text-to-3D Generation with Variational Score Distillation",
        "venue": "Neural Information Processing Systems",
        "year": 2023,
        "citationCount": 1142,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2305.16213",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.16213, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2145912320",
                "name": "Zhengyi Wang"
            },
            {
                "authorId": "102517285",
                "name": "Cheng Lu"
            },
            {
                "authorId": null,
                "name": "Yikai Wang"
            },
            {
                "authorId": "2071898125",
                "name": "Fan Bao"
            },
            {
                "authorId": "2399563",
                "name": "Chongxuan Li"
            },
            {
                "authorId": "2093561216",
                "name": "Hang Su"
            },
            {
                "authorId": "2155220672",
                "name": "Jun Zhu"
            }
        ],
        "abstract": "Score distillation sampling (SDS) has shown great promise in text-to-3D generation by distilling pretrained large-scale text-to-image diffusion models, but suffers from over-saturation, over-smoothing, and low-diversity problems. In this work, we propose to model the 3D parameter as a random variable instead of a constant as in SDS and present variational score distillation (VSD), a principled particle-based variational framework to explain and address the aforementioned issues in text-to-3D generation. We show that SDS is a special case of VSD and leads to poor samples with both small and large CFG weights. In comparison, VSD works well with various CFG weights as ancestral sampling from diffusion models and simultaneously improves the diversity and sample quality with a common CFG weight (i.e., $7.5$). We further present various improvements in the design space for text-to-3D such as distillation time schedule and density initialization, which are orthogonal to the distillation algorithm yet not well explored. Our overall approach, dubbed ProlificDreamer, can generate high rendering resolution (i.e., $512\\times512$) and high-fidelity NeRF with rich structure and complex effects (e.g., smoke and drops). Further, initialized from NeRF, meshes fine-tuned by VSD are meticulously detailed and photo-realistic. Project page and codes: https://ml.cs.tsinghua.edu.cn/prolificdreamer/"
    },
    {
        "paperId": "d1120d67b700e4dfe8b39eb1e48fbdea4e1a0c43",
        "url": "https://www.semanticscholar.org/paper/d1120d67b700e4dfe8b39eb1e48fbdea4e1a0c43",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "venue": "Neural Information Processing Systems",
        "year": 2023,
        "citationCount": 1210,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2303.17580",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2303.17580, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1471660296",
                "name": "Yongliang Shen"
            },
            {
                "authorId": "50982078",
                "name": "Kaitao Song"
            },
            {
                "authorId": "48391466",
                "name": "Xu Tan"
            },
            {
                "authorId": "2330364101",
                "name": "Dongsheng Li"
            },
            {
                "authorId": "1776903",
                "name": "Weiming Lu"
            },
            {
                "authorId": "2056432541",
                "name": "Y. Zhuang"
            }
        ],
        "abstract": "Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence. While there are numerous AI models available for various domains and modalities, they cannot handle complicated AI tasks autonomously. Considering large language models (LLMs) have exhibited exceptional abilities in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks, with language serving as a generic interface to empower this. Based on this philosophy, we present HuggingGPT, an LLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT can tackle a wide range of sophisticated AI tasks spanning different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards the realization of artificial general intelligence."
    },
    {
        "paperId": "f22d71c7ce9720ba1f717a4f1181488200e78198",
        "url": "https://www.semanticscholar.org/paper/f22d71c7ce9720ba1f717a4f1181488200e78198",
        "title": "LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day",
        "venue": "Neural Information Processing Systems",
        "year": 2023,
        "citationCount": 1276,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2306.00890",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2306.00890, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2109737569",
                "name": "Chunyuan Li"
            },
            {
                "authorId": "2109566188",
                "name": "Cliff Wong"
            },
            {
                "authorId": "72655349",
                "name": "Sheng Zhang"
            },
            {
                "authorId": "2637252",
                "name": "N. Usuyama"
            },
            {
                "authorId": "2143856368",
                "name": "Haotian Liu"
            },
            {
                "authorId": "120157163",
                "name": "Jianwei Yang"
            },
            {
                "authorId": "40466858",
                "name": "Tristan Naumann"
            },
            {
                "authorId": "1759772",
                "name": "Hoifung Poon"
            },
            {
                "authorId": "48441311",
                "name": "Jianfeng Gao"
            }
        ],
        "abstract": "Conversational generative AI has demonstrated remarkable promise for empowering biomedical practitioners, but current investigations focus on unimodal text. Multimodal conversational AI has seen rapid progress by leveraging billions of image-text pairs from the public web, but such general-domain vision-language models still lack sophistication in understanding and conversing about biomedical images. In this paper, we propose a cost-efficient approach for training a vision-language conversational assistant that can answer open-ended research questions of biomedical images. The key idea is to leverage a large-scale, broad-coverage biomedical figure-caption dataset extracted from PubMed Central, use GPT-4 to self-instruct open-ended instruction-following data from the captions, and then fine-tune a large general-domain vision-language model using a novel curriculum learning method. Specifically, the model first learns to align biomedical vocabulary using the figure-caption pairs as is, then learns to master open-ended conversational semantics using GPT-4 generated instruction-following data, broadly mimicking how a layperson gradually acquires biomedical knowledge. This enables us to train a Large Language and Vision Assistant for BioMedicine (LLaVA-Med) in less than 15 hours (with eight A100s). LLaVA-Med exhibits excellent multimodal conversational capability and can follow open-ended instruction to assist with inquiries about a biomedical image. On three standard biomedical visual question answering datasets, LLaVA-Med outperforms previous supervised state-of-the-art on certain metrics. To facilitate biomedical multimodal research, we will release our instruction-following data and the LLaVA-Med model."
    },
    {
        "paperId": "1406bb4cb6801bc4767b661308118c888a9b09da",
        "url": "https://www.semanticscholar.org/paper/1406bb4cb6801bc4767b661308118c888a9b09da",
        "title": "MMLU-Pro: A More Robust and Challenging Multi-Task Language Understanding Benchmark",
        "venue": "Neural Information Processing Systems",
        "year": 2024,
        "citationCount": 1036,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.01574, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2238120100",
                "name": "Yubo Wang"
            },
            {
                "authorId": "2461713",
                "name": "Xueguang Ma"
            },
            {
                "authorId": "2143853895",
                "name": "Ge Zhang"
            },
            {
                "authorId": "2268493966",
                "name": "Yuansheng Ni"
            },
            {
                "authorId": "2304449194",
                "name": "Abhranil Chandra"
            },
            {
                "authorId": "2304752176",
                "name": "Shiguang Guo"
            },
            {
                "authorId": "2268493042",
                "name": "Weiming Ren"
            },
            {
                "authorId": "2304445005",
                "name": "Aaran Arulraj"
            },
            {
                "authorId": "2299486403",
                "name": "Xuan He"
            },
            {
                "authorId": "2112347577",
                "name": "Ziyan Jiang"
            },
            {
                "authorId": "2304932037",
                "name": "Tianle Li"
            },
            {
                "authorId": "2218153604",
                "name": "Max W.F. Ku"
            },
            {
                "authorId": "2304713202",
                "name": "Kai Wang"
            },
            {
                "authorId": "2304444725",
                "name": "Alex Zhuang"
            },
            {
                "authorId": "2304426528",
                "name": "Rongqi \"Richard\" Fan"
            },
            {
                "authorId": "2284988933",
                "name": "Xiang Yue"
            },
            {
                "authorId": "2253811180",
                "name": "Wenhu Chen"
            }
        ],
        "abstract": "In the age of large-scale language models, benchmarks like the Massive Multitask Language Understanding (MMLU) have been pivotal in pushing the boundaries of what AI can achieve in language comprehension and reasoning across diverse domains. However, as models continue to improve, their performance on these benchmarks has begun to plateau, making it increasingly difficult to discern differences in model capabilities. This paper introduces MMLU-Pro, an enhanced dataset designed to extend the mostly knowledge-driven MMLU benchmark by integrating more challenging, reasoning-focused questions and expanding the choice set from four to ten options. Additionally, MMLU-Pro eliminates the trivial and noisy questions in MMLU. Our experimental results show that MMLU-Pro not only raises the challenge, causing a significant drop in accuracy by 16% to 33% compared to MMLU but also demonstrates greater stability under varying prompts. With 24 different prompt styles tested, the sensitivity of model scores to prompt variations decreased from 4-5% in MMLU to just 2% in MMLU-Pro. Additionally, we found that models utilizing Chain of Thought (CoT) reasoning achieved better performance on MMLU-Pro compared to direct answering, which is in stark contrast to the findings on the original MMLU, indicating that MMLU-Pro includes more complex reasoning questions. Our assessments confirm that MMLU-Pro is a more discriminative benchmark to better track progress in the field."
    },
    {
        "paperId": "2091dcd7c80bc27ba7d1fdc0087748bb1e293e2f",
        "url": "https://www.semanticscholar.org/paper/2091dcd7c80bc27ba7d1fdc0087748bb1e293e2f",
        "title": "Depth Anything V2",
        "venue": "Neural Information Processing Systems",
        "year": 2024,
        "citationCount": 1034,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.09414, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2268796616",
                "name": "Lihe Yang"
            },
            {
                "authorId": "2261363653",
                "name": "Bingyi Kang"
            },
            {
                "authorId": "2276315322",
                "name": "Zilong Huang"
            },
            {
                "authorId": "145737114",
                "name": "Zhen Zhao"
            },
            {
                "authorId": "2261385713",
                "name": "Xiaogang Xu"
            },
            {
                "authorId": "2276580610",
                "name": "Jiashi Feng"
            },
            {
                "authorId": "2253834598",
                "name": "Hengshuang Zhao"
            }
        ],
        "abstract": "This work presents Depth Anything V2. Without pursuing fancy techniques, we aim to reveal crucial findings to pave the way towards building a powerful monocular depth estimation model. Notably, compared with V1, this version produces much finer and more robust depth predictions through three key practices: 1) replacing all labeled real images with synthetic images, 2) scaling up the capacity of our teacher model, and 3) teaching student models via the bridge of large-scale pseudo-labeled real images. Compared with the latest models built on Stable Diffusion, our models are significantly more efficient (more than 10x faster) and more accurate. We offer models of different scales (ranging from 25M to 1.3B params) to support extensive scenarios. Benefiting from their strong generalization capability, we fine-tune them with metric depth labels to obtain our metric depth models. In addition to our models, considering the limited diversity and frequent noise in current test sets, we construct a versatile evaluation benchmark with precise annotations and diverse scenes to facilitate future research."
    },
    {
        "paperId": "3723f1406b8f65471030b81fb5045067f0e29c2d",
        "url": "https://www.semanticscholar.org/paper/3723f1406b8f65471030b81fb5045067f0e29c2d",
        "title": "YOLOv10: Real-Time End-to-End Object Detection",
        "venue": "Neural Information Processing Systems",
        "year": 2024,
        "citationCount": 2975,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.14458, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2211980511",
                "name": "Ao Wang"
            },
            {
                "authorId": "2155552964",
                "name": "Hui Chen"
            },
            {
                "authorId": "2302788780",
                "name": "Lihao Liu"
            },
            {
                "authorId": "2157740966",
                "name": "Kai Chen"
            },
            {
                "authorId": "1818920",
                "name": "Zijia Lin"
            },
            {
                "authorId": "2345186205",
                "name": "Jungong Han"
            },
            {
                "authorId": "2242661989",
                "name": "Guiguang Ding"
            }
        ],
        "abstract": "Over the past years, YOLOs have emerged as the predominant paradigm in the field of real-time object detection owing to their effective balance between computational cost and detection performance. Researchers have explored the architectural designs, optimization objectives, data augmentation strategies, and others for YOLOs, achieving notable progress. However, the reliance on the non-maximum suppression (NMS) for post-processing hampers the end-to-end deployment of YOLOs and adversely impacts the inference latency. Besides, the design of various components in YOLOs lacks the comprehensive and thorough inspection, resulting in noticeable computational redundancy and limiting the model's capability. It renders the suboptimal efficiency, along with considerable potential for performance improvements. In this work, we aim to further advance the performance-efficiency boundary of YOLOs from both the post-processing and model architecture. To this end, we first present the consistent dual assignments for NMS-free training of YOLOs, which brings competitive performance and low inference latency simultaneously. Moreover, we introduce the holistic efficiency-accuracy driven model design strategy for YOLOs. We comprehensively optimize various components of YOLOs from both efficiency and accuracy perspectives, which greatly reduces the computational overhead and enhances the capability. The outcome of our effort is a new generation of YOLO series for real-time end-to-end object detection, dubbed YOLOv10. Extensive experiments show that YOLOv10 achieves state-of-the-art performance and efficiency across various model scales. For example, our YOLOv10-S is 1.8$\\times$ faster than RT-DETR-R18 under the similar AP on COCO, meanwhile enjoying 2.8$\\times$ smaller number of parameters and FLOPs. Compared with YOLOv9-C, YOLOv10-B has 46\\% less latency and 25\\% fewer parameters for the same performance."
    },
    {
        "paperId": "b24e899ec0f77eef2fc87a9b8e50516367aa1f97",
        "url": "https://www.semanticscholar.org/paper/b24e899ec0f77eef2fc87a9b8e50516367aa1f97",
        "title": "VMamba: Visual State Space Model",
        "venue": "Neural Information Processing Systems",
        "year": 2024,
        "citationCount": 1474,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2401.10166, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2279865167",
                "name": "Yue Liu"
            },
            {
                "authorId": "145649522",
                "name": "Yunjie Tian"
            },
            {
                "authorId": "2279864665",
                "name": "Yuzhong Zhao"
            },
            {
                "authorId": "2279908067",
                "name": "Hongtian Yu"
            },
            {
                "authorId": "2279860604",
                "name": "Lingxi Xie"
            },
            {
                "authorId": "2274017459",
                "name": "Yaowei Wang"
            },
            {
                "authorId": "2278220085",
                "name": "Qixiang Ye"
            },
            {
                "authorId": "2279787174",
                "name": "Yunfan Liu"
            }
        ],
        "abstract": "Designing computationally efficient network architectures remains an ongoing necessity in computer vision. In this paper, we adapt Mamba, a state-space language model, into VMamba, a vision backbone with linear time complexity. At the core of VMamba is a stack of Visual State-Space (VSS) blocks with the 2D Selective Scan (SS2D) module. By traversing along four scanning routes, SS2D bridges the gap between the ordered nature of 1D selective scan and the non-sequential structure of 2D vision data, which facilitates the collection of contextual information from various sources and perspectives. Based on the VSS blocks, we develop a family of VMamba architectures and accelerate them through a succession of architectural and implementation enhancements. Extensive experiments demonstrate VMamba's promising performance across diverse visual perception tasks, highlighting its superior input scaling efficiency compared to existing benchmark models. Source code is available at https://github.com/MzeroMiko/VMamba."
    },
    {
        "paperId": "1406bb4cb6801bc4767b661308118c888a9b09da",
        "url": "https://www.semanticscholar.org/paper/1406bb4cb6801bc4767b661308118c888a9b09da",
        "title": "MMLU-Pro: A More Robust and Challenging Multi-Task Language Understanding Benchmark",
        "venue": "Neural Information Processing Systems",
        "year": 2024,
        "citationCount": 1036,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.01574, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2238120100",
                "name": "Yubo Wang"
            },
            {
                "authorId": "2461713",
                "name": "Xueguang Ma"
            },
            {
                "authorId": "2143853895",
                "name": "Ge Zhang"
            },
            {
                "authorId": "2268493966",
                "name": "Yuansheng Ni"
            },
            {
                "authorId": "2304449194",
                "name": "Abhranil Chandra"
            },
            {
                "authorId": "2304752176",
                "name": "Shiguang Guo"
            },
            {
                "authorId": "2268493042",
                "name": "Weiming Ren"
            },
            {
                "authorId": "2304445005",
                "name": "Aaran Arulraj"
            },
            {
                "authorId": "2299486403",
                "name": "Xuan He"
            },
            {
                "authorId": "2112347577",
                "name": "Ziyan Jiang"
            },
            {
                "authorId": "2304932037",
                "name": "Tianle Li"
            },
            {
                "authorId": "2218153604",
                "name": "Max W.F. Ku"
            },
            {
                "authorId": "2304713202",
                "name": "Kai Wang"
            },
            {
                "authorId": "2304444725",
                "name": "Alex Zhuang"
            },
            {
                "authorId": "2304426528",
                "name": "Rongqi \"Richard\" Fan"
            },
            {
                "authorId": "2284988933",
                "name": "Xiang Yue"
            },
            {
                "authorId": "2253811180",
                "name": "Wenhu Chen"
            }
        ],
        "abstract": "In the age of large-scale language models, benchmarks like the Massive Multitask Language Understanding (MMLU) have been pivotal in pushing the boundaries of what AI can achieve in language comprehension and reasoning across diverse domains. However, as models continue to improve, their performance on these benchmarks has begun to plateau, making it increasingly difficult to discern differences in model capabilities. This paper introduces MMLU-Pro, an enhanced dataset designed to extend the mostly knowledge-driven MMLU benchmark by integrating more challenging, reasoning-focused questions and expanding the choice set from four to ten options. Additionally, MMLU-Pro eliminates the trivial and noisy questions in MMLU. Our experimental results show that MMLU-Pro not only raises the challenge, causing a significant drop in accuracy by 16% to 33% compared to MMLU but also demonstrates greater stability under varying prompts. With 24 different prompt styles tested, the sensitivity of model scores to prompt variations decreased from 4-5% in MMLU to just 2% in MMLU-Pro. Additionally, we found that models utilizing Chain of Thought (CoT) reasoning achieved better performance on MMLU-Pro compared to direct answering, which is in stark contrast to the findings on the original MMLU, indicating that MMLU-Pro includes more complex reasoning questions. Our assessments confirm that MMLU-Pro is a more discriminative benchmark to better track progress in the field."
    },
    {
        "paperId": "2091dcd7c80bc27ba7d1fdc0087748bb1e293e2f",
        "url": "https://www.semanticscholar.org/paper/2091dcd7c80bc27ba7d1fdc0087748bb1e293e2f",
        "title": "Depth Anything V2",
        "venue": "Neural Information Processing Systems",
        "year": 2024,
        "citationCount": 1034,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.09414, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2268796616",
                "name": "Lihe Yang"
            },
            {
                "authorId": "2261363653",
                "name": "Bingyi Kang"
            },
            {
                "authorId": "2276315322",
                "name": "Zilong Huang"
            },
            {
                "authorId": "145737114",
                "name": "Zhen Zhao"
            },
            {
                "authorId": "2261385713",
                "name": "Xiaogang Xu"
            },
            {
                "authorId": "2276580610",
                "name": "Jiashi Feng"
            },
            {
                "authorId": "2253834598",
                "name": "Hengshuang Zhao"
            }
        ],
        "abstract": "This work presents Depth Anything V2. Without pursuing fancy techniques, we aim to reveal crucial findings to pave the way towards building a powerful monocular depth estimation model. Notably, compared with V1, this version produces much finer and more robust depth predictions through three key practices: 1) replacing all labeled real images with synthetic images, 2) scaling up the capacity of our teacher model, and 3) teaching student models via the bridge of large-scale pseudo-labeled real images. Compared with the latest models built on Stable Diffusion, our models are significantly more efficient (more than 10x faster) and more accurate. We offer models of different scales (ranging from 25M to 1.3B params) to support extensive scenarios. Benefiting from their strong generalization capability, we fine-tune them with metric depth labels to obtain our metric depth models. In addition to our models, considering the limited diversity and frequent noise in current test sets, we construct a versatile evaluation benchmark with precise annotations and diverse scenes to facilitate future research."
    },
    {
        "paperId": "3723f1406b8f65471030b81fb5045067f0e29c2d",
        "url": "https://www.semanticscholar.org/paper/3723f1406b8f65471030b81fb5045067f0e29c2d",
        "title": "YOLOv10: Real-Time End-to-End Object Detection",
        "venue": "Neural Information Processing Systems",
        "year": 2024,
        "citationCount": 2975,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.14458, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2211980511",
                "name": "Ao Wang"
            },
            {
                "authorId": "2155552964",
                "name": "Hui Chen"
            },
            {
                "authorId": "2302788780",
                "name": "Lihao Liu"
            },
            {
                "authorId": "2157740966",
                "name": "Kai Chen"
            },
            {
                "authorId": "1818920",
                "name": "Zijia Lin"
            },
            {
                "authorId": "2345186205",
                "name": "Jungong Han"
            },
            {
                "authorId": "2242661989",
                "name": "Guiguang Ding"
            }
        ],
        "abstract": "Over the past years, YOLOs have emerged as the predominant paradigm in the field of real-time object detection owing to their effective balance between computational cost and detection performance. Researchers have explored the architectural designs, optimization objectives, data augmentation strategies, and others for YOLOs, achieving notable progress. However, the reliance on the non-maximum suppression (NMS) for post-processing hampers the end-to-end deployment of YOLOs and adversely impacts the inference latency. Besides, the design of various components in YOLOs lacks the comprehensive and thorough inspection, resulting in noticeable computational redundancy and limiting the model's capability. It renders the suboptimal efficiency, along with considerable potential for performance improvements. In this work, we aim to further advance the performance-efficiency boundary of YOLOs from both the post-processing and model architecture. To this end, we first present the consistent dual assignments for NMS-free training of YOLOs, which brings competitive performance and low inference latency simultaneously. Moreover, we introduce the holistic efficiency-accuracy driven model design strategy for YOLOs. We comprehensively optimize various components of YOLOs from both efficiency and accuracy perspectives, which greatly reduces the computational overhead and enhances the capability. The outcome of our effort is a new generation of YOLO series for real-time end-to-end object detection, dubbed YOLOv10. Extensive experiments show that YOLOv10 achieves state-of-the-art performance and efficiency across various model scales. For example, our YOLOv10-S is 1.8$\\times$ faster than RT-DETR-R18 under the similar AP on COCO, meanwhile enjoying 2.8$\\times$ smaller number of parameters and FLOPs. Compared with YOLOv9-C, YOLOv10-B has 46\\% less latency and 25\\% fewer parameters for the same performance."
    },
    {
        "paperId": "b24e899ec0f77eef2fc87a9b8e50516367aa1f97",
        "url": "https://www.semanticscholar.org/paper/b24e899ec0f77eef2fc87a9b8e50516367aa1f97",
        "title": "VMamba: Visual State Space Model",
        "venue": "Neural Information Processing Systems",
        "year": 2024,
        "citationCount": 1474,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2401.10166, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2279865167",
                "name": "Yue Liu"
            },
            {
                "authorId": "145649522",
                "name": "Yunjie Tian"
            },
            {
                "authorId": "2279864665",
                "name": "Yuzhong Zhao"
            },
            {
                "authorId": "2279908067",
                "name": "Hongtian Yu"
            },
            {
                "authorId": "2279860604",
                "name": "Lingxi Xie"
            },
            {
                "authorId": "2274017459",
                "name": "Yaowei Wang"
            },
            {
                "authorId": "2278220085",
                "name": "Qixiang Ye"
            },
            {
                "authorId": "2279787174",
                "name": "Yunfan Liu"
            }
        ],
        "abstract": "Designing computationally efficient network architectures remains an ongoing necessity in computer vision. In this paper, we adapt Mamba, a state-space language model, into VMamba, a vision backbone with linear time complexity. At the core of VMamba is a stack of Visual State-Space (VSS) blocks with the 2D Selective Scan (SS2D) module. By traversing along four scanning routes, SS2D bridges the gap between the ordered nature of 1D selective scan and the non-sequential structure of 2D vision data, which facilitates the collection of contextual information from various sources and perspectives. Based on the VSS blocks, we develop a family of VMamba architectures and accelerate them through a succession of architectural and implementation enhancements. Extensive experiments demonstrate VMamba's promising performance across diverse visual perception tasks, highlighting its superior input scaling efficiency compared to existing benchmark models. Source code is available at https://github.com/MzeroMiko/VMamba."
    },
    {
        "paperId": "0ecb33ced5b0976accdf13817151f80568b6fdcb",
        "url": "https://www.semanticscholar.org/paper/0ecb33ced5b0976accdf13817151f80568b6fdcb",
        "title": "Coarse-to-Fine n-Best Parsing and MaxEnt Discriminative Reranking",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2005,
        "citationCount": 1281,
        "openAccessPdf": {
            "url": "http://dl.acm.org/ft_gateway.cfm?id=1219862&type=pdf",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/P05-1022, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1749837",
                "name": "Eugene Charniak"
            },
            {
                "authorId": "145177220",
                "name": "Mark Johnson"
            }
        ],
        "abstract": "Discriminative reranking is one method for constructing high-performance statistical parsers (Collins, 2000). A discriminative reranker requires a source of candidate parses for each sentence. This paper describes a simple yet novel method for constructing sets of 50-best parses based on a coarse-to-fine generative parser (Charniak, 2000). This method generates 50-best lists that are of substantially higher quality than previously obtainable. We used these parses as the input to a MaxEnt reranker (Johnson et al., 1999; Riezler et al., 2002) that selects the best parse from the set of parses for each sentence, obtaining an f-score of 91.0% on sentences of length 100 or less."
    },
    {
        "paperId": "4f410ab5c8b12b34b38421241366ee456bbebab9",
        "url": "https://www.semanticscholar.org/paper/4f410ab5c8b12b34b38421241366ee456bbebab9",
        "title": "Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2005,
        "citationCount": 3441,
        "openAccessPdf": {
            "url": "http://dl.acm.org/ft_gateway.cfm?id=1219885&type=pdf",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/P05-1045, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2784228",
                "name": "J. Finkel"
            },
            {
                "authorId": "3050250",
                "name": "Trond Grenager"
            },
            {
                "authorId": "144783904",
                "name": "Christopher D. Manning"
            }
        ],
        "abstract": "Most current statistical natural language processing models use only local features so as to permit dynamic programming in inference, but this makes them unable to fully account for the long distance structure that is prevalent in language use. We show how to solve this dilemma with Gibbs sampling, a simple Monte Carlo method used to perform approximate inference in factored probabilistic models. By using simulated annealing in place of Viterbi decoding in sequence models such as HMMs, CMMs, and CRFs, it is possible to incorporate non-local structure while preserving tractable inference. We use this technique to augment an existing CRF-based information extraction system with long-distance dependency models, enforcing label consistency and extraction template consistency constraints. This technique results in an error reduction of up to 9% over state-of-the-art systems on two established information extraction tasks."
    },
    {
        "paperId": "6af58c061f2e4f130c3b795c21ff0c7e3903278f",
        "url": "https://www.semanticscholar.org/paper/6af58c061f2e4f130c3b795c21ff0c7e3903278f",
        "title": "Seeing Stars: Exploiting Class Relationships for Sentiment Categorization with Respect to Rating Scales",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2005,
        "citationCount": 3129,
        "openAccessPdf": {
            "url": "https://dl.acm.org/doi/pdf/10.3115/1219840.1219855",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/cs/0506075, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "144865353",
                "name": "B. Pang"
            },
            {
                "authorId": "145810617",
                "name": "Lillian Lee"
            }
        ],
        "abstract": "We address the rating-inference problem, wherein rather than simply decide whether a review is \"thumbs up\" or \"thumbs down\", as in previous sentiment analysis work, one must determine an author's evaluation with respect to a multi-point scale (e.g., one to five \"stars\"). This task represents an interesting twist on standard multi-class text categorization because there are several different degrees of similarity between class labels; for example, \"three stars\" is intuitively closer to \"four stars\" than to \"one star\".We first evaluate human performance at the task. Then, we apply a meta-algorithm, based on a metric labeling formulation of the problem, that alters a given n-ary classifier's output in an explicit attempt to ensure that similar items receive similar labels. We show that the meta-algorithm can provide significant improvements over both multi-class and regression versions of SVMs when we employ a novel similarity measure appropriate to the problem."
    },
    {
        "paperId": "ad3d2f463916784d0c14a19936c1544309a0a440",
        "url": "https://www.semanticscholar.org/paper/ad3d2f463916784d0c14a19936c1544309a0a440",
        "title": "A Hierarchical Phrase-Based Model for Statistical Machine Translation",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2005,
        "citationCount": 1307,
        "openAccessPdf": {
            "url": "https://dl.acm.org/doi/pdf/10.3115/1219840.1219873",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/P05-1033, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "145287425",
                "name": "David Chiang"
            }
        ],
        "abstract": "We present a statistical phrase-based translation model that uses hierarchical phrases---phrases that contain subphrases. The model is formally a synchronous context-free grammar but is learned from a bitext without any syntactic information. Thus it can be seen as a shift to the formal machinery of syntax-based translation systems without any linguistic commitment. In our experiments using BLEU as a metric, the hierarchical phrase-based model achieves a relative improvement of 7.5% over Pharaoh, a state-of-the-art phrase-based system."
    },
    {
        "paperId": "6ed9417eaa7ee16f0563599829a061421a3e0563",
        "url": "https://www.semanticscholar.org/paper/6ed9417eaa7ee16f0563599829a061421a3e0563",
        "title": "NLTK: The Natural Language Toolkit",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2006,
        "citationCount": 5045,
        "openAccessPdf": {
            "url": "https://dl.acm.org/doi/pdf/10.3115/1225403.1225421",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/P06-4018, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "21308992",
                "name": "Steven Bird"
            }
        ],
        "abstract": "The Natural Language Toolkit is a suite of program modules, data sets and tutorials supporting research and teaching in computational linguistics and natural language processing. NLTK is written in Python and distributed under the GPL open source license. Over the past year the toolkit has been rewritten, simplifying many linguistic data structures and taking advantage of recent enhancements in the Python language. This paper reports on the simpli-ed toolkit and explains how it is used in teaching NLP."
    },
    {
        "paperId": "4ee2eab4c298c1824a9fb8799ad8eed21be38d21",
        "url": "https://www.semanticscholar.org/paper/4ee2eab4c298c1824a9fb8799ad8eed21be38d21",
        "title": "Moses: Open Source Toolkit for Statistical Machine Translation",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2007,
        "citationCount": 6507,
        "openAccessPdf": {
            "url": "https://dl.acm.org/doi/pdf/10.5555/1557769.1557821",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/P07-2045, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1755162",
                "name": "Philipp Koehn"
            },
            {
                "authorId": "152378023",
                "name": "Hieu T. Hoang"
            },
            {
                "authorId": "2539211",
                "name": "Alexandra Birch"
            },
            {
                "authorId": "1763608",
                "name": "Chris Callison-Burch"
            },
            {
                "authorId": "102811815",
                "name": "Marcello Federico"
            },
            {
                "authorId": "1895952",
                "name": "N. Bertoldi"
            },
            {
                "authorId": "46898156",
                "name": "Brooke Cowan"
            },
            {
                "authorId": "2529583",
                "name": "Wade Shen"
            },
            {
                "authorId": "2055137469",
                "name": "C. Moran"
            },
            {
                "authorId": "1983801",
                "name": "Richard Zens"
            },
            {
                "authorId": "1745899",
                "name": "Chris Dyer"
            },
            {
                "authorId": "143832874",
                "name": "Ondrej Bojar"
            },
            {
                "authorId": "2057195055",
                "name": "Alexandra Constantin"
            },
            {
                "authorId": "2082901914",
                "name": "Evan Herbst"
            }
        ],
        "abstract": "We describe an open-source toolkit for statistical machine translation whose novel contributions are (a) support for linguistically motivated factors, (b) confusion network decoding, and (c) efficient data formats for translation models and language models. In addition to the SMT decoder, the toolkit also includes a wide variety of tools for training, tuning and applying the system to many translation tasks."
    },
    {
        "paperId": "9f62067945d991cd78a62cf647de17f01d1b54d3",
        "url": "https://www.semanticscholar.org/paper/9f62067945d991cd78a62cf647de17f01d1b54d3",
        "title": "Frustratingly Easy Domain Adaptation",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2007,
        "citationCount": 1830,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/0907.1815, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1722360",
                "name": "Hal Daum"
            }
        ],
        "abstract": "We describe an approach to domain adaptation that is appropriate exactly in the case when one has enough target data to do slightly better than just using only source data. Our approach is incredibly simple, easy to implement as a preprocessing step (10 lines of Perl!) and outperforms stateof-the-art approaches on a range of datasets. Moreover, it is trivially extended to a multidomain adaptation problem, where one has data from a variety of different domains."
    },
    {
        "paperId": "d895647b4a80861703851ef55930a2627fe19492",
        "url": "https://www.semanticscholar.org/paper/d895647b4a80861703851ef55930a2627fe19492",
        "title": "Biographies, Bollywood, Boom-boxes and Blenders: Domain Adaptation for Sentiment Classification",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2007,
        "citationCount": 2408,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://aclanthology.org/P07-1056, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2116927",
                "name": "John Blitzer"
            },
            {
                "authorId": "1782853",
                "name": "Mark Dredze"
            },
            {
                "authorId": "145366908",
                "name": "Fernando C Pereira"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "d84b57362e2010f6f65357267df7e0157af30684",
        "url": "https://www.semanticscholar.org/paper/d84b57362e2010f6f65357267df7e0157af30684",
        "title": "Distant supervision for relation extraction without labeled data",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2009,
        "citationCount": 3156,
        "openAccessPdf": {
            "url": "https://dl.acm.org/doi/pdf/10.5555/1690219.1690287",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/P09-1113, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "36181176",
                "name": "Mike D. Mintz"
            },
            {
                "authorId": "87299088",
                "name": "Steven Bills"
            },
            {
                "authorId": "144621026",
                "name": "R. Snow"
            },
            {
                "authorId": "1746807",
                "name": "Dan Jurafsky"
            }
        ],
        "abstract": "Modern models of relation extraction for tasks like ACE are based on supervised learning of relations from small hand-labeled corpora. We investigate an alternative paradigm that does not require labeled corpora, avoiding the domain dependence of ACE-style algorithms, and allowing the use of corpora of any size. Our experiments use Freebase, a large semantic database of several thousand relations, to provide distant supervision. For each pair of entities that appears in some Freebase relation, we find all sentences containing those entities in a large unlabeled corpus and extract textual features to train a relation classifier. Our algorithm combines the advantages of supervised IE (combining 400,000 noisy pattern features in a probabilistic classifier) and unsupervised IE (extracting large numbers of relations from large corpora of any domain). Our model is able to extract 10,000 instances of 102 relations at a precision of 67.6%. We also analyze feature performance, showing that syntactic parse features are particularly helpful for relations that are ambiguous or lexically distant in their expression."
    },
    {
        "paperId": "7858c5b5f3a097090d24a6eed5439d0622a1afb2",
        "url": "https://www.semanticscholar.org/paper/7858c5b5f3a097090d24a6eed5439d0622a1afb2",
        "title": "Part-of-Speech Tagging for Twitter: Annotation, Features, and Experiments",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2010,
        "citationCount": 1117,
        "openAccessPdf": {
            "url": "https://figshare.com/articles/journal_contribution/Part-of-Speech_Tagging_for_Twitter_Annotation_Features_and_Experiments/6473609/1/files/11903213.pdf",
            "status": "GREEN",
            "license": "CCBYNCSA",
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/P11-2008, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1700980",
                "name": "Kevin Gimpel"
            },
            {
                "authorId": "145254207",
                "name": "Nathan Schneider"
            },
            {
                "authorId": "153724741",
                "name": "Brendan T. O'Connor"
            },
            {
                "authorId": "143790066",
                "name": "Dipanjan Das"
            },
            {
                "authorId": "2064370825",
                "name": "Daniel Mills"
            },
            {
                "authorId": "144154709",
                "name": "Jacob Eisenstein"
            },
            {
                "authorId": "2618874",
                "name": "Michael Heilman"
            },
            {
                "authorId": "1755465",
                "name": "Dani Yogatama"
            },
            {
                "authorId": "144683841",
                "name": "Jeffrey Flanigan"
            },
            {
                "authorId": "144365875",
                "name": "Noah A. Smith"
            }
        ],
        "abstract": "We address the problem of part-of-speech tagging for English data from the popular micro-blogging service Twitter. We develop a tagset, annotate data, develop features, and report tagging results nearing 90% accuracy. The data and tools have been made available to the research community with the goal of enabling richer text analysis of Twitter and related social media data sets."
    },
    {
        "paperId": "8492070dc4031ed825e95e4803781752bb5e909f",
        "url": "https://www.semanticscholar.org/paper/8492070dc4031ed825e95e4803781752bb5e909f",
        "title": "Word Representations: A Simple and General Method for Semi-Supervised Learning",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2010,
        "citationCount": 2334,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://aclanthology.org/P10-1040, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "153160559",
                "name": "Joseph P. Turian"
            },
            {
                "authorId": "2335225",
                "name": "Lev-Arie Ratinov"
            },
            {
                "authorId": "1751762",
                "name": "Yoshua Bengio"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "1c61f9ef06fe74505775a833ff849185757199e7",
        "url": "https://www.semanticscholar.org/paper/1c61f9ef06fe74505775a833ff849185757199e7",
        "title": "Learning Word Vectors for Sentiment Analysis",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2011,
        "citationCount": 5728,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://aclanthology.org/P11-1015, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "34961461",
                "name": "Andrew L. Maas"
            },
            {
                "authorId": "119816047",
                "name": "Raymond E. Daly"
            },
            {
                "authorId": "2061523260",
                "name": "Peter T. Pham"
            },
            {
                "authorId": "2110408720",
                "name": "Dan Huang"
            },
            {
                "authorId": "34699434",
                "name": "A. Ng"
            },
            {
                "authorId": "144922861",
                "name": "Christopher Potts"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "72729882f8fa3d9084eaece513f6bf9630be5901",
        "url": "https://www.semanticscholar.org/paper/72729882f8fa3d9084eaece513f6bf9630be5901",
        "title": "Collecting Highly Parallel Data for Paraphrase Evaluation",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2011,
        "citationCount": 1398,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://aclanthology.org/P11-1020, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "153642390",
                "name": "David L. Chen"
            },
            {
                "authorId": "83415753",
                "name": "W. Dolan"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "7ba9dcbd2e9163f2684b098786bbc3abb4784905",
        "url": "https://www.semanticscholar.org/paper/7ba9dcbd2e9163f2684b098786bbc3abb4784905",
        "title": "Target-dependent Twitter Sentiment Classification",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2011,
        "citationCount": 1098,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://aclanthology.org/P11-1016, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2112532004",
                "name": "Long Jiang"
            },
            {
                "authorId": "2482533",
                "name": "Mo Yu"
            },
            {
                "authorId": "143849609",
                "name": "M. Zhou"
            },
            {
                "authorId": "2110998009",
                "name": "Xiaohua Liu"
            },
            {
                "authorId": "145382463",
                "name": "T. Zhao"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "b8460e242311b7527f1d52438668ab334a04ff31",
        "url": "https://www.semanticscholar.org/paper/b8460e242311b7527f1d52438668ab334a04ff31",
        "title": "Finding Deceptive Opinion Spam by Any Stretch of the Imagination",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2011,
        "citationCount": 1451,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1107.4557, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "40511414",
                "name": "Myle Ott"
            },
            {
                "authorId": "1699545",
                "name": "Yejin Choi"
            },
            {
                "authorId": "1748501",
                "name": "Claire Cardie"
            },
            {
                "authorId": "1697703",
                "name": "Jeffrey T. Hancock"
            }
        ],
        "abstract": "Consumers increasingly rate, review and research products online (Jansen, 2010; Litvin et al., 2008). Consequently, websites containing consumer reviews are becoming targets of opinion spam. While recent work has focused primarily on manually identifiable instances of opinion spam, in this work we study deceptive opinion spam---fictitious opinions that have been deliberately written to sound authentic. Integrating work from psychology and computational linguistics, we develop and compare three approaches to detecting deceptive opinion spam, and ultimately develop a classifier that is nearly 90% accurate on our gold-standard opinion spam dataset. Based on feature analysis of our learned models, we additionally make several theoretical contributions, including revealing a relationship between deceptive opinions and imaginative writing."
    },
    {
        "paperId": "d48edf9e81653f4c3da716b037b0b50d54c5b034",
        "url": "https://www.semanticscholar.org/paper/d48edf9e81653f4c3da716b037b0b50d54c5b034",
        "title": "Knowledge-Based Weak Supervision for Information Extraction of Overlapping Relations",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2011,
        "citationCount": 1049,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://aclanthology.org/P11-1055, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2566295",
                "name": "Raphael Hoffmann"
            },
            {
                "authorId": "1799338",
                "name": "Congle Zhang"
            },
            {
                "authorId": "145787377",
                "name": "Xiao Ling"
            },
            {
                "authorId": "1982950",
                "name": "Luke Zettlemoyer"
            },
            {
                "authorId": "1780531",
                "name": "Daniel S. Weld"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "2b669398c4cf2ebe04375c8b1beae20f4ac802fa",
        "url": "https://www.semanticscholar.org/paper/2b669398c4cf2ebe04375c8b1beae20f4ac802fa",
        "title": "Improving Word Representations via Global Context and Multiple Word Prototypes",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2012,
        "citationCount": 1271,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://aclanthology.org/P12-1092, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "40150953",
                "name": "E. Huang"
            },
            {
                "authorId": "2166511",
                "name": "R. Socher"
            },
            {
                "authorId": "144783904",
                "name": "Christopher D. Manning"
            },
            {
                "authorId": "34699434",
                "name": "A. Ng"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "5e9fa46f231c59e6573f9a116f77f53703347659",
        "url": "https://www.semanticscholar.org/paper/5e9fa46f231c59e6573f9a116f77f53703347659",
        "title": "Baselines and Bigrams: Simple, Good Sentiment and Topic Classification",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2012,
        "citationCount": 1336,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://aclanthology.org/P12-2018, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "8729431",
                "name": "Sida I. Wang"
            },
            {
                "authorId": "144783904",
                "name": "Christopher D. Manning"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "0183b3e9d84c15c7048e6c2149ed86257ccdc6cb",
        "url": "https://www.semanticscholar.org/paper/0183b3e9d84c15c7048e6c2149ed86257ccdc6cb",
        "title": "Dependency-Based Word Embeddings",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2014,
        "citationCount": 1174,
        "openAccessPdf": {
            "url": "https://aclanthology.org/P14-2050.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/P14-2050, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "39455775",
                "name": "Omer Levy"
            },
            {
                "authorId": "2089067",
                "name": "Yoav Goldberg"
            }
        ],
        "abstract": "While continuous word embeddings are gaining popularity, current models are based solely on linear contexts. In this work, we generalize the skip-gram model with negative sampling introduced by Mikolov et al. to include arbitrary contexts. In particular, we perform experiments with dependency-based contexts, and show that they produce markedly different embeddings. The dependencybased embeddings are less topical and exhibit more functional similarity than the original skip-gram embeddings."
    },
    {
        "paperId": "1938624bb9b0f999536dcc8d8f519810bb4e1b3b",
        "url": "https://www.semanticscholar.org/paper/1938624bb9b0f999536dcc8d8f519810bb4e1b3b",
        "title": "On Using Very Large Target Vocabulary for Neural Machine Translation",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2014,
        "citationCount": 1026,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/P15-1001.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1412.2007, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "152857609",
                "name": "Sbastien Jean"
            },
            {
                "authorId": "1979489",
                "name": "Kyunghyun Cho"
            },
            {
                "authorId": "1710604",
                "name": "R. Memisevic"
            },
            {
                "authorId": "1751762",
                "name": "Yoshua Bengio"
            }
        ],
        "abstract": "Neural machine translation, a recently proposed approach to machine translation based purely on neural networks, has shown promising results compared to the existing approaches such as phrase-based statistical machine translation. Despite its recent success, neural machine translation has its limitation in handling a larger vocabulary, as training complexity as well as decoding complexity increase proportionally to the number of target words. In this paper, we propose a method based on importance sampling that allows us to use a very large target vocabulary without increasing training complexity. We show that decoding can be efficiently done even with the model having a very large target vocabulary by selecting only a small subset of the whole target vocabulary. The models trained by the proposed approach are empirically found to outperform the baseline models with a small vocabulary as well as the LSTM-based neural machine translation models. Furthermore, when we use the ensemble of a few models with very large target vocabularies, we achieve the state-of-the-art translation performance (measured by BLEU) on the English!German translation and almost as high performance as state-of-the-art English!French translation system."
    },
    {
        "paperId": "27725a2d2a8cee9bf9fffc6c2167017103aba0fa",
        "url": "https://www.semanticscholar.org/paper/27725a2d2a8cee9bf9fffc6c2167017103aba0fa",
        "title": "A Convolutional Neural Network for Modelling Sentences",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2014,
        "citationCount": 3623,
        "openAccessPdf": {
            "url": "https://doi.org/10.3115/v1/p14-1062",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1404.2188, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2583391",
                "name": "Nal Kalchbrenner"
            },
            {
                "authorId": "1864353",
                "name": "Edward Grefenstette"
            },
            {
                "authorId": "1685771",
                "name": "Phil Blunsom"
            }
        ],
        "abstract": "The ability to accurately represent sentences is central to language understanding. We describe a convolutional architecture dubbed the Dynamic Convolutional Neural Network (DCNN) that we adopt for the semantic modelling of sentences. The network uses Dynamic k-Max Pooling, a global pooling operation over linear sequences. The network handles input sentences of varying length and induces a feature graph over the sentence that is capable of explicitly capturing short and long-range relations. The network does not rely on a parse tree and is easily applicable to any language. We test the DCNN in four experiments: small scale binary and multi-class sentiment prediction, six-way question classification and Twitter sentiment prediction by distant supervision. The network achieves excellent performance in the first three tasks and a greater than 25% error reduction in the last task with respect to the strongest baseline."
    },
    {
        "paperId": "2f5102ec3f70d0dea98c957cc2cab4d15d83a2da",
        "url": "https://www.semanticscholar.org/paper/2f5102ec3f70d0dea98c957cc2cab4d15d83a2da",
        "title": "The Stanford CoreNLP Natural Language Processing Toolkit",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2014,
        "citationCount": 7526,
        "openAccessPdf": {
            "url": "https://aclanthology.org/P14-5010.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/P14-5010, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "144783904",
                "name": "Christopher D. Manning"
            },
            {
                "authorId": "1760868",
                "name": "M. Surdeanu"
            },
            {
                "authorId": "144661918",
                "name": "John Bauer"
            },
            {
                "authorId": "2784228",
                "name": "J. Finkel"
            },
            {
                "authorId": "2105138",
                "name": "Steven Bethard"
            },
            {
                "authorId": "2240597",
                "name": "David McClosky"
            }
        ],
        "abstract": "We describe the design and use of the Stanford CoreNLP toolkit, an extensible pipeline that provides core natural language analysis. This toolkit is quite widely used, both in the research NLP community and also among commercial and government users of open source NLP technology. We suggest that this follows from a simple, approachable design, straightforward interfaces, the inclusion of robust and good quality analysis components, and not requiring use of a large amount of associated baggage."
    },
    {
        "paperId": "3cfbb77e5a0e24772cfdb2eb3d4f35dead54b118",
        "url": "https://www.semanticscholar.org/paper/3cfbb77e5a0e24772cfdb2eb3d4f35dead54b118",
        "title": "Dont count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2014,
        "citationCount": 1499,
        "openAccessPdf": {
            "url": "https://aclanthology.org/P14-1023.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/P14-1023, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "145283199",
                "name": "Marco Baroni"
            },
            {
                "authorId": "145505048",
                "name": "Georgiana Dinu"
            },
            {
                "authorId": "2067996",
                "name": "Germn Kruszewski"
            }
        ],
        "abstract": "Context-predicting models (more commonly known as embeddings or neural language models) are the new kids on the distributional semantics block. Despite the buzz surrounding these models, the literature is still lacking a systematic comparison of the predictive models with classic, count-vector-based distributional semantic approaches. In this paper, we perform such an extensive evaluation, on a wide range of lexical semantics tasks and across many parameter settings. The results, to our own surprise, show that the buzz is fully justified, as the context-predicting models obtain a thorough and resounding victory against their count-based counterparts."
    },
    {
        "paperId": "ab001d508fbb4160e53686e05b800ab4baeb9728",
        "url": "https://www.semanticscholar.org/paper/ab001d508fbb4160e53686e05b800ab4baeb9728",
        "title": "Learning Sentiment-Specific Word Embedding for Twitter Sentiment Classification",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2014,
        "citationCount": 1219,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/P14-1146, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "39483833",
                "name": "Duyu Tang"
            },
            {
                "authorId": "49807919",
                "name": "Furu Wei"
            },
            {
                "authorId": "144610884",
                "name": "Nan Yang"
            },
            {
                "authorId": "143849609",
                "name": "M. Zhou"
            },
            {
                "authorId": "40282288",
                "name": "Ting Liu"
            },
            {
                "authorId": "152277111",
                "name": "Bing Qin"
            }
        ],
        "abstract": "We present a method that learns word embedding for Twitter sentiment classification in this paper. Most existing algorithms for learning continuous word representations typically only model the syntactic context of words but ignore the sentiment of text. This is problematic for sentiment analysis as they usually map words with similar syntactic context but opposite sentiment polarity, such as good and bad, to neighboring word vectors. We address this issue by learning sentimentspecific word embedding (SSWE), which encodes sentiment information in the continuous representation of words. Specifically, we develop three neural networks to effectively incorporate the supervision from sentiment polarity of text (e.g. sentences or tweets) in their loss functions. To obtain large scale training corpora, we learn the sentiment-specific word embedding from massive distant-supervised tweets collected by positive and negative emoticons. Experiments on applying SSWE to a benchmark Twitter sentiment classification dataset in SemEval 2013 show that (1) the SSWE feature performs comparably with hand-crafted features in the top-performed system; (2) the performance is further improved by concatenating SSWE with existing feature set."
    },
    {
        "paperId": "1518039b5001f1836565215eb047526b3ac7f462",
        "url": "https://www.semanticscholar.org/paper/1518039b5001f1836565215eb047526b3ac7f462",
        "title": "Neural Machine Translation of Rare Words with Subword Units",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2015,
        "citationCount": 8328,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/P16-1162.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1508.07909, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2082372",
                "name": "Rico Sennrich"
            },
            {
                "authorId": "2259100",
                "name": "B. Haddow"
            },
            {
                "authorId": "2539211",
                "name": "Alexandra Birch"
            }
        ],
        "abstract": "Neural machine translation (NMT) models typically operate with a fixed vocabulary, but translation is an open-vocabulary problem. Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary. In this paper, we introduce a simpler and more effective approach, making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units. This is based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations). We discuss the suitability of different word segmentation techniques, including simple character n-gram models and a segmentation based on the byte pair encoding compression algorithm, and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English-German and English-Russian by 1.1 and 1.3 BLEU, respectively."
    },
    {
        "paperId": "18bd7cd489874ed9976b4f87a6a558f9533316e0",
        "url": "https://www.semanticscholar.org/paper/18bd7cd489874ed9976b4f87a6a558f9533316e0",
        "title": "Knowledge Graph Embedding via Dynamic Mapping Matrix",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2015,
        "citationCount": 1639,
        "openAccessPdf": {
            "url": "https://aclanthology.org/P15-1067.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/P15-1067, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2055422009",
                "name": "Guoliang Ji"
            },
            {
                "authorId": "1954845",
                "name": "Shizhu He"
            },
            {
                "authorId": "8540973",
                "name": "Liheng Xu"
            },
            {
                "authorId": "2200096",
                "name": "Kang Liu"
            },
            {
                "authorId": "1390572170",
                "name": "Jun Zhao"
            }
        ],
        "abstract": "Knowledge graphs are useful resources for numerous AI applications, but they are far from completeness. Previous work such as TransE, TransH and TransR/CTransR regard a relation as translation from head entity to tail entity and the CTransR achieves state-of-the-art performance. In this paper, we propose a more fine-grained model named TransD, which is an improvement of TransR/CTransR. In TransD, we use two vectors to represent a named symbol object (entity and relation). The first one represents the meaning of a(n) entity (relation), the other one is used to construct mapping matrix dynamically. Compared with TransR/CTransR, TransD not only considers the diversity of relations, but also entities. TransD has less parameters and has no matrix-vector multiplication operations, which makes it can be applied on large scale graphs. In Experiments, we evaluate our model on two typical tasks including triplets classification and link prediction. Evaluation results show that our approach outperforms state-of-the-art methods."
    },
    {
        "paperId": "32de44f01a96d4473d21099d15e25bc2b9f08e2f",
        "url": "https://www.semanticscholar.org/paper/32de44f01a96d4473d21099d15e25bc2b9f08e2f",
        "title": "Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2015,
        "citationCount": 3205,
        "openAccessPdf": {
            "url": "https://aclanthology.org/P15-1150.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1503.00075, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "8421815",
                "name": "Kai Sheng Tai"
            },
            {
                "authorId": "2166511",
                "name": "R. Socher"
            },
            {
                "authorId": "144783904",
                "name": "Christopher D. Manning"
            }
        ],
        "abstract": "Because of their superior ability to preserve sequence information over time, Long Short-Term Memory (LSTM) networks, a type of recurrent neural network with a more complex computational unit, have obtained strong results on a variety of sequence modeling tasks. The only underlying LSTM structure that has been explored so far is a linear chain. However, natural language exhibits syntactic properties that would naturally combine words to phrases. We introduce the Tree-LSTM, a generalization of LSTMs to tree-structured network topologies. Tree-LSTMs outperform all existing systems and strong LSTM baselines on two tasks: predicting the semantic relatedness of two sentences (SemEval 2014, Task 1) and sentiment classification (Stanford Sentiment Treebank)."
    },
    {
        "paperId": "ba49d3823d43515e447296ca4e1e55d3f1fd8c4d",
        "url": "https://www.semanticscholar.org/paper/ba49d3823d43515e447296ca4e1e55d3f1fd8c4d",
        "title": "Neural Responding Machine for Short-Text Conversation",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2015,
        "citationCount": 1161,
        "openAccessPdf": {
            "url": "https://aclanthology.org/P15-1152.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1503.02364, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "50812138",
                "name": "Lifeng Shang"
            },
            {
                "authorId": "11955007",
                "name": "Zhengdong Lu"
            },
            {
                "authorId": "49404233",
                "name": "Hang Li"
            }
        ],
        "abstract": "We propose Neural Responding Machine (NRM), a neural network-based response generator for Short-Text Conversation. NRM takes the general encoder-decoder framework: it formalizes the generation of response as a decoding process based on the latent representation of the input text, while both encoding and decoding are realized with recurrent neural networks (RNN). The NRM is trained with a large amount of one-round conversation data collected from a microblogging service. Empirical study shows that NRM can generate grammatically correct and content-wise appropriate responses to over 75% of the input text, outperforming state-of-the-arts in the same setting, including retrieval-based and SMT-based models."
    },
    {
        "paperId": "f3b96ef2dc1fc5e14982f1b963db8db6a54183bb",
        "url": "https://www.semanticscholar.org/paper/f3b96ef2dc1fc5e14982f1b963db8db6a54183bb",
        "title": "Improving Neural Machine Translation Models with Monolingual Data",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2015,
        "citationCount": 2844,
        "openAccessPdf": {
            "url": "https://aclanthology.org/P16-1009.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1511.06709, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2082372",
                "name": "Rico Sennrich"
            },
            {
                "authorId": "2259100",
                "name": "B. Haddow"
            },
            {
                "authorId": "2539211",
                "name": "Alexandra Birch"
            }
        ],
        "abstract": "Neural Machine Translation (NMT) has obtained state-of-the art performance for several language pairs, while only using parallel data for training. Target-side monolingual data plays an important role in boosting fluency for phrase-based statistical machine translation, and we investigate the use of monolingual data for NMT. In contrast to previous work, which combines NMT models with separately trained language models, we note that encoder-decoder NMT architectures already have the capacity to learn the same information as a language model, and we explore strategies to train with monolingual data without changing the neural network architecture. By pairing monolingual training data with an automatic back-translation, we can treat it as additional parallel training data, and we obtain substantial improvements on the WMT 15 task English German (+2.8-3.7 BLEU), and for the low-resourced IWSLT 14 task Turkish->English (+2.1-3.4 BLEU), obtaining new state-of-the-art results. We also show that fine-tuning on in-domain monolingual and parallel data gives substantial improvements for the IWSLT 15 task English->German."
    },
    {
        "paperId": "02534853626c18c9a097c2712f1ddf3613257d35",
        "url": "https://www.semanticscholar.org/paper/02534853626c18c9a097c2712f1ddf3613257d35",
        "title": "Incorporating Copying Mechanism in Sequence-to-Sequence Learning",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2016,
        "citationCount": 1571,
        "openAccessPdf": {
            "url": "https://doi.org/10.18653/v1/p16-1154",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1603.06393, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3016273",
                "name": "Jiatao Gu"
            },
            {
                "authorId": "11955007",
                "name": "Zhengdong Lu"
            },
            {
                "authorId": "49404233",
                "name": "Hang Li"
            },
            {
                "authorId": "2052674293",
                "name": "V. Li"
            }
        ],
        "abstract": "We address an important problem in sequence-to-sequence (Seq2Seq) learning referred to as copying, in which certain segments in the input sequence are selectively replicated in the output sequence. A similar phenomenon is observable in human language communication. For example, humans tend to repeat entity names or even long phrases in conversation. The challenge with regard to copying in Seq2Seq is that new machinery is needed to decide when to perform the operation. In this paper, we incorporate copying into neural network-based Seq2Seq learning and propose a new model called CopyNet with encoder-decoder structure. CopyNet can nicely integrate the regular way of word generation in the decoder with the new copying mechanism which can choose sub-sequences in the input sequence and put them at proper places in the output sequence. Our empirical study on both synthetic data sets and real world data sets demonstrates the efficacy of CopyNet. For example, CopyNet can outperform regular RNN-based model with remarkable margins on text summarization tasks."
    },
    {
        "paperId": "1ea75cdb7ce8c4f5f2599165e3698034b4142e08",
        "url": "https://www.semanticscholar.org/paper/1ea75cdb7ce8c4f5f2599165e3698034b4142e08",
        "title": "A Persona-Based Neural Conversation Model",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2016,
        "citationCount": 1069,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/P16-1094.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1603.06155, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2386020962",
                "name": "Jiwei Li"
            },
            {
                "authorId": "1947267",
                "name": "Michel Galley"
            },
            {
                "authorId": "3125776",
                "name": "Chris Brockett"
            },
            {
                "authorId": "3130583",
                "name": "Georgios P. Spithourakis"
            },
            {
                "authorId": "1800422",
                "name": "Jianfeng Gao"
            },
            {
                "authorId": "83415753",
                "name": "W. Dolan"
            }
        ],
        "abstract": "We present persona-based models for handling the issue of speaker consistency in neural response generation. A speaker model encodes personas in distributed embeddings that capture individual characteristics such as background information and speaking style. A dyadic speaker-addressee model captures properties of interactions between two interlocutors. Our models yield qualitative performance improvements in both perplexity and BLEU scores over baseline sequence-to-sequence models, with similar gains in speaker consistency as measured by human judges."
    },
    {
        "paperId": "345ef9a7d9af0ac0816d76803ddcf9b6d19404d7",
        "url": "https://www.semanticscholar.org/paper/345ef9a7d9af0ac0816d76803ddcf9b6d19404d7",
        "title": "Neural Relation Extraction with Selective Attention over Instances",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2016,
        "citationCount": 1003,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/P16-1200.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/P16-1200, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2427350",
                "name": "Yankai Lin"
            },
            {
                "authorId": "2589625",
                "name": "Shiqi Shen"
            },
            {
                "authorId": "49293587",
                "name": "Zhiyuan Liu"
            },
            {
                "authorId": "3371599",
                "name": "Huanbo Luan"
            },
            {
                "authorId": "1753344",
                "name": "Maosong Sun"
            }
        ],
        "abstract": "Distant supervised relation extraction has been widely used to nd novel relational facts from text. However, distant supervision inevitably accompanies with the wrong labelling problem, and these noisy data will substantially hurt the performance of relation extraction. To alleviate this issue, we propose a sentence-level attention-based model for relation extraction. In this model, we employ convolutional neural networks to embed the semantics of sentences. Afterwards, we build sentence-level attention over multiple instances, which is expected to dynamically reduce the weights of those noisy instances. Experimental results on real-world datasets show that, our model can make full use of all informative sentences and effectively reduce the inuence of wrong labelled instances. Our model achieves signicant and consistent improvements on relation extraction as compared with baselines. The source code of this paper can be obtained from https: //github.com/thunlp/NRE ."
    },
    {
        "paperId": "3899f87a2031f3434f89beb68c11a1ca6428328a",
        "url": "https://www.semanticscholar.org/paper/3899f87a2031f3434f89beb68c11a1ca6428328a",
        "title": "End-to-End Relation Extraction using LSTMs on Sequences and Tree Structures",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2016,
        "citationCount": 1241,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/P16-1105.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1601.00770, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1731657",
                "name": "Makoto Miwa"
            },
            {
                "authorId": "143977268",
                "name": "Mohit Bansal"
            }
        ],
        "abstract": "We present a novel end-to-end neural model to extract entities and relations between them. Our recurrent neural network based model captures both word sequence and dependency tree substructure information by stacking bidirectional tree-structured LSTM-RNNs on bidirectional sequential LSTM-RNNs. This allows our model to jointly represent both entities and relations with shared parameters in a single model. We further encourage detection of entities during training and use of entity information in relation extraction via entity pretraining and scheduled sampling. Our model improves over the state-of-the-art feature-based model on end-to-end relation extraction, achieving 12.1% and 5.7% relative error reductions in F1-score on ACE2005 and ACE2004, respectively. We also show that our LSTM-RNN based model compares favorably to the state-of-the-art CNN based model (in F1-score) on nominal relation classification (SemEval-2010 Task 8). Finally, we present an extensive ablation analysis of several model components."
    },
    {
        "paperId": "3a29aa4eff48624752c07059a44d3288a678c8ab",
        "url": "https://www.semanticscholar.org/paper/3a29aa4eff48624752c07059a44d3288a678c8ab",
        "title": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2016,
        "citationCount": 1365,
        "openAccessPdf": {
            "url": "https://aclanthology.org/P16-1.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.18653/v1/p16-1?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.18653/v1/p16-1, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1708114",
                "name": "K. Erk"
            },
            {
                "authorId": "144365875",
                "name": "Noah A. Smith"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "6b8b2075319accc23fef43e4cf76bc3682189d82",
        "url": "https://www.semanticscholar.org/paper/6b8b2075319accc23fef43e4cf76bc3682189d82",
        "title": "Attention-Based Bidirectional Long Short-Term Memory Networks for Relation Classification",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2016,
        "citationCount": 1894,
        "openAccessPdf": {
            "url": "https://doi.org/10.18653/v1/p16-2034",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/P16-2034, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "144032121",
                "name": "P. Zhou"
            },
            {
                "authorId": null,
                "name": "Wei Shi"
            },
            {
                "authorId": "2279669419",
                "name": "Jun Tian"
            },
            {
                "authorId": "2072539340",
                "name": "Zhenyu Qi"
            },
            {
                "authorId": "38953293",
                "name": "B. Li"
            },
            {
                "authorId": "143680014",
                "name": "Hongwei Hao"
            },
            {
                "authorId": "2109511511",
                "name": "Bo Xu"
            }
        ],
        "abstract": "Relation classification is an important semantic processing task in the field of natural language processing (NLP). State-ofthe-art systems still rely on lexical resources such as WordNet or NLP systems like dependency parser and named entity recognizers (NER) to get high-level features. Another challenge is that important information can appear at any position in the sentence. To tackle these problems, we propose Attention-Based Bidirectional Long Short-Term Memory Networks(AttBLSTM) to capture the most important semantic information in a sentence. The experimental results on the SemEval-2010 relation classification task show that our method outperforms most of the existing methods, with only word vectors."
    },
    {
        "paperId": "83e7654d545fbbaaf2328df365a781fb67b841b4",
        "url": "https://www.semanticscholar.org/paper/83e7654d545fbbaaf2328df365a781fb67b841b4",
        "title": "Enhanced LSTM for Natural Language Inference",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2016,
        "citationCount": 1171,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/P17-1152.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1609.06038, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "47261124",
                "name": "Qian Chen"
            },
            {
                "authorId": "1854999",
                "name": "Xiao-Dan Zhu"
            },
            {
                "authorId": "1749989",
                "name": "Zhenhua Ling"
            },
            {
                "authorId": "144572674",
                "name": "Si Wei"
            },
            {
                "authorId": "36357862",
                "name": "Hui Jiang"
            },
            {
                "authorId": "1697366",
                "name": "Diana Inkpen"
            }
        ],
        "abstract": "Reasoning and inference are central to human and artificial intelligence. Modeling inference in human language is very challenging. With the availability of large annotated data (Bowman et al., 2015), it has recently become feasible to train neural network based inference models, which have shown to be very effective. In this paper, we present a new state-of-the-art result, achieving the accuracy of 88.6% on the Stanford Natural Language Inference Dataset. Unlike the previous top models that use very complicated network architectures, we first demonstrate that carefully designing sequential inference models based on chain LSTMs can outperform all previous models. Based on this, we further show that by explicitly considering recursive architectures in both local inference modeling and inference composition, we achieve additional improvement. Particularly, incorporating syntactic parsing information contributes to our best resultit further improves the performance even when added to the already very strong model."
    },
    {
        "paperId": "8dd6aae51e31a72752c4be5cddbdd76dfdc6cda4",
        "url": "https://www.semanticscholar.org/paper/8dd6aae51e31a72752c4be5cddbdd76dfdc6cda4",
        "title": "End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2016,
        "citationCount": 2745,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/P16-1101.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1603.01354, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2378954",
                "name": "Xuezhe Ma"
            },
            {
                "authorId": "144547315",
                "name": "E. Hovy"
            }
        ],
        "abstract": "State-of-the-art sequence labeling systems traditionally require large amounts of task-specific knowledge in the form of hand-crafted features and data pre-processing. In this paper, we introduce a novel neutral network architecture that benefits from both word- and character-level representations automatically, by using combination of bidirectional LSTM, CNN and CRF. Our system is truly end-to-end, requiring no feature engineering or data pre-processing, thus making it applicable to a wide range of sequence labeling tasks. We evaluate our system on two data sets for two sequence labeling tasks --- Penn Treebank WSJ corpus for part-of-speech (POS) tagging and CoNLL 2003 corpus for named entity recognition (NER). We obtain state-of-the-art performance on both the two data --- 97.55\\% accuracy for POS tagging and 91.21\\% F1 for NER."
    },
    {
        "paperId": "03c294ad75bd1bac92217419ac25358227f6a901",
        "url": "https://www.semanticscholar.org/paper/03c294ad75bd1bac92217419ac25358227f6a901",
        "title": "Liar, Liar Pants on Fire: A New Benchmark Dataset for Fake News Detection",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2017,
        "citationCount": 1550,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/P17-2067.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1705.00648, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1682479",
                "name": "William Yang Wang"
            }
        ],
        "abstract": "Automatic fake news detection is a challenging problem in deception detection, and it has tremendous real-world political and social impacts. However, statistical approaches to combating fake news has been dramatically limited by the lack of labeled benchmark datasets. In this paper, we present LIAR: a new, publicly available dataset for fake news detection. We collected a decade-long, 12.8K manually labeled short statements in various contexts from PolitiFact.com, which provides detailed analysis report and links to source documents for each case. This dataset can be used for fact-checking research as well. Notably, this new dataset is an order of magnitude larger than previously largest public fake news datasets of similar type. Empirically, we investigate automatic fake news detection based on surface-level linguistic patterns. We have designed a novel, hybrid convolutional neural network to integrate meta-data with text. We show that this hybrid approach can improve a text-only deep learning model."
    },
    {
        "paperId": "104715e1097b7ebee436058bfd9f45540f269845",
        "url": "https://www.semanticscholar.org/paper/104715e1097b7ebee436058bfd9f45540f269845",
        "title": "Reading Wikipedia to Answer Open-Domain Questions",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2017,
        "citationCount": 2140,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/P17-1171.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1704.00051, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "50536468",
                "name": "Danqi Chen"
            },
            {
                "authorId": "2064150446",
                "name": "Adam Fisch"
            },
            {
                "authorId": "145183709",
                "name": "J. Weston"
            },
            {
                "authorId": "1713934",
                "name": "Antoine Bordes"
            }
        ],
        "abstract": "This paper proposes to tackle open-domain question answering using Wikipedia as the unique knowledge source: the answer to any factoid question is a text span in a Wikipedia article. This task of machine reading at scale combines the challenges of document retrieval (finding the relevant articles) with that of machine comprehension of text (identifying the answer spans from those articles). Our approach combines a search component based on bigram hashing and TF-IDF matching with a multi-layer recurrent neural network model trained to detect answers in Wikipedia paragraphs. Our experiments on multiple existing QA datasets indicate that (1) both modules are highly competitive with respect to existing counterparts and (2) multitask learning using distant supervision on their combination is an effective complete system on this challenging task."
    },
    {
        "paperId": "514e7fb769950dbe96eb519c88ca17e04dc829f6",
        "url": "https://www.semanticscholar.org/paper/514e7fb769950dbe96eb519c88ca17e04dc829f6",
        "title": "HotFlip: White-Box Adversarial Examples for Text Classification",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2017,
        "citationCount": 1162,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/P18-2006.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/P18-2006, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "39043512",
                "name": "J. Ebrahimi"
            },
            {
                "authorId": "36290866",
                "name": "Anyi Rao"
            },
            {
                "authorId": "3021654",
                "name": "Daniel Lowd"
            },
            {
                "authorId": "1721158",
                "name": "D. Dou"
            }
        ],
        "abstract": "We propose an efficient method to generate white-box adversarial examples to trick a character-level neural classifier. We find that only a few manipulations are needed to greatly decrease the accuracy. Our method relies on an atomic flip operation, which swaps one token for another, based on the gradients of the one-hot input vectors. Due to efficiency of our method, we can perform adversarial training which makes the model more robust to attacks at test time. With the use of a few semantics-preserving constraints, we demonstrate that HotFlip can be adapted to attack a word-level classifier as well."
    },
    {
        "paperId": "668db48c6a79826456341680ee1175dfc4cced71",
        "url": "https://www.semanticscholar.org/paper/668db48c6a79826456341680ee1175dfc4cced71",
        "title": "Get To The Point: Summarization with Pointer-Generator Networks",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2017,
        "citationCount": 4260,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/P17-1099.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1704.04368, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "13070498",
                "name": "A. See"
            },
            {
                "authorId": "35025299",
                "name": "Peter J. Liu"
            },
            {
                "authorId": "144783904",
                "name": "Christopher D. Manning"
            }
        ],
        "abstract": "Neural sequence-to-sequence models have provided a viable new approach for abstractive text summarization (meaning they are not restricted to simply selecting and rearranging passages from the original text). However, these models have two shortcomings: they are liable to reproduce factual details inaccurately, and they tend to repeat themselves. In this work we propose a novel architecture that augments the standard sequence-to-sequence attentional model in two orthogonal ways. First, we use a hybrid pointer-generator network that can copy words from the source text via pointing, which aids accurate reproduction of information, while retaining the ability to produce novel words through the generator. Second, we use coverage to keep track of what has been summarized, which discourages repetition. We apply our model to the CNN / Daily Mail summarization task, outperforming the current abstractive state-of-the-art by at least 2 ROUGE points."
    },
    {
        "paperId": "aab5002a22b9b4244a8329b140bd0a86021aa2d1",
        "url": "https://www.semanticscholar.org/paper/aab5002a22b9b4244a8329b140bd0a86021aa2d1",
        "title": "OpenNMT: Open-Source Toolkit for Neural Machine Translation",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2017,
        "citationCount": 1936,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/P17-4012.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1709.03815, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "39861444",
                "name": "Guillaume Klein"
            },
            {
                "authorId": "152847918",
                "name": "Yoon Kim"
            },
            {
                "authorId": "2505751",
                "name": "Yuntian Deng"
            },
            {
                "authorId": "3053934",
                "name": "Jean Senellart"
            },
            {
                "authorId": "2531268",
                "name": "Alexander M. Rush"
            }
        ],
        "abstract": "We describe an open-source toolkit for neural machine translation (NMT). The toolkit prioritizes efficiency, modularity, and extensibility with the goal of supporting NMT research into model architectures, feature representations, and source modalities, while maintaining competitive performance and reasonable training requirements. The toolkit consists of modeling and translation support, as well as detailed pedagogical documentation about the underlying techniques."
    },
    {
        "paperId": "f010affab57b5fcf1cd6be23df79d8ec98c7289c",
        "url": "https://www.semanticscholar.org/paper/f010affab57b5fcf1cd6be23df79d8ec98c7289c",
        "title": "TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2017,
        "citationCount": 3293,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/P17-1147.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1705.03551, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "144863691",
                "name": "Mandar Joshi"
            },
            {
                "authorId": "2890423",
                "name": "Eunsol Choi"
            },
            {
                "authorId": "1780531",
                "name": "Daniel S. Weld"
            },
            {
                "authorId": "1982950",
                "name": "Luke Zettlemoyer"
            }
        ],
        "abstract": "We present TriviaQA, a challenging reading comprehension dataset containing over 650K question-answer-evidence triples. TriviaQA includes 95K question-answer pairs authored by trivia enthusiasts and independently gathered evidence documents, six per question on average, that provide high quality distant supervision for answering the questions. We show that, in comparison to other recently introduced large-scale datasets, TriviaQA (1) has relatively complex, compositional questions, (2) has considerable syntactic and lexical variability between questions and corresponding answer-evidence sentences, and (3) requires more cross sentence reasoning to find answers. We also present two baseline algorithms: a feature-based classifier and a state-of-the-art neural network, that performs well on SQuAD reading comprehension. Neither approach comes close to human performance (23% and 40% vs. 80%), suggesting that TriviaQA is a challenging testbed that is worth significant future study."
    },
    {
        "paperId": "006fdeff6e1a81c404317ee4056d6cc72f9c0e50",
        "url": "https://www.semanticscholar.org/paper/006fdeff6e1a81c404317ee4056d6cc72f9c0e50",
        "title": "Multimodal Language Analysis in the Wild: CMU-MOSEI Dataset and Interpretable Dynamic Fusion Graph",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2018,
        "citationCount": 1337,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/P18-1208.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/P18-1208, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "144802290",
                "name": "Amir Zadeh"
            },
            {
                "authorId": "28130078",
                "name": "Paul Pu Liang"
            },
            {
                "authorId": "1746416",
                "name": "Soujanya Poria"
            },
            {
                "authorId": "49943757",
                "name": "E. Cambria"
            },
            {
                "authorId": "49933077",
                "name": "Louis-philippe Morency"
            }
        ],
        "abstract": "Analyzing human multimodal language is an emerging area of research in NLP. Intrinsically this language is multimodal (heterogeneous), sequential and asynchronous; it consists of the language (words), visual (expressions) and acoustic (paralinguistic) modalities all in the form of asynchronous coordinated sequences. From a resource perspective, there is a genuine need for large scale datasets that allow for in-depth studies of this form of language. In this paper we introduce CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI), the largest dataset of sentiment analysis and emotion recognition to date. Using data from CMU-MOSEI and a novel multimodal fusion technique called the Dynamic Fusion Graph (DFG), we conduct experimentation to exploit how modalities interact with each other in human multimodal language. Unlike previously proposed fusion techniques, DFG is highly interpretable and achieves competative performance when compared to the previous state of the art."
    },
    {
        "paperId": "1e077413b25c4d34945cc2707e17e46ed4fe784a",
        "url": "https://www.semanticscholar.org/paper/1e077413b25c4d34945cc2707e17e46ed4fe784a",
        "title": "Universal Language Model Fine-tuning for Text Classification",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2018,
        "citationCount": 3921,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/P18-1031.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/P18-1031, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2093348519",
                "name": "Jeremy Howard"
            },
            {
                "authorId": "2884561",
                "name": "Sebastian Ruder"
            }
        ],
        "abstract": "Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24% on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100 times more data. We open-source our pretrained models and code."
    },
    {
        "paperId": "29de7c0fb3c09eaf55b20619bceaeafe72fd87a6",
        "url": "https://www.semanticscholar.org/paper/29de7c0fb3c09eaf55b20619bceaeafe72fd87a6",
        "title": "Hierarchical Neural Story Generation",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2018,
        "citationCount": 1825,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/P18-1082.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1805.04833, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "144270981",
                "name": "Angela Fan"
            },
            {
                "authorId": "35084211",
                "name": "M. Lewis"
            },
            {
                "authorId": "2921469",
                "name": "Yann Dauphin"
            }
        ],
        "abstract": "We explore story generation: creative systems that can build coherent and fluent passages of text about a topic. We collect a large dataset of 300K human-written stories paired with writing prompts from an online forum. Our dataset enables hierarchical story generation, where the model first generates a premise, and then transforms it into a passage of text. We gain further improvements with a novel form of model fusion that improves the relevance of the story to the prompt, and adding a new gated multi-scale self-attention mechanism to model long-range context. Experiments show large improvements over strong baselines on both automated and human evaluations. Human judges prefer stories generated by our approach to those from a strong non-hierarchical model by a factor of two to one."
    },
    {
        "paperId": "4d1c856275744c0284312a3a50efb6ca9dc4cd4c",
        "url": "https://www.semanticscholar.org/paper/4d1c856275744c0284312a3a50efb6ca9dc4cd4c",
        "title": "Know What You Dont Know: Unanswerable Questions for SQuAD",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2018,
        "citationCount": 3117,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/P18-2124.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1806.03822, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2706258",
                "name": "Pranav Rajpurkar"
            },
            {
                "authorId": "3422908",
                "name": "Robin Jia"
            },
            {
                "authorId": "145419642",
                "name": "Percy Liang"
            }
        ],
        "abstract": "Extractive reading comprehension systems can often locate the correct answer to a question in a context document, but they also tend to make unreliable guesses on questions for which the correct answer is not stated in the context. Existing datasets either focus exclusively on answerable questions, or use automatically generated unanswerable questions that are easy to identify. To address these weaknesses, we present SQuADRUn, a new dataset that combines the existing Stanford Question Answering Dataset (SQuAD) with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuADRUn, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering. SQuADRUn is a challenging natural language understanding task for existing models: a strong neural system that gets 86% F1 on SQuAD achieves only 66% F1 on SQuADRUn. We release SQuADRUn to the community as the successor to SQuAD."
    },
    {
        "paperId": "6c7046195f64cccac1ed3275d88d77655534b5a4",
        "url": "https://www.semanticscholar.org/paper/6c7046195f64cccac1ed3275d88d77655534b5a4",
        "title": "Personalizing Dialogue Agents: I have a dog, do you have pets too?",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2018,
        "citationCount": 1603,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/P18-1205.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1801.07243, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "35097114",
                "name": "Saizheng Zhang"
            },
            {
                "authorId": "31461304",
                "name": "Emily Dinan"
            },
            {
                "authorId": "39219656",
                "name": "Jack Urbanek"
            },
            {
                "authorId": "3149531",
                "name": "Arthur Szlam"
            },
            {
                "authorId": "1743722",
                "name": "Douwe Kiela"
            },
            {
                "authorId": "145183709",
                "name": "J. Weston"
            }
        ],
        "abstract": "Chit-chat models are known to have several problems: they lack specificity, do not display a consistent personality and are often not very captivating. In this work we present the task of making chit-chat more engaging by conditioning on profile information. We collect data and train models to (i)condition on their given profile information; and (ii) information about the person they are talking to, resulting in improved dialogues, as measured by next utterance prediction. Since (ii) is initially unknown our model is trained to engage its partner with personal topics, and we show the resulting dialogue can be used to predict profile information about the interlocutors."
    },
    {
        "paperId": "a33a06ddc762fb855b6954c08d5aca603080b011",
        "url": "https://www.semanticscholar.org/paper/a33a06ddc762fb855b6954c08d5aca603080b011",
        "title": "Towards Empathetic Open-domain Conversation Models: A New Benchmark and Dataset",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2018,
        "citationCount": 1014,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1811.00207",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/P19-1534, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2516777",
                "name": "Hannah Rashkin"
            },
            {
                "authorId": "51324296",
                "name": "Eric Michael Smith"
            },
            {
                "authorId": "6649233",
                "name": "Margaret Li"
            },
            {
                "authorId": "90841478",
                "name": "Y-Lan Boureau"
            }
        ],
        "abstract": "One challenge for dialogue agents is recognizing feelings in the conversation partner and replying accordingly, a key communicative skill. While it is straightforward for humans to recognize and acknowledge others feelings in a conversation, this is a significant challenge for AI systems due to the paucity of suitable publicly-available datasets for training and evaluation. This work proposes a new benchmark for empathetic dialogue generation and EmpatheticDialogues, a novel dataset of 25k conversations grounded in emotional situations. Our experiments indicate that dialogue models that use our dataset are perceived to be more empathetic by human evaluators, compared to models merely trained on large-scale Internet conversation data. We also present empirical comparisons of dialogue model adaptations for empathetic responding, leveraging existing models or datasets without requiring lengthy re-training of the full model."
    },
    {
        "paperId": "b4df354db88a70183a64dbc9e56cf14e7669a6c0",
        "url": "https://www.semanticscholar.org/paper/b4df354db88a70183a64dbc9e56cf14e7669a6c0",
        "title": "Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2018,
        "citationCount": 2782,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/P18-1238.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/P18-1238, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "48267618",
                "name": "Piyush Sharma"
            },
            {
                "authorId": "2066767241",
                "name": "Nan Ding"
            },
            {
                "authorId": "7685850",
                "name": "Sebastian Goodman"
            },
            {
                "authorId": "1737285",
                "name": "Radu Soricut"
            }
        ],
        "abstract": "We present a new dataset of image caption annotations, Conceptual Captions, which contains an order of magnitude more images than the MS-COCO dataset (Lin et al., 2014) and represents a wider variety of both images and image caption styles. We achieve this by extracting and filtering image caption annotations from billions of webpages. We also present quantitative evaluations of a number of image captioning models and show that a model architecture based on Inception-ResNetv2 (Szegedy et al., 2016) for image-feature extraction and Transformer (Vaswani et al., 2017) for sequence modeling achieves the best performance when trained on the Conceptual Captions dataset."
    },
    {
        "paperId": "e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e",
        "url": "https://www.semanticscholar.org/paper/e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e",
        "title": "Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2018,
        "citationCount": 1275,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/P18-1007.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1804.10959, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1765329",
                "name": "Taku Kudo"
            }
        ],
        "abstract": "Subword units are an effective way to alleviate the open vocabulary problems in neural machine translation (NMT). While sentences are usually converted into unique subword sequences, subword segmentation is potentially ambiguous and multiple segmentations are possible even with the same vocabulary. The question addressed in this paper is whether it is possible to harness the segmentation ambiguity as a noise to improve the robustness of NMT. We present a simple regularization method, subword regularization, which trains the model with multiple subword segmentations probabilistically sampled during training. In addition, for better subword sampling, we propose a new subword segmentation algorithm based on a unigram language model. We experiment with multiple corpora and report consistent improvements especially on low resource and out-of-domain settings."
    },
    {
        "paperId": "f2d257625e8029f6f4998deb6279f97e07e2893c",
        "url": "https://www.semanticscholar.org/paper/f2d257625e8029f6f4998deb6279f97e07e2893c",
        "title": "MELD: A Multimodal Multi-Party Dataset for Emotion Recognition in Conversations",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2018,
        "citationCount": 1362,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/P19-1050.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1810.02508, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1746416",
                "name": "Soujanya Poria"
            },
            {
                "authorId": "8223433",
                "name": "Devamanyu Hazarika"
            },
            {
                "authorId": "35122767",
                "name": "Navonil Majumder"
            },
            {
                "authorId": "2064937904",
                "name": "Gautam Naik"
            },
            {
                "authorId": "49943757",
                "name": "E. Cambria"
            },
            {
                "authorId": "2105984203",
                "name": "Rada Mihalcea"
            }
        ],
        "abstract": "Emotion recognition in conversations is a challenging task that has recently gained popularity due to its potential applications. Until now, however, a large-scale multimodal multi-party emotional conversational database containing more than two speakers per dialogue was missing. Thus, we propose the Multimodal EmotionLines Dataset (MELD), an extension and enhancement of EmotionLines. MELD contains about 13,000 utterances from 1,433 dialogues from the TV-series Friends. Each utterance is annotated with emotion and sentiment labels, and encompasses audio, visual and textual modalities. We propose several strong multimodal baselines and show the importance of contextual and multimodal information for emotion recognition in conversations. The full dataset is available for use at http://affective-meld.github.io."
    },
    {
        "paperId": "07a64686ce8e43ac475a8d820a8a9f1d87989583",
        "url": "https://www.semanticscholar.org/paper/07a64686ce8e43ac475a8d820a8a9f1d87989583",
        "title": "Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2019,
        "citationCount": 1321,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/P19-1580.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1905.09418, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "46235299",
                "name": "Elena Voita"
            },
            {
                "authorId": "144251066",
                "name": "David Talbot"
            },
            {
                "authorId": "2157158",
                "name": "F. Moiseev"
            },
            {
                "authorId": "2082372",
                "name": "Rico Sennrich"
            },
            {
                "authorId": "144889265",
                "name": "Ivan Titov"
            }
        ],
        "abstract": "Multi-head self-attention is a key component of the Transformer, a state-of-the-art architecture for neural machine translation. In this work we evaluate the contribution made by individual attention heads to the overall performance of the model and analyze the roles played by them in the encoder. We find that the most important and confident heads play consistent and often linguistically-interpretable roles. When pruning heads using a method based on stochastic gates and a differentiable relaxation of the L0 penalty, we observe that specialized heads are last to be pruned. Our novel pruning method removes the vast majority of heads without seriously affecting performance. For example, on the English-Russian WMT dataset, pruning 38 out of 48 encoder heads results in a drop of only 0.15 BLEU."
    },
    {
        "paperId": "207da6d2c07289bf72a2b5974bb3f011ebb5dd0d",
        "url": "https://www.semanticscholar.org/paper/207da6d2c07289bf72a2b5974bb3f011ebb5dd0d",
        "title": "Adversarial NLI: A New Benchmark for Natural Language Understanding",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2019,
        "citationCount": 1122,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/2020.acl-main.441.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1910.14599, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "40383658",
                "name": "Yixin Nie"
            },
            {
                "authorId": "81840293",
                "name": "Adina Williams"
            },
            {
                "authorId": "31461304",
                "name": "Emily Dinan"
            },
            {
                "authorId": "143977268",
                "name": "Mohit Bansal"
            },
            {
                "authorId": "145183709",
                "name": "J. Weston"
            },
            {
                "authorId": "1743722",
                "name": "Douwe Kiela"
            }
        ],
        "abstract": "We introduce a new large-scale NLI benchmark dataset, collected via an iterative, adversarial human-and-model-in-the-loop procedure. We show that training models on this new dataset leads to state-of-the-art performance on a variety of popular NLI benchmarks, while posing a more difficult challenge with its new test set. Our analysis sheds light on the shortcomings of current state-of-the-art models, and shows that non-expert annotators are successful at finding their weaknesses. The data collection method can be applied in a never-ending learning scenario, becoming a moving target for NLU, rather than a static benchmark that will quickly saturate."
    },
    {
        "paperId": "335613303ebc5eac98de757ed02a56377d99e03a",
        "url": "https://www.semanticscholar.org/paper/335613303ebc5eac98de757ed02a56377d99e03a",
        "title": "What Does BERT Learn about the Structure of Language?",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2019,
        "citationCount": 1420,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/P19-1356.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/P19-1356, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "145123979",
                "name": "Ganesh Jawahar"
            },
            {
                "authorId": "68990982",
                "name": "Benot Sagot"
            },
            {
                "authorId": "1679170",
                "name": "Djam Seddah"
            }
        ],
        "abstract": "BERT is a recent language representation model that has surprisingly performed well in diverse language understanding benchmarks. This result indicates the possibility that BERT networks capture structural information about language. In this work, we provide novel support for this claim by performing a series of experiments to unpack the elements of English language structure learned by BERT. Our findings are fourfold. BERTs phrasal representation captures the phrase-level information in the lower layers. The intermediate layers of BERT compose a rich hierarchy of linguistic information, starting with surface features at the bottom, syntactic features in the middle followed by semantic features at the top. BERT requires deeper layers while tracking subject-verb agreement to handle long-term dependency problem. Finally, the compositional scheme underlying BERT mimics classical, tree-like structures."
    },
    {
        "paperId": "388e2fcdcefbe0834e153ab2a0be127092f9674d",
        "url": "https://www.semanticscholar.org/paper/388e2fcdcefbe0834e153ab2a0be127092f9674d",
        "title": "DIALOGPT : Large-Scale Generative Pre-training for Conversational Response Generation",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2019,
        "citationCount": 1650,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/2020.acl-demos.30.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1911.00536, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3272356",
                "name": "Yizhe Zhang"
            },
            {
                "authorId": "2109508754",
                "name": "Siqi Sun"
            },
            {
                "authorId": "1947267",
                "name": "Michel Galley"
            },
            {
                "authorId": "2378902",
                "name": "Yen-Chun Chen"
            },
            {
                "authorId": "3125776",
                "name": "Chris Brockett"
            },
            {
                "authorId": "71886367",
                "name": "Xiang Gao"
            },
            {
                "authorId": "1800422",
                "name": "Jianfeng Gao"
            },
            {
                "authorId": "46700348",
                "name": "Jingjing Liu"
            },
            {
                "authorId": "83415753",
                "name": "W. Dolan"
            }
        ],
        "abstract": "We present a large, tunable neural conversational response generation model, DIALOGPT (dialogue generative pre-trained transformer). Trained on 147M conversation-like exchanges extracted from Reddit comment chains over a period spanning from 2005 through 2017, DialoGPT extends the Hugging Face PyTorch transformer to attain a performance close to human both in terms of automatic and human evaluation in single-turn dialogue settings. We show that conversational systems that leverage DialoGPT generate more relevant, contentful and context-consistent responses than strong baseline systems. The pre-trained model and training pipeline are publicly released to facilitate research into neural response generation and the development of more intelligent open-domain dialogue systems."
    },
    {
        "paperId": "395de0bd3837fdf4b4b5e5f04835bcc69c279481",
        "url": "https://www.semanticscholar.org/paper/395de0bd3837fdf4b4b5e5f04835bcc69c279481",
        "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2019,
        "citationCount": 12030,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/2020.acl-main.703.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1910.13461, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "35084211",
                "name": "M. Lewis"
            },
            {
                "authorId": "11323179",
                "name": "Yinhan Liu"
            },
            {
                "authorId": "39589154",
                "name": "Naman Goyal"
            },
            {
                "authorId": "2320509",
                "name": "Marjan Ghazvininejad"
            },
            {
                "authorId": "113947684",
                "name": "Abdel-rahman Mohamed"
            },
            {
                "authorId": "39455775",
                "name": "Omer Levy"
            },
            {
                "authorId": "1759422",
                "name": "Veselin Stoyanov"
            },
            {
                "authorId": "1982950",
                "name": "Luke Zettlemoyer"
            }
        ],
        "abstract": "We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance."
    },
    {
        "paperId": "42ed4a9994e6121a9f325f5b901c5b3d7ce104f5",
        "url": "https://www.semanticscholar.org/paper/42ed4a9994e6121a9f325f5b901c5b3d7ce104f5",
        "title": "Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2019,
        "citationCount": 1326,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/P19-1334.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1902.01007, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "145534175",
                "name": "R. Thomas McCoy"
            },
            {
                "authorId": "2949185",
                "name": "Ellie Pavlick"
            },
            {
                "authorId": "2467508",
                "name": "Tal Linzen"
            }
        ],
        "abstract": "A machine learning system can score well on a given test set by relying on heuristics that are effective for frequent example types but break down in more challenging cases. We study this issue within natural language inference (NLI), the task of determining whether one sentence entails another. We hypothesize that statistical NLI models may adopt three fallible syntactic heuristics: the lexical overlap heuristic, the subsequence heuristic, and the constituent heuristic. To determine whether models have adopted these heuristics, we introduce a controlled evaluation set called HANS (Heuristic Analysis for NLI Systems), which contains many examples where the heuristics fail. We find that models trained on MNLI, including BERT, a state-of-the-art model, perform very poorly on HANS, suggesting that they have indeed adopted these heuristics. We conclude that there is substantial room for improvement in NLI systems, and that the HANS dataset can motivate and measure progress in this area."
    },
    {
        "paperId": "5f994dc8cae24ca9d1ed629e517fcc652660ddde",
        "url": "https://www.semanticscholar.org/paper/5f994dc8cae24ca9d1ed629e517fcc652660ddde",
        "title": "ERNIE: Enhanced Language Representation with Informative Entities",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2019,
        "citationCount": 1509,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/P19-1139.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1905.07129, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2621696",
                "name": "Zhengyan Zhang"
            },
            {
                "authorId": "48506411",
                "name": "Xu Han"
            },
            {
                "authorId": "49293587",
                "name": "Zhiyuan Liu"
            },
            {
                "authorId": "145820291",
                "name": "Xin Jiang"
            },
            {
                "authorId": "1753344",
                "name": "Maosong Sun"
            },
            {
                "authorId": "1688015",
                "name": "Qun Liu"
            }
        ],
        "abstract": "Neural language representation models such as BERT pre-trained on large-scale corpora can well capture rich semantic patterns from plain text, and be fine-tuned to consistently improve the performance of various NLP tasks. However, the existing pre-trained language models rarely consider incorporating knowledge graphs (KGs), which can provide rich structured knowledge facts for better language understanding. We argue that informative entities in KGs can enhance language representation with external knowledge. In this paper, we utilize both large-scale textual corpora and KGs to train an enhanced language representation model (ERNIE), which can take full advantage of lexical, syntactic, and knowledge information simultaneously. The experimental results have demonstrated that ERNIE achieves significant improvements on various knowledge-driven tasks, and meanwhile is comparable with the state-of-the-art model BERT on other common NLP tasks. The code and datasets will be available in the future."
    },
    {
        "paperId": "658721bc13b0fa97366d38c05a96bf0a9f4bb0ac",
        "url": "https://www.semanticscholar.org/paper/658721bc13b0fa97366d38c05a96bf0a9f4bb0ac",
        "title": "Multi-Task Deep Neural Networks for Natural Language Understanding",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2019,
        "citationCount": 1324,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/P19-1441.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1901.11504, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "46522098",
                "name": "Xiaodong Liu"
            },
            {
                "authorId": "50462546",
                "name": "Pengcheng He"
            },
            {
                "authorId": "2109136147",
                "name": "Weizhu Chen"
            },
            {
                "authorId": "1800422",
                "name": "Jianfeng Gao"
            }
        ],
        "abstract": "In this paper, we present a Multi-Task Deep Neural Network (MT-DNN) for learning representations across multiple natural language understanding (NLU) tasks. MT-DNN not only leverages large amounts of cross-task data, but also benefits from a regularization effect that leads to more general representations to help adapt to new tasks and domains. MT-DNN extends the model proposed in Liu et al. (2015) by incorporating a pre-trained bidirectional transformer language model, known as BERT (Devlin et al., 2018). MT-DNN obtains new state-of-the-art results on ten NLU tasks, including SNLI, SciTail, and eight out of nine GLUE tasks, pushing the GLUE benchmark to 82.7% (2.2% absolute improvement) as of February 25, 2019 on the latest GLUE test set. We also demonstrate using the SNLI and SciTail datasets that the representations learned by MT-DNN allow domain adaptation with substantially fewer in-domain labels than the pre-trained BERT representations. Our code and pre-trained models will be made publicly available."
    },
    {
        "paperId": "6fec3e579c7cd4f13bdabbee2b6ac2e8ff5941c6",
        "url": "https://www.semanticscholar.org/paper/6fec3e579c7cd4f13bdabbee2b6ac2e8ff5941c6",
        "title": "Unsupervised Cross-lingual Representation Learning at Scale",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2019,
        "citationCount": 7566,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/2020.acl-main.747.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1911.02116, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2480903",
                "name": "Alexis Conneau"
            },
            {
                "authorId": "40267343",
                "name": "Kartikay Khandelwal"
            },
            {
                "authorId": "39589154",
                "name": "Naman Goyal"
            },
            {
                "authorId": "113810201",
                "name": "Vishrav Chaudhary"
            },
            {
                "authorId": "2293203",
                "name": "Guillaume Wenzek"
            },
            {
                "authorId": "144204682",
                "name": "Francisco (Paco) Guzmn"
            },
            {
                "authorId": "3024698",
                "name": "Edouard Grave"
            },
            {
                "authorId": "40511414",
                "name": "Myle Ott"
            },
            {
                "authorId": "1982950",
                "name": "Luke Zettlemoyer"
            },
            {
                "authorId": "1759422",
                "name": "Veselin Stoyanov"
            }
        ],
        "abstract": "This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6% average accuracy on XNLI, +13% average F1 score on MLQA, and +2.4% F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7% in XNLI accuracy for Swahili and 11.4% for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make our code and models publicly available."
    },
    {
        "paperId": "809cc93921e4698bde891475254ad6dfba33d03b",
        "url": "https://www.semanticscholar.org/paper/809cc93921e4698bde891475254ad6dfba33d03b",
        "title": "How Multilingual is Multilingual BERT?",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2019,
        "citationCount": 1563,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/P19-1493.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1906.01502, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "13455512",
                "name": "Telmo Pires"
            },
            {
                "authorId": "2824355",
                "name": "Eva Schlinger"
            },
            {
                "authorId": "2758616",
                "name": "Dan Garrette"
            }
        ],
        "abstract": "In this paper, we show that Multilingual BERT (M-BERT), released by Devlin et al. (2018) as a single language model pre-trained from monolingual corpora in 104 languages, is surprisingly good at zero-shot cross-lingual model transfer, in which task-specific annotations in one language are used to fine-tune the model for evaluation in another language. To understand why, we present a large number of probing experiments, showing that transfer is possible even to languages in different scripts, that transfer works best between typologically similar languages, that monolingual corpora can train models for code-switching, and that the model can find translation pairs. From these results, we can conclude that M-BERT does create multilingual representations, but that these representations exhibit systematic deficiencies affecting certain language pairs."
    },
    {
        "paperId": "8b0f27bb594b1eaaf493eaf1e2ee723a2b0a19ad",
        "url": "https://www.semanticscholar.org/paper/8b0f27bb594b1eaaf493eaf1e2ee723a2b0a19ad",
        "title": "HellaSwag: Can a Machine Really Finish Your Sentence?",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2019,
        "citationCount": 3414,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/P19-1472.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1905.07830, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2545335",
                "name": "Rowan Zellers"
            },
            {
                "authorId": "14487640",
                "name": "Ari Holtzman"
            },
            {
                "authorId": "3312309",
                "name": "Yonatan Bisk"
            },
            {
                "authorId": "143787583",
                "name": "Ali Farhadi"
            },
            {
                "authorId": "1699545",
                "name": "Yejin Choi"
            }
        ],
        "abstract": "Recent work by Zellers et al. (2018) introduced a new task of commonsense natural language inference: given an event description such as A woman sits at a piano, a machine must select the most likely followup: She sets her fingers on the keys. With the introduction of BERT, near human-level performance was reached. Does this mean that machines can perform human level commonsense inference? In this paper, we show that commonsense inference still proves difficult for even state-of-the-art models, by presenting HellaSwag, a new challenge dataset. Though its questions are trivial for humans (>95% accuracy), state-of-the-art models struggle (<48%). We achieve this via Adversarial Filtering (AF), a data collection paradigm wherein a series of discriminators iteratively select an adversarial set of machine-generated wrong answers. AF proves to be surprisingly robust. The key insight is to scale up the length and complexity of the dataset examples towards a critical Goldilocks zone wherein generated text is ridiculous to humans, yet often misclassified by state-of-the-art models. Our construction of HellaSwag, and its resulting difficulty, sheds light on the inner workings of deep pretrained models. More broadly, it suggests a new path forward for NLP research, in which benchmarks co-evolve with the evolving state-of-the-art in an adversarial way, so as to present ever-harder challenges."
    },
    {
        "paperId": "949fef650da4c41afe6049a183b504b3cc91f4bd",
        "url": "https://www.semanticscholar.org/paper/949fef650da4c41afe6049a183b504b3cc91f4bd",
        "title": "Multimodal Transformer for Unaligned Multimodal Language Sequences",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2019,
        "citationCount": 1762,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/P19-1656.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1906.00295, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "145639633",
                "name": "Yao-Hung Hubert Tsai"
            },
            {
                "authorId": "35836381",
                "name": "Shaojie Bai"
            },
            {
                "authorId": "28130078",
                "name": "Paul Pu Liang"
            },
            {
                "authorId": "145116464",
                "name": "J. Z. Kolter"
            },
            {
                "authorId": "49933077",
                "name": "Louis-philippe Morency"
            },
            {
                "authorId": "145124475",
                "name": "R. Salakhutdinov"
            }
        ],
        "abstract": "Human language is often multimodal, which comprehends a mixture of natural language, facial gestures, and acoustic behaviors. However, two major challenges in modeling such multimodal human language time-series data exist: 1) inherent data non-alignment due to variable sampling rates for the sequences from each modality; and 2) long-range dependencies between elements across modalities. In this paper, we introduce the Multimodal Transformer (MulT) to generically address the above issues in an end-to-end manner without explicitly aligning the data. At the heart of our model is the directional pairwise crossmodal attention, which attends to interactions between multimodal sequences across distinct time steps and latently adapt streams from one modality to another. Comprehensive experiments on both aligned and non-aligned multimodal time-series show that our model outperforms state-of-the-art methods by a large margin. In addition, empirical analysis suggests that correlated crossmodal signals are able to be captured by the proposed crossmodal attention mechanism in MulT."
    },
    {
        "paperId": "97906df07855b029b7aae7c2a1c6c5e8df1d531c",
        "url": "https://www.semanticscholar.org/paper/97906df07855b029b7aae7c2a1c6c5e8df1d531c",
        "title": "BERT Rediscovers the Classical NLP Pipeline",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2019,
        "citationCount": 1645,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1905.05950",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1905.05950, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "6117577",
                "name": "Ian Tenney"
            },
            {
                "authorId": "143790066",
                "name": "Dipanjan Das"
            },
            {
                "authorId": "2949185",
                "name": "Ellie Pavlick"
            }
        ],
        "abstract": "Pre-trained text encoders have rapidly advanced the state of the art on many NLP tasks. We focus on one such model, BERT, and aim to quantify where linguistic information is captured within the network. We find that the model represents the steps of the traditional NLP pipeline in an interpretable and localizable way, and that the regions responsible for each step appear in the expected sequence: POS tagging, parsing, NER, semantic roles, then coreference. Qualitative analysis reveals that the model can and often does adjust this pipeline dynamically, revising lower-level decisions on the basis of disambiguating information from higher-level representations."
    },
    {
        "paperId": "a81874b4a651a740fffbfc47ef96515e8c7f782f",
        "url": "https://www.semanticscholar.org/paper/a81874b4a651a740fffbfc47ef96515e8c7f782f",
        "title": "Latent Retrieval for Weakly Supervised Open Domain Question Answering",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2019,
        "citationCount": 1109,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/P19-1612.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1906.00300, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2544107",
                "name": "Kenton Lee"
            },
            {
                "authorId": "1744179",
                "name": "Ming-Wei Chang"
            },
            {
                "authorId": "3259253",
                "name": "Kristina Toutanova"
            }
        ],
        "abstract": "Recent work on open domain question answering (QA) assumes strong supervision of the supporting evidence and/or assumes a blackbox information retrieval (IR) system to retrieve evidence candidates. We argue that both are suboptimal, since gold evidence is not always available, and QA is fundamentally different from IR. We show for the first time that it is possible to jointly learn the retriever and reader from question-answer string pairs and without any IR system. In this setting, evidence retrieval from all of Wikipedia is treated as a latent variable. Since this is impractical to learn from scratch, we pre-train the retriever with an Inverse Cloze Task. We evaluate on open versions of five QA datasets. On datasets where the questioner already knows the answer, a traditional IR system such as BM25 is sufficient. On datasets where a user is genuinely seeking an answer, we show that learned retrieval is crucial, outperforming BM25 by up to 19 points in exact match."
    },
    {
        "paperId": "b61c6405f4de381758e8b52a20313554d68a9d85",
        "url": "https://www.semanticscholar.org/paper/b61c6405f4de381758e8b52a20313554d68a9d85",
        "title": "CamemBERT: a Tasty French Language Model",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2019,
        "citationCount": 1047,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/2020.acl-main.645.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1911.03894, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "143792623",
                "name": "Louis Martin"
            },
            {
                "authorId": "150045488",
                "name": "Benjamin Muller"
            },
            {
                "authorId": "147846651",
                "name": "Pedro Ortiz Suarez"
            },
            {
                "authorId": "2061071908",
                "name": "Yoann Dupont"
            },
            {
                "authorId": "49799441",
                "name": "L. Romary"
            },
            {
                "authorId": "1400417301",
                "name": "Eric Villemonte de la Clergerie"
            },
            {
                "authorId": "1679170",
                "name": "Djam Seddah"
            },
            {
                "authorId": "68990982",
                "name": "Benot Sagot"
            }
        ],
        "abstract": "Pretrained language models are now ubiquitous in Natural Language Processing. Despite their success, most available models have either been trained on English data or on the concatenation of data in multiple languages. This makes practical use of such models in all languages except English very limited. In this paper, we investigate the feasibility of training monolingual Transformer-based language models for other languages, taking French as an example and evaluating our language models on part-of-speech tagging, dependency parsing, named entity recognition and natural language inference tasks. We show that the use of web crawled data is preferable to the use of Wikipedia data. More surprisingly, we show that a relatively small web crawled dataset (4GB) leads to results that are as good as those obtained using larger datasets (130+GB). Our best performing model CamemBERT reaches or improves the state of the art in all four downstream tasks."
    },
    {
        "paperId": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6",
        "url": "https://www.semanticscholar.org/paper/c4744a7c2bb298e4a52289a1e085c71cc3d37bc6",
        "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2019,
        "citationCount": 4098,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/P19-1285.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1901.02860, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3422912",
                "name": "Zihang Dai"
            },
            {
                "authorId": "2109512754",
                "name": "Zhilin Yang"
            },
            {
                "authorId": "35729970",
                "name": "Yiming Yang"
            },
            {
                "authorId": "143712374",
                "name": "J. Carbonell"
            },
            {
                "authorId": "2827616",
                "name": "Quoc V. Le"
            },
            {
                "authorId": "145124475",
                "name": "R. Salakhutdinov"
            }
        ],
        "abstract": "Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80% longer than RNNs and 450% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch."
    },
    {
        "paperId": "d6a083dad7114f3a39adc65c09bfbb6cf3fee9ea",
        "url": "https://www.semanticscholar.org/paper/d6a083dad7114f3a39adc65c09bfbb6cf3fee9ea",
        "title": "Energy and Policy Considerations for Deep Learning in NLP",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2019,
        "citationCount": 3007,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/P19-1355.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1906.02243, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2268272",
                "name": "Emma Strubell"
            },
            {
                "authorId": "47079359",
                "name": "Ananya Ganesh"
            },
            {
                "authorId": "143753639",
                "name": "A. McCallum"
            }
        ],
        "abstract": "Recent progress in hardware and methodology for training neural networks has ushered in a new generation of large networks trained on abundant data. These models have obtained notable gains in accuracy across many NLP tasks. However, these accuracy improvements depend on the availability of exceptionally large computational resources that necessitate similarly substantial energy consumption. As a result these models are costly to train and develop, both financially, due to the cost of hardware and electricity or cloud compute time, and environmentally, due to the carbon footprint required to fuel modern tensor processing hardware. In this paper we bring this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training a variety of recently successful neural network models for NLP. Based on these findings, we propose actionable recommendations to reduce costs and improve equity in NLP research and practice."
    },
    {
        "paperId": "02eaaf87f9cae34cca398fed146079e6eeb1f868",
        "url": "https://www.semanticscholar.org/paper/02eaaf87f9cae34cca398fed146079e6eeb1f868",
        "title": "Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2020,
        "citationCount": 1139,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/2020.acl-main.463.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2020.acl-main.463, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2471699",
                "name": "Emily M. Bender"
            },
            {
                "authorId": "145542037",
                "name": "Alexander Koller"
            }
        ],
        "abstract": "The success of the large neural language models on many NLP tasks is exciting. However, we find that these successes sometimes lead to hype in which these models are being described as understanding language or capturing meaning. In this position paper, we argue that a system trained only on form has a priori no way to learn meaning. In keeping with the ACL 2020 theme of Taking Stock of Where Weve Been and Where Were Going, we argue that a clear understanding of the distinction between form and meaning will help guide the field towards better science around natural language understanding."
    },
    {
        "paperId": "0e141942fa265142f41a2a26eb17b6005d3af29e",
        "url": "https://www.semanticscholar.org/paper/0e141942fa265142f41a2a26eb17b6005d3af29e",
        "title": "The State and Fate of Linguistic Diversity and Inclusion in the NLP World",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2020,
        "citationCount": 1061,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/2020.acl-main.560.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2004.09095, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "47070483",
                "name": "Pratik M. Joshi"
            },
            {
                "authorId": "50074956",
                "name": "Sebastin Santy"
            },
            {
                "authorId": "2410839",
                "name": "A. Budhiraja"
            },
            {
                "authorId": "3086996",
                "name": "Kalika Bali"
            },
            {
                "authorId": "143990839",
                "name": "M. Choudhury"
            }
        ],
        "abstract": "Language technologies contribute to promoting multilingualism and linguistic diversity around the world. However, only a very small number of the over 7000 languages of the world are represented in the rapidly evolving language technologies and applications. In this paper we look at the relation between the types of languages, resources, and their representation in NLP conferences to understand the trajectory that different languages have followed over time. Our quantitative investigation underlines the disparity between languages, especially in terms of their resources, and calls into question the language agnostic status of current models and systems. Through this paper, we attempt to convince the ACL community to prioritise the resolution of the predicaments highlighted here, so that no language is left behind."
    },
    {
        "paperId": "33ec7eb2168e37e3007d1059aa96b9a63254b4da",
        "url": "https://www.semanticscholar.org/paper/33ec7eb2168e37e3007d1059aa96b9a63254b4da",
        "title": "Beyond Accuracy: Behavioral Testing of NLP Models with CheckList",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2020,
        "citationCount": 1230,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/2020.acl-main.442.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2005.04118, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "78846919",
                "name": "Marco Tulio Ribeiro"
            },
            {
                "authorId": "35232494",
                "name": "Tongshuang Sherry Wu"
            },
            {
                "authorId": "1730156",
                "name": "Carlos Guestrin"
            },
            {
                "authorId": "34650964",
                "name": "Sameer Singh"
            }
        ],
        "abstract": "Although measuring held-out accuracy has been the primary approach to evaluate generalization, it often overestimates the performance of NLP models, while alternative approaches for evaluating models either focus on individual tasks or on specific behaviors. Inspired by principles of behavioral testing in software engineering, we introduce CheckList, a task-agnostic methodology for testing NLP models. CheckList includes a matrix of general linguistic capabilities and test types that facilitate comprehensive test ideation, as well as a software tool to generate a large and diverse number of test cases quickly. We illustrate the utility of CheckList with tests for three tasks, identifying critical failures in both commercial and state-of-art models. In a user study, a team responsible for a commercial sentiment analysis model found new and actionable bugs in an extensively tested model. In another user study, NLP practitioners with CheckList created twice as many tests, and found almost three times as many bugs as users without it."
    },
    {
        "paperId": "4ae52766028e69186052ea8f33a137fbbbdb986a",
        "url": "https://www.semanticscholar.org/paper/4ae52766028e69186052ea8f33a137fbbbdb986a",
        "title": "BLEURT: Learning Robust Metrics for Text Generation",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2020,
        "citationCount": 1733,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/2020.acl-main.704.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2004.04696, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "145450400",
                "name": "Thibault Sellam"
            },
            {
                "authorId": "143790066",
                "name": "Dipanjan Das"
            },
            {
                "authorId": "144729897",
                "name": "Ankur P. Parikh"
            }
        ],
        "abstract": "Text generation has made significant advances in the last few years. Yet, evaluation metrics have lagged behind, as the most popular choices (e.g., BLEU and ROUGE) may correlate poorly with human judgment. We propose BLEURT, a learned evaluation metric for English based on BERT. BLEURT can model human judgment with a few thousand possibly biased training examples. A key aspect of our approach is a novel pre-training scheme that uses millions of synthetic examples to help the model generalize. BLEURT provides state-of-the-art results on the last three years of the WMT Metrics shared task and the WebNLG data set. In contrast to a vanilla BERT-based approach, it yields superior results even when the training data is scarce and out-of-distribution."
    },
    {
        "paperId": "641a9749fe546a02bbab9a86bfc91492db1c3bc5",
        "url": "https://www.semanticscholar.org/paper/641a9749fe546a02bbab9a86bfc91492db1c3bc5",
        "title": "Stanza: A Python Natural Language Processing Toolkit for Many Human Languages",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2020,
        "citationCount": 1890,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/2020.acl-demos.14.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2003.07082, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "50531624",
                "name": "Peng Qi"
            },
            {
                "authorId": "49889487",
                "name": "Yuhao Zhang"
            },
            {
                "authorId": "49889860",
                "name": "Yuhui Zhang"
            },
            {
                "authorId": "40756403",
                "name": "Jason Bolton"
            },
            {
                "authorId": "144783904",
                "name": "Christopher D. Manning"
            }
        ],
        "abstract": "We introduce Stanza, an open-source Python natural language processing toolkit supporting 66 human languages. Compared to existing widely used toolkits, Stanza features a language-agnostic fully neural pipeline for text analysis, including tokenization, multi-word token expansion, lemmatization, part-of-speech and morphological feature tagging, dependency parsing, and named entity recognition. We have trained Stanza on a total of 112 datasets, including the Universal Dependencies treebanks and other multilingual corpora, and show that the same neural architecture generalizes well and achieves competitive performance on all languages tested. Additionally, Stanza includes a native Python interface to the widely used Java Stanford CoreNLP software, which further extends its functionality to cover other tasks such as coreference resolution and relation extraction. Source code, documentation, and pretrained models for 66 languages are available at https://stanfordnlp.github.io/stanza/."
    },
    {
        "paperId": "b896b846ae180d804c7290d8b9ae9ffc55325866",
        "url": "https://www.semanticscholar.org/paper/b896b846ae180d804c7290d8b9ae9ffc55325866",
        "title": "Language-agnostic BERT Sentence Embedding",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2020,
        "citationCount": 1103,
        "openAccessPdf": {
            "url": "https://aclanthology.org/2022.acl-long.62.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2007.01852, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3129590",
                "name": "Fangxiaoyu Feng"
            },
            {
                "authorId": "2118771180",
                "name": "Yinfei Yang"
            },
            {
                "authorId": "46724030",
                "name": "Daniel Matthew Cer"
            },
            {
                "authorId": "3365231",
                "name": "N. Arivazhagan"
            },
            {
                "authorId": "2158624629",
                "name": "Wei Wang"
            }
        ],
        "abstract": "While BERT is an effective method for learning monolingual sentence embeddings for semantic similarity and embedding based transfer learning BERT based cross-lingual sentence embeddings have yet to be explored. We systematically investigate methods for learning multilingual sentence embeddings by combining the best methods for learning monolingual and cross-lingual representations including: masked language modeling (MLM), translation language modeling (TLM), dual encoder translation ranking, and additive margin softmax. We show that introducing a pre-trained multilingual language model dramatically reduces the amount of parallel training data required to achieve good performance by 80%. Composing the best of these methods produces a model that achieves 83.7% bi-text retrieval accuracy over 112 languages on Tatoeba, well above the 65.5% achieved by LASER, while still performing competitively on monolingual transfer learning benchmarks. Parallel data mined from CommonCrawl using our best model is shown to train competitive NMT models for en-zh and en-de. We publicly release our best multilingual sentence embedding model for 109+ languages at https://tfhub.dev/google/LaBSE."
    },
    {
        "paperId": "babeda48b10a4d638252118f2238d05a06f4ec55",
        "url": "https://www.semanticscholar.org/paper/babeda48b10a4d638252118f2238d05a06f4ec55",
        "title": "StereoSet: Measuring stereotypical bias in pretrained language models",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2020,
        "citationCount": 1209,
        "openAccessPdf": {
            "url": "https://aclanthology.org/2021.acl-long.416.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2004.09456, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "50411022",
                "name": "Moin Nadeem"
            },
            {
                "authorId": "78850252",
                "name": "Anna Bethke"
            },
            {
                "authorId": "145732771",
                "name": "Siva Reddy"
            }
        ],
        "abstract": "A stereotype is an over-generalized belief about a particular group of people, e.g., Asians are good at math or African Americans are athletic. Such beliefs (biases) are known to hurt target groups. Since pretrained language models are trained on large real-world data, they are known to capture stereotypical biases. It is important to quantify to what extent these biases are present in them. Although this is a rapidly growing area of research, existing literature lacks in two important aspects: 1) they mainly evaluate bias of pretrained language models on a small set of artificial sentences, even though these models are trained on natural data 2) current evaluations focus on measuring bias without considering the language modeling ability of a model, which could lead to misleading trust on a model even if it is a poor language model. We address both these problems. We present StereoSet, a large-scale natural English dataset to measure stereotypical biases in four domains: gender, profession, race, and religion. We contrast both stereotypical bias and language modeling ability of popular models like BERT, GPT-2, RoBERTa, and XLnet. We show that these models exhibit strong stereotypical biases. Our data and code are available at https://stereoset.mit.edu."
    },
    {
        "paperId": "d47a682723f710395454687319bb55635e653105",
        "url": "https://www.semanticscholar.org/paper/d47a682723f710395454687319bb55635e653105",
        "title": "Language (Technology) is Power: A Critical Survey of Bias in NLP",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2020,
        "citationCount": 1481,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/2020.acl-main.485.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2005.14050, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3422038",
                "name": "Su Lin Blodgett"
            },
            {
                "authorId": "2881033",
                "name": "Solon Barocas"
            },
            {
                "authorId": "2065041692",
                "name": "Hal Daum'e"
            },
            {
                "authorId": "1831395",
                "name": "Hanna M. Wallach"
            }
        ],
        "abstract": "We survey 146 papers analyzing bias in NLP systems, finding that their motivations are often vague, inconsistent, and lacking in normative reasoning, despite the fact that analyzing bias is an inherently normative process. We further find that these papers proposed quantitative techniques for measuring or mitigating bias are poorly matched to their motivations and do not engage with the relevant literature outside of NLP. Based on these findings, we describe the beginnings of a path forward by proposing three recommendations that should guide work analyzing bias in NLP systems. These recommendations rest on a greater recognition of the relationships between language and social hierarchies, encouraging researchers and practitioners to articulate their conceptualizations of bias---i.e., what kinds of system behaviors are harmful, in what ways, to whom, and why, as well as the normative reasoning underlying these statementsand to center work around the lived experiences of members of communities affected by NLP systems, while interrogating and reimagining the power relations between technologists and such communities."
    },
    {
        "paperId": "dbeeca8466e0c177ec67c60d529899232415ca87",
        "url": "https://www.semanticscholar.org/paper/dbeeca8466e0c177ec67c60d529899232415ca87",
        "title": "On Faithfulness and Factuality in Abstractive Summarization",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2020,
        "citationCount": 1240,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/2020.acl-main.173.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2005.00661, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "46219105",
                "name": "Joshua Maynez"
            },
            {
                "authorId": "143790499",
                "name": "Shashi Narayan"
            },
            {
                "authorId": "1678747",
                "name": "Bernd Bohnet"
            },
            {
                "authorId": "143957226",
                "name": "Ryan T. McDonald"
            }
        ],
        "abstract": "It is well known that the standard likelihood training and approximate decoding objectives in neural text generation models lead to less human-like responses for open-ended tasks such as language modeling and story generation. In this paper we have analyzed limitations of these models for abstractive document summarization and found that these models are highly prone to hallucinate content that is unfaithful to the input document. We conducted a large scale human evaluation of several neural abstractive summarization systems to better understand the types of hallucinations they produce. Our human annotators found substantial amounts of hallucinated content in all model generated summaries. However, our analysis does show that pretrained models are better summarizers not only in terms of raw metrics, i.e., ROUGE, but also in generating faithful and factual summaries as evaluated by humans. Furthermore, we show that textual entailment measures better correlate with faithfulness than standard metrics, potentially leading the way to automatic evaluation metrics as well as training and decoding criteria."
    },
    {
        "paperId": "e816f788767eec6a8ef0ea9eddd0e902435d4271",
        "url": "https://www.semanticscholar.org/paper/e816f788767eec6a8ef0ea9eddd0e902435d4271",
        "title": "Dont Stop Pretraining: Adapt Language Models to Domains and Tasks",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2020,
        "citationCount": 2729,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/2020.acl-main.740.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2004.10964, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "40895369",
                "name": "Suchin Gururangan"
            },
            {
                "authorId": "3451494",
                "name": "Ana Marasovi"
            },
            {
                "authorId": "2705113",
                "name": "Swabha Swayamdipta"
            },
            {
                "authorId": "46258841",
                "name": "Kyle Lo"
            },
            {
                "authorId": "46181066",
                "name": "Iz Beltagy"
            },
            {
                "authorId": "145612610",
                "name": "Doug Downey"
            },
            {
                "authorId": "144365875",
                "name": "Noah A. Smith"
            }
        ],
        "abstract": "Language models pretrained on text from a wide variety of sources form the foundation of todays NLP. In light of the success of these broad-coverage models, we investigate whether it is still helpful to tailor a pretrained model to the domain of a target task. We present a study across four domains (biomedical and computer science publications, news, and reviews) and eight classification tasks, showing that a second phase of pretraining in-domain (domain-adaptive pretraining) leads to performance gains, under both high- and low-resource settings. Moreover, adapting to the tasks unlabeled data (task-adaptive pretraining) improves performance even after domain-adaptive pretraining. Finally, we show that adapting to a task corpus augmented using simple data selection strategies is an effective alternative, especially when resources for domain-adaptive pretraining might be unavailable. Overall, we consistently find that multi-phase adaptive pretraining offers large gains in task performance."
    },
    {
        "paperId": "0adec918885dff698acf359988ed79a543157f80",
        "url": "https://www.semanticscholar.org/paper/0adec918885dff698acf359988ed79a543157f80",
        "title": "Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2021,
        "citationCount": 1366,
        "openAccessPdf": {
            "url": "https://aclanthology.org/2022.acl-long.556.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2104.08786, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "40282678",
                "name": "Yao Lu"
            },
            {
                "authorId": "153408953",
                "name": "Max Bartolo"
            },
            {
                "authorId": "51114267",
                "name": "Alastair Moore"
            },
            {
                "authorId": "48662861",
                "name": "Sebastian Riedel"
            },
            {
                "authorId": "1918552",
                "name": "Pontus Stenetorp"
            }
        ],
        "abstract": "When primed with only a handful of training samples, very large, pretrained language models such as GPT-3 have shown competitive results when compared to fully-supervised, fine-tuned, large, pretrained language models. We demonstrate that the order in which the samples are provided can make the difference between near state-of-the-art and random guess performance: essentially some permutations are fantastic and some not. We analyse this phenomenon in detail, establishing that: it is present across model sizes (even for the largest current models), it is not related to a specific subset of samples, and that a given good permutation for one model is not transferable to another. While one could use a development set to determine which permutations are performant, this would deviate from the true few-shot setting as it requires additional annotated data. Instead, we use the generative nature of language models to construct an artificial development set and based on entropy statistics of the candidate permutations on this set, we identify performant prompts. Our method yields a 13% relative improvement for GPT-family models across eleven different established text classification tasks."
    },
    {
        "paperId": "339b2b711fb5b228d097b03ebc3e62a521779235",
        "url": "https://www.semanticscholar.org/paper/339b2b711fb5b228d097b03ebc3e62a521779235",
        "title": "BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2021,
        "citationCount": 1526,
        "openAccessPdf": {
            "url": "https://aclanthology.org/2022.acl-short.1.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2106.10199, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2082648761",
                "name": "Elad Ben-Zaken"
            },
            {
                "authorId": "51432464",
                "name": "Shauli Ravfogel"
            },
            {
                "authorId": "79775260",
                "name": "Yoav Goldberg"
            }
        ],
        "abstract": "We introduce BitFit, a sparse-finetuning method where only the bias-terms of the model (or a subset of them) are being modified. We show that with small-to-medium training data, applying BitFit on pre-trained BERT models is competitive with (and sometimes better than) fine-tuning the entire model. For larger data, the method is competitive with other sparse fine-tuning methods.Besides their practical utility, these findings are relevant for the question of understanding the commonly-used process of finetuning: they support the hypothesis that finetuning is mainly about exposing knowledge induced by language-modeling training, rather than learning new task-specific linguistic knowledge."
    },
    {
        "paperId": "50796b0f3edf9cb5ff1e447c298b33755378aa4f",
        "url": "https://www.semanticscholar.org/paper/50796b0f3edf9cb5ff1e447c298b33755378aa4f",
        "title": "GLM: General Language Model Pretraining with Autoregressive Blank Infilling",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2021,
        "citationCount": 1777,
        "openAccessPdf": {
            "url": "https://aclanthology.org/2022.acl-long.26.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2103.10360, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "66395694",
                "name": "Zhengxiao Du"
            },
            {
                "authorId": "5606742",
                "name": "Yujie Qian"
            },
            {
                "authorId": "2111312892",
                "name": "Xiao Liu"
            },
            {
                "authorId": "145573466",
                "name": "Ming Ding"
            },
            {
                "authorId": "40125294",
                "name": "J. Qiu"
            },
            {
                "authorId": "2109512754",
                "name": "Zhilin Yang"
            },
            {
                "authorId": "2109541439",
                "name": "Jie Tang"
            }
        ],
        "abstract": "There have been various types of pretraining architectures including autoencoding models (e.g., BERT), autoregressive models (e.g., GPT), and encoder-decoder models (e.g., T5). However, none of the pretraining frameworks performs the best for all tasks of three main categories including natural language understanding (NLU), unconditional generation, and conditional generation. We propose a General Language Model (GLM) based on autoregressive blank infilling to address this challenge. GLM improves blank filling pretraining by adding 2D positional encodings and allowing an arbitrary order to predict spans, which results in performance gains over BERT and T5 on NLU tasks. Meanwhile, GLM can be pretrained for different types of tasks by varying the number and lengths of blanks. On a wide range of tasks across NLU, conditional and unconditional generation, GLM outperforms BERT, T5, and GPT given the same model sizes and data, and achieves the best performance from a single pretrained model with 1.25 parameters of BERT Large , demonstrating its generalizability to different downstream tasks."
    },
    {
        "paperId": "53d8b356551a2361020a948f64454a6d599af69f",
        "url": "https://www.semanticscholar.org/paper/53d8b356551a2361020a948f64454a6d599af69f",
        "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2021,
        "citationCount": 5165,
        "openAccessPdf": {
            "url": "https://aclanthology.org/2021.acl-long.353.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2101.00190, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "32551341",
                "name": "Xiang Lisa Li"
            },
            {
                "authorId": "145419642",
                "name": "Percy Liang"
            }
        ],
        "abstract": "Fine-tuning is the de facto way of leveraging large pretrained language models for downstream tasks. However, fine-tuning modifies all the language model parameters and therefore necessitates storing a full copy for each task. In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural language generation tasks, which keeps language model parameters frozen and instead optimizes a sequence of continuous task-specific vectors, which we call the prefix. Prefix-tuning draws inspiration from prompting for language models, allowing subsequent tokens to attend to this prefix as if it were virtual tokens. We apply prefix-tuning to GPT-2 for table-to-text generation and to BART for summarization. We show that by learning only 0.1% of the parameters, prefix-tuning obtains comparable performance in the full data setting, outperforms fine-tuning in low-data settings, and extrapolates better to examples with topics that are unseen during training."
    },
    {
        "paperId": "77d956cdab4508d569ae5741549b78e715fd0749",
        "url": "https://www.semanticscholar.org/paper/77d956cdab4508d569ae5741549b78e715fd0749",
        "title": "TruthfulQA: Measuring How Models Mimic Human Falsehoods",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2021,
        "citationCount": 2629,
        "openAccessPdf": {
            "url": "https://aclanthology.org/2022.acl-long.229.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2109.07958, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "48639938",
                "name": "Stephanie C. Lin"
            },
            {
                "authorId": "2052366271",
                "name": "Jacob Hilton"
            },
            {
                "authorId": "47107786",
                "name": "Owain Evans"
            }
        ],
        "abstract": "We propose a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. We crafted questions that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from imitating human texts. We tested GPT-3, GPT-Neo/J, GPT-2 and a T5-based model. The best model was truthful on 58% of questions, while human performance was 94%. Models generated many false answers that mimic popular misconceptions and have the potential to deceive humans. The largest models were generally the least truthful. This contrasts with other NLP tasks, where performance improves with model size. However, this result is expected if false answers are learned from the training distribution. We suggest that scaling up models alone is less promising for improving truthfulness than fine-tuning using training objectives other than imitation of text from the web."
    },
    {
        "paperId": "85e7d63f75c0916bd350a229e040c5fbb1472e7a",
        "url": "https://www.semanticscholar.org/paper/85e7d63f75c0916bd350a229e040c5fbb1472e7a",
        "title": "Making Pre-trained Language Models Better Few-shot Learners",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2021,
        "citationCount": 2174,
        "openAccessPdf": {
            "url": "https://aclanthology.org/2021.acl-long.295.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2012.15723, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "4800645",
                "name": "Tianyu Gao"
            },
            {
                "authorId": "2064150446",
                "name": "Adam Fisch"
            },
            {
                "authorId": "50536468",
                "name": "Danqi Chen"
            }
        ],
        "abstract": "The recent GPT-3 model (Brown et al., 2020) achieves remarkable few-shot performance solely by leveraging a natural-language prompt and a few task demonstrations as input context. Inspired by their findings, we study few-shot learning in a more practical scenario, where we use smaller language models for which fine-tuning is computationally efficient. We present LM-BFFbetter few-shot fine-tuning of language modelsa suite of simple and complementary techniques for fine-tuning language models on a small number of annotated examples. Our approach includes (1) prompt-based fine-tuning together with a novel pipeline for automating prompt generation; and (2) a refined strategy for dynamically and selectively incorporating demonstrations into each context. Finally, we present a systematic evaluation for analyzing few-shot performance on a range of NLP tasks, including classification and regression. Our experiments demonstrate that our methods combine to dramatically outperform standard fine-tuning procedures in this low resource setting, achieving up to 30% absolute improvement, and 11% on average across all tasks. Our approach makes minimal assumptions on task resources and domain expertise, and hence constitutes a strong task-agnostic method for few-shot learning."
    },
    {
        "paperId": "663a41c866d49ce052801fbc88947d39764cad29",
        "url": "https://www.semanticscholar.org/paper/663a41c866d49ce052801fbc88947d39764cad29",
        "title": "Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2022,
        "citationCount": 1529,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2210.09261",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2210.09261, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "51903517",
                "name": "Mirac Suzgun"
            },
            {
                "authorId": "1471909492",
                "name": "Nathan Scales"
            },
            {
                "authorId": "1821614764",
                "name": "Nathanael Scharli"
            },
            {
                "authorId": "3159346",
                "name": "Sebastian Gehrmann"
            },
            {
                "authorId": "144447820",
                "name": "Yi Tay"
            },
            {
                "authorId": "3351938",
                "name": "Hyung Won Chung"
            },
            {
                "authorId": "2841893",
                "name": "A. Chowdhery"
            },
            {
                "authorId": "2827616",
                "name": "Quoc V. Le"
            },
            {
                "authorId": "2226805",
                "name": "Ed H. Chi"
            },
            {
                "authorId": "65855107",
                "name": "Denny Zhou"
            },
            {
                "authorId": "119640649",
                "name": "Jason Wei"
            }
        ],
        "abstract": "BIG-Bench (Srivastava et al., 2022) is a diverse evaluation suite that focuses on tasks believed to be beyond the capabilities of current language models. Language models have already made good progress on this benchmark, with the best model in the BIG-Bench paper outperforming average reported human-rater results on 65% of the BIG-Bench tasks via few-shot prompting. But on what tasks do language models fall short of average human-rater performance, and are those tasks actually unsolvable by current language models? In this work, we focus on a suite of 23 challenging BIG-Bench tasks which we call BIG-Bench Hard (BBH). These are the task for which prior language model evaluations did not outperform the average human-rater. We find that applying chain-of-thought (CoT) prompting to BBH tasks enables PaLM to surpass the average human-rater performance on 10 of the 23 tasks, and Codex (code-davinci-002) to surpass the average human-rater performance on 17 of the 23 tasks. Since many tasks in BBH require multi-step reasoning, few-shot prompting without CoT, as done in the BIG-Bench evaluations (Srivastava et al., 2022), substantially underestimates the best performance and capabilities of language models, which is better captured via CoT prompting. As further analysis, we explore the interaction between CoT and model scale on BBH, finding that CoT enables emergent task performance on several BBH tasks with otherwise flat scaling curves."
    },
    {
        "paperId": "e65b346d442e9962a4276dc1c1af2956d9d5f1eb",
        "url": "https://www.semanticscholar.org/paper/e65b346d442e9962a4276dc1c1af2956d9d5f1eb",
        "title": "Self-Instruct: Aligning Language Models with Self-Generated Instructions",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "year": 2022,
        "citationCount": 2793,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2212.10560",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2212.10560, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1705260",
                "name": "Yizhong Wang"
            },
            {
                "authorId": "2156538832",
                "name": "Yeganeh Kordi"
            },
            {
                "authorId": "1817207",
                "name": "Swaroop Mishra"
            },
            {
                "authorId": "94500147",
                "name": "Alisa Liu"
            },
            {
                "authorId": "144365875",
                "name": "Noah A. Smith"
            },
            {
                "authorId": "1783281",
                "name": "Daniel Khashabi"
            },
            {
                "authorId": "2548384",
                "name": "Hannaneh Hajishirzi"
            }
        ],
        "abstract": "Large instruction-tuned language models (i.e., finetuned to respond to instructions) have demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless, they depend heavily on human-written instruction data that is often limited in quantity, diversity, and creativity, therefore hindering the generality of the tuned model. We introduce Self-Instruct, a framework for improving the instruction-following capabilities of pretrained language models by bootstrapping off their own generations. Our pipeline generates instructions, input, and output samples from a language model, then filters invalid or similar ones before using them to finetune the original model. Applying our method to the vanilla GPT3, we demonstrate a 33% absolute improvement over the original model on Super-NaturalInstructions, on par with the performance of InstructGPT-001, which was trained with private user data and human annotations. For further evaluation, we curate a set of expert-written instructions for novel tasks, and show through human evaluation that tuning GPT3 with Self-Instruct outperforms using existing public instruction datasets by a large margin, leaving only a 5% absolute gap behind InstructGPT-001. Self-Instruct provides an almost annotation-free method for aligning pre-trained language models with instructions, and we release our large synthetic dataset to facilitate future studies on instruction tuning."
    },
    {
        "paperId": "63303c61889dae39895a08b8d910e4511cd2a545",
        "url": "https://www.semanticscholar.org/paper/63303c61889dae39895a08b8d910e4511cd2a545",
        "title": "Probabilistic Linear Discriminant Analysis for Inferences About Identity",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2007,
        "citationCount": 1142,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICCV.2007.4409052?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICCV.2007.4409052, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "144827387",
                "name": "S. Prince"
            },
            {
                "authorId": "1792404",
                "name": "J. Elder"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "804836b8ad86ef8042e3dcbd45442a52f031ee03",
        "url": "https://www.semanticscholar.org/paper/804836b8ad86ef8042e3dcbd45442a52f031ee03",
        "title": "A Database and Evaluation Methodology for Optical Flow",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2007,
        "citationCount": 2871,
        "openAccessPdf": {
            "url": "http://www.cs.brown.edu/people/black/Papers/ofevaltr.pdf",
            "status": "HYBRID",
            "license": "CCBYNC",
            "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s11263-010-0390-2?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s11263-010-0390-2, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "145347688",
                "name": "Simon Baker"
            },
            {
                "authorId": "1709053",
                "name": "D. Scharstein"
            },
            {
                "authorId": "153024876",
                "name": "J. P. Lewis"
            },
            {
                "authorId": "145920814",
                "name": "S. Roth"
            },
            {
                "authorId": "2105795",
                "name": "Michael J. Black"
            },
            {
                "authorId": "1717841",
                "name": "R. Szeliski"
            }
        ],
        "abstract": "The quantitative evaluation of optical flow algorithms by Barron et al. (1994) led to significant advances in performance. The challenges for optical flow algorithms today go beyond the datasets and evaluation methods proposed in that paper. Instead, they center on problems associated with complex natural scenes, including nonrigid motion, real sensor noise, and motion discontinuities. We propose a new set of benchmarks and evaluation methods for the next generation of optical flow algorithms. To that end, we contribute four types of data to test different aspects of optical flow algorithms: (1)sequences with nonrigid motion where the ground-truth flow is determined by tracking hidden fluorescent texture, (2)realistic synthetic sequences, (3)high frame-rate video used to study interpolation error, and (4)modified stereo sequences of static scenes. In addition to the average angular error used by Barron etal., we compute the absolute flow endpoint error, measures for frame interpolation error, improved statistics, and results at motion discontinuities and in textureless regions. In October 2007, we published the performance of several well-known methods on a preliminary version of our data to establish the current state of the art. We also made the data freely available on the web at http://vision.middlebury.edu/flow/. Subsequently a number of researchers have uploaded their results to our website and published papers using the data. A significant improvement in performance has already been achieved. In this paper we analyze the results obtained to date and draw a large number of conclusions from them."
    },
    {
        "paperId": "d175a196816e44c08928ad05e30fd774468d69aa",
        "url": "https://www.semanticscholar.org/paper/d175a196816e44c08928ad05e30fd774468d69aa",
        "title": "Image Classification using Random Forests and Ferns",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2007,
        "citationCount": 1476,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICCV.2007.4409066?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICCV.2007.4409066, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "144423856",
                "name": "Anna Bosch"
            },
            {
                "authorId": "1688869",
                "name": "Andrew Zisserman"
            },
            {
                "authorId": "2062941326",
                "name": "X. Muoz"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "1f88427d7aa8225e47f946ac41a0667d7b69ac52",
        "url": "https://www.semanticscholar.org/paper/1f88427d7aa8225e47f946ac41a0667d7b69ac52",
        "title": "What is the best multi-stage architecture for object recognition?",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2009,
        "citationCount": 2364,
        "openAccessPdf": {
            "url": "http://yann.lecun.com/exdb/publis/pdf/jarrett-iccv-09.pdf",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICCV.2009.5459469?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICCV.2009.5459469, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2077257730",
                "name": "Kevin Jarrett"
            },
            {
                "authorId": "2645384",
                "name": "K. Kavukcuoglu"
            },
            {
                "authorId": "1706809",
                "name": "Marc'Aurelio Ranzato"
            },
            {
                "authorId": "1688882",
                "name": "Yann LeCun"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "55b29a2505149d06d8c1d616cd30edca40cb029c",
        "url": "https://www.semanticscholar.org/paper/55b29a2505149d06d8c1d616cd30edca40cb029c",
        "title": "Poselets: Body part detectors trained using 3D human pose annotations",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2009,
        "citationCount": 1119,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICCV.2009.5459303?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICCV.2009.5459303, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1769383",
                "name": "Lubomir D. Bourdev"
            },
            {
                "authorId": "143751119",
                "name": "Jitendra Malik"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "7098bce6979dc00502f738746c7a6927a3c763d4",
        "url": "https://www.semanticscholar.org/paper/7098bce6979dc00502f738746c7a6927a3c763d4",
        "title": "Fast visibility restoration from a single color or gray level image",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2009,
        "citationCount": 1355,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICCV.2009.5459251?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICCV.2009.5459251, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1682784",
                "name": "Jean-Philippe Tarel"
            },
            {
                "authorId": "3272549",
                "name": "N. Hautire"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "7d7a98a81715e2d3747071722b2deeed8937d122",
        "url": "https://www.semanticscholar.org/paper/7d7a98a81715e2d3747071722b2deeed8937d122",
        "title": "You'll never walk alone: Modeling social behavior for multi-target tracking",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2009,
        "citationCount": 1699,
        "openAccessPdf": {
            "url": "http://vision.cse.psu.edu/courses/Tracking/vlpr12/PellegriniNeverWalkAlone.pdf",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICCV.2009.5459260?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICCV.2009.5459260, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "31458284",
                "name": "S. Pellegrini"
            },
            {
                "authorId": "2433494",
                "name": "Andreas Ess"
            },
            {
                "authorId": "144810819",
                "name": "K. Schindler"
            },
            {
                "authorId": "1681236",
                "name": "L. Gool"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "7ffd2cc1919a3dfd5419d0e7c7656890154e2e18",
        "url": "https://www.semanticscholar.org/paper/7ffd2cc1919a3dfd5419d0e7c7656890154e2e18",
        "title": "Non-local sparse models for image restoration",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2009,
        "citationCount": 1842,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICCV.2009.5459452?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICCV.2009.5459452, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2599292",
                "name": "J. Mairal"
            },
            {
                "authorId": "144570279",
                "name": "F. Bach"
            },
            {
                "authorId": "144189388",
                "name": "J. Ponce"
            },
            {
                "authorId": "1699339",
                "name": "G. Sapiro"
            },
            {
                "authorId": "1688869",
                "name": "Andrew Zisserman"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "89f62dd328cb3e0e7b1a7e03e5b39084b4a91d18",
        "url": "https://www.semanticscholar.org/paper/89f62dd328cb3e0e7b1a7e03e5b39084b4a91d18",
        "title": "Super-resolution from a single image",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2009,
        "citationCount": 1996,
        "openAccessPdf": {
            "url": "http://www.wisdom.weizmann.ac.il/~vision/single_image_SR/files/single_image_SR.pdf",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICCV.2009.5459271?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICCV.2009.5459271, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1752652",
                "name": "Daniel Glasner"
            },
            {
                "authorId": "1944189",
                "name": "Shai Bagon"
            },
            {
                "authorId": "144611617",
                "name": "M. Irani"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "9922e9f7c87b859df2e8af355f03744d33023f0e",
        "url": "https://www.semanticscholar.org/paper/9922e9f7c87b859df2e8af355f03744d33023f0e",
        "title": "Building Rome in a day",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2009,
        "citationCount": 2438,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1145/2001269.2001293?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/2001269.2001293, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "144908136",
                "name": "Sameer Agarwal"
            },
            {
                "authorId": "1798912",
                "name": "Yasutaka Furukawa"
            },
            {
                "authorId": "1830653",
                "name": "Noah Snavely"
            },
            {
                "authorId": "35577716",
                "name": "Ian Simon"
            },
            {
                "authorId": "143800609",
                "name": "B. Curless"
            },
            {
                "authorId": "1679223",
                "name": "S. Seitz"
            },
            {
                "authorId": "1717841",
                "name": "R. Szeliski"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "c7614898de1cfee1a3a1c564f52c1e67879c29b3",
        "url": "https://www.semanticscholar.org/paper/c7614898de1cfee1a3a1c564f52c1e67879c29b3",
        "title": "Learning to predict where humans look",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2009,
        "citationCount": 2144,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICCV.2009.5459462?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICCV.2009.5459462, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "143775672",
                "name": "Tilke Judd"
            },
            {
                "authorId": "1865091",
                "name": "Krista A. Ehinger"
            },
            {
                "authorId": "145403226",
                "name": "F. Durand"
            },
            {
                "authorId": "143805211",
                "name": "A. Torralba"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "d3046251ec5d6e7f90ef5ef2b0ac885c01138555",
        "url": "https://www.semanticscholar.org/paper/d3046251ec5d6e7f90ef5ef2b0ac885c01138555",
        "title": "Attribute and simile classifiers for face verification",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2009,
        "citationCount": 1693,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICCV.2009.5459250?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICCV.2009.5459250, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "144996078",
                "name": "Neeraj Kumar"
            },
            {
                "authorId": "39668247",
                "name": "A. Berg"
            },
            {
                "authorId": "1767767",
                "name": "P. Belhumeur"
            },
            {
                "authorId": "1750470",
                "name": "S. Nayar"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "b4d2151e29fb12dbe5d164b430273de65103d39b",
        "url": "https://www.semanticscholar.org/paper/b4d2151e29fb12dbe5d164b430273de65103d39b",
        "title": "Annotated Facial Landmarks in the Wild: A large-scale, real-world database for facial landmark localization",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2011,
        "citationCount": 1060,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICCVW.2011.6130513?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICCVW.2011.6130513, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1993853",
                "name": "Martin Kstinger"
            },
            {
                "authorId": "3202367",
                "name": "Paul Wohlhart"
            },
            {
                "authorId": "1791182",
                "name": "P. Roth"
            },
            {
                "authorId": "144746444",
                "name": "H. Bischof"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "28ef938aaa312d76df988f636e248a6b267b6352",
        "url": "https://www.semanticscholar.org/paper/28ef938aaa312d76df988f636e248a6b267b6352",
        "title": "Unsupervised Visual Domain Adaptation Using Subspace Alignment",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2013,
        "citationCount": 1339,
        "openAccessPdf": {
            "url": "https://lirias.kuleuven.be/bitstream/123456789/436656/2/3689_final_OA.pdf",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICCV.2013.368?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICCV.2013.368, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1688071",
                "name": "Basura Fernando"
            },
            {
                "authorId": "1749327",
                "name": "Amaury Habrard"
            },
            {
                "authorId": "1738336",
                "name": "M. Sebban"
            },
            {
                "authorId": "1704728",
                "name": "T. Tuytelaars"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "869b17632ed4f19f93b3b58dcaa9f0b8e92108f3",
        "url": "https://www.semanticscholar.org/paper/869b17632ed4f19f93b3b58dcaa9f0b8e92108f3",
        "title": "Abnormal Event Detection at 150 FPS in MATLAB",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2013,
        "citationCount": 1236,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICCV.2013.338?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICCV.2013.338, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2208603293",
                "name": "Cewu Lu"
            },
            {
                "authorId": "1788070",
                "name": "Jianping Shi"
            },
            {
                "authorId": "1729056",
                "name": "Jiaya Jia"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "9c495a9b835f7803746fce1f711dad0eeb411112",
        "url": "https://www.semanticscholar.org/paper/9c495a9b835f7803746fce1f711dad0eeb411112",
        "title": "Transfer Feature Learning with Joint Distribution Adaptation",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2013,
        "citationCount": 1775,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICCV.2013.274?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICCV.2013.274, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "35776445",
                "name": "Mingsheng Long"
            },
            {
                "authorId": "2144499343",
                "name": "Jianmin Wang"
            },
            {
                "authorId": "38329336",
                "name": "Guiguang Ding"
            },
            {
                "authorId": "2109517420",
                "name": "Jiaguang Sun"
            },
            {
                "authorId": "144019071",
                "name": "Philip S. Yu"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "d9f7fbbadc09a032d70f751f1fc865ecf044c03f",
        "url": "https://www.semanticscholar.org/paper/d9f7fbbadc09a032d70f751f1fc865ecf044c03f",
        "title": "DeepFlow: Large Displacement Optical Flow with Deep Matching",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2013,
        "citationCount": 1100,
        "openAccessPdf": {
            "url": "https://hal.inria.fr/hal-00873592/file/DeepFlow_iccv2013.pdf",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICCV.2013.175?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICCV.2013.175, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2492127",
                "name": "Philippe Weinzaepfel"
            },
            {
                "authorId": "3428663",
                "name": "Jrme Revaud"
            },
            {
                "authorId": "1753355",
                "name": "Zad Harchaoui"
            },
            {
                "authorId": "2338341226",
                "name": "Cordelia Schmid"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "e2b99083f3b88691af5553b2a715c1fe43e59246",
        "url": "https://www.semanticscholar.org/paper/e2b99083f3b88691af5553b2a715c1fe43e59246",
        "title": "Efficient Image Dehazing with Boundary Constraint and Contextual Regularization",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2013,
        "citationCount": 1045,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICCV.2013.82?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICCV.2013.82, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3182192",
                "name": "Gaofeng Meng"
            },
            {
                "authorId": "2155514829",
                "name": "Ying Wang"
            },
            {
                "authorId": "2407295",
                "name": "Jiangyong Duan"
            },
            {
                "authorId": "1683738",
                "name": "Shiming Xiang"
            },
            {
                "authorId": "144809241",
                "name": "Chunhong Pan"
            }
        ],
        "abstract": "Images captured in foggy weather conditions often suffer from bad visibility. In this paper, we propose an efficient regularization method to remove hazes from a single input image. Our method benefits much from an exploration on the inherent boundary constraint on the transmission function. This constraint, combined with a weighted L1-norm based contextual regularization, is modeled into an optimization problem to estimate the unknown scene transmission. A quite efficient algorithm based on variable splitting is also presented to solve the problem. The proposed method requires only a few general assumptions and can restore a high-quality haze-free image with faithful colors and fine image details. Experimental results on a variety of haze images demonstrate the effectiveness and efficiency of the proposed method."
    },
    {
        "paperId": "f3cb2b0b4c1fc5cd06d20ceabb7ebfccdce90ad8",
        "url": "https://www.semanticscholar.org/paper/f3cb2b0b4c1fc5cd06d20ceabb7ebfccdce90ad8",
        "title": "Anchored Neighborhood Regression for Fast Example-Based Super-Resolution",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2013,
        "citationCount": 1268,
        "openAccessPdf": {
            "url": "https://lirias.kuleuven.be/bitstream/123456789/436247/1/Timofte_Anchored_Neighborhood_Regression_2013_ICCV_paper.pdf",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICCV.2013.241?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICCV.2013.241, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1732855",
                "name": "Radu Timofte"
            },
            {
                "authorId": "2824105",
                "name": "V. Smet"
            },
            {
                "authorId": "1681236",
                "name": "L. Gool"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "6424b69f3ff4d35249c0bb7ef912fbc2c86f4ff4",
        "url": "https://www.semanticscholar.org/paper/6424b69f3ff4d35249c0bb7ef912fbc2c86f4ff4",
        "title": "Deep Learning Face Attributes in the Wild",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2014,
        "citationCount": 9158,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/1411.7766",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1411.7766, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2117940996",
                "name": "Ziwei Liu"
            },
            {
                "authorId": "47571885",
                "name": "Ping Luo"
            },
            {
                "authorId": "31843833",
                "name": "Xiaogang Wang"
            },
            {
                "authorId": "50295995",
                "name": "Xiaoou Tang"
            }
        ],
        "abstract": "Predicting face attributes in the wild is challenging due to complex face variations. We propose a novel deep learning framework for attribute prediction in the wild. It cascades two CNNs, LNet and ANet, which are fine-tuned jointly with attribute tags, but pre-trained differently. LNet is pre-trained by massive general object categories for face localization, while ANet is pre-trained by massive face identities for attribute prediction. This framework not only outperforms the state-of-the-art with a large margin, but also reveals valuable facts on learning face representation. (1) It shows how the performances of face localization (LNet) and attribute prediction (ANet) can be improved by different pre-training strategies. (2) It reveals that although the filters of LNet are fine-tuned only with image-level attribute tags, their response maps over entire images have strong indication of face locations. This fact enables training LNet for face localization with only image-level annotations, but without face bounding boxes or landmarks, which are required by all attribute recognition works. (3) It also demonstrates that the high-level hidden neurons of ANet automatically discover semantic concepts after pre-training with massive face identities, and such concepts are significantly enriched after fine-tuning with attribute tags. Each attribute can be well explained with a sparse linear combination of these concepts."
    },
    {
        "paperId": "cb3a2ddcf305e2ec0f6b94af13d1e631ed261bdc",
        "url": "https://www.semanticscholar.org/paper/cb3a2ddcf305e2ec0f6b94af13d1e631ed261bdc",
        "title": "Predicting Depth, Surface Normals and Semantic Labels with a Common Multi-scale Convolutional Architecture",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2014,
        "citationCount": 2782,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1411.4734",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1411.4734, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2060028",
                "name": "D. Eigen"
            },
            {
                "authorId": "2276554",
                "name": "R. Fergus"
            }
        ],
        "abstract": "In this paper we address three different computer vision tasks using a single basic architecture: depth prediction, surface normal estimation, and semantic labeling. We use a multiscale convolutional network that is able to adapt easily to each task using only small modifications, regressing from the input image to the output map directly. Our method progressively refines predictions using a sequence of scales, and captures many image details without any superpixels or low-level segmentation. We achieve state-of-the-art performance on benchmarks for all three tasks."
    },
    {
        "paperId": "d25c65d261ea0e6a458be4c50c40ffe5bc508f77",
        "url": "https://www.semanticscholar.org/paper/d25c65d261ea0e6a458be4c50c40ffe5bc508f77",
        "title": "Learning Spatiotemporal Features with 3D Convolutional Networks",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2014,
        "citationCount": 8888,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/1412.0767",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICCV.2015.510?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICCV.2015.510, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1687325",
                "name": "Du Tran"
            },
            {
                "authorId": "1769383",
                "name": "Lubomir D. Bourdev"
            },
            {
                "authorId": "2276554",
                "name": "R. Fergus"
            },
            {
                "authorId": "1732879",
                "name": "L. Torresani"
            },
            {
                "authorId": "2210374",
                "name": "Manohar Paluri"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "09769e80cdf027db32a1fcb695a1aa0937214763",
        "url": "https://www.semanticscholar.org/paper/09769e80cdf027db32a1fcb695a1aa0937214763",
        "title": "Learning Spatially Regularized Correlation Filters for Visual Tracking",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2015,
        "citationCount": 1942,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1608.05571",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1608.05571, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2488938",
                "name": "Martin Danelljan"
            },
            {
                "authorId": "2176196",
                "name": "Gustav Hger"
            },
            {
                "authorId": "2358803",
                "name": "F. Khan"
            },
            {
                "authorId": "2228323",
                "name": "M. Felsberg"
            }
        ],
        "abstract": "Robust and accurate visual tracking is one of the most challenging computer vision problems. Due to the inherent lack of training data, a robust approach for constructing a target appearance model is crucial. Recently, discriminatively learned correlation filters (DCF) have been successfully applied to address this problem for tracking. These methods utilize a periodic assumption of the training samples to efficiently learn a classifier on all patches in the target neighborhood. However, the periodic assumption also introduces unwanted boundary effects, which severely degrade the quality of the tracking model. We propose Spatially Regularized Discriminative Correlation Filters (SRDCF) for tracking. A spatial regularization component is introduced in the learning to penalize correlation filter coefficients depending on their spatial location. Our SRDCF formulation allows the correlation filters to be learned on a significantly larger set of negative training samples, without corrupting the positive samples. We further propose an optimization strategy, based on the iterative Gauss-Seidel method, for efficient online learning of our SRDCF. Experiments are performed on four benchmark datasets: OTB-2013, ALOV++, OTB-2015, and VOT2014. Our approach achieves state-of-the-art results on all four datasets. On OTB-2013 and OTB-2015, we obtain an absolute gain of 8.0% and 8.2% respectively, in mean overlap precision, compared to the best existing trackers."
    },
    {
        "paperId": "0e6824e137847be0599bb0032e37042ed2ef5045",
        "url": "https://www.semanticscholar.org/paper/0e6824e137847be0599bb0032e37042ed2ef5045",
        "title": "Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2015,
        "citationCount": 2669,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1506.06724",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1506.06724, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2334885409",
                "name": "Yukun Zhu"
            },
            {
                "authorId": "3450996",
                "name": "Ryan Kiros"
            },
            {
                "authorId": "1804104",
                "name": "R. Zemel"
            },
            {
                "authorId": "145124475",
                "name": "R. Salakhutdinov"
            },
            {
                "authorId": "2422559",
                "name": "R. Urtasun"
            },
            {
                "authorId": "143805211",
                "name": "A. Torralba"
            },
            {
                "authorId": "37895334",
                "name": "S. Fidler"
            }
        ],
        "abstract": "Books are a rich source of both fine-grained information, how a character, an object or a scene looks like, as well as high-level semantics, what someone is thinking, feeling and how these states evolve through a story. This paper aims to align books to their movie releases in order to provide rich descriptive explanations for visual content that go semantically far beyond the captions available in the current datasets. To align movies and books we propose a neural sentence embedding that is trained in an unsupervised way from a large corpus of books, as well as a video-text neural embedding for computing similarities between movie clips and sentences in the book. We propose a context-aware CNN to combine information from multiple sources. We demonstrate good quantitative performance for movie/book alignment and show several qualitative examples that showcase the diversity of tasks our model can be used for."
    },
    {
        "paperId": "1ba8376f416e90fe434977ae9f300997667498d2",
        "url": "https://www.semanticscholar.org/paper/1ba8376f416e90fe434977ae9f300997667498d2",
        "title": "Multi-view Convolutional Neural Networks for 3D Shape Recognition",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2015,
        "citationCount": 3460,
        "openAccessPdf": {
            "url": "https://people.cs.umass.edu/%7Eelm/papers/HangSu_3D_arXiv.pdf",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1505.00880, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "144904233",
                "name": "Hang Su"
            },
            {
                "authorId": "35208858",
                "name": "Subhransu Maji"
            },
            {
                "authorId": "2808670",
                "name": "E. Kalogerakis"
            },
            {
                "authorId": "1389846455",
                "name": "E. Learned-Miller"
            }
        ],
        "abstract": "A longstanding question in computer vision concerns the representation of 3D shapes for recognition: should 3D shapes be represented with descriptors operating on their native 3D formats, such as voxel grid or polygon mesh, or can they be effectively represented with view-based descriptors? We address this question in the context of learning to recognize 3D shapes from a collection of their rendered views on 2D images. We first present a standard CNN architecture trained to recognize the shapes' rendered views independently of each other, and show that a 3D shape can be recognized even from a single view at an accuracy far higher than using state-of-the-art 3D shape descriptors. Recognition rates further increase when multiple views of the shapes are provided. In addition, we present a novel CNN architecture that combines information from multiple views of a 3D shape into a single and compact shape descriptor offering even better recognition performance. The same architecture can be applied to accurately recognize human hand-drawn sketches of shapes. We conclude that a collection of 2D views can be highly informative for 3D shape recognition and is amenable to emerging CNN architectures and their derivatives."
    },
    {
        "paperId": "1ec7433aeb4777e7d5c903920ae945e5429d3bc4",
        "url": "https://www.semanticscholar.org/paper/1ec7433aeb4777e7d5c903920ae945e5429d3bc4",
        "title": "Recurrent Network Models for Human Dynamics",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2015,
        "citationCount": 1005,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/1508.00271",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICCV.2015.494?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICCV.2015.494, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2301208909",
                "name": "Katerina Fragkiadaki"
            },
            {
                "authorId": "1736651",
                "name": "S. Levine"
            },
            {
                "authorId": "2986395",
                "name": "Panna Felsen"
            },
            {
                "authorId": "143751119",
                "name": "Jitendra Malik"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "3d3f789a56dca288b2c8e23ef047a2b342184950",
        "url": "https://www.semanticscholar.org/paper/3d3f789a56dca288b2c8e23ef047a2b342184950",
        "title": "Bilinear CNN Models for Fine-Grained Visual Recognition",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2015,
        "citationCount": 1989,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1504.07889, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2144284",
                "name": "Tsung-Yu Lin"
            },
            {
                "authorId": "2895705",
                "name": "Aruni RoyChowdhury"
            },
            {
                "authorId": "35208858",
                "name": "Subhransu Maji"
            }
        ],
        "abstract": "We propose bilinear models, a recognition architecture that consists of two feature extractors whose outputs are multiplied using outer product at each location of the image and pooled to obtain an image descriptor. This architecture can model local pairwise feature interactions in a translationally invariant manner which is particularly useful for fine-grained categorization. It also generalizes various orderless texture descriptors such as the Fisher vector, VLAD and O2P. We present experiments with bilinear models where the feature extractors are based on convolutional neural networks. The bilinear form simplifies gradient computation and allows end-to-end training of both networks using image labels only. Using networks initialized from the ImageNet dataset followed by domain specific fine-tuning we obtain 84.1% accuracy of the CUB-200-2011 dataset requiring only category labels at training time. We present experiments and visualizations that analyze the effects of fine-tuning and the choice two networks on the speed and accuracy of the models. Results show that the architecture compares favorably to the existing state of the art on a number of fine-grained datasets while being substantially simpler and easier to train. Moreover, our most accurate model is fairly efficient running at 8 frames/sec on a NVIDIA Tesla K40 GPU. The source code for the complete system will be made available at http://vis-www.cs.umass.edu/bcnn."
    },
    {
        "paperId": "5c8a6874011640981e4103d120957802fa28f004",
        "url": "https://www.semanticscholar.org/paper/5c8a6874011640981e4103d120957802fa28f004",
        "title": "Hierarchical Convolutional Features for Visual Tracking",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2015,
        "citationCount": 1705,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICCV.2015.352?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICCV.2015.352, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "46658056",
                "name": "Chao Ma"
            },
            {
                "authorId": "3068086",
                "name": "Jia-Bin Huang"
            },
            {
                "authorId": "1795291",
                "name": "Xiaokang Yang"
            },
            {
                "authorId": "1715634",
                "name": "Ming-Hsuan Yang"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "5f425b7abf2ed3172ed060df85bb1885860a297e",
        "url": "https://www.semanticscholar.org/paper/5f425b7abf2ed3172ed060df85bb1885860a297e",
        "title": "Describing Videos by Exploiting Temporal Structure",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2015,
        "citationCount": 1093,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/1502.08029",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1502.08029, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "145095579",
                "name": "L. Yao"
            },
            {
                "authorId": "1730844",
                "name": "Atousa Torabi"
            },
            {
                "authorId": "1979489",
                "name": "Kyunghyun Cho"
            },
            {
                "authorId": "2482072",
                "name": "Nicolas Ballas"
            },
            {
                "authorId": "1972076",
                "name": "C. Pal"
            },
            {
                "authorId": "1777528",
                "name": "H. Larochelle"
            },
            {
                "authorId": "1760871",
                "name": "Aaron C. Courville"
            }
        ],
        "abstract": "Recent progress in using recurrent neural networks (RNNs) for image description has motivated the exploration of their application for video description. However, while images are static, working with videos requires modeling their dynamic temporal structure and then properly integrating that information into a natural language description model. In this context, we propose an approach that successfully takes into account both the local and global temporal structure of videos to produce descriptions. First, our approach incorporates a spatial temporal 3-D convolutional neural network (3-D CNN) representation of the short temporal dynamics. The 3-D CNN representation is trained on video action recognition tasks, so as to produce a representation that is tuned to human motion and behavior. Second we propose a temporal attention mechanism that allows to go beyond local temporal modeling and learns to automatically select the most relevant temporal segments given the text-generating RNN. Our approach exceeds the current state-of-art for both BLEU and METEOR metrics on the Youtube2Text dataset. We also present results on a new, larger and more challenging dataset of paired video and natural language descriptions."
    },
    {
        "paperId": "83f200fdef3f1b1778a3b46eabd44d5e2b305e2e",
        "url": "https://www.semanticscholar.org/paper/83f200fdef3f1b1778a3b46eabd44d5e2b305e2e",
        "title": "Simultaneous Deep Transfer Across Domains and Tasks",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2015,
        "citationCount": 1396,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1510.02192",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1510.02192, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "50196944",
                "name": "Judy Hoffman"
            },
            {
                "authorId": "2368132",
                "name": "Eric Tzeng"
            },
            {
                "authorId": "1753210",
                "name": "Trevor Darrell"
            },
            {
                "authorId": "2903226",
                "name": "Kate Saenko"
            }
        ],
        "abstract": "Recent reports suggest that a generic supervised deep CNN model trained on a large-scale dataset reduces, but does not remove, dataset bias. Fine-tuning deep models in a new domain can require a significant amount of labeled data, which for many applications is simply not available. We propose a new CNN architecture to exploit unlabeled and sparsely labeled target domain data. Our approach simultaneously optimizes for domain invariance to facilitate domain transfer and uses a soft label distribution matching loss to transfer information between tasks. Our proposed adaptation method offers empirical performance which exceeds previously published results on two standard benchmark visual domain adaptation tasks, evaluated across supervised and semi-supervised adaptation settings."
    },
    {
        "paperId": "9c6d223eac55c14ef94447211a7855a032beb11a",
        "url": "https://www.semanticscholar.org/paper/9c6d223eac55c14ef94447211a7855a032beb11a",
        "title": "PoseNet: A Convolutional Network for Real-Time 6-DOF Camera Relocalization",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2015,
        "citationCount": 2309,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/1505.07427",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICCV.2015.336?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICCV.2015.336, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "47645184",
                "name": "Alex Kendall"
            },
            {
                "authorId": "2167293",
                "name": "M. Grimes"
            },
            {
                "authorId": "1745672",
                "name": "R. Cipolla"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "c2fb5b39428818d7ec8cc78e152e19c21b7db568",
        "url": "https://www.semanticscholar.org/paper/c2fb5b39428818d7ec8cc78e152e19c21b7db568",
        "title": "FlowNet: Learning Optical Flow with Convolutional Networks",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2015,
        "citationCount": 4465,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1504.06852",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1504.06852, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2841331",
                "name": "Alexey Dosovitskiy"
            },
            {
                "authorId": "152702479",
                "name": "P. Fischer"
            },
            {
                "authorId": "48105320",
                "name": "Eddy Ilg"
            },
            {
                "authorId": "2880264",
                "name": "Philip Husser"
            },
            {
                "authorId": "3322806",
                "name": "C. Hazirbas"
            },
            {
                "authorId": "2943639",
                "name": "Vladimir Golkov"
            },
            {
                "authorId": "1715782",
                "name": "Patrick van der Smagt"
            },
            {
                "authorId": "1695302",
                "name": "D. Cremers"
            },
            {
                "authorId": "1710872",
                "name": "T. Brox"
            }
        ],
        "abstract": "Convolutional neural networks (CNNs) have recently been very successful in a variety of computer vision tasks, especially on those linked to recognition. Optical flow estimation has not been among the tasks CNNs succeeded at. In this paper we construct CNNs which are capable of solving the optical flow estimation problem as a supervised learning task. We propose and compare two architectures: a generic architecture and another one including a layer that correlates feature vectors at different image locations. Since existing ground truth data sets are not sufficiently large to train a CNN, we generate a large synthetic Flying Chairs dataset. We show that networks trained on this unrealistic data still generalize very well to existing datasets such as Sintel and KITTI, achieving competitive accuracy at frame rates of 5 to 10 fps."
    },
    {
        "paperId": "ca5c766b2d31a1f5ce8896a0a42b40a2bff9323a",
        "url": "https://www.semanticscholar.org/paper/ca5c766b2d31a1f5ce8896a0a42b40a2bff9323a",
        "title": "Conditional Random Fields as Recurrent Neural Networks",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2015,
        "citationCount": 2573,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/1502.03240",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1502.03240, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "40474289",
                "name": "Shuai Zheng"
            },
            {
                "authorId": "3078751",
                "name": "Sadeep Jayasumana"
            },
            {
                "authorId": "1403031665",
                "name": "Bernardino Romera-Paredes"
            },
            {
                "authorId": "143729959",
                "name": "Vibhav Vineet"
            },
            {
                "authorId": "3118650",
                "name": "Zhizhong Su"
            },
            {
                "authorId": "40359161",
                "name": "Dalong Du"
            },
            {
                "authorId": "48908475",
                "name": "Chang Huang"
            },
            {
                "authorId": "143635540",
                "name": "Philip H. S. Torr"
            }
        ],
        "abstract": "Pixel-level labelling tasks, such as semantic segmentation, play a central role in image understanding. Recent approaches have attempted to harness the capabilities of deep learning techniques for image recognition to tackle pixel-level labelling tasks. One central issue in this methodology is the limited capacity of deep learning techniques to delineate visual objects. To solve this problem, we introduce a new form of convolutional neural network that combines the strengths of Convolutional Neural Networks (CNNs) and Conditional Random Fields (CRFs)-based probabilistic graphical modelling. To this end, we formulate Conditional Random Fields with Gaussian pairwise potentials and mean-field approximate inference as Recurrent Neural Networks. This network, called CRF-RNN, is then plugged in as a part of a CNN to obtain a deep network that has desirable properties of both CNNs and CRFs. Importantly, our system fully integrates CRF modelling with CNNs, making it possible to train the whole deep network end-to-end with the usual back-propagation algorithm, avoiding offline post-processing methods for object delineation. We apply the proposed method to the problem of semantic image segmentation, obtaining top results on the challenging Pascal VOC 2012 segmentation benchmark."
    },
    {
        "paperId": "cf986bfe13a24d4739f95df3a856a3c6e4ed4c1c",
        "url": "https://www.semanticscholar.org/paper/cf986bfe13a24d4739f95df3a856a3c6e4ed4c1c",
        "title": "Learning Deconvolution Network for Semantic Segmentation",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2015,
        "citationCount": 4303,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1505.04366",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1505.04366, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2018393",
                "name": "Hyeonwoo Noh"
            },
            {
                "authorId": "2241528",
                "name": "Seunghoon Hong"
            },
            {
                "authorId": "40030651",
                "name": "Bohyung Han"
            }
        ],
        "abstract": "We propose a novel semantic segmentation algorithm by learning a deep deconvolution network. We learn the network on top of the convolutional layers adopted from VGG 16-layer net. The deconvolution network is composed of deconvolution and unpooling layers, which identify pixelwise class labels and predict segmentation masks. We apply the trained network to each proposal in an input image, and construct the final semantic segmentation map by combining the results from all proposals in a simple manner. The proposed algorithm mitigates the limitations of the existing methods based on fully convolutional networks by integrating deep deconvolution network and proposal-wise prediction, our segmentation method typically identifies detailed structures and handles objects in multiple scales naturally. Our network demonstrates outstanding performance in PASCAL VOC 2012 dataset, and we achieve the best accuracy (72.5%) among the methods trained without using Microsoft COCO dataset through ensemble with the fully convolutional network."
    },
    {
        "paperId": "d6b93b766dff2066cebbc897c9ec7fbc44848ad7",
        "url": "https://www.semanticscholar.org/paper/d6b93b766dff2066cebbc897c9ec7fbc44848ad7",
        "title": "DeepDriving: Learning Affordance for Direct Perception in Autonomous Driving",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2015,
        "citationCount": 1830,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/1505.00256",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1505.00256, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2364084832",
                "name": "Chenyi Chen"
            },
            {
                "authorId": "2233674",
                "name": "Ari Seff"
            },
            {
                "authorId": "17434392",
                "name": "A. Kornhauser"
            },
            {
                "authorId": "40599257",
                "name": "Jianxiong Xiao"
            }
        ],
        "abstract": "Today, there are two major paradigms for vision-based autonomous driving systems: mediated perception approaches that parse an entire scene to make a driving decision, and behavior reflex approaches that directly map an input image to a driving action by a regressor. In this paper, we propose a third paradigm: a direct perception approach to estimate the affordance for driving. We propose to map an input image to a small number of key perception indicators that directly relate to the affordance of a road/traffic state for driving. Our representation provides a set of compact yet complete descriptions of the scene to enable a simple controller to drive autonomously. Falling in between the two extremes of mediated perception and behavior reflex, we argue that our direct perception representation provides the right level of abstraction. To demonstrate this, we train a deep Convolutional Neural Network using recording from 12 hours of human driving in a video game and show that our model can work well to drive a car in a very diverse set of virtual environments. We also train a model for car distance estimation on the KITTI dataset. Results show that our direct perception approach can generalize well to real driving images. Source code and data are available on our project website."
    },
    {
        "paperId": "d6f2f611da110b5b5061731be3fc4c7f45d8ee23",
        "url": "https://www.semanticscholar.org/paper/d6f2f611da110b5b5061731be3fc4c7f45d8ee23",
        "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2015,
        "citationCount": 19838,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/1502.01852",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1502.01852, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "39353098",
                "name": "Kaiming He"
            },
            {
                "authorId": "1771551",
                "name": "X. Zhang"
            },
            {
                "authorId": "3080683",
                "name": "Shaoqing Ren"
            },
            {
                "authorId": null,
                "name": "Jian Sun"
            }
        ],
        "abstract": "Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on the learnable activation and advanced initialization, we achieve 4.94% top-5 test error on the ImageNet 2012 classification dataset. This is a 26% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66% [33]). To our knowledge, our result is the first to surpass the reported human-level performance (5.1%, [26]) on this dataset."
    },
    {
        "paperId": "e24c261f5cfcd58a595efb7ca684aedcb2a2f22c",
        "url": "https://www.semanticscholar.org/paper/e24c261f5cfcd58a595efb7ca684aedcb2a2f22c",
        "title": "Scalable Person Re-identification: A Benchmark",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2015,
        "citationCount": 4335,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICCV.2015.133?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICCV.2015.133, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "144802394",
                "name": "Liang Zheng"
            },
            {
                "authorId": "1970631",
                "name": "Liyue Shen"
            },
            {
                "authorId": "152755696",
                "name": "Lu Tian"
            },
            {
                "authorId": "1678689",
                "name": "Shengjin Wang"
            },
            {
                "authorId": "1688516",
                "name": "Jingdong Wang"
            },
            {
                "authorId": "144876831",
                "name": "Q. Tian"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "e58a110fa1e4ddf247d5c614d117d64bfbe135c4",
        "url": "https://www.semanticscholar.org/paper/e58a110fa1e4ddf247d5c614d117d64bfbe135c4",
        "title": "Sequence to Sequence -- Video to Text",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2015,
        "citationCount": 1467,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1505.00487",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1505.00487, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1811430",
                "name": "Subhashini Venugopalan"
            },
            {
                "authorId": "34849128",
                "name": "Marcus Rohrbach"
            },
            {
                "authorId": "7408951",
                "name": "Jeff Donahue"
            },
            {
                "authorId": "1797655",
                "name": "R. Mooney"
            },
            {
                "authorId": "1753210",
                "name": "Trevor Darrell"
            },
            {
                "authorId": "2903226",
                "name": "Kate Saenko"
            }
        ],
        "abstract": "Real-world videos often have complex dynamics, methods for generating open-domain video descriptions should be sensitive to temporal structure and allow both input (sequence of frames) and output (sequence of words) of variable length. To approach this problem we propose a novel end-to-end sequence-to-sequence model to generate captions for videos. For this we exploit recurrent neural networks, specifically LSTMs, which have demonstrated state-of-the-art performance in image caption generation. Our LSTM model is trained on video-sentence pairs and learns to associate a sequence of video frames to a sequence of words in order to generate a description of the event in the video clip. Our model naturally is able to learn the temporal structure of the sequence of frames as well as the sequence model of the generated sentences, i.e. a language model. We evaluate several variants of our model that exploit different visual features on a standard set of YouTube videos and two movie description datasets (M-VAD and MPII-MD)."
    },
    {
        "paperId": "f084f0126d48a0793cf7e60830089b93ef09c844",
        "url": "https://www.semanticscholar.org/paper/f084f0126d48a0793cf7e60830089b93ef09c844",
        "title": "BoxSup: Exploiting Bounding Boxes to Supervise Convolutional Networks for Semantic Segmentation",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2015,
        "citationCount": 1088,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/1503.01640",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1503.01640, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3304536",
                "name": "Jifeng Dai"
            },
            {
                "authorId": "39353098",
                "name": "Kaiming He"
            },
            {
                "authorId": null,
                "name": "Jian Sun"
            }
        ],
        "abstract": "Recent leading approaches to semantic segmentation rely on deep convolutional networks trained with human-annotated, pixel-level segmentation masks. Such pixel-accurate supervision demands expensive labeling effort and limits the performance of deep networks that usually benefit from more training data. In this paper, we propose a method that achieves competitive accuracy but only requires easily obtained bounding box annotations. The basic idea is to iterate between automatically generating region proposals and training convolutional networks. These two steps gradually recover segmentation masks for improving the networks, and vise versa. Our method, called \"BoxSup\", produces competitive results (e.g., 62.0% mAP for validation) supervised by boxes only, on par with strong baselines (e.g., 63.8% mAP) fully supervised by masks under the same setting. By leveraging a large amount of bounding boxes, BoxSup further yields state-of-the-art results on PASCAL VOC 2012 and PASCAL-CONTEXT [26]."
    },
    {
        "paperId": "fc1b1c9364c58ec406f494dd944b609a6a038ba6",
        "url": "https://www.semanticscholar.org/paper/fc1b1c9364c58ec406f494dd944b609a6a038ba6",
        "title": "Unsupervised Visual Representation Learning by Context Prediction",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2015,
        "citationCount": 2904,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/1505.05192",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1505.05192, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2786693",
                "name": "Carl Doersch"
            },
            {
                "authorId": "1726095131",
                "name": "A. Gupta"
            },
            {
                "authorId": "1763086",
                "name": "Alexei A. Efros"
            }
        ],
        "abstract": "This work explores the use of spatial context as a source of free and plentiful supervisory signal for training a rich visual representation. Given only a large, unlabeled image collection, we extract random pairs of patches from each image and train a convolutional neural net to predict the position of the second patch relative to the first. We argue that doing well on this task requires the model to learn to recognize objects and their parts. We demonstrate that the feature representation learned using this within-image context indeed captures visual similarity across images. For example, this representation allows us to perform unsupervised visual discovery of objects like cats, people, and even birds from the Pascal VOC 2011 detection dataset. Furthermore, we show that the learned ConvNet can be used in the R-CNN framework [19] and provides a significant boost over a randomly-initialized ConvNet, resulting in state-of-the-art performance among algorithms which use only Pascal-provided training set annotations."
    },
    {
        "paperId": "74ff6d48f9c62e937023106629d27ef2d2ddf8bc",
        "url": "https://www.semanticscholar.org/paper/74ff6d48f9c62e937023106629d27ef2d2ddf8bc",
        "title": "Least Squares Generative Adversarial Networks",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2016,
        "citationCount": 4872,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1611.04076",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1611.04076, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "34443348",
                "name": "Xudong Mao"
            },
            {
                "authorId": "2117895101",
                "name": "Qing Li"
            },
            {
                "authorId": "3607957",
                "name": "Haoran Xie"
            },
            {
                "authorId": "144031692",
                "name": "Raymond Y. K. Lau"
            },
            {
                "authorId": "2118453660",
                "name": "Zhen Wang"
            },
            {
                "authorId": "32309056",
                "name": "Stephen Paul Smolley"
            }
        ],
        "abstract": "Unsupervised learning with generative adversarial networks (GANs) has proven hugely successful. Regular GANs hypothesize the discriminator as a classifier with the sigmoid cross entropy loss function. However, we found that this loss function may lead to the vanishing gradients problem during the learning process. To overcome such a problem, we propose in this paper the Least Squares Generative Adversarial Networks (LSGANs) which adopt the least squares loss function for the discriminator. We show that minimizing the objective function of LSGAN yields minimizing the Pearson X2 divergence. There are two benefits of LSGANs over regular GANs. First, LSGANs are able to generate higher quality images than regular GANs. Second, LSGANs perform more stable during the learning process. We evaluate LSGANs on LSUN and CIFAR-10 datasets and the experimental results show that the images generated by LSGANs are of better quality than the ones generated by regular GANs. We also conduct two comparison experiments between LSGANs and regular GANs to illustrate the stability of LSGANs."
    },
    {
        "paperId": "c70c6dafc7276177225f4604cb285db07881aa6f",
        "url": "https://www.semanticscholar.org/paper/c70c6dafc7276177225f4604cb285db07881aa6f",
        "title": "RMPE: Regional Multi-person Pose Estimation",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2016,
        "citationCount": 1707,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/1612.00137",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1612.00137, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "122851212",
                "name": "Haoshu Fang"
            },
            {
                "authorId": "8483323",
                "name": "Shuqin Xie"
            },
            {
                "authorId": "5068280",
                "name": "Yu-Wing Tai"
            },
            {
                "authorId": "2281998765",
                "name": "Cewu Lu"
            }
        ],
        "abstract": "Multi-person pose estimation in the wild is challenging. Although state-of-the-art human detectors have demonstrated good performance, small errors in localization and recognition are inevitable. These errors can cause failures for a single-person pose estimator (SPPE), especially for methods that solely depend on human detection results. In this paper, we propose a novel regional multi-person pose estimation (RMPE) framework to facilitate pose estimation in the presence of inaccurate human bounding boxes. Our framework consists of three components: Symmetric Spatial Transformer Network (SSTN), Parametric Pose Non-Maximum-Suppression (NMS), and Pose-Guided Proposals Generator (PGPG). Our method is able to handle inaccurate bounding boxes and redundant detections, allowing it to achieve 76:7 mAP on the MPII (multi person) dataset[3]. Our model and source codes are made publicly available."
    },
    {
        "paperId": "ea67d2d5f2a7d5760ec6b67ea93d11dd5affa921",
        "url": "https://www.semanticscholar.org/paper/ea67d2d5f2a7d5760ec6b67ea93d11dd5affa921",
        "title": "StackGAN: Text to Photo-Realistic Image Synthesis with Stacked Generative Adversarial Networks",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2016,
        "citationCount": 2879,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1612.03242",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1612.03242, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": null,
                "name": "Han Zhang"
            },
            {
                "authorId": "2118716442",
                "name": "Tao Xu"
            },
            {
                "authorId": "47893312",
                "name": "Hongsheng Li"
            },
            {
                "authorId": "1753384",
                "name": "Shaoting Zhang"
            },
            {
                "authorId": "31843833",
                "name": "Xiaogang Wang"
            },
            {
                "authorId": "143713756",
                "name": "Xiaolei Huang"
            },
            {
                "authorId": "1711560",
                "name": "Dimitris N. Metaxas"
            }
        ],
        "abstract": "Synthesizing high-quality images from text descriptions is a challenging problem in computer vision and has many practical applications. Samples generated by existing textto- image approaches can roughly reflect the meaning of the given descriptions, but they fail to contain necessary details and vivid object parts. In this paper, we propose Stacked Generative Adversarial Networks (StackGAN) to generate 256.256 photo-realistic images conditioned on text descriptions. We decompose the hard problem into more manageable sub-problems through a sketch-refinement process. The Stage-I GAN sketches the primitive shape and colors of the object based on the given text description, yielding Stage-I low-resolution images. The Stage-II GAN takes Stage-I results and text descriptions as inputs, and generates high-resolution images with photo-realistic details. It is able to rectify defects in Stage-I results and add compelling details with the refinement process. To improve the diversity of the synthesized images and stabilize the training of the conditional-GAN, we introduce a novel Conditioning Augmentation technique that encourages smoothness in the latent conditioning manifold. Extensive experiments and comparisons with state-of-the-arts on benchmark datasets demonstrate that the proposed method achieves significant improvements on generating photo-realistic images conditioned on text descriptions."
    },
    {
        "paperId": "fddc32f3880688238847077fd927ab3025db7a6a",
        "url": "https://www.semanticscholar.org/paper/fddc32f3880688238847077fd927ab3025db7a6a",
        "title": "EnhanceNet: Single Image Super-Resolution Through Automated Texture Synthesis",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2016,
        "citationCount": 1026,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1612.07919",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1612.07919, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2283034",
                "name": "Mehdi S. M. Sajjadi"
            },
            {
                "authorId": "1707625",
                "name": "B. Scholkopf"
            },
            {
                "authorId": "144566512",
                "name": "M. Hirsch"
            }
        ],
        "abstract": "Single image super-resolution is the task of inferring a high-resolution image from a single low-resolution input. Traditionally, the performance of algorithms for this task is measured using pixel-wise reconstruction measures such as peak signal-to-noise ratio (PSNR) which have been shown to correlate poorly with the human perception of image quality. As a result, algorithms minimizing these metrics tend to produce over-smoothed images that lack highfrequency textures and do not look natural despite yielding high PSNR values.,,We propose a novel application of automated texture synthesis in combination with a perceptual loss focusing on creating realistic textures rather than optimizing for a pixelaccurate reproduction of ground truth images during training. By using feed-forward fully convolutional neural networks in an adversarial training setting, we achieve a significant boost in image quality at high magnification ratios. Extensive experiments on a number of datasets show the effectiveness of our approach, yielding state-of-the-art results in both quantitative and qualitative benchmarks."
    },
    {
        "paperId": "01c40508dcb6f8e9efcdefe49e22bc0ccaf8881c",
        "url": "https://www.semanticscholar.org/paper/01c40508dcb6f8e9efcdefe49e22bc0ccaf8881c",
        "title": "Learning Background-Aware Correlation Filters for Visual Tracking",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2017,
        "citationCount": 1103,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1703.04590",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1703.04590, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2860592",
                "name": "Hamed Kiani Galoogahi"
            },
            {
                "authorId": "3231493",
                "name": "Ashton Fagg"
            },
            {
                "authorId": "1820249",
                "name": "S. Lucey"
            }
        ],
        "abstract": "Correlation Filters (CFs) have recently demonstrated excellent performance in terms of rapidly tracking objects under challenging photometric and geometric variations. The strength of the approach comes from its ability to efficiently learn - on the fly - how the object is changing over time. A fundamental drawback to CFs, however, is that the background of the target is not modeled over time which can result in suboptimal performance. Recent tracking algorithms have suggested to resolve this drawback by either learning CFs from more discriminative deep features (e.g. DeepSRDCF [9] and CCOT [11]) or learning complex deep trackers (e.g. MDNet [28] and FCNT [33]). While such methods have been shown to work well, they suffer from high complexity: extracting deep features or applying deep tracking frameworks is very computationally expensive. This limits the real-time performance of such methods, even on high-end GPUs. This work proposes a Background-Aware CF based on hand-crafted features (HOG [6]) that can efficiently model how both the foreground and background of the object varies over time. Our approach, like conventional CFs, is extremely computationally efficient- and extensive experiments over multiple tracking benchmarks demonstrate the superior accuracy and real-time performance of our method compared to the state-of-the-art trackers."
    },
    {
        "paperId": "024d037d46ae933c7e12fd16af61953c7161773a",
        "url": "https://www.semanticscholar.org/paper/024d037d46ae933c7e12fd16af61953c7161773a",
        "title": "Learning Spatio-Temporal Representation with Pseudo-3D Residual Networks",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2017,
        "citationCount": 1769,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/1711.10305",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1711.10305, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3430743",
                "name": "Zhaofan Qiu"
            },
            {
                "authorId": "145690248",
                "name": "Ting Yao"
            },
            {
                "authorId": "144025741",
                "name": "Tao Mei"
            }
        ],
        "abstract": "Convolutional Neural Networks (CNN) have been regarded as a powerful class of models for image recognition problems. Nevertheless, it is not trivial when utilizing a CNN for learning spatio-temporal video representation. A few studies have shown that performing 3D convolutions is a rewarding approach to capture both spatial and temporal dimensions in videos. However, the development of a very deep 3D CNN from scratch results in expensive computational cost and memory demand. A valid question is why not recycle off-the-shelf 2D networks for a 3D CNN. In this paper, we devise multiple variants of bottleneck building blocks in a residual learning framework by simulating 3 x 3 x 3 convolutions with 1  3  3 convolutional filters on spatial domain (equivalent to 2D CNN) plus 3  1  1 convolutions to construct temporal connections on adjacent feature maps in time. Furthermore, we propose a new architecture, named Pseudo-3D Residual Net (P3D ResNet), that exploits all the variants of blocks but composes each in different placement of ResNet, following the philosophy that enhancing structural diversity with going deep could improve the power of neural networks. Our P3D ResNet achieves clear improvements on Sports-1M video classification dataset against 3D CNN and frame-based 2D CNN by 5.3% and 1.8%, respectively. We further examine the generalization performance of video representation produced by our pre-trained P3D ResNet on five different benchmarks and three different tasks, demonstrating superior performances over several state-of-the-art techniques."
    },
    {
        "paperId": "049fd80f52c0b1fa4d532945d95a24734b62bdf3",
        "url": "https://www.semanticscholar.org/paper/049fd80f52c0b1fa4d532945d95a24734b62bdf3",
        "title": "ThiNet: A Filter Level Pruning Method for Deep Neural Network Compression",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2017,
        "citationCount": 1826,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/1707.06342",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1707.06342, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2119576",
                "name": "Jian-Hao Luo"
            },
            {
                "authorId": "1808816",
                "name": "Jianxin Wu"
            },
            {
                "authorId": "8131625",
                "name": "Weiyao Lin"
            }
        ],
        "abstract": "We propose an efficient and unified framework, namely ThiNet, to simultaneously accelerate and compress CNN models in both training and inference stages. We focus on the filter level pruning, i.e., the whole filter would be discarded if it is less important. Our method does not change the original network structure, thus it can be perfectly supported by any off-the-shelf deep learning libraries. We formally establish filter pruning as an optimization problem, and reveal that we need to prune filters based on statistics information computed from its next layer, not the current layer, which differentiates ThiNet from existing methods. Experimental results demonstrate the effectiveness of this strategy, which has advanced the state-of-the-art. We also show the performance of ThiNet on ILSVRC-12 benchmark. ThiNet achieves 3.31 x FLOPs reduction and 16.63 compression on VGG-16, with only 0.52% top-5 accuracy drop. Similar experiments with ResNet-50 reveal that even for a compact network, ThiNet can also reduce more than half of the parameters and FLOPs, at the cost of roughly 1% top-5 accuracy drop. Moreover, the original VGG-16 model can be further pruned into a very small model with only 5.05MB model size, preserving AlexNet level accuracy but showing much stronger generalization ability."
    },
    {
        "paperId": "0ba19bb88c856b880d198f4b6e9dcf8144657df8",
        "url": "https://www.semanticscholar.org/paper/0ba19bb88c856b880d198f4b6e9dcf8144657df8",
        "title": "End-to-End Learning of Geometry and Context for Deep Stereo Regression",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2017,
        "citationCount": 1441,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/1703.04309",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1703.04309, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "47645184",
                "name": "Alex Kendall"
            },
            {
                "authorId": "9746545",
                "name": "H. Martirosyan"
            },
            {
                "authorId": "5762869",
                "name": "Saumitro Dasgupta"
            },
            {
                "authorId": "1791800",
                "name": "Peter Henry"
            }
        ],
        "abstract": "We propose a novel deep learning architecture for regressing disparity from a rectified pair of stereo images. We leverage knowledge of the problems geometry to form a cost volume using deep feature representations. We learn to incorporate contextual information using 3-D convolutions over this volume. Disparity values are regressed from the cost volume using a proposed differentiable soft argmin operation, which allows us to train our method end-to-end to sub-pixel accuracy without any additional post-processing or regularization. We evaluate our method on the Scene Flow and KITTI datasets and on KITTI we set a new stateof-the-art benchmark, while being significantly faster than competing approaches."
    },
    {
        "paperId": "129256ab0fd6785766a6709f107610deb3d9bfcd",
        "url": "https://www.semanticscholar.org/paper/129256ab0fd6785766a6709f107610deb3d9bfcd",
        "title": "A Simple Yet Effective Baseline for 3d Human Pose Estimation",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2017,
        "citationCount": 1427,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1705.03098",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1705.03098, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2109650391",
                "name": "Julieta Martinez"
            },
            {
                "authorId": "46979707",
                "name": "Rayat Hossain"
            },
            {
                "authorId": "143881914",
                "name": "J. Romero"
            },
            {
                "authorId": "1710980",
                "name": "J. Little"
            }
        ],
        "abstract": "Following the success of deep convolutional networks, state-of-the-art methods for 3d human pose estimation have focused on deep end-to-end systems that predict 3d joint locations given raw image pixels. Despite their excellent performance, it is often not easy to understand whether their remaining error stems from a limited 2d pose (visual) understanding, or from a failure to map 2d poses into 3- dimensional positions.,,With the goal of understanding these sources of error, we set out to build a system that given 2d joint locations predicts 3d positions. Much to our surprise, we have found that, with current technology, \"lifting\" ground truth 2d joint locations to 3d space is a task that can be solved with a remarkably low error rate: a relatively simple deep feedforward network outperforms the best reported result by about 30% on Human3.6M, the largest publicly available 3d pose estimation benchmark. Furthermore, training our system on the output of an off-the-shelf state-of-the-art 2d detector (i.e., using images as input) yields state of the art results  this includes an array of systems that have been trained end-to-end specifically for this task. Our results indicate that a large portion of the error of modern deep 3d pose estimation systems stems from their visual analysis, and suggests directions to further advance the state of the art in 3d human pose estimation."
    },
    {
        "paperId": "4a73a1840945e87583d89ca0216a2c449d50a4a3",
        "url": "https://www.semanticscholar.org/paper/4a73a1840945e87583d89ca0216a2c449d50a4a3",
        "title": "Deformable Convolutional Networks",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2017,
        "citationCount": 6094,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1703.06211",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1703.06211, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3304536",
                "name": "Jifeng Dai"
            },
            {
                "authorId": "7217794",
                "name": "Haozhi Qi"
            },
            {
                "authorId": "3372084",
                "name": "Yuwen Xiong"
            },
            {
                "authorId": "2153682629",
                "name": "Yi Li"
            },
            {
                "authorId": "46266081",
                "name": "Guodong Zhang"
            },
            {
                "authorId": "1823518756",
                "name": "Han Hu"
            },
            {
                "authorId": "1732264",
                "name": "Yichen Wei"
            }
        ],
        "abstract": "Convolutional neural networks (CNNs) are inherently limited to model geometric transformations due to the fixed geometric structures in their building modules. In this work, we introduce two new modules to enhance the transformation modeling capability of CNNs, namely, deformable convolution and deformable RoI pooling. Both are based on the idea of augmenting the spatial sampling locations in the modules with additional offsets and learning the offsets from the target tasks, without additional supervision. The new modules can readily replace their plain counterparts in existing CNNs and can be easily trained end-to-end by standard back-propagation, giving rise to deformable convolutional networks. Extensive experiments validate the performance of our approach. For the first time, we show that learning dense spatial transformation in deep CNNs is effective for sophisticated vision tasks such as object detection and semantic segmentation. The code is released at https://github.com/msracver/Deformable-ConvNets."
    },
    {
        "paperId": "53c0aa8d33d240197caff824a6225fb223c1181c",
        "url": "https://www.semanticscholar.org/paper/53c0aa8d33d240197caff824a6225fb223c1181c",
        "title": "Soft-NMS  Improving Object Detection with One Line of Code",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2017,
        "citationCount": 1971,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1704.04503",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1704.04503, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "9541177",
                "name": "Navaneeth Bodla"
            },
            {
                "authorId": "2111282196",
                "name": "Bharat Singh"
            },
            {
                "authorId": "9215658",
                "name": "R. Chellappa"
            },
            {
                "authorId": "1693428",
                "name": "L. Davis"
            }
        ],
        "abstract": "Non-maximum suppression is an integral part of the object detection pipeline. First, it sorts all detection boxes on the basis of their scores. The detection box M with the maximum score is selected and all other detection boxes with a significant overlap (using a pre-defined threshold) with M are suppressed. This process is recursively applied on the remaining boxes. As per the design of the algorithm, if an object lies within the predefined overlap threshold, it leads to a miss. To this end, we propose Soft-NMS, an algorithm which decays the detection scores of all other objects as a continuous function of their overlap with M. Hence, no object is eliminated in this process. Soft-NMS obtains consistent improvements for the coco-style mAP metric on standard datasets like PASCAL VOC2007 (1.7% for both R-FCN and Faster-RCNN) and MS-COCO (1.3% for R-FCN and 1.1% for Faster-RCNN) by just changing the NMS algorithm without any additional hyper-parameters. Using Deformable-RFCN, Soft-NMS improves state-of-the-art in object detection from 39.8% to 40.9% with a single model. Further, the computational complexity of Soft-NMS is the same as traditional NMS and hence it can be efficiently implemented. Since Soft-NMS does not require any extra training and is simple to implement, it can be easily integrated into any object detection pipeline. Code for Soft-NMS is publicly available on GitHub http://bit.ly/2nJLNMu."
    },
    {
        "paperId": "5fdd40601a2ccdaa2d2ade27872bd8b3f43b2c1c",
        "url": "https://www.semanticscholar.org/paper/5fdd40601a2ccdaa2d2ade27872bd8b3f43b2c1c",
        "title": "MemNet: A Persistent Memory Network for Image Restoration",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2017,
        "citationCount": 1688,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1708.02209",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1708.02209, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "144970872",
                "name": "Ying Tai"
            },
            {
                "authorId": "51460259",
                "name": "Jian Yang"
            },
            {
                "authorId": "2108960018",
                "name": "Xiaoming Liu"
            },
            {
                "authorId": "48258938",
                "name": "Chunyan Xu"
            }
        ],
        "abstract": "Recently, very deep convolutional neural networks (CNNs) have been attracting considerable attention in image restoration. However, as the depth grows, the longterm dependency problem is rarely realized for these very deep models, which results in the prior states/layers having little influence on the subsequent ones. Motivated by the fact that human thoughts have persistency, we propose a very deep persistent memory network (MemNet) that introduces a memory block, consisting of a recursive unit and a gate unit, to explicitly mine persistent memory through an adaptive learning process. The recursive unit learns multi-level representations of the current state under different receptive fields. The representations and the outputs from the previous memory blocks are concatenated and sent to the gate unit, which adaptively controls how much of the previous states should be reserved, and decides how much of the current state should be stored. We apply MemNet to three image restoration tasks, i.e., image denosing, super-resolution and JPEG deblocking. Comprehensive experiments demonstrate the necessity of the MemNet and its unanimous superiority on all three tasks over the state of the arts. Code is available at https://github.com/tyshiwo/MemNet."
    },
    {
        "paperId": "7380e343dd4547e21d5118b16daf03d021d98c4e",
        "url": "https://www.semanticscholar.org/paper/7380e343dd4547e21d5118b16daf03d021d98c4e",
        "title": "Interpretable Explanations of Black Boxes by Meaningful Perturbation",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2017,
        "citationCount": 1617,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1704.03296",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1704.03296, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "25576460",
                "name": "Ruth C. Fong"
            },
            {
                "authorId": "1687524",
                "name": "A. Vedaldi"
            }
        ],
        "abstract": "As machine learning algorithms are increasingly applied to high impact yet high risk tasks, such as medical diagnosis or autonomous driving, it is critical that researchers can explain how such algorithms arrived at their predictions. In recent years, a number of image saliency methods have been developed to summarize where highly complex neural networks look in an image for evidence for their predictions. However, these techniques are limited by their heuristic nature and architectural constraints. In this paper, we make two main contributions: First, we propose a general framework for learning different kinds of explanations for any black box algorithm. Second, we specialise the framework to find the part of an image most responsible for a classifier decision. Unlike previous works, our method is model-agnostic and testable because it is grounded in explicit and interpretable image perturbations."
    },
    {
        "paperId": "79828e6e9f137a583082b8b5a9dfce0c301989b8",
        "url": "https://www.semanticscholar.org/paper/79828e6e9f137a583082b8b5a9dfce0c301989b8",
        "title": "The Mapillary Vistas Dataset for Semantic Understanding of Street Scenes",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2017,
        "citationCount": 1413,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICCV.2017.534?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICCV.2017.534, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "10789199",
                "name": "Gerhard Neuhold"
            },
            {
                "authorId": "31422278",
                "name": "Tobias Ollmann"
            },
            {
                "authorId": "2145174",
                "name": "S. R. Bul"
            },
            {
                "authorId": "2049889",
                "name": "P. Kontschieder"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "79cfb51a51fc093f66aac8e858afe2e14d4a1f20",
        "url": "https://www.semanticscholar.org/paper/79cfb51a51fc093f66aac8e858afe2e14d4a1f20",
        "title": "Focal Loss for Dense Object Detection",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2017,
        "citationCount": 28997,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/1708.02002",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICCV.2017.324?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICCV.2017.324, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "33493200",
                "name": "Tsung-Yi Lin"
            },
            {
                "authorId": "47316088",
                "name": "Priya Goyal"
            },
            {
                "authorId": "2983898",
                "name": "Ross B. Girshick"
            },
            {
                "authorId": "39353098",
                "name": "Kaiming He"
            },
            {
                "authorId": "3127283",
                "name": "Piotr Dollr"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "84d440eac8a3fb52ea5708e4943d02fc3fcfe009",
        "url": "https://www.semanticscholar.org/paper/84d440eac8a3fb52ea5708e4943d02fc3fcfe009",
        "title": "Mask R-CNN",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2017,
        "citationCount": 1992,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "39353098",
                "name": "Kaiming He"
            },
            {
                "authorId": "2082991",
                "name": "Georgia Gkioxari"
            },
            {
                "authorId": "3127283",
                "name": "Piotr Dollr"
            },
            {
                "authorId": "2983898",
                "name": "Ross B. Girshick"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "867f1a262a704ef4cabe84899310370182dd598f",
        "url": "https://www.semanticscholar.org/paper/867f1a262a704ef4cabe84899310370182dd598f",
        "title": "AOD-Net: All-in-One Dehazing Network",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2017,
        "citationCount": 1853,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICCV.2017.511?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICCV.2017.511, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2132473300",
                "name": "Boyi Li"
            },
            {
                "authorId": "3050945",
                "name": "Xiulian Peng"
            },
            {
                "authorId": "2969311",
                "name": "Zhangyang Wang"
            },
            {
                "authorId": "1697982",
                "name": "Jizheng Xu"
            },
            {
                "authorId": "2064895478",
                "name": "Dan Feng"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "8760bc7631c0cb04e7138254e9fd6451b7def8ca",
        "url": "https://www.semanticscholar.org/paper/8760bc7631c0cb04e7138254e9fd6451b7def8ca",
        "title": "Revisiting Unreasonable Effectiveness of Data in Deep Learning Era",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2017,
        "citationCount": 2605,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1707.02968",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1707.02968, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1491624845",
                "name": "Chen Sun"
            },
            {
                "authorId": "1781242",
                "name": "Abhinav Shrivastava"
            },
            {
                "authorId": "2108498897",
                "name": "Saurabh Singh"
            },
            {
                "authorId": "1726095131",
                "name": "A. Gupta"
            }
        ],
        "abstract": "The success of deep learning in vision can be attributed to: (a) models with high capacity; (b) increased computational power; and (c) availability of large-scale labeled data. Since 2012, there have been significant advances in representation capabilities of the models and computational capabilities of GPUs. But the size of the biggest dataset has surprisingly remained constant. What will happen if we increase the dataset size by 10  or 100  ? This paper takes a step towards clearing the clouds of mystery surrounding the relationship between enormous data and visual deep learning. By exploiting the JFT-300M dataset which has more than 375M noisy labels for 300M images, we investigate how the performance of current vision tasks would change if this data was used for representation learning. Our paper delivers some surprising (and some expected) findings. First, we find that the performance on vision tasks increases logarithmically based on volume of training data size. Second, we show that representation learning (or pre-training) still holds a lot of promise. One can improve performance on many vision tasks by just training a better base model. Finally, as expected, we present new state-of-the-art results for different vision tasks including image classification, object detection, semantic segmentation and human pose estimation. Our sincere hope is that this inspires vision community to not undervalue the data and develop collective efforts in building larger datasets."
    },
    {
        "paperId": "90a16f34d109b63d95ab4da2d491cbe3a1c8b656",
        "url": "https://www.semanticscholar.org/paper/90a16f34d109b63d95ab4da2d491cbe3a1c8b656",
        "title": "Learning Efficient Convolutional Networks through Network Slimming",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2017,
        "citationCount": 2646,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1708.06519",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1708.06519, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2109168016",
                "name": "Zhuang Liu"
            },
            {
                "authorId": "2118505169",
                "name": "Jianguo Li"
            },
            {
                "authorId": "145314568",
                "name": "Zhiqiang Shen"
            },
            {
                "authorId": "143983679",
                "name": "Gao Huang"
            },
            {
                "authorId": "3024017",
                "name": "Shoumeng Yan"
            },
            {
                "authorId": "14966740",
                "name": "Changshui Zhang"
            }
        ],
        "abstract": "The deployment of deep convolutional neural networks (CNNs) in many real world applications is largely hindered by their high computational cost. In this paper, we propose a novel learning scheme for CNNs to simultaneously 1) reduce the model size; 2) decrease the run-time memory footprint; and 3) lower the number of computing operations, without compromising accuracy. This is achieved by enforcing channel-level sparsity in the network in a simple but effective way. Different from many existing approaches, the proposed method directly applies to modern CNN architectures, introduces minimum overhead to the training process, and requires no special software/hardware accelerators for the resulting models. We call our approach network slimming, which takes wide and large networks as input models, but during training insignificant channels are automatically identified and pruned afterwards, yielding thin and compact models with comparable accuracy. We empirically demonstrate the effectiveness of our approach with several state-of-the-art CNN models, including VGGNet, ResNet and DenseNet, on various image classification datasets. For VGGNet, a multi-pass version of network slimming gives a 20 reduction in model size and a 5 reduction in computing operations."
    },
    {
        "paperId": "96dd1fc39a368d23291816d57763bc6eb4f7b8d6",
        "url": "https://www.semanticscholar.org/paper/96dd1fc39a368d23291816d57763bc6eb4f7b8d6",
        "title": "Dense-Captioning Events in Videos",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2017,
        "citationCount": 1431,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1705.00754",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1705.00754, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "145237361",
                "name": "Ranjay Krishna"
            },
            {
                "authorId": "1382195702",
                "name": "K. Hata"
            },
            {
                "authorId": "3260219",
                "name": "F. Ren"
            },
            {
                "authorId": "48004138",
                "name": "Li Fei-Fei"
            },
            {
                "authorId": "9200530",
                "name": "Juan Carlos Niebles"
            }
        ],
        "abstract": "Most natural videos contain numerous events. For example, in a video of a man playing a piano, the video might also contain another man dancing or a crowd clapping. We introduce the task of dense-captioning events, which involves both detecting and describing events in a video. We propose a new model that is able to identify all events in a single pass of the video while simultaneously describing the detected events with natural language. Our model introduces a variant of an existing proposal module that is designed to capture both short as well as long events that span minutes. To capture the dependencies between the events in a video, our model introduces a new captioning module that uses contextual information from past and future events to jointly describe all events. We also introduce ActivityNet Captions, a large-scale benchmark for dense-captioning events. ActivityNet Captions contains 20k videos amounting to 849 video hours with 100k total descriptions, each with its unique start and end time. Finally, we report performances of our model for dense-captioning events, video retrieval and localization."
    },
    {
        "paperId": "aadf777ef924ac93317550fbdfb9649a10d8aa82",
        "url": "https://www.semanticscholar.org/paper/aadf777ef924ac93317550fbdfb9649a10d8aa82",
        "title": "How Far are We from Solving the 2D & 3D Face Alignment Problem? (and a Dataset of 230,000 3D Facial Landmarks)",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2017,
        "citationCount": 1586,
        "openAccessPdf": {
            "url": "https://nottingham-repository.worktribe.com/preview/889442/0560.pdf",
            "status": "GREEN",
            "license": "other-oa",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1703.07332, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "145245424",
                "name": "Adrian Bulat"
            },
            {
                "authorId": "2610880",
                "name": "Georgios Tzimiropoulos"
            }
        ],
        "abstract": "This paper investigates how far a very deep neural network is from attaining close to saturating performance on existing 2D and 3D face alignment datasets. To this end, we make the following 5 contributions: (a) we construct, for the first time, a very strong baseline by combining a state-of-the-art architecture for landmark localization with a state-of-the-art residual block, train it on a very large yet synthetically expanded 2D facial landmark dataset and finally evaluate it on all other 2D facial landmark datasets. (b)We create a guided by 2D landmarks network which converts 2D landmark annotations to 3D and unifies all existing datasets, leading to the creation of LS3D-W, the largest and most challenging 3D facial landmark dataset to date (~230,000 images). (c) Following that, we train a neural network for 3D face alignment and evaluate it on the newly introduced LS3D-W. (d) We further look into the effect of all traditional factors affecting face alignment performance like large pose, initialization and resolution, and introduce a new one, namely the size of the network. (e) We show that both 2D and 3D face alignment networks achieve performance of remarkable accuracy which is probably close to saturating the datasets used. Training and testing code as well as the dataset can be downloaded from https://www.adrianbulat.com/face-alignment/"
    },
    {
        "paperId": "b1e7f07965a53491690bd31fdab626bfac606eae",
        "url": "https://www.semanticscholar.org/paper/b1e7f07965a53491690bd31fdab626bfac606eae",
        "title": "Deeper, Broader and Artier Domain Generalization",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2017,
        "citationCount": 1644,
        "openAccessPdf": {
            "url": "https://www.pure.ed.ac.uk/ws/files/41072820/li2017dg.pdf",
            "status": "GREEN",
            "license": "other-oa",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1710.03077, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2108338672",
                "name": "Da Li"
            },
            {
                "authorId": "2653152",
                "name": "Yongxin Yang"
            },
            {
                "authorId": "1705408",
                "name": "Yi-Zhe Song"
            },
            {
                "authorId": "1697755",
                "name": "Timothy M. Hospedales"
            }
        ],
        "abstract": "The problem of domain generalization is to learn from multiple training domains, and extract a domain-agnostic model that can then be applied to an unseen domain. Domain generalization (DG) has a clear motivation in contexts where there are target domains with distinct characteristics, yet sparse data for training. For example recognition in sketch images, which are distinctly more abstract and rarer than photos. Nevertheless, DG methods have primarily been evaluated on photo-only benchmarks focusing on alleviating the dataset bias where both problems of domain distinctiveness and data sparsity can be minimal. We argue that these benchmarks are overly straightforward, and show that simple deep learning baselines perform surprisingly well on them. In this paper, we make two main contributions: Firstly, we build upon the favorable domain shift-robust properties of deep learning methods, and develop a low-rank parameterized CNN model for end-to-end DG learning. Secondly, we develop a DG benchmark dataset covering photo, sketch, cartoon and painting domains. This is both more practically relevant, and harder (bigger domain shift) than existing benchmarks. The results show that our method outperforms existing DG alternatives, and our dataset provides a more significant DG challenge to drive future research."
    },
    {
        "paperId": "b68811a9b5cafe4795a11c1048541750068b7ad0",
        "url": "https://www.semanticscholar.org/paper/b68811a9b5cafe4795a11c1048541750068b7ad0",
        "title": "The Something Something Video Database for Learning and Evaluating Visual Common Sense",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2017,
        "citationCount": 1763,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1706.04261",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1706.04261, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "38962424",
                "name": "Raghav Goyal"
            },
            {
                "authorId": "3127597",
                "name": "Samira Ebrahimi Kahou"
            },
            {
                "authorId": "1748421",
                "name": "Vincent Michalski"
            },
            {
                "authorId": "2273472082",
                "name": "Joanna Materzynska"
            },
            {
                "authorId": "12929417",
                "name": "S. Westphal"
            },
            {
                "authorId": "2233986",
                "name": "Heuna Kim"
            },
            {
                "authorId": "7241984",
                "name": "V. Haenel"
            },
            {
                "authorId": "1848689",
                "name": "Ingo Frnd"
            },
            {
                "authorId": "3203897",
                "name": "P. Yianilos"
            },
            {
                "authorId": "1414405239",
                "name": "Moritz Mueller-Freitag"
            },
            {
                "authorId": "2069898274",
                "name": "F. Hoppe"
            },
            {
                "authorId": "2020614",
                "name": "Christian Thurau"
            },
            {
                "authorId": "2443288",
                "name": "Ingo Bax"
            },
            {
                "authorId": "1710604",
                "name": "R. Memisevic"
            }
        ],
        "abstract": "Neural networks trained on datasets such as ImageNet have led to major advances in visual object classification. One obstacle that prevents networks from reasoning more deeply about complex scenes and situations, and from integrating visual knowledge with natural language, like humans do, is their lack of common sense knowledge about the physical world. Videos, unlike still images, contain a wealth of detailed information about the physical world. However, most labelled video datasets represent high-level concepts rather than detailed physical aspects about actions and scenes. In this work, we describe our ongoing collection of the something-something database of video prediction tasks whose solutions require a common sense understanding of the depicted situation. The database currently contains more than 100,000 videos across 174 classes, which are defined as caption-templates. We also describe the challenges in crowd-sourcing this data at scale."
    },
    {
        "paperId": "be0ef77fb0345c5851bb5d297f3ed84ae3c581ee",
        "url": "https://www.semanticscholar.org/paper/be0ef77fb0345c5851bb5d297f3ed84ae3c581ee",
        "title": "Arbitrary Style Transfer in Real-Time with Adaptive Instance Normalization",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2017,
        "citationCount": 4931,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1703.06868",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1703.06868, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "144247007",
                "name": "Xun Huang"
            },
            {
                "authorId": "50172592",
                "name": "Serge J. Belongie"
            }
        ],
        "abstract": "Gatys et al. recently introduced a neural algorithm that renders a content image in the style of another image, achieving so-called style transfer. However, their framework requires a slow iterative optimization process, which limits its practical application. Fast approximations with feed-forward neural networks have been proposed to speed up neural style transfer. Unfortunately, the speed improvement comes at a cost: the network is usually tied to a fixed set of styles and cannot adapt to arbitrary new styles. In this paper, we present a simple yet effective approach that for the first time enables arbitrary style transfer in real-time. At the heart of our method is a novel adaptive instance normalization (AdaIN) layer that aligns the mean and variance of the content features with those of the style features. Our method achieves speed comparable to the fastest existing approach, without the restriction to a pre-defined set of styles. In addition, our approach allows flexible user controls such as content-style trade-off, style interpolation, color & spatial controls, all using a single feed-forward neural network."
    },
    {
        "paperId": "c1cb7a1efb1a47348e3a25c21ff0a3ff192d7058",
        "url": "https://www.semanticscholar.org/paper/c1cb7a1efb1a47348e3a25c21ff0a3ff192d7058",
        "title": "Image Super-Resolution Using Dense Skip Connections",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2017,
        "citationCount": 1233,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICCV.2017.514?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICCV.2017.514, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "144061555",
                "name": "T. Tong"
            },
            {
                "authorId": "2108550321",
                "name": "Gen Li"
            },
            {
                "authorId": "32488502",
                "name": "Xiejie Liu"
            },
            {
                "authorId": "40214487",
                "name": "Qinquan Gao"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "c43d954cf8133e6254499f3d68e45218067e4941",
        "url": "https://www.semanticscholar.org/paper/c43d954cf8133e6254499f3d68e45218067e4941",
        "title": "Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2017,
        "citationCount": 5601,
        "openAccessPdf": {
            "url": "https://repositorio.unal.edu.co/bitstream/unal/82529/2/98562187.2022.pdf",
            "status": "GREEN",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1703.10593, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2436356",
                "name": "Jun-Yan Zhu"
            },
            {
                "authorId": "2071929129",
                "name": "Taesung Park"
            },
            {
                "authorId": "2094770",
                "name": "Phillip Isola"
            },
            {
                "authorId": "1763086",
                "name": "Alexei A. Efros"
            }
        ],
        "abstract": "Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain X to a target domain Y in the absence of paired examples. Our goal is to learn a mapping G : X  Y such that the distribution of images from G(X) is indistinguishable from the distribution Y using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping F : Y  X and introduce a cycle consistency loss to push F(G(X))  X (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach."
    },
    {
        "paperId": "ee53c9480132fc0d09b1192226cb2c460462fd6d",
        "url": "https://www.semanticscholar.org/paper/ee53c9480132fc0d09b1192226cb2c460462fd6d",
        "title": "Channel Pruning for Accelerating Very Deep Neural Networks",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2017,
        "citationCount": 2673,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1707.06168",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1707.06168, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "39838894",
                "name": "Yihui He"
            },
            {
                "authorId": "50875121",
                "name": "Xiangyu Zhang"
            },
            {
                "authorId": null,
                "name": "Jian Sun"
            }
        ],
        "abstract": "In this paper, we introduce a new channel pruning method to accelerate very deep convolutional neural networks. Given a trained CNN model, we propose an iterative two-step algorithm to effectively prune each layer, by a LASSO regression based channel selection and least square reconstruction. We further generalize this algorithm to multi-layer and multi-branch cases. Our method reduces the accumulated error and enhance the compatibility with various architectures. Our pruned VGG-16 achieves the state-of-the-art results by 5 speed-up along with only 0.3% increase of error. More importantly, our method is able to accelerate modern networks like ResNet, Xception and suffers only 1.4%, 1.0% accuracy loss under 2 speedup respectively, which is significant."
    },
    {
        "paperId": "ee909ad489244016cf301bb7d7d8eeea423dbf35",
        "url": "https://www.semanticscholar.org/paper/ee909ad489244016cf301bb7d7d8eeea423dbf35",
        "title": "Localizing Moments in Video with Natural Language",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2017,
        "citationCount": 1100,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1708.01641",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1708.01641, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2234342",
                "name": "Lisa Anne Hendricks"
            },
            {
                "authorId": "39231399",
                "name": "Oliver Wang"
            },
            {
                "authorId": "2177801",
                "name": "Eli Shechtman"
            },
            {
                "authorId": "1782755",
                "name": "Josef Sivic"
            },
            {
                "authorId": "1753210",
                "name": "Trevor Darrell"
            },
            {
                "authorId": "145160921",
                "name": "Bryan C. Russell"
            }
        ],
        "abstract": "We consider retrieving a specific temporal segment, or moment, from a video given a natural language text description. Methods designed to retrieve whole video clips with natural language determine what occurs in a video but not when. To address this issue, we propose the Moment Context Network (MCN) which effectively localizes natural language queries in videos by integrating local and global video features over time. A key obstacle to training our MCN model is that current video datasets do not include pairs of localized video segments and referring expressions, or text descriptions which uniquely identify a corresponding moment. Therefore, we collect the Distinct Describable Moments (DiDeMo) dataset which consists of over 10,000 unedited, personal videos in diverse visual settings with pairs of localized video segments and referring expressions. We demonstrate that MCN outperforms several baseline methods and believe that our initial results together with the release of DiDeMo will inspire further research on localizing video moments with natural language."
    },
    {
        "paperId": "ef3c1f6c177e37f1d0d2a61702b60c766971700b",
        "url": "https://www.semanticscholar.org/paper/ef3c1f6c177e37f1d0d2a61702b60c766971700b",
        "title": "DualGAN: Unsupervised Dual Learning for Image-to-Image Translation",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2017,
        "citationCount": 2040,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1704.02510",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1704.02510, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "39737792",
                "name": "Zili Yi"
            },
            {
                "authorId": "39497427",
                "name": "Hao Zhang"
            },
            {
                "authorId": "145604260",
                "name": "P. Tan"
            },
            {
                "authorId": "34077629",
                "name": "Minglun Gong"
            }
        ],
        "abstract": "Conditional Generative Adversarial Networks (GANs) for cross-domain image-to-image translation have made much progress recently [7, 8, 21, 12, 4, 18]. Depending on the task complexity, thousands to millions of labeled image pairs are needed to train a conditional GAN. However, human labeling is expensive, even impractical, and large quantities of data may not always be available. Inspired by dual learning from natural language translation [23], we develop a novel dual-GAN mechanism, which enables image translators to be trained from two sets of unlabeled images from two domains. In our architecture, the primal GAN learns to translate images from domain U to those in domain V, while the dual GAN learns to invert the task. The closed loop made by the primal and dual tasks allows images from either domain to be translated and then reconstructed. Hence a loss function that accounts for the reconstruction error of images can be used to train the translators. Experiments on multiple image translation tasks with unlabeled data show considerable performance gain of DualGAN over a single GAN. For some tasks, DualGAN can even achieve comparable or slightly better results than conditional GAN trained on fully labeled data."
    },
    {
        "paperId": "f48d322244c906b45792b28206df7cfb23495004",
        "url": "https://www.semanticscholar.org/paper/f48d322244c906b45792b28206df7cfb23495004",
        "title": "Escape from Cells: Deep Kd-Networks for the Recognition of 3D Point Cloud Models",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2017,
        "citationCount": 1012,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1704.01222",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1704.01222, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "10714572",
                "name": "Roman Klokov"
            },
            {
                "authorId": "1740145",
                "name": "V. Lempitsky"
            }
        ],
        "abstract": "We present a new deep learning architecture (called Kdnetwork) that is designed for 3D model recognition tasks and works with unstructured point clouds. The new architecture performs multiplicative transformations and shares parameters of these transformations according to the subdivisions of the point clouds imposed onto them by kdtrees. Unlike the currently dominant convolutional architectures that usually require rasterization on uniform twodimensional or three-dimensional grids, Kd-networks do not rely on such grids in any way and therefore avoid poor scaling behavior. In a series of experiments with popular shape recognition benchmarks, Kd-networks demonstrate competitive performance in a number of shape recognition tasks such as shape classification, shape retrieval and shape part segmentation."
    },
    {
        "paperId": "f9b7783448f65205e085bd4e6fdfa2c8bfa9a4df",
        "url": "https://www.semanticscholar.org/paper/f9b7783448f65205e085bd4e6fdfa2c8bfa9a4df",
        "title": "Unlabeled Samples Generated by GAN Improve the Person Re-identification Baseline in Vitro",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2017,
        "citationCount": 1995,
        "openAccessPdf": {
            "url": "https://opus.lib.uts.edu.au/bitstream/10453/118067/4/FF67E427-6528-4081-B0B7-C3EB797E0421.pdf",
            "status": "GREEN",
            "license": "other-oa",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1701.07717, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "7435343",
                "name": "Zhedong Zheng"
            },
            {
                "authorId": "144802394",
                "name": "Liang Zheng"
            },
            {
                "authorId": "7179962",
                "name": "Yi Yang"
            }
        ],
        "abstract": "The main contribution of this paper is a simple semisupervised pipeline that only uses the original training set without collecting extra data. It is challenging in 1) how to obtain more training data only from the training set and 2) how to use the newly generated data. In this work, the generative adversarial network (GAN) is used to generate unlabeled samples. We propose the label smoothing regularization for outliers (LSRO). This method assigns a uniform label distribution to the unlabeled images, which regularizes the supervised model and improves the baseline. We verify the proposed method on a practical problem: person re-identification (re-ID). This task aims to retrieve a query person from other cameras. We adopt the deep convolutional generative adversarial network (DCGAN) for sample generation, and a baseline convolutional neural network (CNN) for representation learning. Experiments show that adding the GAN-generated data effectively improves the discriminative ability of learned CNN embeddings. On three large-scale datasets, Market- 1501, CUHK03 and DukeMTMC-reID, we obtain +4.37%, +1.6% and +2.46% improvement in rank-1 precision over the baseline CNN, respectively. We additionally apply the proposed method to fine-grained bird recognition and achieve a +0.6% improvement over a strong baseline. The code is available at https://github.com/layumi/ Person-reID_GAN."
    },
    {
        "paperId": "3217278e346fefbd34f0727321059c7ea5792612",
        "url": "https://www.semanticscholar.org/paper/3217278e346fefbd34f0727321059c7ea5792612",
        "title": "Moment Matching for Multi-Source Domain Adaptation",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2018,
        "citationCount": 2103,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1812.01754",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1812.01754, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2960713",
                "name": "Xingchao Peng"
            },
            {
                "authorId": "2132890",
                "name": "Qinxun Bai"
            },
            {
                "authorId": "3302135",
                "name": "Xide Xia"
            },
            {
                "authorId": "2109595998",
                "name": "Zijun Huang"
            },
            {
                "authorId": "2903226",
                "name": "Kate Saenko"
            },
            {
                "authorId": null,
                "name": "Bo Wang"
            }
        ],
        "abstract": "Conventional unsupervised domain adaptation (UDA) assumes that training data are sampled from a single domain. This neglects the more practical scenario where training data are collected from multiple sources, requiring multi-source domain adaptation. We make three major contributions towards addressing this problem. First, we collect and annotate by far the largest UDA dataset, called DomainNet, which contains six domains and about 0.6 million images distributed among 345 categories, addressing the gap in data availability for multi-source UDA research. Second, we propose a new deep learning approach, Moment Matching for Multi-Source Domain Adaptation (M3SDA), which aims to transfer knowledge learned from multiple labeled source domains to an unlabeled target domain by dynamically aligning moments of their feature distributions. Third, we provide new theoretical insights specifically for moment matching approaches in both single and multiple source domain adaptation. Extensive experiments are conducted to demonstrate the power of our new dataset in benchmarking state-of-the-art multi-source domain adaptation methods, as well as the advantage of our proposed model. Dataset and Code are available at http://ai.bu.edu/M3SDA/"
    },
    {
        "paperId": "4152d2c8585f7e3f85d3b3d84036171de104cbd7",
        "url": "https://www.semanticscholar.org/paper/4152d2c8585f7e3f85d3b3d84036171de104cbd7",
        "title": "Rethinking ImageNet Pre-Training",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2018,
        "citationCount": 1157,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1811.08883",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1811.08883, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "39353098",
                "name": "Kaiming He"
            },
            {
                "authorId": "2983898",
                "name": "Ross B. Girshick"
            },
            {
                "authorId": "3127283",
                "name": "Piotr Dollr"
            }
        ],
        "abstract": "We report competitive results on object detection and instance segmentation on the COCO dataset using standard models trained \\textbf{from random initialization}. The results are \\textbf{no worse} than their ImageNet pre-training counterparts even when using the hyper-parameters of the baseline system (Mask R-CNN) that were optimized for fine-tuning pre-trained models, with the sole exception of increasing the number of training iterations so the randomly initialized models may converge. Training from random initialization is surprisingly robust; our results hold even when: (i) using only 10\\% of the training data, (ii) for deeper and wider models, and (iii) for multiple tasks and metrics. Experiments show that ImageNet pre-training speeds up convergence early in training, but does not necessarily provide regularization or improve final target task accuracy. To push the envelope we demonstrate {50.9}~AP on COCO object detection without using any external data---a result on par with the top COCO 2017 competition results that used ImageNet pre-training. These observations challenge the conventional wisdom of ImageNet pre-training for dependent tasks and we expect these discoveries will encourage people to rethink the current de facto paradigm of `pre-training and fine-tuning' in computer vision."
    },
    {
        "paperId": "4bbfd46721c145852e443ae4aad35148b814bf91",
        "url": "https://www.semanticscholar.org/paper/4bbfd46721c145852e443ae4aad35148b814bf91",
        "title": "TSM: Temporal Shift Module for Efficient Video Understanding",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2018,
        "citationCount": 1924,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1811.08383",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1811.08383, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "46698300",
                "name": "Ji Lin"
            },
            {
                "authorId": "144158271",
                "name": "Chuang Gan"
            },
            {
                "authorId": "143840275",
                "name": "Song Han"
            }
        ],
        "abstract": "The explosive growth in video streaming gives rise to challenges on performing video understanding at high accuracy and low computation cost. Conventional 2D CNNs are computationally cheap but cannot capture temporal relationships; 3D CNN based methods can achieve good performance but are computationally intensive, making it expensive to deploy. In this paper, we propose a generic and effective Temporal Shift Module (TSM) that enjoys both high efficiency and high performance. Specifically, it can achieve the performance of 3D CNN but maintain 2D CNNs complexity. TSM shifts part of the channels along the temporal dimension; thus facilitate information exchanged among neighboring frames. It can be inserted into 2D CNNs to achieve temporal modeling at zero computation and zero parameters. We also extended TSM to online setting, which enables real-time low-latency online video recognition and video object detection. TSM is accurate and efficient: it ranks the first place on the Something-Something leaderboard upon publication; on Jetson Nano and Galaxy Note8, it achieves a low latency of 13ms and 35ms for online video recognition. The code is available at: https://github. com/mit-han-lab/temporal-shift-module."
    },
    {
        "paperId": "5132500b23d2da47129b3f4f68dd30947a29e502",
        "url": "https://www.semanticscholar.org/paper/5132500b23d2da47129b3f4f68dd30947a29e502",
        "title": "CCNet: Criss-Cross Attention for Semantic Segmentation",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2018,
        "citationCount": 2856,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1811.11721",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1811.11721, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3462332",
                "name": "Zilong Huang"
            },
            {
                "authorId": "2443233",
                "name": "Xinggang Wang"
            },
            {
                "authorId": "47033130",
                "name": "Lichao Huang"
            },
            {
                "authorId": "48908475",
                "name": "Chang Huang"
            },
            {
                "authorId": "49020088",
                "name": "Yunchao Wei"
            },
            {
                "authorId": "48667025",
                "name": "Humphrey Shi"
            },
            {
                "authorId": null,
                "name": "Wenyu Liu"
            }
        ],
        "abstract": "Full-image dependencies provide useful contextual information to benefit visual understanding problems. In this work, we propose a Criss-Cross Network (CCNet) for obtaining such contextual information in a more effective and efficient way. Concretely, for each pixel, a novel criss-cross attention module in CCNet harvests the contextual information of all the pixels on its criss-cross path. By taking a further recurrent operation, each pixel can finally capture the full-image dependencies from all pixels. Overall, CCNet is with the following merits: 1) GPU memory friendly. Compared with the non-local block, the proposed recurrent criss-cross attention module requires 11x less GPU memory usage. 2) High computational efficiency. The recurrent criss-cross attention significantly reduces FLOPs by about 85% of the non-local block in computing full-image dependencies. 3) The state-of-the-art performance. We conduct extensive experiments on popular semantic segmentation benchmarks including Cityscapes, ADE20K, and instance segmentation benchmark COCO. In particular, our CCNet achieves the mIoU score of 81.4 and 45.22 on Cityscapes test set and ADE20K validation set, respectively, which are the new state-of-the-art results. The source code is available at https://github.com/speedinghzl/CCNet."
    },
    {
        "paperId": "589cfcb2f995c94b0a98c902cc1f5e0f27cbd927",
        "url": "https://www.semanticscholar.org/paper/589cfcb2f995c94b0a98c902cc1f5e0f27cbd927",
        "title": "Digging Into Self-Supervised Monocular Depth Estimation",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2018,
        "citationCount": 2274,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1806.01260",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICCV.2019.00393?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICCV.2019.00393, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "31082236",
                "name": "Clment Godard"
            },
            {
                "authorId": "2918822",
                "name": "Oisin Mac Aodha"
            },
            {
                "authorId": "3309893",
                "name": "G. Brostow"
            }
        ],
        "abstract": "Per-pixel ground-truth depth data is challenging to acquire at scale. To overcome this limitation, self-supervised learning has emerged as a promising alternative for training models to perform monocular depth estimation. In this paper, we propose a set of improvements, which together result in both quantitatively and qualitatively improved depth maps compared to competing self-supervised methods. Research on self-supervised monocular training usually explores increasingly complex architectures, loss functions, and image formation models, all of which have recently helped to close the gap with fully-supervised methods. We show that a surprisingly simple model, and associated design choices, lead to superior predictions. In particular, we propose (i) a minimum reprojection loss, designed to robustly handle occlusions, (ii) a full-resolution multi-scale sampling method that reduces visual artifacts, and (iii) an auto-masking loss to ignore training pixels that violate camera motion assumptions. We demonstrate the effectiveness of each component in isolation, and show high quality, state-of-the-art results on the KITTI benchmark."
    },
    {
        "paperId": "8b47b9c3c35b2b2a78bff7822605b3040f87d699",
        "url": "https://www.semanticscholar.org/paper/8b47b9c3c35b2b2a78bff7822605b3040f87d699",
        "title": "SlowFast Networks for Video Recognition",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2018,
        "citationCount": 3818,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1812.03982",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1812.03982, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2322150",
                "name": "Christoph Feichtenhofer"
            },
            {
                "authorId": "146884473",
                "name": "Haoqi Fan"
            },
            {
                "authorId": "143751119",
                "name": "Jitendra Malik"
            },
            {
                "authorId": "39353098",
                "name": "Kaiming He"
            }
        ],
        "abstract": "We present SlowFast networks for video recognition. Our model involves (i) a Slow pathway, operating at low frame rate, to capture spatial semantics, and (ii) a Fast pathway, operating at high frame rate, to capture motion at fine temporal resolution. The Fast pathway can be made very lightweight by reducing its channel capacity, yet can learn useful temporal information for video recognition. Our models achieve strong performance for both action classification and detection in video, and large improvements are pin-pointed as contributions by our SlowFast concept. We report state-of-the-art accuracy on major video recognition benchmarks, Kinetics, Charades and AVA. Code has been made available at: https://github.com/facebookresearch/SlowFast."
    },
    {
        "paperId": "a997f1ecd85e1467d11252741d188fac8db22722",
        "url": "https://www.semanticscholar.org/paper/a997f1ecd85e1467d11252741d188fac8db22722",
        "title": "Free-Form Image Inpainting With Gated Convolution",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2018,
        "citationCount": 1912,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1806.03589",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1806.03589, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "150167366",
                "name": "Jiahui Yu"
            },
            {
                "authorId": "145527707",
                "name": "Zhe L. Lin"
            },
            {
                "authorId": "1768964",
                "name": "Jimei Yang"
            },
            {
                "authorId": "1720987",
                "name": "Xiaohui Shen"
            },
            {
                "authorId": "145574672",
                "name": "Xin Lu"
            },
            {
                "authorId": "153652752",
                "name": "Thomas S. Huang"
            }
        ],
        "abstract": "We present a generative image inpainting system to complete images with free-form mask and guidance. The system is based on gated convolutions learned from millions of images without additional labelling efforts. The proposed gated convolution solves the issue of vanilla convolution that treats all input pixels as valid ones, generalizes partial convolution by providing a learnable dynamic feature selection mechanism for each channel at each spatial location across all layers. Moreover, as free-form masks may appear anywhere in images with any shape, global and local GANs designed for a single rectangular mask are not applicable. Thus, we also present a patch-based GAN loss, named SN-PatchGAN, by applying spectral-normalized discriminator on dense image patches. SN-PatchGAN is simple in formulation, fast and stable in training. Results on automatic image inpainting and user-guided extension demonstrate that our system generates higher-quality and more flexible results than previous methods. Our system helps user quickly remove distracting objects, modify image layouts, clear watermarks and edit faces. Code, demo and models are available at: \\url{https://github.com/JiahuiYu/generative_inpainting}."
    },
    {
        "paperId": "0c2b90305dbeb11950ff71718cb3331048124d50",
        "url": "https://www.semanticscholar.org/paper/0c2b90305dbeb11950ff71718cb3331048124d50",
        "title": "Learning to Reconstruct 3D Human Pose and Shape via Model-Fitting in the Loop",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2019,
        "citationCount": 1083,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1909.12828",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1909.12828, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "113950826",
                "name": "Nikos Kolotouros"
            },
            {
                "authorId": "2829330",
                "name": "G. Pavlakos"
            },
            {
                "authorId": "2105795",
                "name": "Michael J. Black"
            },
            {
                "authorId": "1751586",
                "name": "Kostas Daniilidis"
            }
        ],
        "abstract": "Model-based human pose estimation is currently approached through two different paradigms. Optimization-based methods fit a parametric body model to 2D observations in an iterative manner, leading to accurate image-model alignments, but are often slow and sensitive to the initialization. In contrast, regression-based methods, that use a deep network to directly estimate the model parameters from pixels, tend to provide reasonable, but not pixel accurate, results while requiring huge amounts of supervision. In this work, instead of investigating which approach is better, our key insight is that the two paradigms can form a strong collaboration. A reasonable, directly regressed estimate from the network can initialize the iterative optimization making the fitting faster and more accurate. Similarly, a pixel accurate fit from iterative optimization can act as strong supervision for the network. This is the core of our proposed approach SPIN (SMPL oPtimization IN the loop). The deep network initializes an iterative optimization routine that fits the body model to 2D joints within the training loop, and the fitted estimate is subsequently used to supervise the network. Our approach is self-improving by nature, since better network estimates can lead the optimization to better solutions, while more accurate optimization fits provide better supervision for the network. We demonstrate the effectiveness of our approach in different settings, where 3D ground truth is scarce, or not available, and we consistently outperform the state-of-the-art model-based pose estimation approaches by significant margins. The project website with videos, results, and code can be found at https://seas.upenn.edu/~nkolot/projects/spin."
    },
    {
        "paperId": "115df25cc9080f0b384b3398a897767b33c42a5f",
        "url": "https://www.semanticscholar.org/paper/115df25cc9080f0b384b3398a897767b33c42a5f",
        "title": "EGNet: Edge Guidance Network for Salient Object Detection",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2019,
        "citationCount": 1021,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1908.08297",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1908.08297, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2109784468",
                "name": "Jiaxing Zhao"
            },
            {
                "authorId": "2119612440",
                "name": "Jiangjiang Liu"
            },
            {
                "authorId": "23999143",
                "name": "Deng-Ping Fan"
            },
            {
                "authorId": "2108097465",
                "name": "Yang Cao"
            },
            {
                "authorId": "1755872",
                "name": "Jufeng Yang"
            },
            {
                "authorId": "37535930",
                "name": "Ming-Ming Cheng"
            }
        ],
        "abstract": "Fully convolutional neural networks (FCNs) have shown their advantages in the salient object detection task. However, most existing FCNs-based methods still suffer from coarse object boundaries. In this paper, to solve this problem, we focus on the complementarity between salient edge information and salient object information. Accordingly, we present an edge guidance network (EGNet) for salient object detection with three steps to simultaneously model these two kinds of complementary information in a single network. In the rst step, we extract the salient object features by a progressive fusion way. In the second step, we integrate the local edge information and global location information to obtain the salient edge features. Finally, to sufciently leverage these complementary features, we couple the same salient edge features with salient object features at various resolutions. Beneting from the rich edge information and location information in salient edge features, the fused features can help locate salient objects, especially their boundaries more accurately. Experimental results demonstrate that the proposed method performs favorably against the state-of-the-art methods on six widely used datasets without any pre-processing and post-processing. The source code is available at http: //mmcheng.net/egnet/."
    },
    {
        "paperId": "27ac832ee83d8b5386917998a171a0257e2151e2",
        "url": "https://www.semanticscholar.org/paper/27ac832ee83d8b5386917998a171a0257e2151e2",
        "title": "Attention Augmented Convolutional Networks",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2019,
        "citationCount": 1119,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1904.09925",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1904.09925, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "4689792",
                "name": "Irwan Bello"
            },
            {
                "authorId": "2368067",
                "name": "Barret Zoph"
            },
            {
                "authorId": "40348417",
                "name": "Ashish Vaswani"
            },
            {
                "authorId": "1789737",
                "name": "Jonathon Shlens"
            },
            {
                "authorId": "2827616",
                "name": "Quoc V. Le"
            }
        ],
        "abstract": "Convolutional networks have enjoyed much success in many computer vision applications. The convolution operation however has a significant weakness in that it only operates on a local neighbourhood, thus missing global information. Self-attention, on the other hand, has emerged as a recent advance to capture long range interactions, but has mostly been applied to sequence modeling and generative modeling tasks. In this paper, we propose to augment convolutional networks with self-attention by concatenating convolutional feature maps with a set of feature maps produced via a novel relative self-attention mechanism. In particular, we extend previous work on relative self-attention over sequences to images and discuss a memory efficient implementation. Unlike Squeeze-and-Excitation, which performs attention over the channels and ignores spatial information, our self-attention mechanism attends jointly to both features and spatial locations while preserving translation equivariance. We find that Attention Augmentation leads to consistent improvements in image classification on ImageNet and object detection on COCO across many different models and scales, including ResNets and a state-of-the art mobile constrained network, while keeping the number of parameters similar. In particular, our method achieves a 1.3% top-1 accuracy improvement on ImageNet classification over a ResNet50 baseline and outperforms other attention mechanisms for images such as Squeeze-and-Excitation. It also achieves an improvement of 1.4 AP in COCO Object Detection on top of a RetinaNet baseline."
    },
    {
        "paperId": "2c8315ae713b3e27c6e9f291a158134d9c516166",
        "url": "https://www.semanticscholar.org/paper/2c8315ae713b3e27c6e9f291a158134d9c516166",
        "title": "Learning Discriminative Model Prediction for Tracking",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2019,
        "citationCount": 1212,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1904.07220",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1904.07220, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "49922196",
                "name": "Goutam Bhat"
            },
            {
                "authorId": "2488938",
                "name": "Martin Danelljan"
            },
            {
                "authorId": "1681236",
                "name": "L. Gool"
            },
            {
                "authorId": "1732855",
                "name": "Radu Timofte"
            }
        ],
        "abstract": "The current strive towards end-to-end trainable computer vision systems imposes major challenges for the task of visual tracking. In contrast to most other vision problems, tracking requires the learning of a robust target-specific appearance model online, during the inference stage. To be end-to-end trainable, the online learning of the target model thus needs to be embedded in the tracking architecture itself. Due to the imposed challenges, the popular Siamese paradigm simply predicts a target feature template, while ignoring the background appearance information during inference. Consequently, the predicted model possesses limited target-background discriminability. We develop an end-to-end tracking architecture, capable of fully exploiting both target and background appearance information for target model prediction. Our architecture is derived from a discriminative learning loss by designing a dedicated optimization process that is capable of predicting a powerful model in only a few iterations. Furthermore, our approach is able to learn key aspects of the discriminative loss itself. The proposed tracker sets a new state-of-the-art on 6 tracking benchmarks, achieving an EAO score of 0.440 on VOT2018, while running at over 40 FPS. The code and models are available at https://github.com/visionml/pytracking."
    },
    {
        "paperId": "343da6d4cff7ce8c04270487a1f7a037ea0572d6",
        "url": "https://www.semanticscholar.org/paper/343da6d4cff7ce8c04270487a1f7a037ea0572d6",
        "title": "PIFu: Pixel-Aligned Implicit Function for High-Resolution Clothed Human Digitization",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2019,
        "citationCount": 1360,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1905.05172",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1905.05172, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1645244475",
                "name": "Shunsuke Saito"
            },
            {
                "authorId": "2109582492",
                "name": "Zeng Huang"
            },
            {
                "authorId": "41015901",
                "name": "Ryota Natsume"
            },
            {
                "authorId": "1731030",
                "name": "S. Morishima"
            },
            {
                "authorId": "20615377",
                "name": "Angjoo Kanazawa"
            },
            {
                "authorId": null,
                "name": "Hao Li"
            }
        ],
        "abstract": "We introduce Pixel-aligned Implicit Function (PIFu), an implicit representation that locally aligns pixels of 2D images with the global context of their corresponding 3D object. Using PIFu, we propose an end-to-end deep learning method for digitizing highly detailed clothed humans that can infer both 3D surface and texture from a single image, and optionally, multiple input images. Highly intricate shapes, such as hairstyles, clothing, as well as their variations and deformations can be digitized in a unified way. Compared to existing representations used for 3D deep learning, PIFu produces high-resolution surfaces including largely unseen regions such as the back of a person. In particular, it is memory efficient unlike the voxel representation, can handle arbitrary topology, and the resulting surface is spatially aligned with the input image. Furthermore, while previous techniques are designed to process either a single image or multiple views, PIFu extends naturally to arbitrary number of views. We demonstrate high-resolution and robust reconstructions on real world images from the DeepFashion dataset, which contains a variety of challenging clothing types. Our method achieves state-of-the-art performance on a public benchmark and outperforms the prior work for clothed human digitization from a single image."
    },
    {
        "paperId": "346f813f3aee101cc288da86c2cf9aad024bb32a",
        "url": "https://www.semanticscholar.org/paper/346f813f3aee101cc288da86c2cf9aad024bb32a",
        "title": "PANet: Few-Shot Image Semantic Segmentation With Prototype Alignment",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2019,
        "citationCount": 1295,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1908.06391",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1908.06391, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2393792699",
                "name": "Kaixin Wang"
            },
            {
                "authorId": "123200208",
                "name": "J. Liew"
            },
            {
                "authorId": "50817744",
                "name": "Yingtian Zou"
            },
            {
                "authorId": "18119920",
                "name": "Daquan Zhou"
            },
            {
                "authorId": "33221685",
                "name": "Jiashi Feng"
            }
        ],
        "abstract": "Despite the great progress made by deep CNNs in image semantic segmentation, they typically require a large number of densely-annotated images for training and are difficult to generalize to unseen object categories. Few-shot segmentation has thus been developed to learn to perform segmentation from only a few annotated examples. In this paper, we tackle the challenging few-shot segmentation problem from a metric learning perspective and present PANet, a novel prototype alignment network to better utilize the information of the support set. Our PANet learns class-specific prototype representations from a few support images within an embedding space and then performs segmentation over the query images through matching each pixel to the learned prototypes. With non-parametric metric learning, PANet offers high-quality prototypes that are representative for each semantic class and meanwhile discriminative for different classes. Moreover, PANet introduces a prototype alignment regularization between support and query. With this, PANet fully exploits knowledge from the support and provides better generalization on few-shot segmentation. Significantly, our model achieves the mIoU score of 48.1% and 55.7% on PASCAL-5i for 1-shot and 5-shot settings respectively, surpassing the state-of-the-art method by 1.8% and 8.6%."
    },
    {
        "paperId": "3ba8d3060731d64cd46d27e933cbdfb8b7853f4b",
        "url": "https://www.semanticscholar.org/paper/3ba8d3060731d64cd46d27e933cbdfb8b7853f4b",
        "title": "Symmetric Cross Entropy for Robust Learning With Noisy Labels",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2019,
        "citationCount": 1047,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1908.06112",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1908.06112, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1919541",
                "name": "Yisen Wang"
            },
            {
                "authorId": "9576855",
                "name": "Xingjun Ma"
            },
            {
                "authorId": "2424252",
                "name": "Zaiyi Chen"
            },
            {
                "authorId": "2118198559",
                "name": "Yuan Luo"
            },
            {
                "authorId": "2882166",
                "name": "Jinfeng Yi"
            },
            {
                "authorId": "145148600",
                "name": "J. Bailey"
            }
        ],
        "abstract": "Training accurate deep neural networks (DNNs) in the presence of noisy labels is an important and challenging task. Though a number of approaches have been proposed for learning with noisy labels, many open issues remain. In this paper, we show that DNN learning with Cross Entropy (CE) exhibits overfitting to noisy labels on some classes (``easy\" classes), but more surprisingly, it also suffers from significant under learning on some other classes (``hard\" classes). Intuitively, CE requires an extra term to facilitate learning of hard classes, and more importantly, this term should be noise tolerant, so as to avoid overfitting to noisy labels. Inspired by the symmetric KL-divergence, we propose the approach of Symmetric cross entropy Learning (SL), boosting CE symmetrically with a noise robust counterpart Reverse Cross Entropy (RCE). Our proposed SL approach simultaneously addresses both the under learning and overfitting problem of CE in the presence of noisy labels. We provide a theoretical analysis of SL and also empirically show, on a range of benchmark and real-world datasets, that SL outperforms state-of-the-art methods. We also show that SL can be easily incorporated into existing methods in order to further enhance their performance."
    },
    {
        "paperId": "4511f4100decc138031f93212cfd921bf42f72e2",
        "url": "https://www.semanticscholar.org/paper/4511f4100decc138031f93212cfd921bf42f72e2",
        "title": "SemanticKITTI: A Dataset for Semantic Scene Understanding of LiDAR Sequences",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2019,
        "citationCount": 2157,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1904.01416",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICCV.2019.00939?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICCV.2019.00939, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3350812",
                "name": "Jens Behley"
            },
            {
                "authorId": "3370510",
                "name": "Martin Garbade"
            },
            {
                "authorId": "26351048",
                "name": "Andres Milioto"
            },
            {
                "authorId": "7547605",
                "name": "Jan Quenzel"
            },
            {
                "authorId": "1699019",
                "name": "Sven Behnke"
            },
            {
                "authorId": "1722062",
                "name": "C. Stachniss"
            },
            {
                "authorId": "145689714",
                "name": "Juergen Gall"
            }
        ],
        "abstract": "Semantic scene understanding is important for various applications. In particular, self-driving cars need a fine-grained understanding of the surfaces and objects in their vicinity. Light detection and ranging (LiDAR) provides precise geometric information about the environment and is thus a part of the sensor suites of almost all self-driving cars. Despite the relevance of semantic scene understanding for this application, there is a lack of a large dataset for this task which is based on an automotive LiDAR. In this paper, we introduce a large dataset to propel research on laser-based semantic segmentation. We annotated all sequences of the KITTI Vision Odometry Benchmark and provide dense point-wise annotations for the complete 360-degree field-of-view of the employed automotive LiDAR. We propose three benchmark tasks based on this dataset: (i) semantic segmentation of point clouds using a single scan, (ii) semantic segmentation using multiple past scans, and (iii) semantic scene completion, which requires to anticipate the semantic scene in the future. We provide baseline experiments and show that there is a need for more sophisticated models to efficiently tackle these tasks. Our dataset opens the door for the development of more advanced methods, but also provides plentiful data to investigate new research directions."
    },
    {
        "paperId": "4e7dd1e79f0f13650b2612325e6ba8d206dc04fb",
        "url": "https://www.semanticscholar.org/paper/4e7dd1e79f0f13650b2612325e6ba8d206dc04fb",
        "title": "DeepGCNs: Can GCNs Go As Deep As CNNs?",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2019,
        "citationCount": 1503,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1904.03751",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1904.03751, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "49461641",
                "name": "G. Li"
            },
            {
                "authorId": "1391027880",
                "name": "Matthias Mller"
            },
            {
                "authorId": "1872964",
                "name": "Ali K. Thabet"
            },
            {
                "authorId": "2931652",
                "name": "Bernard Ghanem"
            }
        ],
        "abstract": "Convolutional Neural Networks (CNNs) achieve impressive performance in a wide variety of fields. Their success benefited from a massive boost when very deep CNN models were able to be reliably trained. Despite their merits, CNNs fail to properly address problems with non-Euclidean data. To overcome this challenge, Graph Convolutional Networks (GCNs) build graphs to represent non-Euclidean data, borrow concepts from CNNs, and apply them in training. GCNs show promising results, but they are usually limited to very shallow models due to the vanishing gradient problem. As a result, most state-of-the-art GCN models are no deeper than 3 or 4 layers. In this work, we present new ways to successfully train very deep GCNs. We do this by borrowing concepts from CNNs, specifically residual/dense connections and dilated convolutions, and adapting them to GCN architectures. Extensive experiments show the positive effect of these deep GCN frameworks. Finally, we use these new concepts to build a very deep 56-layer GCN, and show how it significantly boosts performance (+3.7% mIoU over state-of-the-art) in the task of point cloud semantic segmentation. We believe that the community can greatly benefit from this work, as it opens up many opportunities for advancing GCN-based research."
    },
    {
        "paperId": "5e19eba1e6644f7c83f607383d256deea71f87ae",
        "url": "https://www.semanticscholar.org/paper/5e19eba1e6644f7c83f607383d256deea71f87ae",
        "title": "Searching for MobileNetV3",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2019,
        "citationCount": 8329,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1905.02244, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "144727050",
                "name": "Andrew G. Howard"
            },
            {
                "authorId": "144882893",
                "name": "M. Sandler"
            },
            {
                "authorId": "2054191762",
                "name": "Grace Chu"
            },
            {
                "authorId": "34192119",
                "name": "Liang-Chieh Chen"
            },
            {
                "authorId": "2152688442",
                "name": "Bo Chen"
            },
            {
                "authorId": "120805419",
                "name": "Mingxing Tan"
            },
            {
                "authorId": "2108301428",
                "name": "Weijun Wang"
            },
            {
                "authorId": "1844940337",
                "name": "Yukun Zhu"
            },
            {
                "authorId": "34320634",
                "name": "Ruoming Pang"
            },
            {
                "authorId": "2053781980",
                "name": "Vijay Vasudevan"
            },
            {
                "authorId": "2827616",
                "name": "Quoc V. Le"
            },
            {
                "authorId": "2595180",
                "name": "Hartwig Adam"
            }
        ],
        "abstract": "We present the next generation of MobileNets based on a combination of complementary search techniques as well as a novel architecture design. MobileNetV3 is tuned to mobile phone CPUs through a combination of hardware-aware network architecture search (NAS) complemented by the NetAdapt algorithm and then subsequently improved through novel architecture advances. This paper starts the exploration of how automated search algorithms and network design can work together to harness complementary approaches improving the overall state of the art. Through this process we create two new MobileNet models for release: MobileNetV3-Large and MobileNetV3-Small which are targeted for high and low resource use cases. These models are then adapted and applied to the tasks of object detection and semantic segmentation. For the task of semantic segmentation (or any dense pixel prediction), we propose a new efficient segmentation decoder Lite Reduced Atrous Spatial Pyramid Pooling (LR-ASPP). We achieve new state of the art results for mobile classification, detection and segmentation. MobileNetV3-Large is 3.2% more accurate on ImageNet classification while reducing latency by 20% compared to MobileNetV2. MobileNetV3-Small is 6.6% more accurate compared to a MobileNetV2 model with comparable latency. MobileNetV3-Large detection is over 25% faster at roughly the same accuracy as MobileNetV2 on COCO detection. MobileNetV3-Large LR-ASPP is 34% faster than MobileNetV2 R-ASPP at similar accuracy for Cityscapes segmentation."
    },
    {
        "paperId": "62931b3e0dce8748364e19c87ef318e22ec59c7f",
        "url": "https://www.semanticscholar.org/paper/62931b3e0dce8748364e19c87ef318e22ec59c7f",
        "title": "Image2StyleGAN: How to Embed Images Into the StyleGAN Latent Space?",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2019,
        "citationCount": 1171,
        "openAccessPdf": {
            "url": "https://orca.cardiff.ac.uk/id/eprint/125332/1/ICCV_StyleGAN_HQ.pdf",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1904.03189, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "94395014",
                "name": "Rameen Abdal"
            },
            {
                "authorId": "2408885",
                "name": "Yipeng Qin"
            },
            {
                "authorId": "1798011",
                "name": "Peter Wonka"
            }
        ],
        "abstract": "We propose an efficient algorithm to embed a given image into the latent space of StyleGAN. This embedding enables semantic image editing operations that can be applied to existing photographs. Taking the StyleGAN trained on the FFHD dataset as an example, we show results for image morphing, style transfer, and expression transfer. Studying the results of the embedding algorithm provides valuable insights into the structure of the StyleGAN latent space. We propose a set of experiments to test what class of images can be embedded, how they are embedded, what latent space is suitable for embedding, and if the embedding is semantically meaningful."
    },
    {
        "paperId": "653d92ca61d77a906eabb880f40cac12f6f1dc12",
        "url": "https://www.semanticscholar.org/paper/653d92ca61d77a906eabb880f40cac12f6f1dc12",
        "title": "YOLACT: Real-Time Instance Segmentation",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2019,
        "citationCount": 1950,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1904.02689",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1904.02689, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2265752454",
                "name": "Daniel Bolya"
            },
            {
                "authorId": "50132897",
                "name": "Chong Zhou"
            },
            {
                "authorId": "2299381",
                "name": "Fanyi Xiao"
            },
            {
                "authorId": "144756076",
                "name": "Yong Jae Lee"
            }
        ],
        "abstract": "We present a simple, fully-convolutional model for real-time instance segmentation that achieves 29.8 mAP on MS COCO at 33.5 fps evaluated on a single Titan Xp, which is significantly faster than any previous competitive approach. Moreover, we obtain this result after training on only one GPU. We accomplish this by breaking instance segmentation into two parallel subtasks: (1) generating a set of prototype masks and (2) predicting per-instance mask coefficients. Then we produce instance masks by linearly combining the prototypes with the mask coefficients. We find that because this process doesn't depend on repooling, this approach produces very high-quality masks and exhibits temporal stability for free. Furthermore, we analyze the emergent behavior of our prototypes and show they learn to localize instances on their own in a translation variant manner, despite being fully-convolutional. Finally, we also propose Fast NMS, a drop-in 12 ms faster replacement for standard NMS that only has a marginal performance penalty."
    },
    {
        "paperId": "690c817ab5be8017ff713fa1028669debde205af",
        "url": "https://www.semanticscholar.org/paper/690c817ab5be8017ff713fa1028669debde205af",
        "title": "AMASS: Archive of Motion Capture As Surface Shapes",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2019,
        "citationCount": 1561,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1904.03278",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1904.03278, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1892850",
                "name": "Naureen Mahmood"
            },
            {
                "authorId": "2066023556",
                "name": "N. Ghorbani"
            },
            {
                "authorId": "2932365",
                "name": "N. Troje"
            },
            {
                "authorId": "1403428213",
                "name": "Gerard Pons-Moll"
            },
            {
                "authorId": "2105795",
                "name": "Michael J. Black"
            }
        ],
        "abstract": "Large datasets are the cornerstone of recent advances in computer vision using deep learning. In contrast, existing human motion capture (mocap) datasets are small and the motions limited, hampering progress on learning models of human motion. While there are many different datasets available, they each use a different parameterization of the body, making it difficult to integrate them into a single meta dataset. To address this, we introduce AMASS, a large and varied database of human motion that unifies 15 different optical marker-based mocap datasets by representing them within a common framework and parameterization. We achieve this using a new method, MoSh++, that converts mocap data into realistic 3D human meshes represented by a rigged body model. Here we use SMPL [Loper et al., 2015], which is widely used and provides a standard skeletal representation as well as a fully rigged surface mesh. The method works for arbitrary marker sets, while recovering soft-tissue dynamics and realistic hand motion. We evaluate MoSh++ and tune its hyperparameters using a new dataset of 4D body scans that are jointly recorded with markerbased mocap. The consistent representation of AMASS makes it readily useful for animation, visualization, and generating training data for deep learning. Our dataset is significantly richer than previous human motion collections, having more than 40 hours of motion data, spanning over 300 subjects, more than 11000 motions, and will be publicly available to the research community."
    },
    {
        "paperId": "81c1dc00b32d12edfab00656593f5db56cfa79e0",
        "url": "https://www.semanticscholar.org/paper/81c1dc00b32d12edfab00656593f5db56cfa79e0",
        "title": "DeblurGAN-v2: Deblurring (Orders-of-Magnitude) Faster and Better",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2019,
        "citationCount": 1036,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1908.03826",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1908.03826, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "47133874",
                "name": "Orest Kupyn"
            },
            {
                "authorId": "2067333280",
                "name": "T. Martyniuk"
            },
            {
                "authorId": "14737712",
                "name": "Junru Wu"
            },
            {
                "authorId": "2969311",
                "name": "Zhangyang Wang"
            }
        ],
        "abstract": "We present a new end-to-end generative adversarial network (GAN) for single image motion deblurring, named DeblurGAN-V2, which considerably boosts state-of-the-art deblurring performance while being much more flexible and efficient. DeblurGAN-V2 is based on a relativistic conditional GAN with a double-scale discriminator. For the first time, we introduce the Feature Pyramid Network into deblurring, as a core building block in the generator of DeblurGAN-V2. It can flexibly work with a wide range of backbones, to navigate the balance between performance and efficiency. The plug-in of sophisticated backbones (e.g. Inception ResNet v2) can lead to solid state-of-the-art performance. Meanwhile, with light-weight backbones (e.g. MobileNet and its variants), DeblurGAN-V2 becomes 10-100 times faster than the nearest competitors, while maintaining close to state-of-the-art results, implying the option of real-time video deblurring. We demonstrate that DeblurGAN-V2 has very competitive performance on several popular benchmarks, in terms of deblurring quality (both objective and subjective), as well as efficiency. In addition, we show the architecture to be effective for general image restoration tasks too. Our models and codes will be made available upon acceptance."
    },
    {
        "paperId": "9218b3e7e447048a070e565b9add26e41d861964",
        "url": "https://www.semanticscholar.org/paper/9218b3e7e447048a070e565b9add26e41d861964",
        "title": "KPConv: Flexible and Deformable Convolution for Point Clouds",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2019,
        "citationCount": 2965,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1904.08889",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1904.08889, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "30535797",
                "name": "Hugues Thomas"
            },
            {
                "authorId": "144329939",
                "name": "C. Qi"
            },
            {
                "authorId": "1832264",
                "name": "Jean-Emmanuel Deschaud"
            },
            {
                "authorId": "1740693",
                "name": "B. Marcotegui"
            },
            {
                "authorId": "1698805",
                "name": "F. Goulette"
            },
            {
                "authorId": "51352814",
                "name": "L. Guibas"
            }
        ],
        "abstract": "We present Kernel Point Convolution (KPConv), a new design of point convolution, i.e. that operates on point clouds without any intermediate representation. The convolution weights of KPConv are located in Euclidean space by kernel points, and applied to the input points close to them. Its capacity to use any number of kernel points gives KPConv more flexibility than fixed grid convolutions. Furthermore, these locations are continuous in space and can be learned by the network. Therefore, KPConv can be extended to deformable convolutions that learn to adapt kernel points to local geometry. Thanks to a regular subsampling strategy, KPConv is also efficient and robust to varying densities. Whether they use deformable KPConv for complex tasks, or rigid KPconv for simpler tasks, our networks outperform state-of-the-art classification and segmentation approaches on several datasets. We also offer ablation studies and visualizations to provide understanding of what has been learned by KPConv and to validate the descriptive power of deformable KPConv."
    },
    {
        "paperId": "9311779489e597315488749ee6c386bfa3f3512e",
        "url": "https://www.semanticscholar.org/paper/9311779489e597315488749ee6c386bfa3f3512e",
        "title": "HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2019,
        "citationCount": 1357,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1906.03327",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1906.03327, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "19200186",
                "name": "Antoine Miech"
            },
            {
                "authorId": "35838466",
                "name": "Dimitri Zhukov"
            },
            {
                "authorId": "2285263",
                "name": "Jean-Baptiste Alayrac"
            },
            {
                "authorId": "2103464",
                "name": "Makarand Tapaswi"
            },
            {
                "authorId": "143991676",
                "name": "I. Laptev"
            },
            {
                "authorId": "1782755",
                "name": "Josef Sivic"
            }
        ],
        "abstract": "Learning text-video embeddings usually requires a dataset of video clips with manually provided captions. However, such datasets are expensive and time consuming to create and therefore difficult to obtain on a large scale. In this work, we propose instead to learn such embeddings from video data with readily available natural language annotations in the form of automatically transcribed narrations. The contributions of this work are three-fold. First, we introduce HowTo100M: a large-scale dataset of 136 million video clips sourced from 1.22M narrated instructional web videos depicting humans performing and describing over 23k different visual tasks. Our data collection procedure is fast, scalable and does not require any additional manual annotation. Second, we demonstrate that a text-video embedding trained on this data leads to state-of-the-art results for text-to-video retrieval and action localization on instructional video datasets such as YouCook2 or CrossTask. Finally, we show that this embedding transfers well to other domains: fine-tuning on generic Youtube videos (MSR-VTT dataset) and movies (LSMDC dataset) outperforms models trained on these datasets alone. Our dataset, code and models are publicly available."
    },
    {
        "paperId": "9662b48d9b8a8f2118487a7f3be2d76283848627",
        "url": "https://www.semanticscholar.org/paper/9662b48d9b8a8f2118487a7f3be2d76283848627",
        "title": "CenterNet: Keypoint Triplets for Object Detection",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2019,
        "citationCount": 3155,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1904.08189",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1904.08189, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "36686543",
                "name": "Kaiwen Duan"
            },
            {
                "authorId": "47651566",
                "name": "S. Bai"
            },
            {
                "authorId": "3041937",
                "name": "Lingxi Xie"
            },
            {
                "authorId": "144097734",
                "name": "H. Qi"
            },
            {
                "authorId": "1689702",
                "name": "Qingming Huang"
            },
            {
                "authorId": "144876831",
                "name": "Q. Tian"
            }
        ],
        "abstract": "In object detection, keypoint-based approaches often experience the drawback of a large number of incorrect object bounding boxes, arguably due to the lack of an additional assessment inside cropped regions. This paper presents an efficient solution that explores the visual patterns within individual cropped regions with minimal costs. We build our framework upon a representative one-stage keypoint-based detector named CornerNet. Our approach, named CenterNet, detects each object as a triplet, rather than a pair, of keypoints, which improves both precision and recall. Accordingly, we design two customized modules, cascade corner pooling, and center pooling, that enrich information collected by both the top-left and bottom-right corners and provide more recognizable information from the central regions. On the MS-COCO dataset, CenterNet achieves an AP of 47.0 %, outperforming all existing one-stage detectors by at least 4.9%. Furthermore, with a faster inference speed than the top-ranked two-stage detectors, CenterNet demonstrates a comparable performance to these detectors. Code is available at https://github.com/Duankaiwen/CenterNet."
    },
    {
        "paperId": "9fe3cebb4454abc5d3bcfcad9c3228fbacdbdb08",
        "url": "https://www.semanticscholar.org/paper/9fe3cebb4454abc5d3bcfcad9c3228fbacdbdb08",
        "title": "Similarity-Preserving Knowledge Distillation",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2019,
        "citationCount": 1128,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1907.09682",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1907.09682, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2065573607",
                "name": "Frederick Tung"
            },
            {
                "authorId": "10771328",
                "name": "Greg Mori"
            }
        ],
        "abstract": "Knowledge distillation is a widely applicable technique for training a student neural network under the guidance of a trained teacher network. For example, in neural network compression, a high-capacity teacher is distilled to train a compact student; in privileged learning, a teacher trained with privileged data is distilled to train a student without access to that data. The distillation loss determines how a teacher's knowledge is captured and transferred to the student. In this paper, we propose a new form of knowledge distillation loss that is inspired by the observation that semantically similar inputs tend to elicit similar activation patterns in a trained network. Similarity-preserving knowledge distillation guides the training of a student network such that input pairs that produce similar (dissimilar) activations in the teacher network produce similar (dissimilar) activations in the student network. In contrast to previous distillation methods, the student is not required to mimic the representation space of the teacher, but rather to preserve the pairwise similarities in its own representation space. Experiments on three public datasets demonstrate the potential of our approach."
    },
    {
        "paperId": "a8cab29d2230924dffe89d6dda15ba42790c5ebf",
        "url": "https://www.semanticscholar.org/paper/a8cab29d2230924dffe89d6dda15ba42790c5ebf",
        "title": "Be Your Own Teacher: Improve the Performance of Convolutional Neural Networks via Self Distillation",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2019,
        "citationCount": 1002,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1905.08094",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1905.08094, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "50081570",
                "name": "Linfeng Zhang"
            },
            {
                "authorId": "122151709",
                "name": "Jiebo Song"
            },
            {
                "authorId": "94970361",
                "name": "Anni Gao"
            },
            {
                "authorId": "2144141924",
                "name": "Jingwei Chen"
            },
            {
                "authorId": "2060106235",
                "name": "Chenglong Bao"
            },
            {
                "authorId": "2075321204",
                "name": "Kaisheng Ma"
            }
        ],
        "abstract": "Convolutional neural networks have been widely deployed in various application scenarios. In order to extend the applications' boundaries to some accuracy-crucial domains, researchers have been investigating approaches to boost accuracy through either deeper or wider network structures, which brings with them the exponential increment of the computational and storage cost, delaying the responding time. In this paper, we propose a general training framework named self distillation, which notably enhances the performance (accuracy) of convolutional neural networks through shrinking the size of the network rather than aggrandizing it. Different from traditional knowledge distillation - a knowledge transformation methodology among networks, which forces student neural networks to approximate the softmax layer outputs of pre-trained teacher neural networks, the proposed self distillation framework distills knowledge within network itself. The networks are firstly divided into several sections. Then the knowledge in the deeper portion of the networks is squeezed into the shallow ones. Experiments further prove the generalization of the proposed self distillation framework: enhancement of accuracy at average level is 2.65%, varying from 0.61% in ResNeXt as minimum to 4.07% in VGG19 as maximum. In addition, it can also provide flexibility of depth-wise scalable inference on resource-limited edge devices. Our codes have been released on github."
    },
    {
        "paperId": "b4a35e548de27b6924e5f2ee41d37238a5c4a1d5",
        "url": "https://www.semanticscholar.org/paper/b4a35e548de27b6924e5f2ee41d37238a5c4a1d5",
        "title": "Habitat: A Platform for Embodied AI Research",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2019,
        "citationCount": 1660,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1904.01201",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1904.01201, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2295141",
                "name": "M. Savva"
            },
            {
                "authorId": "89942851",
                "name": "Abhishek Kadian"
            },
            {
                "authorId": "90536527",
                "name": "Oleksandr Maksymets"
            },
            {
                "authorId": "2329715592",
                "name": "Yili Zhao"
            },
            {
                "authorId": "8405939",
                "name": "Erik Wijmans"
            },
            {
                "authorId": "89728623",
                "name": "Bhavana Jain"
            },
            {
                "authorId": "20128275",
                "name": "Julian Straub"
            },
            {
                "authorId": "2116493720",
                "name": "Jia Liu"
            },
            {
                "authorId": "145231047",
                "name": "V. Koltun"
            },
            {
                "authorId": "143751119",
                "name": "Jitendra Malik"
            },
            {
                "authorId": "153432684",
                "name": "Devi Parikh"
            },
            {
                "authorId": "1746610",
                "name": "Dhruv Batra"
            }
        ],
        "abstract": "We present Habitat, a platform for research in embodied artificial intelligence (AI). Habitat enables training embodied agents (virtual robots) in highly efficient photorealistic 3D simulation. Specifically, Habitat consists of: (i) Habitat-Sim: a flexible, high-performance 3D simulator with configurable agents, sensors, and generic 3D dataset handling. Habitat-Sim is fast -- when rendering a scene from Matterport3D, it achieves several thousand frames per second (fps) running single-threaded, and can reach over 10,000 fps multi-process on a single GPU. (ii) Habitat-API: a modular high-level library for end-to-end development of embodied AI algorithms -- defining tasks (e.g., navigation, instruction following, question answering), configuring, training, and benchmarking embodied agents. These large-scale engineering contributions enable us to answer scientific questions requiring experiments that were till now impracticable or 'merely' impractical. Specifically, in the context of point-goal navigation: (1) we revisit the comparison between learning and SLAM approaches from two recent works and find evidence for the opposite conclusion -- that learning outperforms SLAM if scaled to an order of magnitude more experience than previous investigations, and (2) we conduct the first cross-dataset generalization experiments {train, test} x {Matterport3D, Gibson} for multiple sensors {blind, RGB, RGBD, D} and find that only agents with depth (D) sensors generalize across datasets. We hope that our open-source platform and these findings will advance research in embodied AI."
    },
    {
        "paperId": "b4f8c1353aa2d88cacfaef1b3afba74dbf427d89",
        "url": "https://www.semanticscholar.org/paper/b4f8c1353aa2d88cacfaef1b3afba74dbf427d89",
        "title": "FaceForensics++: Learning to Detect Manipulated Facial Images",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2019,
        "citationCount": 2596,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1901.08971",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1901.08971, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1784530",
                "name": "Andreas Rssler"
            },
            {
                "authorId": "34188567",
                "name": "D. Cozzolino"
            },
            {
                "authorId": "1730255",
                "name": "L. Verdoliva"
            },
            {
                "authorId": "145062425",
                "name": "C. Riess"
            },
            {
                "authorId": "34105638",
                "name": "Justus Thies"
            },
            {
                "authorId": "2209612",
                "name": "M. Niener"
            }
        ],
        "abstract": "The rapid progress in synthetic image generation and manipulation has now come to a point where it raises significant concerns for the implications towards society. At best, this leads to a loss of trust in digital content, but could potentially cause further harm by spreading false information or fake news. This paper examines the realism of state-of-the-art image manipulations, and how difficult it is to detect them, either automatically or by humans. To standardize the evaluation of detection methods, we propose an automated benchmark for facial manipulation detection. In particular, the benchmark is based on Deep-Fakes, Face2Face, FaceSwap and NeuralTextures as prominent representatives for facial manipulations at random compression level and size. The benchmark is publicly available and contains a hidden test set as well as a database of over 1.8 million manipulated images. This dataset is over an order of magnitude larger than comparable, publicly available, forgery datasets. Based on this data, we performed a thorough analysis of data-driven forgery detectors. We show that the use of additional domain-specific knowledge improves forgery detection to unprecedented accuracy, even in the presence of strong compression, and clearly outperforms human observers."
    },
    {
        "paperId": "c41a11c0e9b8b92b4faaf97749841170b760760a",
        "url": "https://www.semanticscholar.org/paper/c41a11c0e9b8b92b4faaf97749841170b760760a",
        "title": "VideoBERT: A Joint Model for Video and Language Representation Learning",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2019,
        "citationCount": 1347,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/1904.01766",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1904.01766, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1491624845",
                "name": "Chen Sun"
            },
            {
                "authorId": "49588480",
                "name": "Austin Myers"
            },
            {
                "authorId": "1856025",
                "name": "Carl Vondrick"
            },
            {
                "authorId": "1702318",
                "name": "K. Murphy"
            },
            {
                "authorId": "2462253",
                "name": "C. Schmid"
            }
        ],
        "abstract": "Self-supervised learning has become increasingly important to leverage the abundance of unlabeled data available on platforms like YouTube. Whereas most existing approaches learn low-level representations, we propose a joint visual-linguistic model to learn high-level features without any explicit supervision. In particular, inspired by its recent success in language modeling, we build upon the BERT model to learn bidirectional joint distributions over sequences of visual and linguistic tokens, derived from vector quantization of video data and off-the-shelf speech recognition outputs, respectively. We use VideoBERT in numerous tasks, including action classification and video captioning. We show that it can be applied directly to open-vocabulary classification, and confirm that large amounts of training data and cross-modal information are critical to performance. Furthermore, we outperform the state-of-the-art on video captioning, and quantitative results verify that the model learns high-level semantic features."
    },
    {
        "paperId": "d65eb30e5f0d2013fd5e4f45d1413bc2969ee803",
        "url": "https://www.semanticscholar.org/paper/d65eb30e5f0d2013fd5e4f45d1413bc2969ee803",
        "title": "Memorizing Normality to Detect Anomaly: Memory-Augmented Deep Autoencoder for Unsupervised Anomaly Detection",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2019,
        "citationCount": 1531,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1904.02639",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1904.02639, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "145542268",
                "name": "Dong Gong"
            },
            {
                "authorId": "2161037",
                "name": "Lingqiao Liu"
            },
            {
                "authorId": "144672395",
                "name": "Vuong Le"
            },
            {
                "authorId": "1735360",
                "name": "Budhaditya Saha"
            },
            {
                "authorId": "1761350",
                "name": "M. Mansour"
            },
            {
                "authorId": "143761093",
                "name": "S. Venkatesh"
            },
            {
                "authorId": "5546141",
                "name": "Anton van den Hengel"
            }
        ],
        "abstract": "Deep autoencoder has been extensively used for anomaly detection. Training on the normal data, the autoencoder is expected to produce higher reconstruction error for the abnormal inputs than the normal ones, which is adopted as a criterion for identifying anomalies. However, this assumption does not always hold in practice. It has been observed that sometimes the autoencoder \"generalizes\" so well that it can also reconstruct anomalies well, leading to the miss detection of anomalies. To mitigate this drawback for autoencoder based anomaly detector, we propose to augment the autoencoder with a memory module and develop an improved autoencoder called memory-augmented autoencoder, i.e. MemAE. Given an input, MemAE firstly obtains the encoding from the encoder and then uses it as a query to retrieve the most relevant memory items for reconstruction. At the training stage, the memory contents are updated and are encouraged to represent the prototypical elements of the normal data. At the test stage, the learned memory will be fixed, and the reconstruction is obtained from a few selected memory records of the normal data. The reconstruction will thus tend to be close to a normal sample. Thus the reconstructed errors on anomalies will be strengthened for anomaly detection. MemAE is free of assumptions on the data type and thus general to be applied to different tasks. Experiments on various datasets prove the excellent generalization and high effectiveness of the proposed MemAE."
    },
    {
        "paperId": "e2751a898867ce6687e08a5cc7bdb562e999b841",
        "url": "https://www.semanticscholar.org/paper/e2751a898867ce6687e08a5cc7bdb562e999b841",
        "title": "FCOS: Fully Convolutional One-Stage Object Detection",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2019,
        "citationCount": 5708,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1904.01355",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1904.01355, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "40219976",
                "name": "Zhi Tian"
            },
            {
                "authorId": "1780381",
                "name": "Chunhua Shen"
            },
            {
                "authorId": "2029503517",
                "name": "Hao Chen"
            },
            {
                "authorId": "2118328320",
                "name": "Tong He"
            }
        ],
        "abstract": "We propose a fully convolutional one-stage object detector (FCOS) to solve object detection in a per-pixel prediction fashion, analogue to semantic segmentation. Almost all state-of-the-art object detectors such as RetinaNet, SSD, YOLOv3, and Faster R-CNN rely on pre-defined anchor boxes. In contrast, our proposed detector FCOS is anchor box free, as well as proposal free. By eliminating the pre-defined set of anchor boxes, FCOS completely avoids the complicated computation related to anchor boxes such as calculating overlapping during training. More importantly, we also avoid all hyper-parameters related to anchor boxes, which are often very sensitive to the final detection performance. With the only post-processing non-maximum suppression (NMS), FCOS with ResNeXt-64x4d-101 achieves 44.7% in AP with single-model and single-scale testing, surpassing previous one-stage detectors with the advantage of being much simpler. For the first time, we demonstrate a much simpler and flexible detection framework achieving improved detection accuracy. We hope that the proposed FCOS framework can serve as a simple and strong alternative for many other instance-level tasks. Code is available at: https://tinyurl.com/FCOSv1"
    },
    {
        "paperId": "e60da8d3a79801a3ccbf1abcdd001bb6e001b267",
        "url": "https://www.semanticscholar.org/paper/e60da8d3a79801a3ccbf1abcdd001bb6e001b267",
        "title": "Deep Hough Voting for 3D Object Detection in Point Clouds",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2019,
        "citationCount": 1420,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/1904.09664",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1904.09664, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "144329939",
                "name": "C. Qi"
            },
            {
                "authorId": "2528439",
                "name": "O. Litany"
            },
            {
                "authorId": "39353098",
                "name": "Kaiming He"
            },
            {
                "authorId": "51352814",
                "name": "L. Guibas"
            }
        ],
        "abstract": "Current 3D object detection methods are heavily influenced by 2D detectors. In order to leverage architectures in 2D detectors, they often convert 3D point clouds to regular grids (i.e., to voxel grids or to bird's eye view images), or rely on detection in 2D images to propose 3D boxes. Few works have attempted to directly detect objects in point clouds. In this work, we return to first principles to construct a 3D detection pipeline for point cloud data and as generic as possible. However, due to the sparse nature of the data -- samples from 2D manifolds in 3D space -- we face a major challenge when directly predicting bounding box parameters from scene points: a 3D object centroid can be far from any surface point thus hard to regress accurately in one step. To address the challenge, we propose VoteNet, an end-to-end 3D object detection network based on a synergy of deep point set networks and Hough voting. Our model achieves state-of-the-art 3D detection on two large datasets of real 3D scans, ScanNet and SUN RGB-D with a simple design, compact model size and high efficiency. Remarkably, VoteNet outperforms previous methods by using purely geometric information without relying on color images."
    },
    {
        "paperId": "ed17929e66da7f8fbc3666bf5eb613d302ddde0c",
        "url": "https://www.semanticscholar.org/paper/ed17929e66da7f8fbc3666bf5eb613d302ddde0c",
        "title": "CutMix: Regularization Strategy to Train Strong Classifiers With Localizable Features",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2019,
        "citationCount": 5492,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1905.04899",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1905.04899, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2151587",
                "name": "Sangdoo Yun"
            },
            {
                "authorId": "2086576",
                "name": "Dongyoon Han"
            },
            {
                "authorId": "2390510",
                "name": "Seong Joon Oh"
            },
            {
                "authorId": "2647582",
                "name": "Sanghyuk Chun"
            },
            {
                "authorId": "3338475",
                "name": "Junsuk Choe"
            },
            {
                "authorId": "2347316",
                "name": "Y. Yoo"
            }
        ],
        "abstract": "Regional dropout strategies have been proposed to enhance performance of convolutional neural network classifiers. They have proved to be effective for guiding the model to attend on less discriminative parts of objects (e.g. leg as opposed to head of a person), thereby letting the network generalize better and have better object localization capabilities. On the other hand, current methods for regional dropout removes informative pixels on training images by overlaying a patch of either black pixels or random noise. Such removal is not desirable because it suffers from information loss causing inefficiency in training. We therefore propose the CutMix augmentation strategy: patches are cut and pasted among training images where the ground truth labels are also mixed proportionally to the area of the patches. By making efficient use of training pixels and retaining the regularization effect of regional dropout, CutMix consistently outperforms state-of-the-art augmentation strategies on CIFAR and ImageNet classification tasks, as well as on ImageNet weakly-supervised localization task. Moreover, unlike previous augmentation methods, our CutMix-trained ImageNet classifier, when used as a pretrained model, results in consistent performance gain in Pascal detection and MS-COCO image captioning benchmarks. We also show that CutMix can improve the model robustness against input corruptions and its out-of distribution detection performance."
    },
    {
        "paperId": "fab133bb7c0e7208ee68a90fe03f2da4d1dee245",
        "url": "https://www.semanticscholar.org/paper/fab133bb7c0e7208ee68a90fe03f2da4d1dee245",
        "title": "Scale-Aware Trident Networks for Object Detection",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2019,
        "citationCount": 1000,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1901.01892",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1901.01892, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2366569300",
                "name": "Yanghao Li"
            },
            {
                "authorId": "2798406",
                "name": "Yuntao Chen"
            },
            {
                "authorId": "48246959",
                "name": "Naiyan Wang"
            },
            {
                "authorId": "145274329",
                "name": "Zhaoxiang Zhang"
            }
        ],
        "abstract": "Scale variation is one of the key challenges in object detection. In this work, we rst present a controlled experiment to investigate the effect of receptive elds for scale variation in object detection. Based on the ndings from the exploration experiments, we propose a novel Trident Network (TridentNet) aiming to generate scale-specic feature maps with a uniform representational power. We construct a parallel multi-branch architecture in which each branch shares the same transformation parameters but with different receptive elds. Then, we adopt a scale-aware training scheme to specialize each branch by sampling object instances of proper scales for training. As a bonus, a fast approximation version of TridentNet could achieve signicant improvements without any additional parameters and computational cost compared with the vanilla detector. On the COCO dataset, our TridentNet with ResNet-101 backbone achieves state-of-the-art single-model results of 48.4 mAP. Codes are available at https://git.io/fj5vR."
    },
    {
        "paperId": "022622e024890d6e044ac50e2da6b44c59bdf418",
        "url": "https://www.semanticscholar.org/paper/022622e024890d6e044ac50e2da6b44c59bdf418",
        "title": "The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2020,
        "citationCount": 2093,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2006.16241",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2006.16241, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3422872",
                "name": "Dan Hendrycks"
            },
            {
                "authorId": "104444594",
                "name": "Steven Basart"
            },
            {
                "authorId": "52227748",
                "name": "Norman Mu"
            },
            {
                "authorId": "148070327",
                "name": "Saurav Kadavath"
            },
            {
                "authorId": "2112315803",
                "name": "Frank Wang"
            },
            {
                "authorId": "1779776376",
                "name": "Evan Dorundo"
            },
            {
                "authorId": "2060225103",
                "name": "R. Desai"
            },
            {
                "authorId": "8791781",
                "name": "Tyler Lixuan Zhu"
            },
            {
                "authorId": "2310137191",
                "name": "Samyak Parajuli"
            },
            {
                "authorId": "2107683161",
                "name": "Mike Guo"
            },
            {
                "authorId": "143711382",
                "name": "D. Song"
            },
            {
                "authorId": "5164568",
                "name": "J. Steinhardt"
            },
            {
                "authorId": "2058362",
                "name": "J. Gilmer"
            }
        ],
        "abstract": "We introduce four new real-world distribution shift datasets consisting of changes in image style, image blurriness, geographic location, camera operation, and more. With our new datasets, we take stock of previously proposed methods for improving out-of-distribution robustness and put them to the test. We find that using larger models and artificial data augmentations can improve robustness on real-world distribution shifts, contrary to claims in prior work. We find improvements in artificial robustness benchmarks can transfer to real-world distribution shifts, contrary to claims in prior work. Motivated by our observation that data augmentations can help with real-world distribution shifts, we also introduce a new data augmentation method which advances the state-of-the-art and outperforms models pre-trained with 1000 more labeled data. Overall we find that some methods consistently help with distribution shifts in texture and local image statistics, but these methods do not help with some other distribution shifts like geographic changes. Our results show that future research must study multiple distribution shifts simultaneously, as we demonstrate that no evaluated method consistently improves robustness."
    },
    {
        "paperId": "0f1af3f94f4699cd70a554f68f8f9e2c8e3d53dd",
        "url": "https://www.semanticscholar.org/paper/0f1af3f94f4699cd70a554f68f8f9e2c8e3d53dd",
        "title": "Nerfies: Deformable Neural Radiance Fields",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2020,
        "citationCount": 1331,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2011.12948",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2011.12948, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "81720702",
                "name": "Keunhong Park"
            },
            {
                "authorId": "34570587",
                "name": "U. Sinha"
            },
            {
                "authorId": "50329510",
                "name": "J. Barron"
            },
            {
                "authorId": "35119991",
                "name": "Sofien Bouaziz"
            },
            {
                "authorId": "1976171",
                "name": "Dan B. Goldman"
            },
            {
                "authorId": "1679223",
                "name": "S. Seitz"
            },
            {
                "authorId": "1401885873",
                "name": "Ricardo Martin-Brualla"
            }
        ],
        "abstract": "We present the first method capable of photorealistically reconstructing deformable scenes using photos/videos captured casually from mobile phones. Our approach augments neural radiance fields (NeRF) by optimizing an additional continuous volumetric deformation field that warps each observed point into a canonical 5D NeRF. We observe that these NeRF-like deformation fields are prone to local minima, and propose a coarse-to-fine optimization method for coordinate-based models that allows for more robust optimization. By adapting principles from geometry processing and physical simulation to NeRF-like models, we propose an elastic regularization of the deformation field that further improves robustness. We show that our method can turn casually captured selfie photos/videos into deformable NeRF models that allow for photorealistic renderings of the subject from arbitrary viewpoints, which we dub \"nerfies.\" We evaluate our method by collecting time-synchronized data using a rig with two mobile phones, yielding train/validation images of the same pose at different viewpoints. We show that our method faithfully reconstructs non-rigidly deforming scenes and reproduces unseen views with high fidelity."
    },
    {
        "paperId": "0eff37167876356da2163b2e396df2719adf7de9",
        "url": "https://www.semanticscholar.org/paper/0eff37167876356da2163b2e396df2719adf7de9",
        "title": "CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2021,
        "citationCount": 1875,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2103.14899",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2103.14899, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "48239920",
                "name": "Chun-Fu Chen"
            },
            {
                "authorId": "33421444",
                "name": "Quanfu Fan"
            },
            {
                "authorId": "1819152",
                "name": "Rameswar Panda"
            }
        ],
        "abstract": "The recently developed vision transformer (ViT) has achieved promising results on image classification compared to convolutional neural networks. Inspired by this, in this paper, we study how to learn multi-scale feature representations in transformer models for image classification. To this end, we propose a dual-branch transformer to com-bine image patches (i.e., tokens in a transformer) of different sizes to produce stronger image features. Our approach processes small-patch and large-patch tokens with two separate branches of different computational complexity and these tokens are then fused purely by attention multiple times to complement each other. Furthermore, to reduce computation, we develop a simple yet effective token fusion module based on cross attention, which uses a single token for each branch as a query to exchange information with other branches. Our proposed cross-attention only requires linear time for both computational and memory complexity instead of quadratic time otherwise. Extensive experiments demonstrate that our approach performs better than or on par with several concurrent works on vision transformer, in addition to efficient CNN models. For example, on the ImageNet1K dataset, with some architectural changes, our approach outperforms the recent DeiT by a large margin of 2% with a small to moderate increase in FLOPs and model parameters. Our source codes and models are available at https://github.com/IBM/CrossViT."
    },
    {
        "paperId": "18863dbfa32eaa1ccdb56ff180e6ab079a7f1ec6",
        "url": "https://www.semanticscholar.org/paper/18863dbfa32eaa1ccdb56ff180e6ab079a7f1ec6",
        "title": "Multiscale Vision Transformers",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2021,
        "citationCount": 1492,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2104.11227",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2104.11227, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "146884473",
                "name": "Haoqi Fan"
            },
            {
                "authorId": "144752314",
                "name": "Bo Xiong"
            },
            {
                "authorId": "11379939",
                "name": "K. Mangalam"
            },
            {
                "authorId": "2359205979",
                "name": "Yanghao Li"
            },
            {
                "authorId": "151485208",
                "name": "Zhicheng Yan"
            },
            {
                "authorId": "153652147",
                "name": "J. Malik"
            },
            {
                "authorId": "2322150",
                "name": "Christoph Feichtenhofer"
            }
        ],
        "abstract": "We present Multiscale Vision Transformers (MViT) for video and image recognition, by connecting the seminal idea of multiscale feature hierarchies with transformer models. Multiscale Transformers have several channel-resolution scale stages. Starting from the input resolution and a small channel dimension, the stages hierarchically expand the channel capacity while reducing the spatial resolution. This creates a multiscale pyramid of features with early layers operating at high spatial resolution to model simple low-level visual information, and deeper layers at spatially coarse, but complex, high-dimensional features. We evaluate this fundamental architectural prior for modeling the dense nature of visual signals for a variety of video recognition tasks where it outperforms concurrent vision transformers that rely on large scale external pre-training and are 5-10 more costly in computation and parameters. We further remove the temporal dimension and apply our model for image classification where it outperforms prior work on vision transformers. Code is available at: https://github.com/facebookresearch/SlowFast."
    },
    {
        "paperId": "21336e57dc2ab9ae2171a0f6c35f7d1aba584796",
        "url": "https://www.semanticscholar.org/paper/21336e57dc2ab9ae2171a0f6c35f7d1aba584796",
        "title": "Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2021,
        "citationCount": 2437,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2103.13415",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2103.13415, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "50329510",
                "name": "J. Barron"
            },
            {
                "authorId": "2577533",
                "name": "B. Mildenhall"
            },
            {
                "authorId": "7638730",
                "name": "Matthew Tancik"
            },
            {
                "authorId": "33810877",
                "name": "Peter Hedman"
            },
            {
                "authorId": "1401885873",
                "name": "Ricardo Martin-Brualla"
            },
            {
                "authorId": "2179732",
                "name": "Pratul P. Srinivasan"
            }
        ],
        "abstract": "The rendering procedure used by neural radiance fields (NeRF) samples a scene with a single ray per pixel and may therefore produce renderings that are excessively blurred or aliased when training or testing images observe scene content at different resolutions. The straightforward solution of supersampling by rendering with multiple rays per pixel is impractical for NeRF, because rendering each ray requires querying a multilayer perceptron hundreds of times. Our solution, which we call \"mip-NeRF\" ( la \"mipmap\"), extends NeRF to represent the scene at a continuously-valued scale. By efficiently rendering anti-aliased conical frustums instead of rays, mip-NeRF reduces objectionable aliasing artifacts and significantly improves NeRFs ability to represent fine details, while also being 7% faster than NeRF and half the size. Compared to NeRF, mip-NeRF reduces average error rates by 17% on the dataset presented with NeRF and by 60% on a challenging multiscale variant of that dataset that we present. Mip-NeRF is also able to match the accuracy of a brute-force supersampled NeRF on our multiscale dataset while being 22 faster."
    },
    {
        "paperId": "5744fcc21b40327f7ad710de7d947d4584c53012",
        "url": "https://www.semanticscholar.org/paper/5744fcc21b40327f7ad710de7d947d4584c53012",
        "title": "PlenOctrees for Real-time Rendering of Neural Radiance Fields",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2021,
        "citationCount": 1225,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2103.14024",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2103.14024, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2024370854",
                "name": "Alex Yu"
            },
            {
                "authorId": "51104559",
                "name": "Ruilong Li"
            },
            {
                "authorId": "7638730",
                "name": "Matthew Tancik"
            },
            {
                "authorId": "1706574",
                "name": "Hao Li"
            },
            {
                "authorId": "47383180",
                "name": "Ren Ng"
            },
            {
                "authorId": "20615377",
                "name": "Angjoo Kanazawa"
            }
        ],
        "abstract": "We introduce a method to render Neural Radiance Fields (NeRFs) in real time using PlenOctrees, an octree-based 3D representation which supports view-dependent effects. Our method can render 800800 images at more than 150 FPS, which is over 3000 times faster than conventional NeRFs. We do so without sacrificing quality while preserving the ability of NeRFs to perform free-viewpoint rendering of scenes with arbitrary geometry and view-dependent effects. Real-time performance is achieved by pre-tabulating the NeRF into a PlenOctree. In order to preserve view-dependent effects such as specularities, we factorize the appearance via closed-form spherical basis functions. Specifically, we show that it is possible to train NeRFs to predict a spherical harmonic representation of radiance, removing the viewing direction as an input to the neural network. Furthermore, we show that PlenOctrees can be directly optimized to further minimize the reconstruction loss, which leads to equal or better quality compared to competing methods. Moreover, this octree optimization step can be used to reduce the training time, as we no longer need to wait for the NeRF training to converge fully. Our real-time neural rendering approach may potentially enable new applications such as 6-DOF industrial and product visualizations, as well as next generation AR/VR systems. PlenOctrees are amenable to in-browser rendering as well; please visit the project page for the interactive online demo, as well as video and code: https://alexyu.net/plenoctrees."
    },
    {
        "paperId": "68f080e0ac836ea230cb5316fbed273c70422d75",
        "url": "https://www.semanticscholar.org/paper/68f080e0ac836ea230cb5316fbed273c70422d75",
        "title": "Segmenter: Transformer for Semantic Segmentation",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2021,
        "citationCount": 1760,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2105.05633",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2105.05633, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "86898863",
                "name": "Robin Strudel"
            },
            {
                "authorId": "2133420734",
                "name": "Ricardo Garcia Pinel"
            },
            {
                "authorId": "143991676",
                "name": "I. Laptev"
            },
            {
                "authorId": "2462253",
                "name": "C. Schmid"
            }
        ],
        "abstract": "Image segmentation is often ambiguous at the level of individual image patches and requires contextual information to reach label consensus. In this paper we introduce Segmenter, a transformer model for semantic segmentation. In contrast to convolution-based methods, our approach allows to model global context already at the first layer and throughout the network. We build on the recent Vision Transformer (ViT) and extend it to semantic segmentation. To do so, we rely on the output embeddings corresponding to image patches and obtain class labels from these embed-dings with a point-wise linear decoder or a mask trans-former decoder. We leverage models pre-trained for image classification and show that we can fine-tune them on moderate sized datasets available for semantic segmentation. The linear decoder allows to obtain excellent results already, but the performance can be further improved by a mask transformer generating class masks. We conduct an extensive ablation study to show the impact of the different parameters, in particular the performance is better for large models and small patch sizes. Segmenter attains excellent results for semantic segmentation. It outperforms the state of the art on both ADE20K and Pascal Context datasets and is competitive on Cityscapes."
    },
    {
        "paperId": "739ceacfafb1c4eaa17509351b647c773270b3ae",
        "url": "https://www.semanticscholar.org/paper/739ceacfafb1c4eaa17509351b647c773270b3ae",
        "title": "An Empirical Study of Training Self-Supervised Vision Transformers",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2021,
        "citationCount": 2161,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2104.02057",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2104.02057, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "39717886",
                "name": "Xinlei Chen"
            },
            {
                "authorId": "1817030",
                "name": "Saining Xie"
            },
            {
                "authorId": "2058350112",
                "name": "Kaiming He"
            }
        ],
        "abstract": "This paper does not describe a novel method. Instead, it studies a straightforward, incremental, yet must-know baseline given the recent progress in computer vision: self-supervised learning for Vision Transformers (ViT). While the training recipes for standard convolutional networks have been highly mature and robust, the recipes for ViT are yet to be built, especially in the self-supervised scenarios where training becomes more challenging. In this work, we go back to basics and investigate the effects of several fundamental components for training self-supervised ViT. We observe that instability is a major issue that degrades accuracy, and it can be hidden by apparently good results. We reveal that these results are indeed partial failure, and they can be improved when training is made more stable. We benchmark ViT results in MoCo v3 and several other self-supervised frameworks, with ablations in various aspects. We discuss the currently positive evidence as well as challenges and open questions. We hope that this work will provide useful data points and experience for future research."
    },
    {
        "paperId": "7438524bf00d7c5a22cb8799797f57c3a794b220",
        "url": "https://www.semanticscholar.org/paper/7438524bf00d7c5a22cb8799797f57c3a794b220",
        "title": "TOOD: Task-aligned One-stage Object Detection",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2021,
        "citationCount": 1047,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2108.07755",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2108.07755, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2117343912",
                "name": "Chengjian Feng"
            },
            {
                "authorId": "1624475253",
                "name": "Yujie Zhong"
            },
            {
                "authorId": "2143443111",
                "name": "Yu Gao"
            },
            {
                "authorId": "1915350",
                "name": "Matthew R. Scott"
            },
            {
                "authorId": "49015548",
                "name": "Weilin Huang"
            }
        ],
        "abstract": "One-stage object detection is commonly implemented by optimizing two sub-tasks: object classification and localization, using heads with two parallel branches, which might lead to a certain level of spatial misalignment in predictions between the two tasks. In this work, we propose a Task-aligned One-stage Object Detection (TOOD) that explicitly aligns the two tasks in a learning-based manner. First, we design a novel Task-aligned Head (T-Head) which offers a better balance between learning task-interactive and task-specific features, as well as a greater flexibility to learn the alignment via a task-aligned predictor. Second, we propose Task Alignment Learning (TAL) to explicitly pull closer (or even unify) the optimal anchors for the two tasks during training via a designed sample assignment scheme and a task-aligned loss. Extensive experiments are conducted on MS-COCO, where TOOD achieves a 51.1 AP at single-model single-scale testing. This surpasses the recent one-stage detectors by a large margin, such as ATSS [30] (47.7 AP), GFL [14] (48.2 AP), and PAA [9] (49.0 AP), with fewer parameters and FLOPs. Qualitative results also demonstrate the effectiveness of TOOD for better aligning the tasks of object classification and localization. Code is available at https://github.com/fcjian/TOOD."
    },
    {
        "paperId": "7ba9c013988eaff5cd186d73704af329d027872d",
        "url": "https://www.semanticscholar.org/paper/7ba9c013988eaff5cd186d73704af329d027872d",
        "title": "MDETR - Modulated Detection for End-to-End Multi-Modal Understanding",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2021,
        "citationCount": 1044,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2104.12763",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2104.12763, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "46174952",
                "name": "Aishwarya Kamath"
            },
            {
                "authorId": "152964870",
                "name": "Mannat Singh"
            },
            {
                "authorId": "1688882",
                "name": "Yann LeCun"
            },
            {
                "authorId": "1806773",
                "name": "Ishan Misra"
            },
            {
                "authorId": "2282478",
                "name": "Gabriel Synnaeve"
            },
            {
                "authorId": "3422899",
                "name": "Nicolas Carion"
            }
        ],
        "abstract": "Multi-modal reasoning systems rely on a pre-trained object detector to extract regions of interest from the image. However, this crucial module is typically used as a black box, trained independently of the downstream task and on a fixed vocabulary of objects and attributes. This makes it challenging for such systems to capture the long tail of visual concepts expressed in free form text. In this paper we propose MDETR, an end-to-end modulated detector that detects objects in an image conditioned on a raw text query, like a caption or a question. We use a transformer-based architecture to reason jointly over text and image by fusing the two modalities at an early stage of the model. We pre-train the network on 1.3M text-image pairs, mined from pre-existing multi-modal datasets having explicit alignment between phrases in text and objects in the image. We then fine-tune on several downstream tasks such as phrase grounding, referring expression comprehension and segmentation, achieving state-of-the-art results on popular benchmarks. We also investigate the utility of our model as an object detector on a given label set when fine-tuned in a few-shot setting. We show that our pre-training approach provides a way to handle the long tail of object categories which have very few labelled instances. Our approach can be easily extended for visual question answering, achieving competitive performance on GQA and CLEVR. The code and models are available at https://github.com/ashkamath/mdetr."
    },
    {
        "paperId": "8e33914d6051dd031a5e096962b9398fc1d16067",
        "url": "https://www.semanticscholar.org/paper/8e33914d6051dd031a5e096962b9398fc1d16067",
        "title": "Vision Transformers for Dense Prediction",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2021,
        "citationCount": 2292,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2103.13413",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2103.13413, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2774325",
                "name": "Ren Ranftl"
            },
            {
                "authorId": "1651204675",
                "name": "Alexey Bochkovskiy"
            },
            {
                "authorId": "145231047",
                "name": "V. Koltun"
            }
        ],
        "abstract": "We introduce dense prediction transformers, an architecture that leverages vision transformers in place of convolutional networks as a backbone for dense prediction tasks. We assemble tokens from various stages of the vision transformer into image-like representations at various resolutions and progressively combine them into full-resolution predictions using a convolutional decoder. The transformer backbone processes representations at a constant and relatively high resolution and has a global receptive field at every stage. These properties allow the dense prediction transformer to provide finer-grained and more globally coherent predictions when compared to fully-convolutional networks. Our experiments show that this architecture yields substantial improvements on dense prediction tasks, especially when a large amount of training data is available. For monocular depth estimation, we observe an improvement of up to 28% in relative performance when compared to a state-of-the-art fully-convolutional network. When applied to semantic segmentation, dense prediction transformers set a new state of the art on ADE20K with 49.02% mIoU. We further show that the architecture can be fine-tuned on smaller datasets such as NYUv2, KITTI, and Pascal Context where it also sets the new state of the art. Our models are available at https://github.com/intel-isl/DPT."
    },
    {
        "paperId": "8fb1c04dab87ca6c116495e4d03c46c9547e4ec3",
        "url": "https://www.semanticscholar.org/paper/8fb1c04dab87ca6c116495e4d03c46c9547e4ec3",
        "title": "Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2021,
        "citationCount": 4504,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2102.12122",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2102.12122, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "71074736",
                "name": "Wenhai Wang"
            },
            {
                "authorId": "41020000",
                "name": "Enze Xie"
            },
            {
                "authorId": null,
                "name": "Xiang Li"
            },
            {
                "authorId": "23999143",
                "name": "Deng-Ping Fan"
            },
            {
                "authorId": "50982078",
                "name": "Kaitao Song"
            },
            {
                "authorId": "152335674",
                "name": "Ding Liang"
            },
            {
                "authorId": "144720255",
                "name": "Tong Lu"
            },
            {
                "authorId": "144389940",
                "name": "P. Luo"
            },
            {
                "authorId": "144082425",
                "name": "Ling Shao"
            }
        ],
        "abstract": "Although convolutional neural networks (CNNs) have achieved great success in computer vision, this work investigates a simpler, convolution-free backbone network use-fid for many dense prediction tasks. Unlike the recently-proposed Vision Transformer (ViT) that was designed for image classification specifically, we introduce the Pyramid Vision Transformer (PVT), which overcomes the difficulties of porting Transformer to various dense prediction tasks. PVT has several merits compared to current state of the arts. (1) Different from ViT that typically yields low-resolution outputs and incurs high computational and memory costs, PVT not only can be trained on dense partitions of an image to achieve high output resolution, which is important for dense prediction, but also uses a progressive shrinking pyramid to reduce the computations of large feature maps. (2) PVT inherits the advantages of both CNN and Transformer, making it a unified backbone for various vision tasks without convolutions, where it can be used as a direct replacement for CNN backbones. (3) We validate PVT through extensive experiments, showing that it boosts the performance of many downstream tasks, including object detection, instance and semantic segmentation. For example, with a comparable number of parameters, PVT+RetinaNet achieves 40.4 AP on the COCO dataset, surpassing ResNet50+RetinNet (36.3 AP) by 4.1 absolute AP (see Figure 2). We hope that PVT could, serre as an alternative and useful backbone for pixel-level predictions and facilitate future research."
    },
    {
        "paperId": "aaa99de83292370a964fcaa51e6e866a96726bb2",
        "url": "https://www.semanticscholar.org/paper/aaa99de83292370a964fcaa51e6e866a96726bb2",
        "title": "StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2021,
        "citationCount": 1361,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2103.17249",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2103.17249, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2819477",
                "name": "Or Patashnik"
            },
            {
                "authorId": "34815981",
                "name": "Zongze Wu"
            },
            {
                "authorId": "2177801",
                "name": "Eli Shechtman"
            },
            {
                "authorId": "1388323541",
                "name": "D. Cohen-Or"
            },
            {
                "authorId": "70018371",
                "name": "D. Lischinski"
            }
        ],
        "abstract": "Inspired by the ability of StyleGAN to generate highly realistic images in a variety of domains, much recent work has focused on understanding how to use the latent spaces of StyleGAN to manipulate generated and real images. However, discovering semantically meaningful latent manipulations typically involves painstaking human examination of the many degrees of freedom, or an annotated collection of images for each desired manipulation. In this work, we explore leveraging the power of recently introduced Contrastive Language-Image Pre-training (CLIP) models in order to develop a text-based interface for StyleGAN image manipulation that does not require such manual effort. We first introduce an optimization scheme that utilizes a CLIP-based loss to modify an input latent vector in response to a user-provided text prompt. Next, we describe a latent mapper that infers a text-guided latent manipulation step for a given input image, allowing faster and more stable text-based manipulation. Finally, we present a method for mapping text prompts to input-agnostic directions in StyleGANs style space, enabling interactive text-driven image manipulation. Extensive results and comparisons demonstrate the effectiveness of our approaches."
    },
    {
        "paperId": "ad4a0938c48e61b7827869e4ac3baffd0aefab35",
        "url": "https://www.semanticscholar.org/paper/ad4a0938c48e61b7827869e4ac3baffd0aefab35",
        "title": "Emerging Properties in Self-Supervised Vision Transformers",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2021,
        "citationCount": 7813,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2104.14294",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2104.14294, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2062862676",
                "name": "Mathilde Caron"
            },
            {
                "authorId": "2113243762",
                "name": "Hugo Touvron"
            },
            {
                "authorId": "1806773",
                "name": "Ishan Misra"
            },
            {
                "authorId": "2065248680",
                "name": "Herv'e J'egou"
            },
            {
                "authorId": "2599292",
                "name": "J. Mairal"
            },
            {
                "authorId": "2329288",
                "name": "Piotr Bojanowski"
            },
            {
                "authorId": "2319608",
                "name": "Armand Joulin"
            }
        ],
        "abstract": "In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) [16] that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder [26], multi-crop training [9], and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1% top-1 on ImageNet in linear evaluation with ViT-Base."
    },
    {
        "paperId": "b364cdb02d18b9d9a3c097f5ea446f7e9ab10325",
        "url": "https://www.semanticscholar.org/paper/b364cdb02d18b9d9a3c097f5ea446f7e9ab10325",
        "title": "Going deeper with Image Transformers",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2021,
        "citationCount": 1178,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2103.17239",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2103.17239, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2113243762",
                "name": "Hugo Touvron"
            },
            {
                "authorId": "51021910",
                "name": "M. Cord"
            },
            {
                "authorId": "3469062",
                "name": "Alexandre Sablayrolles"
            },
            {
                "authorId": "2282478",
                "name": "Gabriel Synnaeve"
            },
            {
                "authorId": "2065248680",
                "name": "Herv'e J'egou"
            }
        ],
        "abstract": "Transformers have been recently adapted for large scale image classification, achieving high scores shaking up the long supremacy of convolutional neural networks. However the optimization of vision transformers has been little studied so far. In this work, we build and optimize deeper transformer networks for image classification. In particular, we investigate the interplay of architecture and optimization of such dedicated transformers. We make two architecture changes that significantly improve the accuracy of deep transformers. This leads us to produce models whose performance does not saturate early with more depth, for in-stance we obtain 86.5% top-1 accuracy on Imagenet when training with no external data, we thus attain the current sate of the art with less floating-point operations and parameters. Our best model establishes the new state of the art on Imagenet with Reassessed labels and Imagenet-V2 / match frequency, in the setting with no additional training data. We share our code and models1."
    },
    {
        "paperId": "b4ce7f92a8b987b5e76d580bf5076e2495f06883",
        "url": "https://www.semanticscholar.org/paper/b4ce7f92a8b987b5e76d580bf5076e2495f06883",
        "title": "TransReID: Transformer-based Object Re-Identification",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2021,
        "citationCount": 1048,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2102.04378",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2102.04378, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2115300944",
                "name": "Shuting He"
            },
            {
                "authorId": "2110564218",
                "name": "Haowen Luo"
            },
            {
                "authorId": "8120382",
                "name": "Pichao Wang"
            },
            {
                "authorId": "1716453",
                "name": "F. Wang"
            },
            {
                "authorId": "144966714",
                "name": "Hao Li"
            },
            {
                "authorId": "1753694704",
                "name": "Wei Jiang"
            }
        ],
        "abstract": "Extracting robust feature representation is one of the key challenges in object re-identification (ReID). Although convolution neural network (CNN)-based methods have achieved great success, they only process one local neighborhood at a time and suffer from information loss on details caused by convolution and downsampling operators (e.g. pooling and strided convolution). To overcome these limitations, we propose a pure transformer-based object ReID framework named TransReID. Specifically, we first encode an image as a sequence of patches and build a transformer-based strong baseline with a few critical improvements, which achieves competitive results on several ReID benchmarks with CNN-based methods. To further enhance the robust feature learning in the context of transformers, two novel modules are carefully designed. (i) The jigsaw patch module (JPM) is proposed to rearrange the patch embeddings via shift and patch shuffle operations which generates robust features with improved discrimination ability and more diversified coverage. (ii) The side information embeddings (SIE) is introduced to mitigate feature bias towards camera/view variations by plugging in learnable embeddings to incorporate these non-visual clues. To the best of our knowledge, this is the first work to adopt a pure transformer for ReID research. Experimental results of TransReID are superior promising, which achieve state-of-the-art performance on both person and vehicle ReID benchmarks. Code is available at https://github.com/heshuting555/TransReID."
    },
    {
        "paperId": "b6382a7351c0c595f91472ac71d3b2d87b3c4844",
        "url": "https://www.semanticscholar.org/paper/b6382a7351c0c595f91472ac71d3b2d87b3c4844",
        "title": "ViViT: A Video Vision Transformer",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2021,
        "citationCount": 2664,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2103.15691",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2103.15691, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "31638576",
                "name": "Anurag Arnab"
            },
            {
                "authorId": "3226635",
                "name": "Mostafa Dehghani"
            },
            {
                "authorId": "2280399",
                "name": "G. Heigold"
            },
            {
                "authorId": "1491624845",
                "name": "Chen Sun"
            },
            {
                "authorId": "34302129",
                "name": "Mario Lucic"
            },
            {
                "authorId": "2462253",
                "name": "C. Schmid"
            }
        ],
        "abstract": "We present pure-transformer based models for video classification, drawing upon the recent success of such models in image classification. Our model extracts spatiotemporal tokens from the input video, which are then encoded by a series of transformer layers. In order to handle the long sequences of tokens encountered in video, we propose several, efficient variants of our model which factorise the spatial- and temporal-dimensions of the input. Although transformer-based models are known to only be effective when large training datasets are available, we show how we can effectively regularise the model during training and leverage pretrained image models to be able to train on comparatively small datasets. We conduct thorough ablation studies, and achieve state-of-the-art results on multiple video classification benchmarks including Kinetics 400 and 600, Epic Kitchens, Something-Something v2 and Moments in Time, outperforming prior methods based on deep 3D convolutional networks."
    },
    {
        "paperId": "bac87bdb1cabc35fafb8176a234d332ebcc02864",
        "url": "https://www.semanticscholar.org/paper/bac87bdb1cabc35fafb8176a234d332ebcc02864",
        "title": "Frozen in Time: A Joint Video and Image Encoder for End-to-End Retrieval",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2021,
        "citationCount": 1432,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2104.00650",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2104.00650, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "153000035",
                "name": "Max Bain"
            },
            {
                "authorId": "19263506",
                "name": "Arsha Nagrani"
            },
            {
                "authorId": "2668759",
                "name": "Gl Varol"
            },
            {
                "authorId": "1688869",
                "name": "Andrew Zisserman"
            }
        ],
        "abstract": "Our objective in this work is video-text retrieval  in particular a joint embedding that enables efficient text-to-video retrieval. The challenges in this area include the design of the visual architecture and the nature of the training data, in that the available large scale video-text training datasets, such as HowTo100M, are noisy and hence competitive performance is achieved only at scale through large amounts of compute.We address both these challenges in this paper. We propose an end-to-end trainable model that is designed to take advantage of both large-scale image and video captioning datasets. Our model is an adaptation and extension of the recent ViT and Timesformer architectures, and consists of attention in both space and time. The model is flexible and can be trained on both image and video text datasets, either independently or in conjunction. It is trained with a curriculum learning schedule that begins by treating images as frozen snapshots of video, and then gradually learns to attend to increasing temporal context when trained on video datasets. We also provide a new video-text pretraining dataset WebVid-2M, comprised of over two million videos with weak captions scraped from the internet. Despite training on datasets that are an order of magnitude smaller, we show that this approach yields state-of-the-art results on standard downstream video-retrieval benchmarks including MSR-VTT, MSVD, DiDeMo and LSMDC."
    },
    {
        "paperId": "c8b25fab5608c3e033d34b4483ec47e68ba109b7",
        "url": "https://www.semanticscholar.org/paper/c8b25fab5608c3e033d34b4483ec47e68ba109b7",
        "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2021,
        "citationCount": 28055,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2103.14030",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2103.14030, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2109371439",
                "name": "Ze Liu"
            },
            {
                "authorId": "51091819",
                "name": "Yutong Lin"
            },
            {
                "authorId": "2112823372",
                "name": "Yue Cao"
            },
            {
                "authorId": "1823518756",
                "name": "Han Hu"
            },
            {
                "authorId": "2107995927",
                "name": "Yixuan Wei"
            },
            {
                "authorId": "2148904543",
                "name": "Zheng Zhang"
            },
            {
                "authorId": "145676588",
                "name": "Stephen Lin"
            },
            {
                "authorId": "2261753424",
                "name": "B. Guo"
            }
        ],
        "abstract": "This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with Shifted windows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The hierarchical design and the shifted window approach also prove beneficial for all-MLP architectures. The code and models are publicly available at https://github.com/microsoft/Swin-Transformer."
    },
    {
        "paperId": "dbe077f8521ecbe0a1477d6148c726d4f053d9c9",
        "url": "https://www.semanticscholar.org/paper/dbe077f8521ecbe0a1477d6148c726d4f053d9c9",
        "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2021,
        "citationCount": 2312,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2101.11986",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2101.11986, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2087091296",
                "name": "Li Yuan"
            },
            {
                "authorId": "2144861793",
                "name": "Yunpeng Chen"
            },
            {
                "authorId": null,
                "name": "Tao Wang"
            },
            {
                "authorId": "23476952",
                "name": "Weihao Yu"
            },
            {
                "authorId": "145356288",
                "name": "Yujun Shi"
            },
            {
                "authorId": "40983412",
                "name": "Francis E. H. Tay"
            },
            {
                "authorId": "33221685",
                "name": "Jiashi Feng"
            },
            {
                "authorId": "143653681",
                "name": "Shuicheng Yan"
            }
        ],
        "abstract": "Transformers, which are popular for language modeling, have been explored for solving vision tasks recently, e.g., the Vision Transformer (ViT) for image classification. The ViT model splits each image into a sequence of tokens with fixed length and then applies multiple Transformer layers to model their global relation for classification. However, ViT achieves inferior performance to CNNs when trained from scratch on a midsize dataset like ImageNet. We find it is because: 1) the simple tokenization of input images fails to model the important local structure such as edges and lines among neighboring pixels, leading to low training sample efficiency; 2) the redundant attention backbone design of ViT leads to limited feature richness for fixed computation budgets and limited training samples. To overcome such limitations, we propose a new Tokens-To-Token Vision Transformer (T2T-VTT), which incorporates 1) a layer-wise Tokens-to-Token (T2T) transformation to progressively structurize the image to tokens by recursively aggregating neighboring Tokens into one Token (Tokens-to-Token), such that local structure represented by surrounding tokens can be modeled and tokens length can be reduced; 2) an efficient backbone with a deep-narrow structure for vision transformer motivated by CNN architecture design after empirical study. Notably, T2T-ViT reduces the parameter count and MACs of vanilla ViT by half, while achieving more than 3.0% improvement when trained from scratch on ImageNet. It also outperforms ResNets and achieves comparable performance with MobileNets by directly training on ImageNet. For example, T2T-ViT with comparable size to ResNet50 (21.5M parameters) can achieve 83.3% top1 accuracy in image resolution 384x384 on ImageNet.1"
    },
    {
        "paperId": "e3d06054af531ee2f42270d43100b309c28546ef",
        "url": "https://www.semanticscholar.org/paper/e3d06054af531ee2f42270d43100b309c28546ef",
        "title": "MUSIQ: Multi-scale Image Quality Transformer",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2021,
        "citationCount": 1023,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2108.05997",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2108.05997, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "49287230",
                "name": "Junjie Ke"
            },
            {
                "authorId": "2291080430",
                "name": "Qifei Wang"
            },
            {
                "authorId": "2108103589",
                "name": "Yilin Wang"
            },
            {
                "authorId": "1718280",
                "name": "P. Milanfar"
            },
            {
                "authorId": "1454990616",
                "name": "Feng Yang"
            }
        ],
        "abstract": "Image quality assessment (IQA) is an important research topic for understanding and improving visual experience. The current state-of-the-art IQA methods are based on convolutional neural networks (CNNs). The performance of CNN-based models is often compromised by the fixed shape constraint in batch training. To accommodate this, the input images are usually resized and cropped to a fixed shape, causing image quality degradation. To address this, we design a multi-scale image quality Transformer (MUSIQ) to process native resolution images with varying sizes and aspect ratios. With a multi-scale image representation, our proposed method can capture image quality at different granularities. Furthermore, a novel hash-based 2D spatial embedding and a scale embedding is proposed to support the positional embedding in the multi-scale representation. Experimental results verify that our method can achieve state-of-the-art performance on multiple large scale IQA datasets such as PaQ-2-PiQ [41], SPAQ [11], and KonIQ-10k [16]. 1"
    },
    {
        "paperId": "e775e649d815a02373eac840cf5e33a04ff85c95",
        "url": "https://www.semanticscholar.org/paper/e775e649d815a02373eac840cf5e33a04ff85c95",
        "title": "CvT: Introducing Convolutions to Vision Transformers",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2021,
        "citationCount": 2247,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2103.15808",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2103.15808, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2119019500",
                "name": "Haiping Wu"
            },
            {
                "authorId": "2054421528",
                "name": "Bin Xiao"
            },
            {
                "authorId": "40589056",
                "name": "N. Codella"
            },
            {
                "authorId": "2152968847",
                "name": "Mengchen Liu"
            },
            {
                "authorId": "3386593",
                "name": "Xiyang Dai"
            },
            {
                "authorId": "145347147",
                "name": "Lu Yuan"
            },
            {
                "authorId": "2152828578",
                "name": "Lei Zhang"
            }
        ],
        "abstract": "We present in this paper a new architecture, named Convolutional vision Transformer (CvT), that improves Vision Transformer (ViT) in performance and efficiency by introducing convolutions into ViT to yield the best of both de-signs. This is accomplished through two primary modifications: a hierarchy of Transformers containing a new convolutional token embedding, and a convolutional Transformer block leveraging a convolutional projection. These changes introduce desirable properties of convolutional neural networks (CNNs) to the ViT architecture (i.e. shift, scale, and distortion invariance) while maintaining the merits of Transformers (i.e. dynamic attention, global context, and better generalization). We validate CvT by conducting extensive experiments, showing that this approach achieves state-of-the-art performance over other Vision Transformers and ResNets on ImageNet-1k, with fewer parameters and lower FLOPs. In addition, performance gains are maintained when pretrained on larger datasets (e.g. ImageNet-22k) and fine-tuned to downstream tasks. Pretrained on ImageNet-22k, our CvT-W24 obtains a top-1 accuracy of 87.7% on the ImageNet-1k val set. Finally, our results show that the positional encoding, a crucial component in existing Vision Transformers, can be safely re-moved in our model, simplifying the design for higher resolution vision tasks. Code will be released at https://github.com/microsoft/CvT."
    },
    {
        "paperId": "736973165f98105fec3729b7db414ae4d80fcbeb",
        "url": "https://www.semanticscholar.org/paper/736973165f98105fec3729b7db414ae4d80fcbeb",
        "title": "Scalable Diffusion Models with Transformers",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2022,
        "citationCount": 4144,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2212.09748",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2212.09748, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "35235273",
                "name": "William S. Peebles"
            },
            {
                "authorId": "1817030",
                "name": "Saining Xie"
            }
        ],
        "abstract": "We explore a new class of diffusion models based on the transformer architecture. We train latent diffusion models of images, replacing the commonly-used U-Net backbone with a transformer that operates on latent patches. We analyze the scalability of our Diffusion Transformers (DiTs) through the lens of forward pass complexity as measured by Gflops. We find that DiTs with higher Gflopsthrough increased transformer depth/width or increased number of input tokensconsistently have lower FID. In addition to possessing good scalability properties, our largest DiT-XL/2 models outperform all prior diffusion models on the class-conditional ImageNet 512512 and 256256 benchmarks, achieving a state-of-the-art FID of 2.27 on the latter."
    },
    {
        "paperId": "2c70684973bc4d7b6f8404a647b8031c4d3c8383",
        "url": "https://www.semanticscholar.org/paper/2c70684973bc4d7b6f8404a647b8031c4d3c8383",
        "title": "Zero-1-to-3: Zero-shot One Image to 3D Object",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2023,
        "citationCount": 1463,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2303.11328, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2143183492",
                "name": "Ruoshi Liu"
            },
            {
                "authorId": "1406236938",
                "name": "Rundi Wu"
            },
            {
                "authorId": "1470838102",
                "name": "Basile Van Hoorick"
            },
            {
                "authorId": "2931554",
                "name": "P. Tokmakov"
            },
            {
                "authorId": "144506587",
                "name": "Sergey Zakharov"
            },
            {
                "authorId": "1856025",
                "name": "Carl Vondrick"
            }
        ],
        "abstract": "We introduce Zero-1-to-3, a framework for changing the camera viewpoint of an object given just a single RGB image. To perform novel view synthesis in this under-constrained setting, we capitalize on the geometric priors that large-scale diffusion models learn about natural images. Our conditional diffusion model uses a synthetic dataset to learn controls of the relative camera viewpoint, which allow new images to be generated of the same object under a specified camera transformation. Even though it is trained on a synthetic dataset, our model retains a strong zero-shot generalization ability to out-of-distribution datasets as well as in-the-wild images, including impressionist paintings. Our viewpoint-conditioned diffusion approach can further be used for the task of 3D reconstruction from a single image. Qualitative and quantitative experiments show that our method significantly outperforms state-of-the-art single-view 3D reconstruction and novel view synthesis models by leveraging Internet-scale pre-training."
    },
    {
        "paperId": "35aba190f28b5c39df333c06ca21f46bd4845eba",
        "url": "https://www.semanticscholar.org/paper/35aba190f28b5c39df333c06ca21f46bd4845eba",
        "title": "Sigmoid Loss for Language Image Pre-Training",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2023,
        "citationCount": 2160,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2303.15343, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2743563",
                "name": "Xiaohua Zhai"
            },
            {
                "authorId": "40608942",
                "name": "Basil Mustafa"
            },
            {
                "authorId": "144629422",
                "name": "Alexander Kolesnikov"
            },
            {
                "authorId": "39611591",
                "name": "Lucas Beyer"
            }
        ],
        "abstract": "We propose a simple pairwise sigmoid loss for imagetext pre-training. Unlike standard contrastive learning with softmax normalization, the sigmoid loss operates solely on image-text pairs and does not require a global view of the pairwise similarities for normalization. The sigmoid loss simultaneously allows further scaling up the batch size, while also performing better at smaller batch sizes. With only four TPUv4 chips, we can train a Base CLIP model at 4k batch size and a Large LiT model at 20k batch size, the latter achieves 84.5% ImageNet zero-shot accuracy in two days. This disentanglement of the batch size from the loss further allows us to study the impact of examples vs pairs and negative to positive ratio. Finally, we push the batch size to the extreme, up to one million, and find that the benefits of growing batch size quickly diminish, with a more reasonable batch size of 32k being sufficient. We hope our research motivates further explorations in improving the quality and efficiency of language-image pre-training."
    },
    {
        "paperId": "7470a1702c8c86e6f28d32cfa315381150102f5b",
        "url": "https://www.semanticscholar.org/paper/7470a1702c8c86e6f28d32cfa315381150102f5b",
        "title": "Segment Anything",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2023,
        "citationCount": 10923,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2304.02643, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2064802835",
                "name": "A. Kirillov"
            },
            {
                "authorId": "13131689",
                "name": "Eric Mintun"
            },
            {
                "authorId": "2065647966",
                "name": "Nikhila Ravi"
            },
            {
                "authorId": "2053590350",
                "name": "Hanzi Mao"
            },
            {
                "authorId": "2213549340",
                "name": "Chlo Rolland"
            },
            {
                "authorId": "47029037",
                "name": "Laura Gustafson"
            },
            {
                "authorId": "15727192",
                "name": "Tete Xiao"
            },
            {
                "authorId": "153188991",
                "name": "Spencer Whitehead"
            },
            {
                "authorId": "39668247",
                "name": "A. Berg"
            },
            {
                "authorId": "3317278",
                "name": "Wan-Yen Lo"
            },
            {
                "authorId": "3127283",
                "name": "Piotr Dollr"
            },
            {
                "authorId": "2983898",
                "name": "Ross B. Girshick"
            }
        ],
        "abstract": "We introduce the Segment Anything (SA) project: a new task, model, and dataset for image segmentation. Using our efficient model in a data collection loop, we built the largest segmentation dataset to date (by far), with over 1 billion masks on 11M licensed and privacy respecting images. The model is designed and trained to be promptable, so it can transfer zero-shot to new image distributions and tasks. We evaluate its capabilities on numerous tasks and find that its zero-shot performance is impressive  often competitive with or even superior to prior fully supervised results. We are releasing the Segment Anything Model (SAM) and corresponding dataset (SA-1B) of 1B masks and 11M images at segment-anything.com to foster research into foundation models for computer vision. We recommend reading the full paper at: arxiv.org/abs/2304.02643."
    },
    {
        "paperId": "efbe97d20c4ffe356e8826c01dc550bacc405add",
        "url": "https://www.semanticscholar.org/paper/efbe97d20c4ffe356e8826c01dc550bacc405add",
        "title": "Adding Conditional Control to Text-to-Image Diffusion Models",
        "venue": "IEEE International Conference on Computer Vision",
        "year": 2023,
        "citationCount": 5706,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2302.05543",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2302.05543, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2287850511",
                "name": "Lvmin Zhang"
            },
            {
                "authorId": "36290866",
                "name": "Anyi Rao"
            },
            {
                "authorId": "1820412",
                "name": "Maneesh Agrawala"
            }
        ],
        "abstract": "We present ControlNet, a neural network architecture to add spatial conditioning controls to large, pretrained text-to-image diffusion models. ControlNet locks the production-ready large diffusion models, and reuses their deep and robust encoding layers pretrained with billions of images as a strong backbone to learn a diverse set of conditional controls. The neural architecture is connected with \"zero convolutions\" (zero-initialized convolution layers) that progressively grow the parameters from zero and ensure that no harmful noise could affect the finetuning. We test various conditioning controls, e.g., edges, depth, segmentation, human pose, etc., with Stable Diffusion, using single or multiple conditions, with or without prompts. We show that the training of ControlNets is robust with small (<50k) and large (>1m) datasets. Extensive results show that ControlNet may facilitate wider applications to control image diffusion models."
    },
    {
        "paperId": "39e1c9b8d38bfed261674e24e9ef6bd1695b4b77",
        "url": "https://www.semanticscholar.org/paper/39e1c9b8d38bfed261674e24e9ef6bd1695b4b77",
        "title": "Near-optimal sensor placements in Gaussian processes",
        "venue": "International Conference on Machine Learning",
        "year": 2005,
        "citationCount": 1931,
        "openAccessPdf": {
            "url": "https://figshare.com/articles/journal_contribution/Near-optimal_sensor_placements_in_Gaussian_processes/6607649/1/files/12098201.pdf",
            "status": "GREEN",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/1102351.1102385?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/1102351.1102385, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "145343838",
                "name": "Andreas Krause"
            },
            {
                "authorId": "2117775861",
                "name": "A. Singh"
            },
            {
                "authorId": "1730156",
                "name": "Carlos Guestrin"
            }
        ],
        "abstract": "When monitoring spatial phenomena, which are often modeled as Gaussian Processes (GPs), choosing sensor locations is a fundamental task. A common strategy is to place sensors at the points of highest entropy (variance) in the GP model. We propose a mutual information criteria, and show that it produces better placements. Furthermore, we prove that finding the configuration that maximizes mutual information is NP-complete. To address this issue, we describe a polynomial-time approximation that is within (1 -- 1/e) of the optimum by exploiting the submodularity of our criterion. This algorithm is extended to handle local structure in the GP, yielding significant speedups. We demonstrate the advantages of our approach on two real-world data sets."
    },
    {
        "paperId": "63aaf12163fe9735dfe9a69114937c4fa34f303a",
        "url": "https://www.semanticscholar.org/paper/63aaf12163fe9735dfe9a69114937c4fa34f303a",
        "title": "Learning to rank using gradient descent",
        "venue": "International Conference on Machine Learning",
        "year": 2005,
        "citationCount": 3074,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1145/1102351.1102363?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/1102351.1102363, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2676309",
                "name": "C. Burges"
            },
            {
                "authorId": "3296031",
                "name": "T. Shaked"
            },
            {
                "authorId": "1859813",
                "name": "Erin Renshaw"
            },
            {
                "authorId": "2078999999",
                "name": "Ari Lazier"
            },
            {
                "authorId": "1398663319",
                "name": "Matt Deeds"
            },
            {
                "authorId": "2067005422",
                "name": "Nicole Hamilton"
            },
            {
                "authorId": "2095493",
                "name": "Gregory N. Hullender"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "6cd49dd5d26d1e8e33891f8e64ad3b5012e90ba6",
        "url": "https://www.semanticscholar.org/paper/6cd49dd5d26d1e8e33891f8e64ad3b5012e90ba6",
        "title": "Fast maximum margin matrix factorization for collaborative prediction",
        "venue": "International Conference on Machine Learning",
        "year": 2005,
        "citationCount": 1112,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1145/1102351.1102441?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/1102351.1102441, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "35211659",
                "name": "Jason D. M. Rennie"
            },
            {
                "authorId": "1706280",
                "name": "N. Srebro"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "eae3948747c9d051314c1f5851957b833aa83eca",
        "url": "https://www.semanticscholar.org/paper/eae3948747c9d051314c1f5851957b833aa83eca",
        "title": "Predicting good probabilities with supervised learning",
        "venue": "International Conference on Machine Learning",
        "year": 2005,
        "citationCount": 1783,
        "openAccessPdf": {
            "url": "https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.60.7135&rep=rep1&type=pdf",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1145/1102351.1102430?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/1102351.1102430, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1399048849",
                "name": "Alexandru Niculescu-Mizil"
            },
            {
                "authorId": "145727186",
                "name": "R. Caruana"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "5feca7d699227265f607baee1f74b5449994a6b7",
        "url": "https://www.semanticscholar.org/paper/5feca7d699227265f607baee1f74b5449994a6b7",
        "title": "Topic modeling: beyond bag-of-words",
        "venue": "International Conference on Machine Learning",
        "year": 2006,
        "citationCount": 1236,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1145/1143844.1143967?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/1143844.1143967, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1831395",
                "name": "Hanna M. Wallach"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "955fcf6643c6946f491e70a96db3ffe3bc719a14",
        "url": "https://www.semanticscholar.org/paper/955fcf6643c6946f491e70a96db3ffe3bc719a14",
        "title": "An empirical comparison of supervised learning algorithms",
        "venue": "International Conference on Machine Learning",
        "year": 2006,
        "citationCount": 2821,
        "openAccessPdf": {
            "url": "http://www.eecs.wsu.edu/~holder/courses/CptS570/fall07/present/CaruanaICML06.pdf",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/1143844.1143865?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/1143844.1143865, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "145727186",
                "name": "R. Caruana"
            },
            {
                "authorId": "1399048849",
                "name": "Alexandru Niculescu-Mizil"
            }
        ],
        "abstract": "A number of supervised learning methods have been introduced in the last decade. Unfortunately, the last comprehensive empirical evaluation of supervised learning was the Statlog Project in the early 90's. We present a large-scale empirical comparison between ten supervised learning methods: SVMs, neural nets, logistic regression, naive bayes, memory-based learning, random forests, decision trees, bagged trees, boosted trees, and boosted stumps. We also examine the effect that calibrating the models via Platt Scaling and Isotonic Regression has on their performance. An important aspect of our study is the use of a variety of performance criteria to evaluate the learning methods."
    },
    {
        "paperId": "a883cacbc8f9b021b2a63f0453307855fa075d33",
        "url": "https://www.semanticscholar.org/paper/a883cacbc8f9b021b2a63f0453307855fa075d33",
        "title": "The relationship between Precision-Recall and ROC curves",
        "venue": "International Conference on Machine Learning",
        "year": 2006,
        "citationCount": 6342,
        "openAccessPdf": {
            "url": "https://lirias.kuleuven.be/bitstream/123456789/295592/1/davisgoadrichcamera2.pdf",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1145/1143844.1143874?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/1143844.1143874, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "152658362",
                "name": "Jesse Davis"
            },
            {
                "authorId": "2853980",
                "name": "Mark H. Goadrich"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "b81381d17baf6750c09bd58e96f4660d25be9225",
        "url": "https://www.semanticscholar.org/paper/b81381d17baf6750c09bd58e96f4660d25be9225",
        "title": "Information-theoretic metric learning",
        "venue": "International Conference on Machine Learning",
        "year": 2006,
        "citationCount": 2116,
        "openAccessPdf": {
            "url": "http://bengio.abracadoudou.com/lce/papers/7.pdf",
            "status": "GREEN",
            "license": "other-oa",
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1145/1273496.1273523?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/1273496.1273523, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2111090524",
                "name": "Jason V. Davis"
            },
            {
                "authorId": "1692670",
                "name": "Brian Kulis"
            },
            {
                "authorId": "48964143",
                "name": "Prateek Jain"
            },
            {
                "authorId": "3072326",
                "name": "S. Sra"
            },
            {
                "authorId": "1783667",
                "name": "I. Dhillon"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "1626c940a64ad96a7ed53d7d6c0df63c6696956b",
        "url": "https://www.semanticscholar.org/paper/1626c940a64ad96a7ed53d7d6c0df63c6696956b",
        "title": "Restricted Boltzmann machines for collaborative filtering",
        "venue": "International Conference on Machine Learning",
        "year": 2007,
        "citationCount": 2071,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1145/1273496.1273596?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/1273496.1273596, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "145124475",
                "name": "R. Salakhutdinov"
            },
            {
                "authorId": "1714004",
                "name": "A. Mnih"
            },
            {
                "authorId": "1695689",
                "name": "Geoffrey E. Hinton"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "884895a86fe15cb9601df4a15a1475c07f28da3c",
        "url": "https://www.semanticscholar.org/paper/884895a86fe15cb9601df4a15a1475c07f28da3c",
        "title": "Boosting for transfer learning",
        "venue": "International Conference on Machine Learning",
        "year": 2007,
        "citationCount": 1847,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1145/1273496.1273521?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/1273496.1273521, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1752769",
                "name": "Wenyuan Dai"
            },
            {
                "authorId": "152290618",
                "name": "Qiang Yang"
            },
            {
                "authorId": "1701421",
                "name": "Gui-Rong Xue"
            },
            {
                "authorId": "1811427",
                "name": "Yong Yu"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "9691f67f5075bde2fd70da0135a4a70f25ef042b",
        "url": "https://www.semanticscholar.org/paper/9691f67f5075bde2fd70da0135a4a70f25ef042b",
        "title": "Pegasos: primal estimated sub-gradient solver for SVM",
        "venue": "International Conference on Machine Learning",
        "year": 2007,
        "citationCount": 2331,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1145/1273496.1273598?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/1273496.1273598, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1389955537",
                "name": "Shai Shalev-Shwartz"
            },
            {
                "authorId": "1740765",
                "name": "Y. Singer"
            },
            {
                "authorId": "1706280",
                "name": "N. Srebro"
            },
            {
                "authorId": "145658292",
                "name": "Andrew Cotter"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "b3852f0113fcf8a3913c55ae92393ae6ccde347e",
        "url": "https://www.semanticscholar.org/paper/b3852f0113fcf8a3913c55ae92393ae6ccde347e",
        "title": "Self-taught learning: transfer learning from unlabeled data",
        "venue": "International Conference on Machine Learning",
        "year": 2007,
        "citationCount": 1804,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1145/1273496.1273592?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/1273496.1273592, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2979876",
                "name": "Rajat Raina"
            },
            {
                "authorId": "2078284037",
                "name": "Alexis Battle"
            },
            {
                "authorId": "1697141",
                "name": "Honglak Lee"
            },
            {
                "authorId": "1409971380",
                "name": "Ben Packer"
            },
            {
                "authorId": "34699434",
                "name": "A. Ng"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "b8012351bc5ebce4a4b3039bbbba3ce393bc3315",
        "url": "https://www.semanticscholar.org/paper/b8012351bc5ebce4a4b3039bbbba3ce393bc3315",
        "title": "An empirical evaluation of deep architectures on problems with many factors of variation",
        "venue": "International Conference on Machine Learning",
        "year": 2007,
        "citationCount": 1187,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1145/1273496.1273556?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/1273496.1273556, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1777528",
                "name": "H. Larochelle"
            },
            {
                "authorId": "1761978",
                "name": "D. Erhan"
            },
            {
                "authorId": "1760871",
                "name": "Aaron C. Courville"
            },
            {
                "authorId": "32837403",
                "name": "J. Bergstra"
            },
            {
                "authorId": "1751762",
                "name": "Yoshua Bengio"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "db196fd70f0b54d72aedc75bd74c9b2f826925d7",
        "url": "https://www.semanticscholar.org/paper/db196fd70f0b54d72aedc75bd74c9b2f826925d7",
        "title": "Learning to rank: from pairwise approach to listwise approach",
        "venue": "International Conference on Machine Learning",
        "year": 2007,
        "citationCount": 2296,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1145/1273496.1273513?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/1273496.1273513, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2275821157",
                "name": "Zhe Cao"
            },
            {
                "authorId": "2265984980",
                "name": "Tao Qin"
            },
            {
                "authorId": "2256760771",
                "name": "Tie-Yan Liu"
            },
            {
                "authorId": "2257249370",
                "name": "Ming-Feng Tsai"
            },
            {
                "authorId": "2265994866",
                "name": "Hang Li"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "0389a414c5d0ef50e06fe0c15f6102f374ce1b04",
        "url": "https://www.semanticscholar.org/paper/0389a414c5d0ef50e06fe0c15f6102f374ce1b04",
        "title": "A dual coordinate descent method for large-scale linear SVM",
        "venue": "International Conference on Machine Learning",
        "year": 2008,
        "citationCount": 1022,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1145/1390156.1390208?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/1390156.1390208, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1793529",
                "name": "Cho-Jui Hsieh"
            },
            {
                "authorId": "2782886",
                "name": "Kai-Wei Chang"
            },
            {
                "authorId": "1711460",
                "name": "Chih-Jen Lin"
            },
            {
                "authorId": "144106136",
                "name": "S. Keerthi"
            },
            {
                "authorId": "144833733",
                "name": "S. Sundararajan"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "5262fe8369992259be27165ccd09d1d31c7a4def",
        "url": "https://www.semanticscholar.org/paper/5262fe8369992259be27165ccd09d1d31c7a4def",
        "title": "Bayesian probabilistic matrix factorization using Markov chain Monte Carlo",
        "venue": "International Conference on Machine Learning",
        "year": 2008,
        "citationCount": 1527,
        "openAccessPdf": {
            "url": "http://icml2008.cs.helsinki.fi/papers/600.pdf",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1145/1390156.1390267?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/1390156.1390267, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "145124475",
                "name": "R. Salakhutdinov"
            },
            {
                "authorId": "1714004",
                "name": "A. Mnih"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "57458bc1cffe5caa45a885af986d70f723f406b4",
        "url": "https://www.semanticscholar.org/paper/57458bc1cffe5caa45a885af986d70f723f406b4",
        "title": "A unified architecture for natural language processing: deep neural networks with multitask learning",
        "venue": "International Conference on Machine Learning",
        "year": 2008,
        "citationCount": 5953,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1145/1390156.1390177?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/1390156.1390177, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2939803",
                "name": "R. Collobert"
            },
            {
                "authorId": "145183709",
                "name": "J. Weston"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "73d6a26f407db77506959fdf3f7b853e44f3844a",
        "url": "https://www.semanticscholar.org/paper/73d6a26f407db77506959fdf3f7b853e44f3844a",
        "title": "Training restricted Boltzmann machines using approximations to the likelihood gradient",
        "venue": "International Conference on Machine Learning",
        "year": 2008,
        "citationCount": 1052,
        "openAccessPdf": {
            "url": "http://icml2008.cs.helsinki.fi/papers/638.pdf",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1145/1390156.1390290?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/1390156.1390290, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2957517",
                "name": "T. Tieleman"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "7ee368e60d0b826e78f965aad8d6c7d406127104",
        "url": "https://www.semanticscholar.org/paper/7ee368e60d0b826e78f965aad8d6c7d406127104",
        "title": "Deep learning via semi-supervised embedding",
        "venue": "International Conference on Machine Learning",
        "year": 2008,
        "citationCount": 1064,
        "openAccessPdf": {
            "url": "http://icml2008.cs.helsinki.fi/papers/340.pdf",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1145/1390156.1390303?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/1390156.1390303, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "145183709",
                "name": "J. Weston"
            },
            {
                "authorId": "1799147",
                "name": "F. Ratle"
            },
            {
                "authorId": "3232655",
                "name": "H. Mobahi"
            },
            {
                "authorId": "2939803",
                "name": "R. Collobert"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "843959ffdccf31c6694d135fad07425924f785b1",
        "url": "https://www.semanticscholar.org/paper/843959ffdccf31c6694d135fad07425924f785b1",
        "title": "Extracting and composing robust features with denoising autoencoders",
        "venue": "International Conference on Machine Learning",
        "year": 2008,
        "citationCount": 7907,
        "openAccessPdf": {
            "url": "http://www.iro.umontreal.ca/~vincentp/Publications/denoising_autoencoders_tr1316.pdf",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1145/1390156.1390294?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/1390156.1390294, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "120247189",
                "name": "Pascal Vincent"
            },
            {
                "authorId": "1777528",
                "name": "H. Larochelle"
            },
            {
                "authorId": "1751762",
                "name": "Yoshua Bengio"
            },
            {
                "authorId": "1798462",
                "name": "Pierre-Antoine Manzagol"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "ed7c7c079c8c54d3b82e016cc52a7a2c3a61f237",
        "url": "https://www.semanticscholar.org/paper/ed7c7c079c8c54d3b82e016cc52a7a2c3a61f237",
        "title": "Efficient projections onto the l1-ball for learning in high dimensions",
        "venue": "International Conference on Machine Learning",
        "year": 2008,
        "citationCount": 1531,
        "openAccessPdf": {
            "url": "http://icml2008.cs.helsinki.fi/papers/361.pdf",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1145/1390156.1390191?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/1390156.1390191, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1734693",
                "name": "John C. Duchi"
            },
            {
                "authorId": "1389955537",
                "name": "Shai Shalev-Shwartz"
            },
            {
                "authorId": "1740765",
                "name": "Y. Singer"
            },
            {
                "authorId": "2073806959",
                "name": "Tushar Chandra"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "00bbfde6af97ce5efcf86b3401d265d42a95603d",
        "url": "https://www.semanticscholar.org/paper/00bbfde6af97ce5efcf86b3401d265d42a95603d",
        "title": "Feature hashing for large scale multitask learning",
        "venue": "International Conference on Machine Learning",
        "year": 2009,
        "citationCount": 1057,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/0902.2206",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/0902.2206, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "7446832",
                "name": "Kilian Q. Weinberger"
            },
            {
                "authorId": "144029093",
                "name": "A. Dasgupta"
            },
            {
                "authorId": "1708597",
                "name": "Josh Attenberg"
            },
            {
                "authorId": "144162125",
                "name": "J. Langford"
            },
            {
                "authorId": "46234526",
                "name": "Alex Smola"
            }
        ],
        "abstract": "Empirical evidence suggests that hashing is an effective strategy for dimensionality reduction and practical nonparametric estimation. In this paper we provide exponential tail bounds for feature hashing and show that the interaction between random subspaces is negligible with high probability. We demonstrate the feasibility of this approach with experimental results for a new use case --- multitask learning with hundreds of thousands of tasks."
    },
    {
        "paperId": "12439a6ff384e95ee2262ee982bc055534e30487",
        "url": "https://www.semanticscholar.org/paper/12439a6ff384e95ee2262ee982bc055534e30487",
        "title": "Online dictionary learning for sparse coding",
        "venue": "International Conference on Machine Learning",
        "year": 2009,
        "citationCount": 2409,
        "openAccessPdf": {
            "url": "https://conservancy.umn.edu/bitstreams/3b39023f-fe59-47f1-bd0b-cab83cba2bdf/download",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1145/1553374.1553463?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/1553374.1553463, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2599292",
                "name": "J. Mairal"
            },
            {
                "authorId": "144570279",
                "name": "F. Bach"
            },
            {
                "authorId": "144189388",
                "name": "J. Ponce"
            },
            {
                "authorId": "1699339",
                "name": "G. Sapiro"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "1e80f755bcbf10479afd2338cec05211fdbd325c",
        "url": "https://www.semanticscholar.org/paper/1e80f755bcbf10479afd2338cec05211fdbd325c",
        "title": "Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations",
        "venue": "International Conference on Machine Learning",
        "year": 2009,
        "citationCount": 2699,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1145/1553374.1553453?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/1553374.1553453, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1697141",
                "name": "Honglak Lee"
            },
            {
                "authorId": "1785346",
                "name": "R. Grosse"
            },
            {
                "authorId": "2615814",
                "name": "R. Ranganath"
            },
            {
                "authorId": "34699434",
                "name": "A. Ng"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "35c15d7dfa1fddd2c0292146412ebbec704e8be9",
        "url": "https://www.semanticscholar.org/paper/35c15d7dfa1fddd2c0292146412ebbec704e8be9",
        "title": "Information theoretic measures for clusterings comparison: is a correction for chance necessary?",
        "venue": "International Conference on Machine Learning",
        "year": 2009,
        "citationCount": 1114,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1145/1553374.1553511?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/1553374.1553511, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1817564",
                "name": "X. Nguyen"
            },
            {
                "authorId": "145815422",
                "name": "J. Epps"
            },
            {
                "authorId": "145148600",
                "name": "J. Bailey"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "8de174ab5419b9d3127695405efd079808e956e8",
        "url": "https://www.semanticscholar.org/paper/8de174ab5419b9d3127695405efd079808e956e8",
        "title": "Curriculum learning",
        "venue": "International Conference on Machine Learning",
        "year": 2009,
        "citationCount": 6423,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1145/1553374.1553380?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/1553374.1553380, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1751762",
                "name": "Yoshua Bengio"
            },
            {
                "authorId": "2373952",
                "name": "J. Louradour"
            },
            {
                "authorId": "2939803",
                "name": "R. Collobert"
            },
            {
                "authorId": "145183709",
                "name": "J. Weston"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "047175fb23f6f152d86e81100ba7140dd2847636",
        "url": "https://www.semanticscholar.org/paper/047175fb23f6f152d86e81100ba7140dd2847636",
        "title": "Robust Subspace Segmentation by Low-Rank Representation",
        "venue": "International Conference on Machine Learning",
        "year": 2010,
        "citationCount": 1709,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "1990768",
                "name": "Guangcan Liu"
            },
            {
                "authorId": "33383055",
                "name": "Zhouchen Lin"
            },
            {
                "authorId": "1811427",
                "name": "Yong Yu"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "405aed4b8ecdd869b2e83095dde51c396334115f",
        "url": "https://www.semanticscholar.org/paper/405aed4b8ecdd869b2e83095dde51c396334115f",
        "title": "A Theoretical Analysis of Feature Pooling in Visual Recognition",
        "venue": "International Conference on Machine Learning",
        "year": 2010,
        "citationCount": 1372,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "90841478",
                "name": "Y-Lan Boureau"
            },
            {
                "authorId": "144189388",
                "name": "J. Ponce"
            },
            {
                "authorId": "1688882",
                "name": "Yann LeCun"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "4c46347fbc272b21468efe3d9af34b4b2bad6684",
        "url": "https://www.semanticscholar.org/paper/4c46347fbc272b21468efe3d9af34b4b2bad6684",
        "title": "Deep learning via Hessian-free optimization",
        "venue": "International Conference on Machine Learning",
        "year": 2010,
        "citationCount": 1090,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "145704247",
                "name": "James Martens"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "a538b05ebb01a40323997629e171c91aa28b8e2f",
        "url": "https://www.semanticscholar.org/paper/a538b05ebb01a40323997629e171c91aa28b8e2f",
        "title": "Rectified Linear Units Improve Restricted Boltzmann Machines",
        "venue": "International Conference on Machine Learning",
        "year": 2010,
        "citationCount": 18277,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "2073603971",
                "name": "Vinod Nair"
            },
            {
                "authorId": "1695689",
                "name": "Geoffrey E. Hinton"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "e8f811399746c059bf4d4c3d43334045e0222209",
        "url": "https://www.semanticscholar.org/paper/e8f811399746c059bf4d4c3d43334045e0222209",
        "title": "Learning Fast Approximations of Sparse Coding",
        "venue": "International Conference on Machine Learning",
        "year": 2010,
        "citationCount": 1924,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "144717963",
                "name": "Karol Gregor"
            },
            {
                "authorId": "1688882",
                "name": "Yann LeCun"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "053912e76e50c9f923a1fc1c173f1365776060cc",
        "url": "https://www.semanticscholar.org/paper/053912e76e50c9f923a1fc1c173f1365776060cc",
        "title": "On optimization methods for deep learning",
        "venue": "International Conference on Machine Learning",
        "year": 2011,
        "citationCount": 1043,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "2827616",
                "name": "Quoc V. Le"
            },
            {
                "authorId": "2020608",
                "name": "Jiquan Ngiam"
            },
            {
                "authorId": "144638694",
                "name": "Adam Coates"
            },
            {
                "authorId": "47778994",
                "name": "A. Lahiri"
            },
            {
                "authorId": "41227297",
                "name": "B. Prochnow"
            },
            {
                "authorId": "34699434",
                "name": "A. Ng"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "195d0a8233a7a46329c742eaff56c276f847fadc",
        "url": "https://www.semanticscholar.org/paper/195d0a8233a7a46329c742eaff56c276f847fadc",
        "title": "Contractive Auto-Encoders: Explicit Invariance During Feature Extraction",
        "venue": "International Conference on Machine Learning",
        "year": 2011,
        "citationCount": 1548,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "2425018",
                "name": "Salah Rifai"
            },
            {
                "authorId": "145467703",
                "name": "Pascal Vincent"
            },
            {
                "authorId": "2090922238",
                "name": "X. Muller"
            },
            {
                "authorId": "3119801",
                "name": "Xavier Glorot"
            },
            {
                "authorId": "1751762",
                "name": "Yoshua Bengio"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "60b7d47758a71978e74edff6dd8dea4d9c791d7a",
        "url": "https://www.semanticscholar.org/paper/60b7d47758a71978e74edff6dd8dea4d9c791d7a",
        "title": "PILCO: A Model-Based and Data-Efficient Approach to Policy Search",
        "venue": "International Conference on Machine Learning",
        "year": 2011,
        "citationCount": 1744,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "2261881",
                "name": "M. Deisenroth"
            },
            {
                "authorId": "3472959",
                "name": "C. Rasmussen"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "6f4065f0cc99a0839b0248ffb4457e5f0277b30d",
        "url": "https://www.semanticscholar.org/paper/6f4065f0cc99a0839b0248ffb4457e5f0277b30d",
        "title": "Domain Adaptation for Large-Scale Sentiment Classification: A Deep Learning Approach",
        "venue": "International Conference on Machine Learning",
        "year": 2011,
        "citationCount": 1845,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "3119801",
                "name": "Xavier Glorot"
            },
            {
                "authorId": "1713934",
                "name": "Antoine Bordes"
            },
            {
                "authorId": "1751762",
                "name": "Yoshua Bengio"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "93c20e38c85b69fc2d2eb314b3c1217913f7db11",
        "url": "https://www.semanticscholar.org/paper/93c20e38c85b69fc2d2eb314b3c1217913f7db11",
        "title": "Generating Text with Recurrent Neural Networks",
        "venue": "International Conference on Machine Learning",
        "year": 2011,
        "citationCount": 1542,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "1701686",
                "name": "I. Sutskever"
            },
            {
                "authorId": "2257176728",
                "name": "James Martens"
            },
            {
                "authorId": "2256589446",
                "name": "Geoffrey E. Hinton"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "9c0ddf74f87d154db88d79c640578c1610451eec",
        "url": "https://www.semanticscholar.org/paper/9c0ddf74f87d154db88d79c640578c1610451eec",
        "title": "Parsing Natural Scenes and Natural Language with Recursive Neural Networks",
        "venue": "International Conference on Machine Learning",
        "year": 2011,
        "citationCount": 1476,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "2166511",
                "name": "R. Socher"
            },
            {
                "authorId": "2585821",
                "name": "Cliff Chiung-Yu Lin"
            },
            {
                "authorId": "34699434",
                "name": "A. Ng"
            },
            {
                "authorId": "144783904",
                "name": "Christopher D. Manning"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "a78273144520d57e150744cf75206e881e11cc5b",
        "url": "https://www.semanticscholar.org/paper/a78273144520d57e150744cf75206e881e11cc5b",
        "title": "Multimodal Deep Learning",
        "venue": "International Conference on Machine Learning",
        "year": 2011,
        "citationCount": 3387,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2301.04856",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2301.04856, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2020608",
                "name": "Jiquan Ngiam"
            },
            {
                "authorId": "2556428",
                "name": "A. Khosla"
            },
            {
                "authorId": "1390603950",
                "name": "Mingyu Kim"
            },
            {
                "authorId": "145578392",
                "name": "Juhan Nam"
            },
            {
                "authorId": "1697141",
                "name": "Honglak Lee"
            },
            {
                "authorId": "34699434",
                "name": "A. Ng"
            }
        ],
        "abstract": "Deep networks have been successfully applied to unsupervised feature learning for single modalities (e.g., text, images or audio). In this work, we propose a novel application of deep networks to learn features over multiple modalities. We present a series of tasks for multimodal learning and show how to train deep networks that learn features to address these tasks. In particular, we demonstrate cross modality feature learning, where better features for one modality (e.g., video) can be learned if multiple modalities (e.g., audio and video) are present at feature learning time. Furthermore, we show how to learn a shared representation between modalities and evaluate it on a unique task, where the classifier is trained with audio-only data but tested with video-only data and vice-versa. Our models are validated on the CUAVE and AVLetters datasets on audio-visual speech classification, demonstrating best published visual speech classification on AVLetters and effective shared representation learning."
    },
    {
        "paperId": "aeed631d6a84100b5e9a021ec1914095c66de415",
        "url": "https://www.semanticscholar.org/paper/aeed631d6a84100b5e9a021ec1914095c66de415",
        "title": "Bayesian Learning via Stochastic Gradient Langevin Dynamics",
        "venue": "International Conference on Machine Learning",
        "year": 2011,
        "citationCount": 2890,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "1678311",
                "name": "M. Welling"
            },
            {
                "authorId": "1725303",
                "name": "Y. Teh"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "d8e4936a405bdf58830be2559e9a39e533a0ed42",
        "url": "https://www.semanticscholar.org/paper/d8e4936a405bdf58830be2559e9a39e533a0ed42",
        "title": "Hashing with Graphs",
        "venue": "International Conference on Machine Learning",
        "year": 2011,
        "citationCount": 1068,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "46641573",
                "name": "W. Liu"
            },
            {
                "authorId": "39811558",
                "name": "Jun Wang"
            },
            {
                "authorId": "152663162",
                "name": "Sanjiv Kumar"
            },
            {
                "authorId": "9546964",
                "name": "Shih-Fu Chang"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "f6764d853a14b0c34df1d2283e76277aead40fde",
        "url": "https://www.semanticscholar.org/paper/f6764d853a14b0c34df1d2283e76277aead40fde",
        "title": "A Three-Way Model for Collective Learning on Multi-Relational Data",
        "venue": "International Conference on Machine Learning",
        "year": 2011,
        "citationCount": 2416,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "1729762",
                "name": "Maximilian Nickel"
            },
            {
                "authorId": "1700754",
                "name": "Volker Tresp"
            },
            {
                "authorId": "1688561",
                "name": "H. Kriegel"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "84069287da0a6b488b8c933f3cb5be759cb6237e",
        "url": "https://www.semanticscholar.org/paper/84069287da0a6b488b8c933f3cb5be759cb6237e",
        "title": "On the difficulty of training recurrent neural networks",
        "venue": "International Conference on Machine Learning",
        "year": 2012,
        "citationCount": 5673,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1211.5063, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1996134",
                "name": "Razvan Pascanu"
            },
            {
                "authorId": "2047446108",
                "name": "Tomas Mikolov"
            },
            {
                "authorId": "1751762",
                "name": "Yoshua Bengio"
            }
        ],
        "abstract": "There are two widely known issues with properly training recurrent neural networks, the vanishing and the exploding gradient problems detailed in Bengio et al. (1994). In this paper we attempt to improve the understanding of the underlying issues by exploring these problems from an analytical, a geometric and a dynamical systems perspective. Our analysis is used to justify a simple yet effective solution. We propose a gradient norm clipping strategy to deal with exploding gradients and a soft constraint for the vanishing gradients problem. We validate empirically our hypothesis and proposed solutions in the experimental section."
    },
    {
        "paperId": "990a02f20529f5ce3b382f1d54648afaab391179",
        "url": "https://www.semanticscholar.org/paper/990a02f20529f5ce3b382f1d54648afaab391179",
        "title": "Poisoning Attacks against Support Vector Machines",
        "venue": "International Conference on Machine Learning",
        "year": 2012,
        "citationCount": 1734,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1206.6389, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1684175",
                "name": "B. Biggio"
            },
            {
                "authorId": "39743720",
                "name": "B. Nelson"
            },
            {
                "authorId": "1754215",
                "name": "P. Laskov"
            }
        ],
        "abstract": "We investigate a family of poisoning attacks against Support Vector Machines (SVM). Such attacks inject specially crafted training data that increases the SVM's test error. Central to the motivation for these attacks is the fact that most learning algorithms assume that their training data comes from a natural or well-behaved distribution. However, this assumption does not generally hold in security-sensitive settings. As we demonstrate, an intelligent adversary can, to some extent, predict the change of the SVM's decision function due to malicious input and use this ability to construct malicious data. \n \nThe proposed attack uses a gradient ascent strategy in which the gradient is computed based on properties of the SVM's optimal solution. This method can be kernelized and enables the attack to be constructed in the input space even for non-linear kernels. We experimentally demonstrate that our gradient ascent procedure reliably identifies good local maxima of the non-convex validation error surface, which significantly increases the classifier's test error."
    },
    {
        "paperId": "f26f1a3c034b96514fc092dee99acacedd9c380b",
        "url": "https://www.semanticscholar.org/paper/f26f1a3c034b96514fc092dee99acacedd9c380b",
        "title": "Thompson Sampling for Contextual Bandits with Linear Payoffs",
        "venue": "International Conference on Machine Learning",
        "year": 2012,
        "citationCount": 1072,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1209.3352, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1703744",
                "name": "Shipra Agrawal"
            },
            {
                "authorId": "144260125",
                "name": "Navin Goyal"
            }
        ],
        "abstract": "Thompson Sampling is one of the oldest heuristics for multi-armed bandit problems. It is a randomized algorithm based on Bayesian ideas, and has recently generated significant interest after several studies demonstrated it to have better empirical performance compared to the state-of-the-art methods. However, many questions regarding its theoretical performance remained open. In this paper, we design and analyze a generalization of Thompson Sampling algorithm for the stochastic contextual multi-armed bandit problem with linear payoff functions, when the contexts are provided by an adaptive adversary. This is among the most important and widely studied version of the contextual bandits problem. We prove a high probability regret bound of O(d2/eT1+e) in time T for any 0 < e < 1, where d is the dimension of each context vector and e is a parameter used by the algorithm. Our results provide the first theoretical guarantees for the contextual version of Thompson Sampling, and are close to the lower bound of (dT) for this problem. This essentially solves a COLT open problem of Chapelle and Li [COLT 2012]."
    },
    {
        "paperId": "29935173af73aef20336db72d608e0ef5b0e0c16",
        "url": "https://www.semanticscholar.org/paper/29935173af73aef20336db72d608e0ef5b0e0c16",
        "title": "Making a Science of Model Search: Hyperparameter Optimization in Hundreds of Dimensions for Vision Architectures",
        "venue": "International Conference on Machine Learning",
        "year": 2013,
        "citationCount": 2435,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "32837403",
                "name": "J. Bergstra"
            },
            {
                "authorId": "2292273",
                "name": "Daniel Yamins"
            },
            {
                "authorId": "2042941",
                "name": "David D. Cox"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "37c3303d173c055592ef923235837e1cbc6bd986",
        "url": "https://www.semanticscholar.org/paper/37c3303d173c055592ef923235837e1cbc6bd986",
        "title": "Learning Fair Representations",
        "venue": "International Conference on Machine Learning",
        "year": 2013,
        "citationCount": 1951,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "1804104",
                "name": "R. Zemel"
            },
            {
                "authorId": "51183248",
                "name": "Ledell Yu Wu"
            },
            {
                "authorId": "1754860",
                "name": "Kevin Swersky"
            },
            {
                "authorId": "1695317",
                "name": "T. Pitassi"
            },
            {
                "authorId": "1781565",
                "name": "C. Dwork"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "38f35dd624cd1cf827416e31ac5e0e0454028eca",
        "url": "https://www.semanticscholar.org/paper/38f35dd624cd1cf827416e31ac5e0e0454028eca",
        "title": "Regularization of Neural Networks using DropConnect",
        "venue": "International Conference on Machine Learning",
        "year": 2013,
        "citationCount": 2614,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "2053671127",
                "name": "Li Wan"
            },
            {
                "authorId": "48799969",
                "name": "Matthew D. Zeiler"
            },
            {
                "authorId": "33551113",
                "name": "Sixin Zhang"
            },
            {
                "authorId": "1688882",
                "name": "Yann LeCun"
            },
            {
                "authorId": "2276554",
                "name": "R. Fergus"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "79c286bf03ed97fb94d33511f3355770dcee0aec",
        "url": "https://www.semanticscholar.org/paper/79c286bf03ed97fb94d33511f3355770dcee0aec",
        "title": "Domain Generalization via Invariant Feature Representation",
        "venue": "International Conference on Machine Learning",
        "year": 2013,
        "citationCount": 1276,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1301.2115, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2276351",
                "name": "Krikamol Muandet"
            },
            {
                "authorId": "1722983",
                "name": "David Balduzzi"
            },
            {
                "authorId": "1707625",
                "name": "B. Scholkopf"
            }
        ],
        "abstract": "This paper investigates domain generalization: How to take knowledge acquired from an arbitrary number of related domains and apply it to previously unseen domains? We propose Domain-Invariant Component Analysis (DICA), a kernel-based optimization algorithm that learns an invariant transformation by minimizing the dissimilarity across domains, whilst preserving the functional relationship between input and output variables. A learning-theoretic analysis shows that reducing dissimilarity improves the expected generalization ability of classifiers on new domains, motivating the proposed algorithm. Experimental results on synthetic and real-world datasets demonstrate that DICA successfully learns invariant features and improves classifier performance in practice."
    },
    {
        "paperId": "961eabeaebd7035cd7668c9917fa9c39462e1113",
        "url": "https://www.semanticscholar.org/paper/961eabeaebd7035cd7668c9917fa9c39462e1113",
        "title": "Revisiting Frank-Wolfe: Projection-Free Sparse Convex Optimization",
        "venue": "International Conference on Machine Learning",
        "year": 2013,
        "citationCount": 1416,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "2456863",
                "name": "Martin Jaggi"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "aa7bfd2304201afbb19971ebde87b17e40242e91",
        "url": "https://www.semanticscholar.org/paper/aa7bfd2304201afbb19971ebde87b17e40242e91",
        "title": "On the importance of initialization and momentum in deep learning",
        "venue": "International Conference on Machine Learning",
        "year": 2013,
        "citationCount": 5094,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "1701686",
                "name": "I. Sutskever"
            },
            {
                "authorId": "145704247",
                "name": "James Martens"
            },
            {
                "authorId": "35188630",
                "name": "George E. Dahl"
            },
            {
                "authorId": "1695689",
                "name": "Geoffrey E. Hinton"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "b7b915d508987b73b61eccd2b237e7ed099a2d29",
        "url": "https://www.semanticscholar.org/paper/b7b915d508987b73b61eccd2b237e7ed099a2d29",
        "title": "Maxout Networks",
        "venue": "International Conference on Machine Learning",
        "year": 2013,
        "citationCount": 2232,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1302.4389, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "153440022",
                "name": "I. Goodfellow"
            },
            {
                "authorId": "1393680089",
                "name": "David Warde-Farley"
            },
            {
                "authorId": "153583218",
                "name": "Mehdi Mirza"
            },
            {
                "authorId": "1760871",
                "name": "Aaron C. Courville"
            },
            {
                "authorId": "1751762",
                "name": "Yoshua Bengio"
            }
        ],
        "abstract": "We consider the problem of designing models to leverage a recently introduced approximate model averaging technique called dropout. We define a simple new model called maxout (so named because its output is the max of a set of inputs, and because it is a natural companion to dropout) designed to both facilitate optimization by dropout and improve the accuracy of dropout's fast approximate model averaging technique. We empirically verify that the model successfully accomplishes both of these tasks. We use maxout and dropout to demonstrate state of the art classification performance on four benchmark datasets: MNIST, CIFAR-10, CIFAR-100, and SVHN."
    },
    {
        "paperId": "b8de958fead0d8a9619b55c7299df3257c624a96",
        "url": "https://www.semanticscholar.org/paper/b8de958fead0d8a9619b55c7299df3257c624a96",
        "title": "DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition",
        "venue": "International Conference on Machine Learning",
        "year": 2013,
        "citationCount": 5052,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1310.1531, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "7408951",
                "name": "Jeff Donahue"
            },
            {
                "authorId": "39978391",
                "name": "Yangqing Jia"
            },
            {
                "authorId": "1689108",
                "name": "O. Vinyals"
            },
            {
                "authorId": "50196944",
                "name": "Judy Hoffman"
            },
            {
                "authorId": "2152329702",
                "name": "Ning Zhang"
            },
            {
                "authorId": "2368132",
                "name": "Eric Tzeng"
            },
            {
                "authorId": "1753210",
                "name": "Trevor Darrell"
            }
        ],
        "abstract": "We evaluate whether features extracted from the activation of a deep convolutional network trained in a fully supervised fashion on a large, fixed set of object recognition tasks can be repurposed to novel generic tasks. Our generic tasks may differ significantly from the originally trained tasks and there may be insufficient labeled or unlabeled data to conventionally train or adapt a deep architecture to the new tasks. We investigate and visualize the semantic clustering of deep convolutional features with respect to a variety of such tasks, including scene recognition, domain adaptation, and fine-grained recognition challenges. We compare the efficacy of relying on various network levels to define a fixed feature, and report novel results that significantly outperform the state-of-the-art on several important vision challenges. We are releasing DeCAF, an open-source implementation of these deep convolutional activation features, along with all associated network parameters to enable vision researchers to be able to conduct experimentation with deep representations across a range of visual concept learning paradigms."
    },
    {
        "paperId": "e2257e3f56ccb12875a57bc0a8cca1d9d7e93ec6",
        "url": "https://www.semanticscholar.org/paper/e2257e3f56ccb12875a57bc0a8cca1d9d7e93ec6",
        "title": "Deep Canonical Correlation Analysis",
        "venue": "International Conference on Machine Learning",
        "year": 2013,
        "citationCount": 1971,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "144339350",
                "name": "Galen Andrew"
            },
            {
                "authorId": "144365054",
                "name": "R. Arora"
            },
            {
                "authorId": "1748118",
                "name": "J. Bilmes"
            },
            {
                "authorId": "2924113",
                "name": "Karen Livescu"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "0fa553cfa0cf3cbdf7a913aa2ae789a757dfb32f",
        "url": "https://www.semanticscholar.org/paper/0fa553cfa0cf3cbdf7a913aa2ae789a757dfb32f",
        "title": "Towards End-To-End Speech Recognition with Recurrent Neural Networks",
        "venue": "International Conference on Machine Learning",
        "year": 2014,
        "citationCount": 2268,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "1753223",
                "name": "Alex Graves"
            },
            {
                "authorId": "3111912",
                "name": "N. Jaitly"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "2530cfc7764bda1330c48c0c8e2cd0e0c671d7e1",
        "url": "https://www.semanticscholar.org/paper/2530cfc7764bda1330c48c0c8e2cd0e0c671d7e1",
        "title": "Unsupervised Domain Adaptation by Backpropagation",
        "venue": "International Conference on Machine Learning",
        "year": 2014,
        "citationCount": 6557,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1409.7495, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2825246",
                "name": "Yaroslav Ganin"
            },
            {
                "authorId": "1740145",
                "name": "V. Lempitsky"
            }
        ],
        "abstract": "Top-performing deep architectures are trained on massive amounts of labeled data. In the absence of labeled data for a certain task, domain adaptation often provides an attractive option given that labeled data of similar nature but from a different domain (e.g. synthetic images) are available. Here, we propose a new approach to domain adaptation in deep architectures that can be trained on large amount of labeled data from the source domain and large amount of unlabeled data from the target domain (no labeled target-domain data is necessary). \nAs the training progresses, the approach promotes the emergence of \"deep\" features that are (i) discriminative for the main learning task on the source domain and (ii) invariant with respect to the shift between the domains. We show that this adaptation behaviour can be achieved in almost any feed-forward model by augmenting it with few standard layers and a simple new gradient reversal layer. The resulting augmented architecture can be trained using standard backpropagation. \nOverall, the approach can be implemented with little effort using any of the deep-learning packages. The method performs very well in a series of image classification experiments, achieving adaptation effect in the presence of big domain shifts and outperforming previous state-of-the-art on Office datasets."
    },
    {
        "paperId": "484ad17c926292fbe0d5211540832a8c8a8e958b",
        "url": "https://www.semanticscholar.org/paper/484ad17c926292fbe0d5211540832a8c8a8e958b",
        "title": "Stochastic Backpropagation and Approximate Inference in Deep Generative Models",
        "venue": "International Conference on Machine Learning",
        "year": 2014,
        "citationCount": 5505,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "1748523",
                "name": "Danilo Jimenez Rezende"
            },
            {
                "authorId": "14594344",
                "name": "S. Mohamed"
            },
            {
                "authorId": "1688276",
                "name": "Daan Wierstra"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "687d0e59d5c35f022ce4638b3e3a6142068efc94",
        "url": "https://www.semanticscholar.org/paper/687d0e59d5c35f022ce4638b3e3a6142068efc94",
        "title": "Deterministic Policy Gradient Algorithms",
        "venue": "International Conference on Machine Learning",
        "year": 2014,
        "citationCount": 4309,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "145824029",
                "name": "David Silver"
            },
            {
                "authorId": "3276293",
                "name": "Guy Lever"
            },
            {
                "authorId": "2801204",
                "name": "N. Heess"
            },
            {
                "authorId": "1804488",
                "name": "T. Degris"
            },
            {
                "authorId": "1688276",
                "name": "Daan Wierstra"
            },
            {
                "authorId": "3137672",
                "name": "Martin A. Riedmiller"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "f3de86aeb442216a8391befcacb49e58b478f512",
        "url": "https://www.semanticscholar.org/paper/f3de86aeb442216a8391befcacb49e58b478f512",
        "title": "Distributed Representations of Sentences and Documents",
        "venue": "International Conference on Machine Learning",
        "year": 2014,
        "citationCount": 9594,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1405.4053, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2827616",
                "name": "Quoc V. Le"
            },
            {
                "authorId": "2047446108",
                "name": "Tomas Mikolov"
            }
        ],
        "abstract": "Many machine learning algorithms require the input to be represented as a fixed-length feature vector. When it comes to texts, one of the most common fixed-length features is bag-of-words. Despite their popularity, bag-of-words features have two major weaknesses: they lose the ordering of the words and they also ignore semantics of the words. For example, \"powerful,\" \"strong\" and \"Paris\" are equally distant. In this paper, we propose Paragraph Vector, an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents. Our algorithm represents each document by a dense vector which is trained to predict words in the document. Its construction gives our algorithm the potential to overcome the weaknesses of bag-of-words models. Empirical results show that Paragraph Vectors outperforms bag-of-words models as well as other techniques for text representations. Finally, we achieve new state-of-the-art results on several text classification and sentiment analysis tasks."
    },
    {
        "paperId": "0f7c85357c366b314b5b55c400869a62fd23372c",
        "url": "https://www.semanticscholar.org/paper/0f7c85357c366b314b5b55c400869a62fd23372c",
        "title": "Train faster, generalize better: Stability of stochastic gradient descent",
        "venue": "International Conference on Machine Learning",
        "year": 2015,
        "citationCount": 1335,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1509.01240, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1775622",
                "name": "Moritz Hardt"
            },
            {
                "authorId": "9229182",
                "name": "B. Recht"
            },
            {
                "authorId": "1740765",
                "name": "Y. Singer"
            }
        ],
        "abstract": "We show that parametric models trained by a stochastic gradient method (SGM) with few iterations have vanishing generalization error. We prove our results by arguing that SGM is algorithmically stable in the sense of Bousquet and Elisseeff. Our analysis only employs elementary tools from convex and continuous optimization. We derive stability bounds for both convex and non-convex optimization under standard Lipschitz and smoothness assumptions. \nApplying our results to the convex case, we provide new insights for why multiple epochs of stochastic gradient methods generalize well in practice. In the non-convex case, we give a new interpretation of common practices in neural networks, and formally show that popular techniques for training large deep models are indeed stability-promoting. Our findings conceptually underscore the importance of reducing training time beyond its obvious benefit."
    },
    {
        "paperId": "0f899b92b7fb03b609fee887e4b6f3b633eaf30d",
        "url": "https://www.semanticscholar.org/paper/0f899b92b7fb03b609fee887e4b6f3b633eaf30d",
        "title": "Variational Inference with Normalizing Flows",
        "venue": "International Conference on Machine Learning",
        "year": 2015,
        "citationCount": 4618,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1505.05770, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1748523",
                "name": "Danilo Jimenez Rezende"
            },
            {
                "authorId": "14594344",
                "name": "S. Mohamed"
            }
        ],
        "abstract": "The choice of approximate posterior distribution is one of the core problems in variational inference. Most applications of variational inference employ simple families of posterior approximations in order to allow for efficient inference, focusing on mean-field or other simple structured approximations. This restriction has a significant impact on the quality of inferences made using variational methods. We introduce a new approach for specifying flexible, arbitrarily complex and scalable approximate posterior distributions. Our approximations are distributions constructed through a normalizing flow, whereby a simple initial density is transformed into a more complex one by applying a sequence of invertible transformations until a desired level of complexity is attained. We use this view of normalizing flows to develop categories of finite and infinitesimal flows and provide a unified view of approaches for constructing rich posterior approximations. We demonstrate that the theoretical advantages of having posteriors that better match the true posterior, combined with the scalability of amortized variational approaches, provides a clear improvement in performance and applicability of variational inference."
    },
    {
        "paperId": "13497bd108d4412d02050e646235f456568cf822",
        "url": "https://www.semanticscholar.org/paper/13497bd108d4412d02050e646235f456568cf822",
        "title": "Deep Speech 2 : End-to-End Speech Recognition in English and Mandarin",
        "venue": "International Conference on Machine Learning",
        "year": 2015,
        "citationCount": 3098,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1512.02595, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2698777",
                "name": "Dario Amodei"
            },
            {
                "authorId": "39436202",
                "name": "S. Ananthanarayanan"
            },
            {
                "authorId": "2432216",
                "name": "Rishita Anubhai"
            },
            {
                "authorId": "2113830293",
                "name": "Jin Bai"
            },
            {
                "authorId": "5697774",
                "name": "Eric Battenberg"
            },
            {
                "authorId": "2065131508",
                "name": "Carl Case"
            },
            {
                "authorId": "48991386",
                "name": "J. Casper"
            },
            {
                "authorId": "2301680",
                "name": "Bryan Catanzaro"
            },
            {
                "authorId": "2108756214",
                "name": "Jingdong Chen"
            },
            {
                "authorId": "35977287",
                "name": "Mike Chrzanowski"
            },
            {
                "authorId": "144638694",
                "name": "Adam Coates"
            },
            {
                "authorId": "2040049",
                "name": "G. Diamos"
            },
            {
                "authorId": "152585800",
                "name": "Erich Elsen"
            },
            {
                "authorId": "9695761",
                "name": "Jesse Engel"
            },
            {
                "authorId": "3275727",
                "name": "Linxi (Jim) Fan"
            },
            {
                "authorId": "2910729",
                "name": "Christopher Fougner"
            },
            {
                "authorId": "144479015",
                "name": "Awni Y. Hannun"
            },
            {
                "authorId": "34601942",
                "name": "Billy Jun"
            },
            {
                "authorId": "2075302316",
                "name": "T. Han"
            },
            {
                "authorId": "3081566",
                "name": "P. LeGresley"
            },
            {
                "authorId": "1898780",
                "name": "Xiangang Li"
            },
            {
                "authorId": "2217692742",
                "name": "Libby Lin"
            },
            {
                "authorId": "46617804",
                "name": "Sharan Narang"
            },
            {
                "authorId": "34699434",
                "name": "A. Ng"
            },
            {
                "authorId": "1955694",
                "name": "Sherjil Ozair"
            },
            {
                "authorId": "3283879",
                "name": "R. Prenger"
            },
            {
                "authorId": "2112296522",
                "name": "Sheng Qian"
            },
            {
                "authorId": "34042420",
                "name": "Jonathan Raiman"
            },
            {
                "authorId": "145031342",
                "name": "S. Satheesh"
            },
            {
                "authorId": "2100685",
                "name": "David Seetapun"
            },
            {
                "authorId": "2264597",
                "name": "Shubho Sengupta"
            },
            {
                "authorId": "8401284",
                "name": "Anuroop Sriram"
            },
            {
                "authorId": "2118358652",
                "name": "Chong-Jun Wang"
            },
            {
                "authorId": "46393411",
                "name": "Yi Wang"
            },
            {
                "authorId": "2108331749",
                "name": "Zhiqian Wang"
            },
            {
                "authorId": "2054422231",
                "name": "Bo Xiao"
            },
            {
                "authorId": "2116345992",
                "name": "Yan Xie"
            },
            {
                "authorId": "1755465",
                "name": "Dani Yogatama"
            },
            {
                "authorId": "71424435",
                "name": "J. Zhan"
            },
            {
                "authorId": "2042558",
                "name": "Zhenyao Zhu"
            }
        ],
        "abstract": "We show that an end-to-end deep learning approach can be used to recognize either English or Mandarin Chinese speech-two vastly different languages. Because it replaces entire pipelines of hand-engineered components with neural networks, end-to-end learning allows us to handle a diverse variety of speech including noisy environments, accents and different languages. Key to our approach is our application of HPC techniques, enabling experiments that previously took weeks to now run in days. This allows us to iterate more quickly to identify superior architectures and algorithms. As a result, in several cases, our system is competitive with the transcription of human workers when benchmarked on standard datasets. Finally, using a technique called Batch Dispatch with GPUs in the data center, we show that our system can be inexpensively deployed in an online setting, delivering low latency when serving users at scale."
    },
    {
        "paperId": "2dcef55a07f8607a819c21fe84131ea269cc2e3c",
        "url": "https://www.semanticscholar.org/paper/2dcef55a07f8607a819c21fe84131ea269cc2e3c",
        "title": "Deep Unsupervised Learning using Nonequilibrium Thermodynamics",
        "venue": "International Conference on Machine Learning",
        "year": 2015,
        "citationCount": 8772,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1503.03585, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1407546424",
                "name": "Jascha Narain Sohl-Dickstein"
            },
            {
                "authorId": "2144479710",
                "name": "Eric A. Weiss"
            },
            {
                "authorId": "2333223",
                "name": "Niru Maheswaranathan"
            },
            {
                "authorId": "25769960",
                "name": "S. Ganguli"
            }
        ],
        "abstract": "A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm."
    },
    {
        "paperId": "449532187c94af3dd3aa55e16d2c50f7854d2199",
        "url": "https://www.semanticscholar.org/paper/449532187c94af3dd3aa55e16d2c50f7854d2199",
        "title": "Trust Region Policy Optimization",
        "venue": "International Conference on Machine Learning",
        "year": 2015,
        "citationCount": 7459,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1502.05477, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "47971768",
                "name": "John Schulman"
            },
            {
                "authorId": "1736651",
                "name": "S. Levine"
            },
            {
                "authorId": "1689992",
                "name": "P. Abbeel"
            },
            {
                "authorId": "1694621",
                "name": "Michael I. Jordan"
            },
            {
                "authorId": "29912342",
                "name": "Philipp Moritz"
            }
        ],
        "abstract": "In this article, we describe a method for optimizing control policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified scheme, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters."
    },
    {
        "paperId": "452059171226626718eb677358836328f884298e",
        "url": "https://www.semanticscholar.org/paper/452059171226626718eb677358836328f884298e",
        "title": "Ask Me Anything: Dynamic Memory Networks for Natural Language Processing",
        "venue": "International Conference on Machine Learning",
        "year": 2015,
        "citationCount": 1209,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1506.07285, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2119320633",
                "name": "A. Kumar"
            },
            {
                "authorId": "2329943",
                "name": "Ozan Irsoy"
            },
            {
                "authorId": "3214791",
                "name": "Peter Ondruska"
            },
            {
                "authorId": "2136562",
                "name": "Mohit Iyyer"
            },
            {
                "authorId": "2065251344",
                "name": "James Bradbury"
            },
            {
                "authorId": "2708454",
                "name": "Ishaan Gulrajani"
            },
            {
                "authorId": "3428769",
                "name": "Victor Zhong"
            },
            {
                "authorId": "2896063",
                "name": "Romain Paulus"
            },
            {
                "authorId": "2166511",
                "name": "R. Socher"
            }
        ],
        "abstract": "Most tasks in natural language processing can be cast into question answering (QA) problems over language input. We introduce the dynamic memory network (DMN), a neural network architecture which processes input sequences and questions, forms episodic memories, and generates relevant answers. Questions trigger an iterative attention process which allows the model to condition its attention on the inputs and the result of previous iterations. These results are then reasoned over in a hierarchical recurrent sequence model to generate answers. The DMN can be trained end-to-end and obtains state-of-the-art results on several types of tasks and datasets: question answering (Facebook's bAbI dataset), text classification for sentiment analysis (Stanford Sentiment Treebank) and sequence modeling for part-of-speech tagging (WSJ-PTB). The training for these different tasks relies exclusively on trained word vector representations and input-question-answer triplets."
    },
    {
        "paperId": "4b18303edf701e41a288da36f8f1ba129da67eb7",
        "url": "https://www.semanticscholar.org/paper/4b18303edf701e41a288da36f8f1ba129da67eb7",
        "title": "An embarrassingly simple approach to zero-shot learning",
        "venue": "International Conference on Machine Learning",
        "year": 2015,
        "citationCount": 1309,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/978-3-319-50077-5_2?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/978-3-319-50077-5_2, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1403031665",
                "name": "Bernardino Romera-Paredes"
            },
            {
                "authorId": "143635540",
                "name": "Philip H. S. Torr"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "4c05d7caa357148f0bbd61720bdd35f0bc05eb81",
        "url": "https://www.semanticscholar.org/paper/4c05d7caa357148f0bbd61720bdd35f0bc05eb81",
        "title": "Dueling Network Architectures for Deep Reinforcement Learning",
        "venue": "International Conference on Machine Learning",
        "year": 2015,
        "citationCount": 4157,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1511.06581, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2117966548",
                "name": "Ziyun Wang"
            },
            {
                "authorId": "1725157",
                "name": "T. Schaul"
            },
            {
                "authorId": "39357484",
                "name": "Matteo Hessel"
            },
            {
                "authorId": "7634925",
                "name": "H. V. Hasselt"
            },
            {
                "authorId": "1975889",
                "name": "Marc Lanctot"
            },
            {
                "authorId": "1737568",
                "name": "Nando de Freitas"
            }
        ],
        "abstract": "In recent years there have been many successes of using deep representations in reinforcement learning. Still, many of these applications use conventional architectures, such as convolutional networks, LSTMs, or auto-encoders. In this paper, we present a new neural network architecture for model-free reinforcement learning. Our dueling network represents two separate estimators: one for the state value function and one for the state-dependent action advantage function. The main benefit of this factoring is to generalize learning across actions without imposing any change to the underlying reinforcement learning algorithm. Our results show that this architecture leads to better policy evaluation in the presence of many similar-valued actions. Moreover, the dueling architecture enables our RL agent to outperform the state-of-the-art on the Atari 2600 domain."
    },
    {
        "paperId": "4d8f2d14af5991d4f0d050d22216825cac3157bd",
        "url": "https://www.semanticscholar.org/paper/4d8f2d14af5991d4f0d050d22216825cac3157bd",
        "title": "Show, Attend and Tell: Neural Image Caption Generation with Visual Attention",
        "venue": "International Conference on Machine Learning",
        "year": 2015,
        "citationCount": 10558,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1502.03044, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2117101253",
                "name": "Ke Xu"
            },
            {
                "authorId": "2503659",
                "name": "Jimmy Ba"
            },
            {
                "authorId": "3450996",
                "name": "Ryan Kiros"
            },
            {
                "authorId": "1979489",
                "name": "Kyunghyun Cho"
            },
            {
                "authorId": "1760871",
                "name": "Aaron C. Courville"
            },
            {
                "authorId": "145124475",
                "name": "R. Salakhutdinov"
            },
            {
                "authorId": "1804104",
                "name": "R. Zemel"
            },
            {
                "authorId": "1751762",
                "name": "Yoshua Bengio"
            }
        ],
        "abstract": "Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-the-art performance on three benchmark datasets: Flickr9k, Flickr30k and MS COCO."
    },
    {
        "paperId": "5b8364c21155d3d2cd38ea4c8b8580beba9a3250",
        "url": "https://www.semanticscholar.org/paper/5b8364c21155d3d2cd38ea4c8b8580beba9a3250",
        "title": "An Empirical Exploration of Recurrent Network Architectures",
        "venue": "International Conference on Machine Learning",
        "year": 2015,
        "citationCount": 1763,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "1944541",
                "name": "R. Jzefowicz"
            },
            {
                "authorId": "2563432",
                "name": "Wojciech Zaremba"
            },
            {
                "authorId": "1701686",
                "name": "I. Sutskever"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "5dc2a215bd7cd5bdd3a0baa8c967575632696fac",
        "url": "https://www.semanticscholar.org/paper/5dc2a215bd7cd5bdd3a0baa8c967575632696fac",
        "title": "Universal Value Function Approximators",
        "venue": "International Conference on Machine Learning",
        "year": 2015,
        "citationCount": 1140,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "1725157",
                "name": "T. Schaul"
            },
            {
                "authorId": "48257711",
                "name": "Dan Horgan"
            },
            {
                "authorId": "144717963",
                "name": "Karol Gregor"
            },
            {
                "authorId": "145824029",
                "name": "David Silver"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "62adfea3cc1cd9eb6b53e0e8a40be5dfda2adf8d",
        "url": "https://www.semanticscholar.org/paper/62adfea3cc1cd9eb6b53e0e8a40be5dfda2adf8d",
        "title": "Weight Uncertainty in Neural Network",
        "venue": "International Conference on Machine Learning",
        "year": 2015,
        "citationCount": 2191,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "1723876",
                "name": "C. Blundell"
            },
            {
                "authorId": "3248702",
                "name": "Julien Cornebise"
            },
            {
                "authorId": "2645384",
                "name": "K. Kavukcuoglu"
            },
            {
                "authorId": "1688276",
                "name": "Daan Wierstra"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "66021a920001bc3e6258bffe7076d647614147b7",
        "url": "https://www.semanticscholar.org/paper/66021a920001bc3e6258bffe7076d647614147b7",
        "title": "From Word Embeddings To Document Distances",
        "venue": "International Conference on Machine Learning",
        "year": 2015,
        "citationCount": 2207,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "1940272",
                "name": "Matt J. Kusner"
            },
            {
                "authorId": "2117103358",
                "name": "Yu Sun"
            },
            {
                "authorId": "1971973",
                "name": "Nicholas I. Kolkin"
            },
            {
                "authorId": "7446832",
                "name": "Kilian Q. Weinberger"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "7340f090f8a0df5b109682e9f6d57e4b8ca1a2f7",
        "url": "https://www.semanticscholar.org/paper/7340f090f8a0df5b109682e9f6d57e4b8ca1a2f7",
        "title": "Learning Transferable Features with Deep Adaptation Networks",
        "venue": "International Conference on Machine Learning",
        "year": 2015,
        "citationCount": 5642,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1502.02791, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "35776445",
                "name": "Mingsheng Long"
            },
            {
                "authorId": "2146174097",
                "name": "Yue Cao"
            },
            {
                "authorId": "2144499343",
                "name": "Jianmin Wang"
            },
            {
                "authorId": "1694621",
                "name": "Michael I. Jordan"
            }
        ],
        "abstract": "Recent studies reveal that a deep neural network can learn transferable features which generalize well to novel tasks for domain adaptation. However, as deep features eventually transition from general to specific along the network, the feature transferability drops significantly in higher layers with increasing domain discrepancy. Hence, it is important to formally reduce the dataset bias and enhance the transferability in task-specific layers. In this paper, we propose a new Deep Adaptation Network (DAN) architecture, which generalizes deep convolutional neural network to the domain adaptation scenario. In DAN, hidden representations of all task-specific layers are embedded in a reproducing kernel Hilbert space where the mean embeddings of different domain distributions can be explicitly matched. The domain discrepancy is further reduced using an optimal multikernel selection method for mean embedding matching. DAN can learn transferable features with statistical guarantees, and can scale linearly by unbiased estimate of kernel embedding. Extensive empirical evidence shows that the proposed architecture yields state-of-the-art image classification error rates on standard domain adaptation benchmarks."
    },
    {
        "paperId": "829510ad6f975c939d589eeb01a3cf6fc6c8ce4d",
        "url": "https://www.semanticscholar.org/paper/829510ad6f975c939d589eeb01a3cf6fc6c8ce4d",
        "title": "Unsupervised Learning of Video Representations using LSTMs",
        "venue": "International Conference on Machine Learning",
        "year": 2015,
        "citationCount": 2675,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1502.04681, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2897313",
                "name": "Nitish Srivastava"
            },
            {
                "authorId": "2711409",
                "name": "Elman Mansimov"
            },
            {
                "authorId": "145124475",
                "name": "R. Salakhutdinov"
            }
        ],
        "abstract": "We use Long Short Term Memory (LSTM) networks to learn representations of video sequences. Our model uses an encoder LSTM to map an input sequence into a fixed length representation. This representation is decoded using single or multiple decoder LSTMs to perform different tasks, such as reconstructing the input sequence, or predicting the future sequence. We experiment with two kinds of input sequences - patches of image pixels and high-level representations (\"percepts\") of video frames extracted using a pretrained convolutional net. We explore different design choices such as whether the decoder LSTMs should condition on the generated output. We analyze the outputs of the model qualitatively to see how well the model can extrapolate the learned video representation into the future and into the past. We further evaluate the representations by finetuning them for a supervised learning problem - human action recognition on the UCF-101 and HMDB-51 datasets. We show that the representations help improve classification accuracy, especially when there are only few training examples. Even models pretrained on unrelated datasets (300 hours of YouTube videos) can help action recognition performance."
    },
    {
        "paperId": "93bc65d2842b8cc5f3cf72ebc5b8f75daeacea35",
        "url": "https://www.semanticscholar.org/paper/93bc65d2842b8cc5f3cf72ebc5b8f75daeacea35",
        "title": "Scalable Bayesian Optimization Using Deep Neural Networks",
        "venue": "International Conference on Machine Learning",
        "year": 2015,
        "citationCount": 1098,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1502.05700, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "144108062",
                "name": "Jasper Snoek"
            },
            {
                "authorId": "1757324",
                "name": "Oren Rippel"
            },
            {
                "authorId": "1754860",
                "name": "Kevin Swersky"
            },
            {
                "authorId": "3450996",
                "name": "Ryan Kiros"
            },
            {
                "authorId": "143758120",
                "name": "N. Satish"
            },
            {
                "authorId": "1789372",
                "name": "N. Sundaram"
            },
            {
                "authorId": "8176660",
                "name": "Md. Mostofa Ali Patwary"
            },
            {
                "authorId": "1764912",
                "name": "Prabhat"
            },
            {
                "authorId": "1722180",
                "name": "Ryan P. Adams"
            }
        ],
        "abstract": "Bayesian optimization is an effective methodology for the global optimization of functions with expensive evaluations. It relies on querying a distribution over functions defined by a relatively cheap surrogate model. An accurate model for this distribution over functions is critical to the effectiveness of the approach, and is typically fit using Gaussian processes (GPs). However, since GPs scale cubically with the number of observations, it has been challenging to handle objectives whose optimization requires many evaluations, and as such, massively parallelizing the optimization. \n \nIn this work, we explore the use of neural networks as an alternative to GPs to model distributions over functions. We show that performing adaptive basis function regression with a neural network as the parametric form performs competitively with state-of-the-art GP-based approaches, but scales linearly with the number of data rather than cubically. This allows us to achieve a previously intractable degree of parallelism, which we apply to large scale hyperparameter optimization, rapidly finding competitive models on benchmark object recognition tasks using convolutional networks, and image caption generation using neural language models."
    },
    {
        "paperId": "995c5f5e62614fcb4d2796ad2faab969da51713e",
        "url": "https://www.semanticscholar.org/paper/995c5f5e62614fcb4d2796ad2faab969da51713e",
        "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
        "venue": "International Conference on Machine Learning",
        "year": 2015,
        "citationCount": 45617,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1502.03167, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2054165706",
                "name": "Sergey Ioffe"
            },
            {
                "authorId": "2574060",
                "name": "Christian Szegedy"
            }
        ],
        "abstract": "Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82% top-5 test error, exceeding the accuracy of human raters."
    },
    {
        "paperId": "a2785f66c20fbdf30ec26c0931584c6d6a0f4fca",
        "url": "https://www.semanticscholar.org/paper/a2785f66c20fbdf30ec26c0931584c6d6a0f4fca",
        "title": "DRAW: A Recurrent Neural Network For Image Generation",
        "venue": "International Conference on Machine Learning",
        "year": 2015,
        "citationCount": 1998,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1502.04623, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "144717963",
                "name": "Karol Gregor"
            },
            {
                "authorId": "1841008",
                "name": "Ivo Danihelka"
            },
            {
                "authorId": "1753223",
                "name": "Alex Graves"
            },
            {
                "authorId": "1748523",
                "name": "Danilo Jimenez Rezende"
            },
            {
                "authorId": "1688276",
                "name": "Daan Wierstra"
            }
        ],
        "abstract": "This paper introduces the Deep Recurrent Attentive Writer (DRAW) neural network architecture for image generation. DRAW networks combine a novel spatial attention mechanism that mimics the foveation of the human eye, with a sequential variational auto-encoding framework that allows for the iterative construction of complex images. The system substantially improves on the state of the art for generative models on MNIST, and, when trained on the Street View House Numbers dataset, it generates images that cannot be distinguished from real data with the naked eye."
    },
    {
        "paperId": "b7cf49e30355633af2db19f35189410c8515e91f",
        "url": "https://www.semanticscholar.org/paper/b7cf49e30355633af2db19f35189410c8515e91f",
        "title": "Deep Learning with Limited Numerical Precision",
        "venue": "International Conference on Machine Learning",
        "year": 2015,
        "citationCount": 2128,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1502.02551, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2116011472",
                "name": "Suyog Gupta"
            },
            {
                "authorId": "31651864",
                "name": "A. Agrawal"
            },
            {
                "authorId": "33678523",
                "name": "K. Gopalakrishnan"
            },
            {
                "authorId": "32967358",
                "name": "P. Narayanan"
            }
        ],
        "abstract": "Training of large-scale deep neural networks is often constrained by the available computational resources. We study the effect of limited precision data representation and computation on neural network training. Within the context of low-precision fixed-point computations, we observe the rounding scheme to play a crucial role in determining the network's behavior during training. Our results show that deep networks can be trained using only 16-bit wide fixed-point number representation when using stochastic rounding, and incur little to no degradation in the classification accuracy. We also demonstrate an energy-efficient hardware accelerator that implements low-precision fixed-point arithmetic with stochastic rounding."
    },
    {
        "paperId": "cb4dc7277d1c8c3bc76dd7425eb1cc7cbaf99487",
        "url": "https://www.semanticscholar.org/paper/cb4dc7277d1c8c3bc76dd7425eb1cc7cbaf99487",
        "title": "Optimizing Neural Networks with Kronecker-factored Approximate Curvature",
        "venue": "International Conference on Machine Learning",
        "year": 2015,
        "citationCount": 1151,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1503.05671, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "145704247",
                "name": "James Martens"
            },
            {
                "authorId": "1785346",
                "name": "R. Grosse"
            }
        ],
        "abstract": "We propose an efficient method for approximating natural gradient descent in neural networks which we call Kronecker-Factored Approximate Curvature (K-FAC). K-FAC is based on an efficiently invertible approximation of a neural network's Fisher information matrix which is neither diagonal nor low-rank, and in some cases is completely non-sparse. It is derived by approximating various large blocks of the Fisher (corresponding to entire layers) as being the Kronecker product of two much smaller matrices. While only several times more expensive to compute than the plain stochastic gradient, the updates produced by K-FAC make much more progress optimizing the objective, which results in an algorithm that can be much faster than stochastic gradient descent with momentum in practice. And unlike some previously proposed approximate natural-gradient/Newton methods which use high-quality non-diagonal curvature matrices (such as Hessian-free optimization), K-FAC works very well in highly stochastic optimization regimes. This is because the cost of storing and inverting K-FAC's approximation to the curvature matrix does not depend on the amount of data used to estimate it, which is a feature typically associated only with diagonal or low-rank approximations to the curvature matrix."
    },
    {
        "paperId": "e2820bffe5b42cb7d88b7f65c12171c62ab4aae2",
        "url": "https://www.semanticscholar.org/paper/e2820bffe5b42cb7d88b7f65c12171c62ab4aae2",
        "title": "Gradient-based Hyperparameter Optimization through Reversible Learning",
        "venue": "International Conference on Machine Learning",
        "year": 2015,
        "citationCount": 1000,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1502.03492, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1683298",
                "name": "D. Maclaurin"
            },
            {
                "authorId": "1704657",
                "name": "D. Duvenaud"
            },
            {
                "authorId": "1722180",
                "name": "Ryan P. Adams"
            }
        ],
        "abstract": "Tuning hyperparameters of learning algorithms is hard because gradients are usually unavailable. We compute exact gradients of cross-validation performance with respect to all hyperparameters by chaining derivatives backwards through the entire training procedure. These gradients allow us to optimize thousands of hyperparameters, including step-size and momentum schedules, weight initialization distributions, richly parameterized regularization schemes, and neural network architectures. We compute hyperparameter gradients by exactly reversing the dynamics of stochastic gradient descent with momentum."
    },
    {
        "paperId": "e8b8a7778ace2a02f8db6fe321a54520c6b283ca",
        "url": "https://www.semanticscholar.org/paper/e8b8a7778ace2a02f8db6fe321a54520c6b283ca",
        "title": "Autoencoding beyond pixels using a learned similarity metric",
        "venue": "International Conference on Machine Learning",
        "year": 2015,
        "citationCount": 2174,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1512.09300, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "46560485",
                "name": "Anders Boesen Lindbo Larsen"
            },
            {
                "authorId": "1388358166",
                "name": "Sren Kaae Snderby"
            },
            {
                "authorId": "1777528",
                "name": "H. Larochelle"
            },
            {
                "authorId": "1724252",
                "name": "O. Winther"
            }
        ],
        "abstract": "We present an autoencoder that leverages learned representations to better measure similarities in data space. By combining a variational autoencoder with a generative adversarial network we can use learned feature representations in the GAN discriminator as basis for the VAE reconstruction objective. Thereby, we replace element-wise errors with feature-wise errors to better capture the data distribution while offering invariance towards e.g. translation. We apply our method to images of faces and show that it outperforms VAEs with element-wise similarity measures in terms of visual fidelity. Moreover, we show that the method learns an embedding in which high-level abstract visual features (e.g. wearing glasses) can be modified using simple arithmetic."
    },
    {
        "paperId": "efb5032e6199c80f83309fd866b25be9545831fd",
        "url": "https://www.semanticscholar.org/paper/efb5032e6199c80f83309fd866b25be9545831fd",
        "title": "Compressing Neural Networks with the Hashing Trick",
        "venue": "International Conference on Machine Learning",
        "year": 2015,
        "citationCount": 1223,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1504.04788, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": null,
                "name": "Wenlin Chen"
            },
            {
                "authorId": "152983984",
                "name": "James T. Wilson"
            },
            {
                "authorId": "2342481",
                "name": "Stephen Tyree"
            },
            {
                "authorId": "7446832",
                "name": "Kilian Q. Weinberger"
            },
            {
                "authorId": "2116664181",
                "name": "Yixin Chen"
            }
        ],
        "abstract": "As deep nets are increasingly used in applications suited for mobile devices, a fundamental dilemma becomes apparent: the trend in deep learning is to grow models to absorb ever-increasing data set sizes; however mobile devices are designed with very little memory and cannot store such large models. We present a novel network architecture, HashedNets, that exploits inherent redundancy in neural networks to achieve drastic reductions in model sizes. HashedNets uses a low-cost hash function to randomly group connection weights into hash buckets, and all connections within the same hash bucket share a single parameter value. These parameters are tuned to adjust to the HashedNets weight sharing architecture with standard backprop during training. Our hashing procedure introduces no additional memory overhead, and we demonstrate on several benchmark data sets that HashedNets shrink the storage requirements of neural networks substantially while mostly preserving generalization performance."
    },
    {
        "paperId": "f35de4f9b1a7c4d3fa96a0d2ab1bf8937671f6b6",
        "url": "https://www.semanticscholar.org/paper/f35de4f9b1a7c4d3fa96a0d2ab1bf8937671f6b6",
        "title": "Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning",
        "venue": "International Conference on Machine Learning",
        "year": 2015,
        "citationCount": 10628,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1506.02142, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2681954",
                "name": "Y. Gal"
            },
            {
                "authorId": "1744700",
                "name": "Zoubin Ghahramani"
            }
        ],
        "abstract": "Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs -- extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout's uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout's uncertainty in deep reinforcement learning."
    },
    {
        "paperId": "f44ff4fc0ed0142cb18472a5ba421bb538aa837e",
        "url": "https://www.semanticscholar.org/paper/f44ff4fc0ed0142cb18472a5ba421bb538aa837e",
        "title": "Unsupervised Deep Embedding for Clustering Analysis",
        "venue": "International Conference on Machine Learning",
        "year": 2015,
        "citationCount": 3198,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1511.06335, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2369548",
                "name": "Junyuan Xie"
            },
            {
                "authorId": "2983898",
                "name": "Ross B. Girshick"
            },
            {
                "authorId": "143787583",
                "name": "Ali Farhadi"
            }
        ],
        "abstract": "Clustering is central to many data-driven application domains and has been studied extensively in terms of distance functions and grouping algorithms. Relatively little work has focused on learning representations for clustering. In this paper, we propose Deep Embedded Clustering (DEC), a method that simultaneously learns feature representations and cluster assignments using deep neural networks. DEC learns a mapping from the data space to a lower-dimensional feature space in which it iteratively optimizes a clustering objective. Our experimental evaluations on image and text corpora show significant improvement over state-of-the-art methods."
    },
    {
        "paperId": "04162cb8cfaa0f7e37586823ff4ad0bff09ed21d",
        "url": "https://www.semanticscholar.org/paper/04162cb8cfaa0f7e37586823ff4ad0bff09ed21d",
        "title": "Guided Cost Learning: Deep Inverse Optimal Control via Policy Optimization",
        "venue": "International Conference on Machine Learning",
        "year": 2016,
        "citationCount": 1011,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1603.00448, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "46881670",
                "name": "Chelsea Finn"
            },
            {
                "authorId": "1736651",
                "name": "S. Levine"
            },
            {
                "authorId": "1689992",
                "name": "P. Abbeel"
            }
        ],
        "abstract": "Reinforcement learning can acquire complex behaviors from high-level specifications. However, defining a cost function that can be optimized effectively and encodes the correct task is challenging in practice. We explore how inverse optimal control (IOC) can be used to learn behaviors from demonstrations, with applications to torque control of high-dimensional robotic systems. Our method addresses two key challenges in inverse optimal control: first, the need for informative features and effective regularization to impose structure on the cost, and second, the difficulty of learning the cost function under unknown dynamics for high-dimensional continuous systems. To address the former challenge, we present an algorithm capable of learning arbitrary nonlinear cost functions, such as neural networks, without meticulous feature engineering. To address the latter challenge, we formulate an efficient sample-based approximation for MaxEnt IOC. We evaluate our method on a series of simulated tasks and real-world robotic manipulation problems, demonstrating substantial improvement over prior methods both in terms of task complexity and sample efficiency."
    },
    {
        "paperId": "1464776f20e2bccb6182f183b5ff2e15b0ae5e56",
        "url": "https://www.semanticscholar.org/paper/1464776f20e2bccb6182f183b5ff2e15b0ae5e56",
        "title": "Benchmarking Deep Reinforcement Learning for Continuous Control",
        "venue": "International Conference on Machine Learning",
        "year": 2016,
        "citationCount": 1762,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1604.06778, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "144581158",
                "name": "Yan Duan"
            },
            {
                "authorId": "41192764",
                "name": "Xi Chen"
            },
            {
                "authorId": "3127100",
                "name": "Rein Houthooft"
            },
            {
                "authorId": "47971768",
                "name": "John Schulman"
            },
            {
                "authorId": "1689992",
                "name": "P. Abbeel"
            }
        ],
        "abstract": "Recently, researchers have made significant progress combining the advances in deep learning for learning feature representations with reinforcement learning. Some notable examples include training agents to play Atari games based on raw pixel data and to acquire advanced manipulation skills using raw sensory inputs. However, it has been difficult to quantify progress in the domain of continuous control due to the lack of a commonly adopted benchmark. In this work, we present a benchmark suite of continuous control tasks, including classic tasks like cart-pole swing-up, tasks with very high state and action dimensionality such as 3D humanoid locomotion, tasks with partial observations, and tasks with hierarchical structure. We report novel findings based on the systematic evaluation of a range of implemented reinforcement learning algorithms. Both the benchmark and reference implementations are released at this https URL in order to facilitate experimental reproducibility and to encourage adoption by other researchers."
    },
    {
        "paperId": "17409ae85c237983dc22a5e4750d8054e4b6edc9",
        "url": "https://www.semanticscholar.org/paper/17409ae85c237983dc22a5e4750d8054e4b6edc9",
        "title": "Estimating individual treatment effect: generalization bounds and algorithms",
        "venue": "International Conference on Machine Learning",
        "year": 2016,
        "citationCount": 1152,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "2304764",
                "name": "Uri Shalit"
            },
            {
                "authorId": "144602383",
                "name": "Fredrik D. Johansson"
            },
            {
                "authorId": "1746662",
                "name": "D. Sontag"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "2218e2e1df2c3adfb70e0def2e326a39928aacfc",
        "url": "https://www.semanticscholar.org/paper/2218e2e1df2c3adfb70e0def2e326a39928aacfc",
        "title": "Complex Embeddings for Simple Link Prediction",
        "venue": "International Conference on Machine Learning",
        "year": 2016,
        "citationCount": 3359,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1606.06357, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2057146",
                "name": "Tho Trouillon"
            },
            {
                "authorId": "1851564",
                "name": "Johannes Welbl"
            },
            {
                "authorId": "48662861",
                "name": "Sebastian Riedel"
            },
            {
                "authorId": "1732180",
                "name": "ric Gaussier"
            },
            {
                "authorId": "1684865",
                "name": "Guillaume Bouchard"
            }
        ],
        "abstract": "In statistical relational learning, the link prediction problem is key to automatically understand the structure of large knowledge bases. As in previous studies, we propose to solve this problem through latent factorization. However, here we make use of complex valued embeddings. The composition of complex embeddings can handle a large variety of binary relations, among them symmetric and antisymmetric relations. Compared to state-of-the-art models such as Neural Tensor Network and Holographic Embeddings, our approach based on complex embeddings is arguably simpler, as it only uses the Hermitian dot product, the complex counterpart of the standard dot product between real vectors. Our approach is scalable to large datasets as it remains linear in both space and time, while consistently outperforming alternative approaches on standard link prediction benchmarks."
    },
    {
        "paperId": "3904315e2eca50d0086e4b7273f7fd707c652230",
        "url": "https://www.semanticscholar.org/paper/3904315e2eca50d0086e4b7273f7fd707c652230",
        "title": "Meta-Learning with Memory-Augmented Neural Networks",
        "venue": "International Conference on Machine Learning",
        "year": 2016,
        "citationCount": 1946,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "35030998",
                "name": "Adam Santoro"
            },
            {
                "authorId": "2258504",
                "name": "Sergey Bartunov"
            },
            {
                "authorId": "46378362",
                "name": "M. Botvinick"
            },
            {
                "authorId": "1688276",
                "name": "Daan Wierstra"
            },
            {
                "authorId": "2542999",
                "name": "T. Lillicrap"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "3d846cb01f6a975554035d2210b578ca61344b22",
        "url": "https://www.semanticscholar.org/paper/3d846cb01f6a975554035d2210b578ca61344b22",
        "title": "Revisiting Semi-Supervised Learning with Graph Embeddings",
        "venue": "International Conference on Machine Learning",
        "year": 2016,
        "citationCount": 2341,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1603.08861, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2109512754",
                "name": "Zhilin Yang"
            },
            {
                "authorId": "50056360",
                "name": "William W. Cohen"
            },
            {
                "authorId": "145124475",
                "name": "R. Salakhutdinov"
            }
        ],
        "abstract": "We present a semi-supervised learning framework based on graph embeddings. Given a graph between instances, we train an embedding for each instance to jointly predict the class label and the neighborhood context in the graph. We develop both transductive and inductive variants of our method. In the transductive variant of our method, the class labels are determined by both the learned embeddings and input feature vectors, while in the inductive variant, the embeddings are defined as a parametric function of the feature vectors, so predictions can be made on instances not seen during training. On a large and diverse set of benchmark tasks, including text classification, distantly supervised entity extraction, and entity classification, we show improved performance over many of the existing models."
    },
    {
        "paperId": "41f1d50c85d3180476c4c7b3eea121278b0d8474",
        "url": "https://www.semanticscholar.org/paper/41f1d50c85d3180476c4c7b3eea121278b0d8474",
        "title": "Pixel Recurrent Neural Networks",
        "venue": "International Conference on Machine Learning",
        "year": 2016,
        "citationCount": 2730,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1601.06759, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3422336",
                "name": "Aron van den Oord"
            },
            {
                "authorId": "2583391",
                "name": "Nal Kalchbrenner"
            },
            {
                "authorId": "2645384",
                "name": "K. Kavukcuoglu"
            }
        ],
        "abstract": "Modeling the distribution of natural images is a landmark problem in unsupervised learning. This task requires an image model that is at once expressive, tractable and scalable. We present a deep neural network that sequentially predicts the pixels in an image along the two spatial dimensions. Our method models the discrete probability of the raw pixel values and encodes the complete set of dependencies in the image. Architectural novelties include fast two-dimensional recurrent layers and an effective use of residual connections in deep recurrent networks. We achieve log-likelihood scores on natural images that are considerably better than the previous state of the art. Our main results also provide benchmarks on the diverse ImageNet dataset. Samples generated from the model appear crisp, varied and globally coherent."
    },
    {
        "paperId": "69e76e16740ed69f4dc55361a3d319ac2f1293dd",
        "url": "https://www.semanticscholar.org/paper/69e76e16740ed69f4dc55361a3d319ac2f1293dd",
        "title": "Asynchronous Methods for Deep Reinforcement Learning",
        "venue": "International Conference on Machine Learning",
        "year": 2016,
        "citationCount": 9606,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1602.01783, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3255983",
                "name": "Volodymyr Mnih"
            },
            {
                "authorId": "36045539",
                "name": "Adri Puigdomnech Badia"
            },
            {
                "authorId": "153583218",
                "name": "Mehdi Mirza"
            },
            {
                "authorId": "1753223",
                "name": "Alex Graves"
            },
            {
                "authorId": "2542999",
                "name": "T. Lillicrap"
            },
            {
                "authorId": "3367786",
                "name": "Tim Harley"
            },
            {
                "authorId": "145824029",
                "name": "David Silver"
            },
            {
                "authorId": "2645384",
                "name": "K. Kavukcuoglu"
            }
        ],
        "abstract": "We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input."
    },
    {
        "paperId": "6c7f040a150abf21dbcefe1f22e0f98fa184f41a",
        "url": "https://www.semanticscholar.org/paper/6c7f040a150abf21dbcefe1f22e0f98fa184f41a",
        "title": "Generative Adversarial Text to Image Synthesis",
        "venue": "International Conference on Machine Learning",
        "year": 2016,
        "citationCount": 3334,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1605.05396, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "144828948",
                "name": "Scott E. Reed"
            },
            {
                "authorId": "2893664",
                "name": "Zeynep Akata"
            },
            {
                "authorId": "3084614",
                "name": "Xinchen Yan"
            },
            {
                "authorId": "2876316",
                "name": "Lajanugen Logeswaran"
            },
            {
                "authorId": "48920094",
                "name": "B. Schiele"
            },
            {
                "authorId": "1697141",
                "name": "Honglak Lee"
            }
        ],
        "abstract": "Automatic synthesis of realistic images from text would be interesting and useful, but current AI systems are still far from this goal. However, in recent years generic and powerful recurrent neural network architectures have been developed to learn discriminative text feature representations. Meanwhile, deep convolutional generative adversarial networks (GANs) have begun to generate highly compelling images of specific categories, such as faces, album covers, and room interiors. In this work, we develop a novel deep architecture and GAN formulation to effectively bridge these advances in text and image modeling, translating visual concepts from characters to pixels. We demonstrate the capability of our model to generate plausible images of birds and flowers from detailed text descriptions."
    },
    {
        "paperId": "6fd9e3cb0cf23c8ef4aa7065d9be407c45250bff",
        "url": "https://www.semanticscholar.org/paper/6fd9e3cb0cf23c8ef4aa7065d9be407c45250bff",
        "title": "Large-Margin Softmax Loss for Convolutional Neural Networks",
        "venue": "International Conference on Machine Learning",
        "year": 2016,
        "citationCount": 1536,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1612.02295, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "36326884",
                "name": "Weiyang Liu"
            },
            {
                "authorId": "145357606",
                "name": "Yandong Wen"
            },
            {
                "authorId": "1751019",
                "name": "Zhiding Yu"
            },
            {
                "authorId": "145339754",
                "name": "Meng Yang"
            }
        ],
        "abstract": "Cross-entropy loss together with softmax is arguably one of the most common used supervision components in convolutional neural networks (CNNs). Despite its simplicity, popularity and excellent performance, the component does not explicitly encourage discriminative learning of features. In this paper, we propose a generalized large-margin softmax (L-Softmax) loss which explicitly encourages intra-class compactness and inter-class separability between learned features. Moreover, L-Softmax not only can adjust the desired margin but also can avoid overfitting. We also show that the L-Softmax loss can be optimized by typical stochastic gradient descent. Extensive experiments on four benchmark datasets demonstrate that the deeply-learned features with L-softmax loss become more discriminative, hence significantly boosting the performance on a variety of visual classification and verification tasks."
    },
    {
        "paperId": "7c6de5a9e02a779e24504619050c6118f4eac181",
        "url": "https://www.semanticscholar.org/paper/7c6de5a9e02a779e24504619050c6118f4eac181",
        "title": "Learning Convolutional Neural Networks for Graphs",
        "venue": "International Conference on Machine Learning",
        "year": 2016,
        "citationCount": 2244,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1605.05273, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2780262",
                "name": "Mathias Niepert"
            },
            {
                "authorId": "24931083",
                "name": "Mohamed Ahmed"
            },
            {
                "authorId": "1712289",
                "name": "Konstantin Kutzkov"
            }
        ],
        "abstract": "Numerous important problems can be framed as learning from graph data. We propose a framework for learning convolutional neural networks for arbitrary graphs. These graphs may be undirected, directed, and with both discrete and continuous node and edge attributes. Analogous to image-based convolutional networks that operate on locally connected regions of the input, we present a general approach to extracting locally connected regions from graphs. Using established benchmark data sets, we demonstrate that the learned feature representations are competitive with state of the art graph kernels and that their computation is highly efficient."
    },
    {
        "paperId": "88caa4a0253a8b0076176745ebc072864eab66e1",
        "url": "https://www.semanticscholar.org/paper/88caa4a0253a8b0076176745ebc072864eab66e1",
        "title": "Language Modeling with Gated Convolutional Networks",
        "venue": "International Conference on Machine Learning",
        "year": 2016,
        "citationCount": 2682,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1612.08083, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2921469",
                "name": "Yann Dauphin"
            },
            {
                "authorId": "144270981",
                "name": "Angela Fan"
            },
            {
                "authorId": "2325985",
                "name": "Michael Auli"
            },
            {
                "authorId": "2529182",
                "name": "David Grangier"
            }
        ],
        "abstract": "The pre-dominant approach to language modeling to date is based on recurrent neural networks. Their success on this task is often linked to their ability to capture unbounded context. In this paper we develop a finite context approach through stacked convolutions, which can be more efficient since they allow parallelization over sequential tokens. We propose a novel simplified gating mechanism that outperforms Oord et al (2016) and investigate the impact of key architectural decisions. The proposed approach achieves state-of-the-art on the WikiText-103 benchmark, even though it features long-term dependencies, as well as competitive results on the Google Billion Words benchmark. Our model reduces the latency to score a sentence by an order of magnitude compared to a recurrent baseline. To our knowledge, this is the first time a non-recurrent approach is competitive with strong recurrent models on these large scale language tasks."
    },
    {
        "paperId": "ae9e5e72aefd19b81c1fe75d7baf6c0bedad75e5",
        "url": "https://www.semanticscholar.org/paper/ae9e5e72aefd19b81c1fe75d7baf6c0bedad75e5",
        "title": "Deep Transfer Learning with Joint Adaptation Networks",
        "venue": "International Conference on Machine Learning",
        "year": 2016,
        "citationCount": 2654,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1605.06636, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "35776445",
                "name": "Mingsheng Long"
            },
            {
                "authorId": "1485919236",
                "name": "Hanhua Zhu"
            },
            {
                "authorId": "2144499343",
                "name": "Jianmin Wang"
            },
            {
                "authorId": "1694621",
                "name": "Michael I. Jordan"
            }
        ],
        "abstract": "Deep networks have been successfully applied to learn transferable features for adapting models from a source domain to a different target domain. In this paper, we present joint adaptation networks (JAN), which learn a transfer network by aligning the joint distributions of multiple domain-specific layers across domains based on a joint maximum mean discrepancy (JMMD) criterion. Adversarial training strategy is adopted to maximize JMMD such that the distributions of the source and target domains are made more distinguishable. Learning can be performed by stochastic gradient descent with the gradients computed by back-propagation in linear-time. Experiments testify that our model yields state of the art results on standard datasets."
    },
    {
        "paperId": "c80d112ce59c72f943dc7b3e56e4c77dc3af1146",
        "url": "https://www.semanticscholar.org/paper/c80d112ce59c72f943dc7b3e56e4c77dc3af1146",
        "title": "CryptoNets: Applying Neural Networks to Encrypted Data with High Throughput and Accuracy",
        "venue": "International Conference on Machine Learning",
        "year": 2016,
        "citationCount": 1799,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "3429011",
                "name": "Nathan Dowlin"
            },
            {
                "authorId": "1388775848",
                "name": "Ran Gilad-Bachrach"
            },
            {
                "authorId": "39763956",
                "name": "Kim Laine"
            },
            {
                "authorId": "2679550",
                "name": "K. Lauter"
            },
            {
                "authorId": "1813607",
                "name": "M. Naehrig"
            },
            {
                "authorId": "2372116",
                "name": "J. Wernsing"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "d358d41c69450b171327ebd99462b6afef687269",
        "url": "https://www.semanticscholar.org/paper/d358d41c69450b171327ebd99462b6afef687269",
        "title": "Continuous Deep Q-Learning with Model-based Acceleration",
        "venue": "International Conference on Machine Learning",
        "year": 2016,
        "citationCount": 1043,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1603.00748, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2046135",
                "name": "S. Gu"
            },
            {
                "authorId": "2542999",
                "name": "T. Lillicrap"
            },
            {
                "authorId": "1701686",
                "name": "I. Sutskever"
            },
            {
                "authorId": "1736651",
                "name": "S. Levine"
            }
        ],
        "abstract": "Model-free reinforcement learning has been successfully applied to a range of challenging problems, and has recently been extended to handle large neural network policies and value functions. However, the sample complexity of modelfree algorithms, particularly when using high-dimensional function approximators, tends to limit their applicability to physical systems. In this paper, we explore algorithms and representations to reduce the sample complexity of deep reinforcement learning for continuous control tasks. We propose two complementary techniques for improving the efficiency of such algorithms. First, we derive a continuous variant of the Q-learning algorithm, which we call normalized advantage functions (NAF), as an alternative to the more commonly used policy gradient and actor-critic methods. NAF representation allows us to apply Q-learning with experience replay to continuous tasks, and substantially improves performance on a set of simulated robotic control tasks. To further improve the efficiency of our approach, we explore the use of learned models for accelerating model-free reinforcement learning. We show that iteratively refitted local linear models are especially effective for this, and demonstrate substantially faster learning on domains where such models are applicable."
    },
    {
        "paperId": "ecc0edd450ae7e52f65ddf61405b30ad6dbabdd7",
        "url": "https://www.semanticscholar.org/paper/ecc0edd450ae7e52f65ddf61405b30ad6dbabdd7",
        "title": "Conditional Image Synthesis with Auxiliary Classifier GANs",
        "venue": "International Conference on Machine Learning",
        "year": 2016,
        "citationCount": 3421,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1610.09585, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2624088",
                "name": "Augustus Odena"
            },
            {
                "authorId": "2357599482",
                "name": "Christopher Olah"
            },
            {
                "authorId": "1789737",
                "name": "Jonathon Shlens"
            }
        ],
        "abstract": "In this paper we introduce new methods for the improved training of generative adversarial networks (GANs) for image synthesis. We construct a variant of GANs employing label conditioning that results in 128 x 128 resolution image samples exhibiting global coherence. We expand on previous work for image quality assessment to provide two new analyses for assessing the discriminability and diversity of samples from class-conditional image synthesis models. These analyses demonstrate that high resolution samples provide class information not present in low resolution samples. Across 1000 ImageNet classes, 128 x 128 samples are more than twice as discriminable as artificially resized 32 x 32 samples. In addition, 84.7% of the classes have samples exhibiting diversity comparable to real ImageNet data."
    },
    {
        "paperId": "fafcaf5ca3fab8dc4fad15c2391c0fdb4a7dc005",
        "url": "https://www.semanticscholar.org/paper/fafcaf5ca3fab8dc4fad15c2391c0fdb4a7dc005",
        "title": "Group Equivariant Convolutional Networks",
        "venue": "International Conference on Machine Learning",
        "year": 2016,
        "citationCount": 2155,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1602.07576, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2056266",
                "name": "Taco Cohen"
            },
            {
                "authorId": "1678311",
                "name": "M. Welling"
            }
        ],
        "abstract": "We introduce Group equivariant Convolutional Neural Networks (G-CNNs), a natural generalization of convolutional neural networks that reduces sample complexity by exploiting symmetries. G-CNNs use G-convolutions, a new type of layer that enjoys a substantially higher degree of weight sharing than regular convolution layers. G-convolutions increase the expressive capacity of the network without increasing the number of parameters. Group convolution layers are easy to use and can be implemented with negligible computational overhead for discrete groups generated by translations, reflections and rotations. G-CNNs achieve state of the art results on CI- FAR10 and rotated MNIST."
    },
    {
        "paperId": "0076b232181e4e5be58dce8354a813ad2bbf663a",
        "url": "https://www.semanticscholar.org/paper/0076b232181e4e5be58dce8354a813ad2bbf663a",
        "title": "OptNet: Differentiable Optimization as a Layer in Neural Networks",
        "venue": "International Conference on Machine Learning",
        "year": 2017,
        "citationCount": 1101,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1703.00443, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1773498",
                "name": "Brandon Amos"
            },
            {
                "authorId": "145116464",
                "name": "J. Z. Kolter"
            }
        ],
        "abstract": "This paper presents OptNet, a network architecture that integrates optimization problems (here, specifically in the form of quadratic programs) as individual layers in larger end-to-end trainable deep networks. These layers encode constraints and complex dependencies between the hidden states that traditional convolutional and fully-connected layers often cannot capture. In this paper, we explore the foundations for such an architecture: we show how techniques from sensitivity analysis, bilevel optimization, and implicit differentiation can be used to exactly differentiate through these layers and with respect to layer parameters; we develop a highly efficient solver for these layers that exploits fast GPU-based batch solves within a primal-dual interior point method, and which provides backpropagation gradients with virtually no additional cost on top of the solve; and we highlight the application of these approaches in several problems. In one notable example, we show that the method is capable of learning to play mini-Sudoku (4x4) given just input and output games, with no a priori information about the rules of the game; this highlights the ability of our architecture to learn hard constraints better than other neural architectures."
    },
    {
        "paperId": "08ad8fad21f6ec4cda4d56be1ca5e146b7c913a1",
        "url": "https://www.semanticscholar.org/paper/08ad8fad21f6ec4cda4d56be1ca5e146b7c913a1",
        "title": "Understanding Black-box Predictions via Influence Functions",
        "venue": "International Conference on Machine Learning",
        "year": 2017,
        "citationCount": 3276,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1703.04730, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2572525",
                "name": "Pang Wei Koh"
            },
            {
                "authorId": "145419642",
                "name": "Percy Liang"
            }
        ],
        "abstract": "How can we explain the predictions of a black-box model? In this paper, we use influence functions  a classic technique from robust statistics  to trace a model's prediction through the learning algorithm and back to its training data, thereby identifying training points most responsible for a given prediction. To scale up influence functions to modern machine learning settings, we develop a simple, efficient implementation that requires only oracle access to gradients and Hessian-vector products. We show that even on non-convex and non-differentiable models where the theory breaks down, approximations to influence functions can still provide valuable information. On linear models and convolutional neural networks, we demonstrate that influence functions are useful for multiple purposes: understanding model behavior, debugging models, detecting dataset errors, and even creating visually-indistinguishable training-set attacks."
    },
    {
        "paperId": "1a2118bed729579528deb51e745d58dd3629baf6",
        "url": "https://www.semanticscholar.org/paper/1a2118bed729579528deb51e745d58dd3629baf6",
        "title": "Learning Important Features Through Propagating Activation Differences",
        "venue": "International Conference on Machine Learning",
        "year": 2017,
        "citationCount": 4308,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1704.02685, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3407268",
                "name": "Avanti Shrikumar"
            },
            {
                "authorId": "3407175",
                "name": "Peyton Greenside"
            },
            {
                "authorId": "2844479",
                "name": "Anshul B Kundaje"
            }
        ],
        "abstract": "The purported \"black box\" nature of neural networks is a barrier to adoption in applications where interpretability is essential. Here we present DeepLIFT (Deep Learning Important FeaTures), a method for decomposing the output prediction of a neural network on a specific input by backpropagating the contributions of all neurons in the network to every feature of the input. DeepLIFT compares the activation of each neuron to its 'reference activation' and assigns contribution scores according to the difference. By optionally giving separate consideration to positive and negative contributions, DeepLIFT can also reveal dependencies which are missed by other approaches. Scores can be computed efficiently in a single backward pass. We apply DeepLIFT to models trained on MNIST and simulated genomic data, and show significant advantages over gradient-based methods. Video tutorial: http://goo.gl/qKb7pL, code: http://goo.gl/RM8jvH."
    },
    {
        "paperId": "2a215755d7548ffc82079ce734c4ac60b62f6f56",
        "url": "https://www.semanticscholar.org/paper/2a215755d7548ffc82079ce734c4ac60b62f6f56",
        "title": "Toward Controlled Generation of Text",
        "venue": "International Conference on Machine Learning",
        "year": 2017,
        "citationCount": 1013,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1703.00955, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2749311",
                "name": "Zhiting Hu"
            },
            {
                "authorId": "8387085",
                "name": "Zichao Yang"
            },
            {
                "authorId": "40250403",
                "name": "Xiaodan Liang"
            },
            {
                "authorId": "145124475",
                "name": "R. Salakhutdinov"
            },
            {
                "authorId": "143977260",
                "name": "E. Xing"
            }
        ],
        "abstract": "Generic generation and manipulation of text is challenging and has limited success compared to recent deep generative modeling in visual domain. This paper aims at generating plausible natural language sentences, whose attributes are dynamically controlled by learning disentangled latent representations with designated semantics. We propose a new neural generative model which combines variational auto-encoders and holistic attribute discriminators for effective imposition of semantic structures. With differentiable approximation to discrete text samples, explicit constraints on independent attribute controls, and efficient collaborative learning of generator and discriminators, our model learns highly interpretable representations from even only word annotations, and produces realistic sentences with desired attributes. Quantitative evaluation validates the accuracy of sentence and attribute generation."
    },
    {
        "paperId": "306a2e8ca31fdcc148618d37074785c290f96375",
        "url": "https://www.semanticscholar.org/paper/306a2e8ca31fdcc148618d37074785c290f96375",
        "title": "MentorNet: Learning Data-Driven Curriculum for Very Deep Neural Networks on Corrupted Labels",
        "venue": "International Conference on Machine Learning",
        "year": 2017,
        "citationCount": 1577,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1712.05055, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "39978626",
                "name": "Lu Jiang"
            },
            {
                "authorId": "47230320",
                "name": "Zhengyuan Zhou"
            },
            {
                "authorId": "152456068",
                "name": "Thomas Leung"
            },
            {
                "authorId": "2040091191",
                "name": "Li-Jia Li"
            },
            {
                "authorId": "48004138",
                "name": "Li Fei-Fei"
            }
        ],
        "abstract": "Recent deep networks are capable of memorizing the entire data even when the labels are completely random. To overcome the overfitting on corrupted labels, we propose a novel technique of learning another neural network, called MentorNet, to supervise the training of the base deep networks, namely, StudentNet. During training, MentorNet provides a curriculum (sample weighting scheme) for StudentNet to focus on the sample the label of which is probably correct. Unlike the existing curriculum that is usually predefined by human experts, MentorNet learns a data-driven curriculum dynamically with StudentNet. Experimental results demonstrate that our approach can significantly improve the generalization performance of deep networks trained on corrupted training data. Notably, to the best of our knowledge, we achieve the best-published result on WebVision, a large benchmark containing 2.2 million images of real-world noisy labels. The code are at this https URL"
    },
    {
        "paperId": "43428880d75b3a14257c3ee9bda054e61eb869c0",
        "url": "https://www.semanticscholar.org/paper/43428880d75b3a14257c3ee9bda054e61eb869c0",
        "title": "Convolutional Sequence to Sequence Learning",
        "venue": "International Conference on Machine Learning",
        "year": 2017,
        "citationCount": 3455,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1705.03122, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2401865",
                "name": "Jonas Gehring"
            },
            {
                "authorId": "2325985",
                "name": "Michael Auli"
            },
            {
                "authorId": "2529182",
                "name": "David Grangier"
            },
            {
                "authorId": "13759615",
                "name": "Denis Yarats"
            },
            {
                "authorId": "2921469",
                "name": "Yann Dauphin"
            }
        ],
        "abstract": "The prevalent approach to sequence to sequence learning maps an input sequence to a variable length output sequence via recurrent neural networks. We introduce an architecture based entirely on convolutional neural networks. Compared to recurrent models, computations over all elements can be fully parallelized during training and optimization is easier since the number of non-linearities is fixed and independent of the input length. Our use of gated linear units eases gradient propagation and we equip each decoder layer with a separate attention module. We outperform the accuracy of the deep LSTM setup of Wu et al. (2016) on both WMT'14 English-German and WMT'14 English-French translation at an order of magnitude faster speed, both on GPU and CPU."
    },
    {
        "paperId": "470d11b8ca4586c930adbbfc3f60bff08f2a0161",
        "url": "https://www.semanticscholar.org/paper/470d11b8ca4586c930adbbfc3f60bff08f2a0161",
        "title": "Meta Networks",
        "venue": "International Conference on Machine Learning",
        "year": 2017,
        "citationCount": 1134,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1703.00837, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2227827",
                "name": "Tsendsuren Munkhdalai"
            },
            {
                "authorId": "2119120474",
                "name": "Hong Yu"
            }
        ],
        "abstract": "Neural networks have been successfully applied in applications with a large amount of labeled data. However, the task of rapid generalization on new concepts with small training data while preserving performances on previously learned ones still presents a significant challenge to neural network models. In this work, we introduce a novel meta learning method, Meta Networks (MetaNet), that learns a meta-level knowledge across tasks and shifts its inductive biases via fast parameterization for rapid generalization. When evaluated on Omniglot and Mini-ImageNet benchmarks, our MetaNet models achieve a near human-level performance and outperform the baseline approaches by up to 6% accuracy. We demonstrate several appealing properties of MetaNet relating to generalization and continual learning."
    },
    {
        "paperId": "4b23012689e0f17912fb38d4984775e567cff8d6",
        "url": "https://www.semanticscholar.org/paper/4b23012689e0f17912fb38d4984775e567cff8d6",
        "title": "Provable defenses against adversarial examples via the convex outer adversarial polytope",
        "venue": "International Conference on Machine Learning",
        "year": 2017,
        "citationCount": 1561,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1711.00851, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "145116464",
                "name": "J. Z. Kolter"
            },
            {
                "authorId": "51026953",
                "name": "Eric Wong"
            }
        ],
        "abstract": "We propose a method to learn deep ReLU-based classifiers that are provably robust against norm-bounded adversarial perturbations (on the training data; for previously unseen examples, the approach will be guaranteed to detect all adversarial examples, though it may flag some non-adversarial examples as well). The basic idea of the approach is to consider a convex outer approximation of the set of activations reachable through a norm-bounded perturbation, and we develop a robust optimization procedure that minimizes the worst case loss over this outer region (via a linear program). Crucially, we show that the dual problem to this linear program can be represented itself as a deep network similar to the backpropagation network, leading to very efficient optimization approaches that produce guaranteed bounds on the robust loss. The end result is that by executing a few more forward and backward passes through a slightly modified version of the original network (though possibly with much larger batch sizes), we can learn a classifier that is provably robust to any norm-bounded adversarial attack. We illustrate the approach on a toy 2D robust classification task, and on a simple convolutional architecture applied to MNIST, where we produce a classifier that provably has less than 8.4% test error for any adversarial attack with bounded $\\ell_\\infty$ norm less than $\\epsilon = 0.1$. This represents the largest verified network that we are aware of, and we discuss future challenges in scaling the approach to much larger domains."
    },
    {
        "paperId": "582157494d72dd28dc28172f04dfd5f25fb17bff",
        "url": "https://www.semanticscholar.org/paper/582157494d72dd28dc28172f04dfd5f25fb17bff",
        "title": "GradNorm: Gradient Normalization for Adaptive Loss Balancing in Deep Multitask Networks",
        "venue": "International Conference on Machine Learning",
        "year": 2017,
        "citationCount": 1572,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1711.02257, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2111606331",
                "name": "Zhao Chen"
            },
            {
                "authorId": "2442177",
                "name": "Vijay Badrinarayanan"
            },
            {
                "authorId": "50521003",
                "name": "Chen-Yu Lee"
            },
            {
                "authorId": "39863668",
                "name": "Andrew Rabinovich"
            }
        ],
        "abstract": "Deep multitask networks, in which one neural network produces multiple predictive outputs, can offer better speed and performance than their single-task counterparts but are challenging to train properly. We present a gradient normalization (GradNorm) algorithm that automatically balances training in deep multitask models by dynamically tuning gradient magnitudes. We show that for various network architectures, for both regression and classification tasks, and on both synthetic and real datasets, GradNorm improves accuracy and reduces overfitting across multiple tasks when compared to single-task networks, static baselines, and other adaptive multitask loss balancing techniques. GradNorm also matches or surpasses the performance of exhaustive grid search methods, despite only involving a single asymmetry hyperparameter $\\alpha$. Thus, what was once a tedious search process that incurred exponentially more compute for each task added can now be accomplished within a few training runs, irrespective of the number of tasks. Ultimately, we will demonstrate that gradient manipulation affords us great control over the training dynamics of multitask networks and may be one of the keys to unlocking the potential of multitask learning."
    },
    {
        "paperId": "5ddd38a5df945e4afee68d96ed51fd6ca1f7d4cf",
        "url": "https://www.semanticscholar.org/paper/5ddd38a5df945e4afee68d96ed51fd6ca1f7d4cf",
        "title": "A Closer Look at Memorization in Deep Networks",
        "venue": "International Conference on Machine Learning",
        "year": 2017,
        "citationCount": 2032,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1706.05394, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2309967",
                "name": "Devansh Arpit"
            },
            {
                "authorId": "40569328",
                "name": "Stanislaw Jastrzebski"
            },
            {
                "authorId": "2482072",
                "name": "Nicolas Ballas"
            },
            {
                "authorId": "145055042",
                "name": "David Krueger"
            },
            {
                "authorId": "2286872375",
                "name": "Emmanuel Bengio"
            },
            {
                "authorId": "19308176",
                "name": "Maxinder S. Kanwal"
            },
            {
                "authorId": "3422058",
                "name": "Tegan Maharaj"
            },
            {
                "authorId": "35988982",
                "name": "Asja Fischer"
            },
            {
                "authorId": "1760871",
                "name": "Aaron C. Courville"
            },
            {
                "authorId": "1751762",
                "name": "Yoshua Bengio"
            },
            {
                "authorId": "1388317459",
                "name": "Simon Lacoste-Julien"
            }
        ],
        "abstract": "We examine the role of memorization in deep learning, drawing connections to capacity, generalization, and adversarial robustness. While deep networks are capable of memorizing noise data, our results suggest that they tend to prioritize learning simple patterns first. In our experiments, we expose qualitative differences in gradient-based optimization of deep neural networks (DNNs) on noise vs. real data. We also demonstrate that for appropriately tuned explicit regularization (e.g., dropout) we can degrade DNN training performance on noise datasets without compromising generalization on real data. Our analysis suggests that the notions of effective capacity which are dataset independent are unlikely to explain the generalization performance of deep networks when trained with gradient based methods because training data itself plays an important role in determining the degree of memorization."
    },
    {
        "paperId": "682b9d2212258fd5edbfca589c86390c31a956b0",
        "url": "https://www.semanticscholar.org/paper/682b9d2212258fd5edbfca589c86390c31a956b0",
        "title": "Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV)",
        "venue": "International Conference on Machine Learning",
        "year": 2017,
        "citationCount": 2094,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1711.11279, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3351164",
                "name": "Been Kim"
            },
            {
                "authorId": "145233583",
                "name": "M. Wattenberg"
            },
            {
                "authorId": "2058362",
                "name": "J. Gilmer"
            },
            {
                "authorId": "145941081",
                "name": "Carrie J. Cai"
            },
            {
                "authorId": "49556437",
                "name": "James Wexler"
            },
            {
                "authorId": "1765169",
                "name": "F. Vigas"
            },
            {
                "authorId": "144042306",
                "name": "R. Sayres"
            }
        ],
        "abstract": "The interpretation of deep learning models is a challenge due to their size, complexity, and often opaque internal state. In addition, many systems, such as image classifiers, operate on low-level features rather than high-level concepts. To address these challenges, we introduce Concept Activation Vectors (CAVs), which provide an interpretation of a neural net's internal state in terms of human-friendly concepts. The key idea is to view the high-dimensional internal state of a neural net as an aid, not an obstacle. We show how to use CAVs as part of a technique, Testing with CAVs (TCAV), that uses directional derivatives to quantify the degree to which a user-defined concept is important to a classification result--for example, how sensitive a prediction of \"zebra\" is to the presence of stripes. Using the domain of image classification as a testing ground, we describe how CAVs may be used to explore hypotheses and generate insights for a standard image classification network as well as a medical application."
    },
    {
        "paperId": "7778b2b6ee67df2a0a06fe82e79acfdf714c7399",
        "url": "https://www.semanticscholar.org/paper/7778b2b6ee67df2a0a06fe82e79acfdf714c7399",
        "title": "Learning to Discover Cross-Domain Relations with Generative Adversarial Networks",
        "venue": "International Conference on Machine Learning",
        "year": 2017,
        "citationCount": 2060,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1703.05192, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2509132",
                "name": "Taeksoo Kim"
            },
            {
                "authorId": "9959922",
                "name": "Moonsu Cha"
            },
            {
                "authorId": "2118020246",
                "name": "Hyunsoo Kim"
            },
            {
                "authorId": "2119170990",
                "name": "Jung Kwon Lee"
            },
            {
                "authorId": "3968500",
                "name": "Jiwon Kim"
            }
        ],
        "abstract": "While humans easily recognize relations between data from different domains without any supervision, learning to automatically discover them is in general very challenging and needs many ground-truth pairs that illustrate the relations. To avoid costly pairing, we address the task of discovering cross-domain relations when given unpaired data. We propose a method based on generative adversarial networks that learns to discover relations between different domains (DiscoGAN). Using the discovered relations, our proposed network successfully transfers style from one domain to another while preserving key attributes such as orientation and face identity."
    },
    {
        "paperId": "7a4193d0b042643a8bb9ec262ed7f9d509bdb12e",
        "url": "https://www.semanticscholar.org/paper/7a4193d0b042643a8bb9ec262ed7f9d509bdb12e",
        "title": "Constrained Policy Optimization",
        "venue": "International Conference on Machine Learning",
        "year": 2017,
        "citationCount": 1545,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1705.10528, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3381809",
                "name": "Joshua Achiam"
            },
            {
                "authorId": "145641013",
                "name": "David Held"
            },
            {
                "authorId": "3025260",
                "name": "Aviv Tamar"
            },
            {
                "authorId": "1689992",
                "name": "P. Abbeel"
            }
        ],
        "abstract": "For many applications of reinforcement learning it can be more convenient to specify both a reward function and constraints, rather than trying to design behavior through the reward function. For example, systems that physically interact with or around humans should satisfy safety constraints. Recent advances in policy search algorithms (Mnih et al., 2016, Schulman et al., 2015, Lillicrap et al., 2016, Levine et al., 2016) have enabled new capabilities in high-dimensional control, but do not consider the constrained setting. \nWe propose Constrained Policy Optimization (CPO), the first general-purpose policy search algorithm for constrained reinforcement learning with guarantees for near-constraint satisfaction at each iteration. Our method allows us to train neural network policies for high-dimensional control while making guarantees about policy behavior all throughout training. Our guarantees are based on a new theoretical result, which is of independent interest: we prove a bound relating the expected returns of two policies to an average divergence between them. We demonstrate the effectiveness of our approach on simulated robot locomotion tasks where the agent must satisfy constraints motivated by safety."
    },
    {
        "paperId": "8dce99e33c6fceb3e79023f5894fdbe733c91e92",
        "url": "https://www.semanticscholar.org/paper/8dce99e33c6fceb3e79023f5894fdbe733c91e92",
        "title": "Synthesizing Robust Adversarial Examples",
        "venue": "International Conference on Machine Learning",
        "year": 2017,
        "citationCount": 1750,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "38939786",
                "name": "Anish Athalye"
            },
            {
                "authorId": "39468283",
                "name": "Logan Engstrom"
            },
            {
                "authorId": "34562927",
                "name": "Andrew Ilyas"
            },
            {
                "authorId": "2058062760",
                "name": "K. Kwok"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "907a90967f68da4311802247408e0515e363f930",
        "url": "https://www.semanticscholar.org/paper/907a90967f68da4311802247408e0515e363f930",
        "title": "CyCADA: Cycle-Consistent Adversarial Domain Adaptation",
        "venue": "International Conference on Machine Learning",
        "year": 2017,
        "citationCount": 3180,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1711.03213, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "50196944",
                "name": "Judy Hoffman"
            },
            {
                "authorId": "2368132",
                "name": "Eric Tzeng"
            },
            {
                "authorId": "2071929129",
                "name": "Taesung Park"
            },
            {
                "authorId": "2436356",
                "name": "Jun-Yan Zhu"
            },
            {
                "authorId": "2094770",
                "name": "Phillip Isola"
            },
            {
                "authorId": "2903226",
                "name": "Kate Saenko"
            },
            {
                "authorId": "1763086",
                "name": "Alexei A. Efros"
            },
            {
                "authorId": "1753210",
                "name": "Trevor Darrell"
            }
        ],
        "abstract": "Domain adaptation is critical for success in new, unseen environments. Adversarial adaptation models applied in feature spaces discover domain invariant representations, but are difficult to visualize and sometimes fail to capture pixel-level and low-level domain shifts. Recent work has shown that generative adversarial networks combined with cycle-consistency constraints are surprisingly effective at mapping images between domains, even without the use of aligned image pairs. We propose a novel discriminatively-trained Cycle-Consistent Adversarial Domain Adaptation model. CyCADA adapts representations at both the pixel-level and feature-level, enforces cycle-consistency while leveraging a task loss, and does not require aligned pairs. Our model can be applied in a variety of visual recognition and prediction settings. We show new state-of-the-art results across multiple adaptation tasks, including digit classification and semantic segmentation of road scenes demonstrating transfer from synthetic to real world domains."
    },
    {
        "paperId": "9172cd6c253edf7c3a1568e03577db20648ad0c4",
        "url": "https://www.semanticscholar.org/paper/9172cd6c253edf7c3a1568e03577db20648ad0c4",
        "title": "Reinforcement Learning with Deep Energy-Based Policies",
        "venue": "International Conference on Machine Learning",
        "year": 2017,
        "citationCount": 1484,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1702.08165, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2587648",
                "name": "Tuomas Haarnoja"
            },
            {
                "authorId": "4990833",
                "name": "Haoran Tang"
            },
            {
                "authorId": "1689992",
                "name": "P. Abbeel"
            },
            {
                "authorId": "1736651",
                "name": "S. Levine"
            }
        ],
        "abstract": "We propose a method for learning expressive energy-based policies for continuous states and actions, which has been feasible only in tabular domains before. We apply our method to learning maximum entropy policies, resulting into a new algorithm, called soft Q-learning, that expresses the optimal policy via a Boltzmann distribution. We use the recently proposed amortized Stein variational gradient descent to learn a stochastic sampling network that approximates samples from this distribution. The benefits of the proposed algorithm include improved exploration and compositionality that allows transferring skills between tasks, which we confirm in simulated experiments with swimming and walking robots. We also draw a connection to actor-critic methods, which can be viewed performing approximate inference on the corresponding energy-based model."
    },
    {
        "paperId": "a99d857ecc78316a0d9a774972b775058d5644ca",
        "url": "https://www.semanticscholar.org/paper/a99d857ecc78316a0d9a774972b775058d5644ca",
        "title": "Continual Learning Through Synaptic Intelligence",
        "venue": "International Conference on Machine Learning",
        "year": 2017,
        "citationCount": 2994,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "39979946",
                "name": "Friedemann Zenke"
            },
            {
                "authorId": "16443937",
                "name": "Ben Poole"
            },
            {
                "authorId": "25769960",
                "name": "S. Ganguli"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "acd87843a451d18b4dc6474ddce1ae946429eaf1",
        "url": "https://www.semanticscholar.org/paper/acd87843a451d18b4dc6474ddce1ae946429eaf1",
        "title": "Wasserstein Generative Adversarial Networks",
        "venue": "International Conference on Machine Learning",
        "year": 2017,
        "citationCount": 8981,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "2877311",
                "name": "Martn Arjovsky"
            },
            {
                "authorId": "2127604",
                "name": "Soumith Chintala"
            },
            {
                "authorId": "52184096",
                "name": "L. Bottou"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "b3159fd22e19e24d7cde9d37b3e482b832d4fa58",
        "url": "https://www.semanticscholar.org/paper/b3159fd22e19e24d7cde9d37b3e482b832d4fa58",
        "title": "Learning Representations and Generative Models for 3D Point Clouds",
        "venue": "International Conference on Machine Learning",
        "year": 2017,
        "citationCount": 1579,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "22199114",
                "name": "Panos Achlioptas"
            },
            {
                "authorId": "1868022",
                "name": "Olga Diamanti"
            },
            {
                "authorId": "3168518",
                "name": "Ioannis Mitliagkas"
            },
            {
                "authorId": "51352814",
                "name": "L. Guibas"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "c1f4ef741242d629d1f56e442a09a7ba29595a0e",
        "url": "https://www.semanticscholar.org/paper/c1f4ef741242d629d1f56e442a09a7ba29595a0e",
        "title": "A Distributional Perspective on Reinforcement Learning",
        "venue": "International Conference on Machine Learning",
        "year": 2017,
        "citationCount": 1694,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1707.06887, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1792298",
                "name": "Marc G. Bellemare"
            },
            {
                "authorId": "2605877",
                "name": "Will Dabney"
            },
            {
                "authorId": "1708654",
                "name": "R. Munos"
            }
        ],
        "abstract": "In this paper we argue for the fundamental importance of the value distribution: the distribution of the random return received by a reinforcement learning agent. This is in contrast to the common approach to reinforcement learning which models the expectation of this return, or value. Although there is an established body of literature studying the value distribution, thus far it has always been used for a specific purpose such as implementing risk-aware behaviour. We begin with theoretical results in both the policy evaluation and control settings, exposing a significant distributional instability in the latter. We then use the distributional perspective to design a new algorithm which applies Bellman's equation to the learning of approximate value distributions. We evaluate our algorithm using the suite of games from the Arcade Learning Environment. We obtain both state-of-the-art results and anecdotal evidence demonstrating the importance of the value distribution in approximate reinforcement learning. Finally, we combine theoretical and empirical evidence to highlight the ways in which the value distribution impacts learning in the approximate setting."
    },
    {
        "paperId": "c889d6f98e6d79b89c3a6adf8a921f88fa6ba518",
        "url": "https://www.semanticscholar.org/paper/c889d6f98e6d79b89c3a6adf8a921f88fa6ba518",
        "title": "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks",
        "venue": "International Conference on Machine Learning",
        "year": 2017,
        "citationCount": 13402,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1703.03400, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "46881670",
                "name": "Chelsea Finn"
            },
            {
                "authorId": "1689992",
                "name": "P. Abbeel"
            },
            {
                "authorId": "1736651",
                "name": "S. Levine"
            }
        ],
        "abstract": "We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies."
    },
    {
        "paperId": "d65ce2b8300541414bfe51d03906fca72e93523c",
        "url": "https://www.semanticscholar.org/paper/d65ce2b8300541414bfe51d03906fca72e93523c",
        "title": "On Calibration of Modern Neural Networks",
        "venue": "International Conference on Machine Learning",
        "year": 2017,
        "citationCount": 6845,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1706.04599, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "144993411",
                "name": "Chuan Guo"
            },
            {
                "authorId": "10804137",
                "name": "Geoff Pleiss"
            },
            {
                "authorId": "2117103358",
                "name": "Yu Sun"
            },
            {
                "authorId": "7446832",
                "name": "Kilian Q. Weinberger"
            }
        ],
        "abstract": "Confidence calibration -- the problem of predicting probability estimates representative of the true correctness likelihood -- is important for classification models in many applications. We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors influencing calibration. We evaluate the performance of various post-processing calibration methods on state-of-the-art architectures with image and document classification datasets. Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling -- a single-parameter variant of Platt Scaling -- is surprisingly effective at calibrating predictions."
    },
    {
        "paperId": "da5c65b0ac8b525c3d3d4889bf44d8a48d254a07",
        "url": "https://www.semanticscholar.org/paper/da5c65b0ac8b525c3d3d4889bf44d8a48d254a07",
        "title": "Deep Bayesian Active Learning with Image Data",
        "venue": "International Conference on Machine Learning",
        "year": 2017,
        "citationCount": 1875,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1703.02910, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2681954",
                "name": "Y. Gal"
            },
            {
                "authorId": "18014232",
                "name": "Riashat Islam"
            },
            {
                "authorId": "1744700",
                "name": "Zoubin Ghahramani"
            }
        ],
        "abstract": "Even though active learning forms an important pillar of machine learning, deep learning tools are not prevalent within it. Deep learning poses several difficulties when used in an active learning setting. First, active learning (AL) methods generally rely on being able to learn and update models from small amounts of data. Recent advances in deep learning, on the other hand, are notorious for their dependence on large amounts of data. Second, many AL acquisition functions rely on model uncertainty, yet deep learning methods rarely represent such model uncertainty. In this paper we combine recent advances in Bayesian deep learning into the active learning framework in a practical way. We develop an active learning framework for high dimensional data, a task which has been extremely challenging so far, with very sparse existing literature. Taking advantage of specialised models such as Bayesian convolutional neural networks, we demonstrate our active learning techniques with image data, obtaining a significant improvement on existing active learning approaches. We demonstrate this on both the MNIST dataset, as well as for skin cancer diagnosis from lesion images (ISIC2016 task)."
    },
    {
        "paperId": "e24cdf73b3e7e590c2fe5ecac9ae8aa983801367",
        "url": "https://www.semanticscholar.org/paper/e24cdf73b3e7e590c2fe5ecac9ae8aa983801367",
        "title": "Neural Message Passing for Quantum Chemistry",
        "venue": "International Conference on Machine Learning",
        "year": 2017,
        "citationCount": 8381,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1704.01212, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2058362",
                "name": "J. Gilmer"
            },
            {
                "authorId": "2601641",
                "name": "S. Schoenholz"
            },
            {
                "authorId": "119508204",
                "name": "Patrick F. Riley"
            },
            {
                "authorId": "1689108",
                "name": "O. Vinyals"
            },
            {
                "authorId": "35188630",
                "name": "George E. Dahl"
            }
        ],
        "abstract": "Supervised learning on molecules has incredible potential to be useful in chemistry, drug discovery, and materials science. Luckily, several promising and closely related neural network models invariant to molecular symmetries have already been described in the literature. These models learn a message passing algorithm and aggregation procedure to compute a function of their entire input graph. At this point, the next step is to find a particularly effective variant of this general approach and apply it to chemical prediction benchmarks until we either solve them or reach the limits of the approach. In this paper, we reformulate existing models into a single common framework we call Message Passing Neural Networks (MPNNs) and explore additional novel variations within this framework. Using MPNNs we demonstrate state of the art results on an important molecular property prediction benchmark; these results are strong enough that we believe future work should focus on datasets with larger molecules or more accurate ground truth labels."
    },
    {
        "paperId": "f108b65fe0003e387e1cd7e50f537af0531818e4",
        "url": "https://www.semanticscholar.org/paper/f108b65fe0003e387e1cd7e50f537af0531818e4",
        "title": "Large-Scale Evolution of Image Classifiers",
        "venue": "International Conference on Machine Learning",
        "year": 2017,
        "citationCount": 1716,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1703.01041, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2892780",
                "name": "Esteban Real"
            },
            {
                "authorId": "144375552",
                "name": "Sherry Moore"
            },
            {
                "authorId": "1714992",
                "name": "Andrew Selle"
            },
            {
                "authorId": "2054003577",
                "name": "Saurabh Saxena"
            },
            {
                "authorId": "46901409",
                "name": "Y. Suematsu"
            },
            {
                "authorId": "1739176520",
                "name": "Jie Tan"
            },
            {
                "authorId": "2827616",
                "name": "Quoc V. Le"
            },
            {
                "authorId": "145714153",
                "name": "Alexey Kurakin"
            }
        ],
        "abstract": "Neural networks have proven effective at solving difficult problems but designing their architectures can be challenging, even for image classification problems alone. Our goal is to minimize human participation, so we employ evolutionary algorithms to discover such networks automatically. Despite significant computational requirements, we show that it is now possible to evolve models with accuracies within the range of those published in the last year. Specifically, we employ simple evolutionary techniques at unprecedented scales to discover models for the CIFAR-10 and CIFAR-100 datasets, starting from trivial initial conditions and reaching accuracies of 94.6% (95.6% for ensemble) and 77.0%, respectively. To do this, we use novel and intuitive mutation operators that navigate large search spaces; we stress that no human participation is required once evolution starts and that the output is a fully-trained model. Throughout this work, we place special emphasis on the repeatability of results, the variability in the outcomes and the computational requirements."
    },
    {
        "paperId": "f302e136c41db5de1d624412f68c9174cf7ae8be",
        "url": "https://www.semanticscholar.org/paper/f302e136c41db5de1d624412f68c9174cf7ae8be",
        "title": "Axiomatic Attribution for Deep Networks",
        "venue": "International Conference on Machine Learning",
        "year": 2017,
        "citationCount": 7062,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1703.01365, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "30740726",
                "name": "Mukund Sundararajan"
            },
            {
                "authorId": "40511120",
                "name": "Ankur Taly"
            },
            {
                "authorId": "34789908",
                "name": "Qiqi Yan"
            }
        ],
        "abstract": "We study the problem of attributing the prediction of a deep network to its input features, a problem previously studied by several other works. We identify two fundamental axioms Sensitivity and Implementation Invariance that attribution methods ought to satisfy. We show that they are not satisfied by most known attribution methods, which we consider to be a fundamental weakness of those methods. We use the axioms to guide the design of a new attribution method called Integrated Gradients. Our method requires no modification to the original network and is extremely simple to implement; it just needs a few calls to the standard gradient operator. We apply this method to a couple of image models, a couple of text models and a chemistry model, demonstrating its ability to debug networks, to extract rules from a network, and to enable users to engage with models better."
    },
    {
        "paperId": "02ccfc9b550d381b5df4365a2ae48bb5f7f7578e",
        "url": "https://www.semanticscholar.org/paper/02ccfc9b550d381b5df4365a2ae48bb5f7f7578e",
        "title": "Noise2Noise: Learning Image Restoration without Clean Data",
        "venue": "International Conference on Machine Learning",
        "year": 2018,
        "citationCount": 1839,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1803.04189, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "49244945",
                "name": "J. Lehtinen"
            },
            {
                "authorId": "1762967",
                "name": "Jacob Munkberg"
            },
            {
                "authorId": "2266452",
                "name": "J. Hasselgren"
            },
            {
                "authorId": "36436218",
                "name": "S. Laine"
            },
            {
                "authorId": "2976930",
                "name": "Tero Karras"
            },
            {
                "authorId": "1907688",
                "name": "M. Aittala"
            },
            {
                "authorId": "1761103",
                "name": "Timo Aila"
            }
        ],
        "abstract": "We apply basic statistical reasoning to signal reconstruction by machine learning -- learning to map corrupted observations to clean signals -- with a simple and powerful conclusion: under certain common circumstances, it is possible to learn to restore signals without ever observing clean ones, at performance close or equal to training using clean exemplars. We show applications in photographic noise removal, denoising of synthetic Monte Carlo images, and reconstruction of MRI scans from undersampled inputs, all based on only observing corrupted data."
    },
    {
        "paperId": "03e7e8663c69e691be6b6403b1eb1bbf593d31f2",
        "url": "https://www.semanticscholar.org/paper/03e7e8663c69e691be6b6403b1eb1bbf593d31f2",
        "title": "Gradient Descent Finds Global Minima of Deep Neural Networks",
        "venue": "International Conference on Machine Learning",
        "year": 2018,
        "citationCount": 1188,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1811.03804, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "145697585",
                "name": "S. Du"
            },
            {
                "authorId": "2421201",
                "name": "J. Lee"
            },
            {
                "authorId": "2382821970",
                "name": "Haochuan Li"
            },
            {
                "authorId": "24952249",
                "name": "Liwei Wang"
            },
            {
                "authorId": "22226408",
                "name": "Xiyu Zhai"
            }
        ],
        "abstract": "Gradient descent finds a global minimum in training deep neural networks despite the objective function being non-convex. The current paper proves gradient descent achieves zero training loss in polynomial time for a deep over-parameterized neural network with residual connections (ResNet). Our analysis relies on the particular structure of the Gram matrix induced by the neural network architecture. This structure allows us to show the Gram matrix is stable throughout the training process and this stability implies the global optimality of the gradient descent algorithm. We further extend our analysis to deep residual convolutional neural networks and obtain a similar convergence result."
    },
    {
        "paperId": "04541599accc47d8174f63345ce9c987ef21685b",
        "url": "https://www.semanticscholar.org/paper/04541599accc47d8174f63345ce9c987ef21685b",
        "title": "Disentangling by Factorising",
        "venue": "International Conference on Machine Learning",
        "year": 2018,
        "citationCount": 1450,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1802.05983, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3407176",
                "name": "Hyunjik Kim"
            },
            {
                "authorId": "1714004",
                "name": "A. Mnih"
            }
        ],
        "abstract": "We define and address the problem of unsupervised learning of disentangled representations on data generated from independent factors of variation. We propose FactorVAE, a method that disentangles by encouraging the distribution of representations to be factorial and hence independent across the dimensions. We show that it improves upon $\\beta$-VAE by providing a better trade-off between disentanglement and reconstruction quality. Moreover, we highlight the problems of a commonly used disentanglement metric and introduce a new metric that does not suffer from them."
    },
    {
        "paperId": "1b59eea8ec4684381a885b59acd09c9151a49487",
        "url": "https://www.semanticscholar.org/paper/1b59eea8ec4684381a885b59acd09c9151a49487",
        "title": "Manifold Mixup: Better Representations by Interpolating Hidden States",
        "venue": "International Conference on Machine Learning",
        "year": 2018,
        "citationCount": 1472,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "50346446",
                "name": "Vikas Verma"
            },
            {
                "authorId": "49071560",
                "name": "Alex Lamb"
            },
            {
                "authorId": "12757989",
                "name": "Christopher Beckham"
            },
            {
                "authorId": "2249759218",
                "name": "Amir Najafi"
            },
            {
                "authorId": "3168518",
                "name": "Ioannis Mitliagkas"
            },
            {
                "authorId": "1401804750",
                "name": "David Lopez-Paz"
            },
            {
                "authorId": "1751762",
                "name": "Yoshua Bengio"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "1db9bd18681b96473f3c82b21edc9240b44dc329",
        "url": "https://www.semanticscholar.org/paper/1db9bd18681b96473f3c82b21edc9240b44dc329",
        "title": "Image Transformer",
        "venue": "International Conference on Machine Learning",
        "year": 2018,
        "citationCount": 1825,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1802.05751, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3877127",
                "name": "Niki Parmar"
            },
            {
                "authorId": "40348417",
                "name": "Ashish Vaswani"
            },
            {
                "authorId": "39328010",
                "name": "Jakob Uszkoreit"
            },
            {
                "authorId": "40527594",
                "name": "Lukasz Kaiser"
            },
            {
                "authorId": "1846258",
                "name": "Noam Shazeer"
            },
            {
                "authorId": "31702389",
                "name": "Alexander Ku"
            },
            {
                "authorId": "47497262",
                "name": "Dustin Tran"
            }
        ],
        "abstract": "Image generation has been successfully cast as an autoregressive sequence generation or transformation problem. Recent work has shown that self-attention is an effective way of modeling textual sequences. In this work, we generalize a recently proposed model architecture based on self-attention, the Transformer, to a sequence modeling formulation of image generation with a tractable likelihood. By restricting the self-attention mechanism to attend to local neighborhoods we significantly increase the size of images the model can process in practice, despite maintaining significantly larger receptive fields per layer than typical convolutional neural networks. We propose another extension of self-attention allowing it to efficiently take advantage of the two-dimensional nature of images. While conceptually simple, our generative models trained on two image data sets are competitive with or significantly outperform the current state of the art in autoregressive image generation on two different data sets, CIFAR-10 and ImageNet. We also present results on image super-resolution with a large magnification ratio, applying an encoder-decoder configuration of our architecture. In a human evaluation study, we show that our super-resolution models improve significantly over previously published autoregressive super-resolution models. Images they generate fool human observers three times more often than the previous state of the art."
    },
    {
        "paperId": "2444be7584d1f5a7e2aa9f65078de09154f14ea1",
        "url": "https://www.semanticscholar.org/paper/2444be7584d1f5a7e2aa9f65078de09154f14ea1",
        "title": "Born Again Neural Networks",
        "venue": "International Conference on Machine Learning",
        "year": 2018,
        "citationCount": 1128,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1805.04770, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2067208583",
                "name": "Tommaso Furlanello"
            },
            {
                "authorId": "32219137",
                "name": "Zachary Chase Lipton"
            },
            {
                "authorId": "143902495",
                "name": "Michael Tschannen"
            },
            {
                "authorId": "7326223",
                "name": "L. Itti"
            },
            {
                "authorId": "2047844",
                "name": "Anima Anandkumar"
            }
        ],
        "abstract": "Knowledge distillation (KD) consists of transferring knowledge from one machine learning model (the teacher}) to another (the student). Commonly, the teacher is a high-capacity model with formidable performance, while the student is more compact. By transferring knowledge, one hopes to benefit from the student's compactness. %we desire a compact model with performance close to the teacher's. We study KD from a new perspective: rather than compressing models, we train students parameterized identically to their teachers. Surprisingly, these {Born-Again Networks (BANs), outperform their teachers significantly, both on computer vision and language modeling tasks. Our experiments with BANs based on DenseNets demonstrate state-of-the-art performance on the CIFAR-10 (3.5%) and CIFAR-100 (15.5%) datasets, by validation error. Additional experiments explore two distillation objectives: (i) Confidence-Weighted by Teacher Max (CWTM) and (ii) Dark Knowledge with Permuted Predictions (DKPP). Both methods elucidate the essential components of KD, demonstrating a role of the teacher outputs on both predicted and non-predicted classes. We present experiments with students of various capacities, focusing on the under-explored case where students overpower teachers. Our experiments show significant advantages from transferring knowledge between DenseNets and ResNets in either direction."
    },
    {
        "paperId": "3aa673abd49f0837ed2fbf5cffcada9ba6f48693",
        "url": "https://www.semanticscholar.org/paper/3aa673abd49f0837ed2fbf5cffcada9ba6f48693",
        "title": "Overcoming catastrophic forgetting with hard attention to the task",
        "venue": "International Conference on Machine Learning",
        "year": 2018,
        "citationCount": 1204,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1801.01423, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "145783638",
                "name": "J. Serr"
            },
            {
                "authorId": "35399640",
                "name": "Ddac Surs"
            },
            {
                "authorId": "144057160",
                "name": "M. Miron"
            },
            {
                "authorId": "1713164",
                "name": "Alexandros Karatzoglou"
            }
        ],
        "abstract": "Catastrophic forgetting occurs when a neural network loses the information learned in a previous task after training on subsequent tasks. This problem remains a hurdle for artificial intelligence systems with sequential learning capabilities. In this paper, we propose a task-based hard attention mechanism that preserves previous tasks' information without affecting the current task's learning. A hard attention mask is learned concurrently to every task, through stochastic gradient descent, and previous masks are exploited to condition such learning. We show that the proposed mechanism is effective for reducing catastrophic forgetting, cutting current rates by 45 to 80%. We also show that it is robust to different hyperparameter choices, and that it offers a number of monitoring capabilities. The approach features the possibility to control both the stability and compactness of the learned knowledge, which we believe makes it also attractive for online learning or network compression applications."
    },
    {
        "paperId": "42ec3db12a2e4628885451b13035c2e975220a25",
        "url": "https://www.semanticscholar.org/paper/42ec3db12a2e4628885451b13035c2e975220a25",
        "title": "A Convergence Theory for Deep Learning via Over-Parameterization",
        "venue": "International Conference on Machine Learning",
        "year": 2018,
        "citationCount": 1551,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1811.03962, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1388725932",
                "name": "Zeyuan Allen-Zhu"
            },
            {
                "authorId": "2110486765",
                "name": "Yuanzhi Li"
            },
            {
                "authorId": "143825455",
                "name": "Zhao Song"
            }
        ],
        "abstract": "Deep neural networks (DNNs) have demonstrated dominating performance in many fields; since AlexNet, networks used in practice are going wider and deeper. On the theoretical side, a long line of works has been focusing on training neural networks with one hidden layer. The theory of multi-layer networks remains largely unsettled. \nIn this work, we prove why stochastic gradient descent (SGD) can find $\\textit{global minima}$ on the training objective of DNNs in $\\textit{polynomial time}$. We only make two assumptions: the inputs are non-degenerate and the network is over-parameterized. The latter means the network width is sufficiently large: $\\textit{polynomial}$ in $L$, the number of layers and in $n$, the number of samples. \nOur key technique is to derive that, in a sufficiently large neighborhood of the random initialization, the optimization landscape is almost-convex and semi-smooth even with ReLU activations. This implies an equivalence between over-parameterized neural networks and neural tangent kernel (NTK) in the finite (and polynomial) width setting. \nAs concrete examples, starting from randomly initialized weights, we prove that SGD can attain 100% training accuracy in classification tasks, or minimize regression loss in linear convergence speed, with running time polynomial in $n,L$. Our theory applies to the widely-used but non-smooth ReLU activation, and to any smooth and possibly non-convex loss functions. In terms of network architectures, our theory at least applies to fully-connected neural networks, convolutional neural networks (CNN), and residual neural networks (ResNet)."
    },
    {
        "paperId": "4debb99c0c63bfaa97dd433bc2828e4dac81c48b",
        "url": "https://www.semanticscholar.org/paper/4debb99c0c63bfaa97dd433bc2828e4dac81c48b",
        "title": "Addressing Function Approximation Error in Actor-Critic Methods",
        "venue": "International Conference on Machine Learning",
        "year": 2018,
        "citationCount": 6157,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1802.09477, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "14637819",
                "name": "Scott Fujimoto"
            },
            {
                "authorId": "47662867",
                "name": "H. V. Hoof"
            },
            {
                "authorId": "2462512",
                "name": "D. Meger"
            }
        ],
        "abstract": "In value-based reinforcement learning methods such as deep Q-learning, function approximation errors are known to lead to overestimated value estimates and suboptimal policies. We show that this problem persists in an actor-critic setting and propose novel mechanisms to minimize its effects on both the actor and the critic. Our algorithm builds on Double Q-learning, by taking the minimum value between a pair of critics to limit overestimation. We draw the connection between target networks and overestimation bias, and suggest delaying policy updates to reduce per-update error and further improve performance. We evaluate our method on the suite of OpenAI gym tasks, outperforming the state of the art in every environment tested."
    },
    {
        "paperId": "5285cb8faada5de8a92a47622950f6cfd476ac1d",
        "url": "https://www.semanticscholar.org/paper/5285cb8faada5de8a92a47622950f6cfd476ac1d",
        "title": "Off-Policy Deep Reinforcement Learning without Exploration",
        "venue": "International Conference on Machine Learning",
        "year": 2018,
        "citationCount": 1838,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1812.02900, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "14637819",
                "name": "Scott Fujimoto"
            },
            {
                "authorId": "2462512",
                "name": "D. Meger"
            },
            {
                "authorId": "144368601",
                "name": "Doina Precup"
            }
        ],
        "abstract": "Many practical applications of reinforcement learning constrain agents to learn from a fixed batch of data which has already been gathered, without offering further possibility for data collection. In this paper, we demonstrate that due to errors introduced by extrapolation, standard off-policy deep reinforcement learning algorithms, such as DQN and DDPG, are incapable of learning with data uncorrelated to the distribution under the current policy, making them ineffective for this fixed batch setting. We introduce a novel class of off-policy algorithms, batch-constrained reinforcement learning, which restricts the action space in order to force the agent towards behaving close to on-policy with respect to a subset of the given data. We present the first continuous control deep reinforcement learning algorithm which can learn effectively from arbitrary, fixed batch data, and empirically demonstrate the quality of its behavior in several tasks."
    },
    {
        "paperId": "54a13bcc9613dcaa76fb25fbe96572f376cfcca9",
        "url": "https://www.semanticscholar.org/paper/54a13bcc9613dcaa76fb25fbe96572f376cfcca9",
        "title": "Adafactor: Adaptive Learning Rates with Sublinear Memory Cost",
        "venue": "International Conference on Machine Learning",
        "year": 2018,
        "citationCount": 1187,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1804.04235, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1846258",
                "name": "Noam Shazeer"
            },
            {
                "authorId": "144872294",
                "name": "Mitchell Stern"
            }
        ],
        "abstract": "In several recently proposed stochastic optimization methods (e.g. RMSProp, Adam, Adadelta), parameter updates are scaled by the inverse square roots of exponential moving averages of squared past gradients. Maintaining these per-parameter second-moment estimators requires memory equal to the number of parameters. For the case of neural network weight matrices, we propose maintaining only the per-row and per-column sums of these moving averages, and estimating the per-parameter second moments based on these sums. We demonstrate empirically that this method produces similar results to the baseline. Secondly, we show that adaptive methods can produce larger-than-desired updates when the decay rate of the second moment accumulator is too slow. We propose update clipping and a gradually increasing decay rate scheme as remedies. Combining these methods and dropping momentum, we achieve comparable results to the published Adam regime in training the Transformer model on the WMT 2014 English-German machine translation task, while using very little auxiliary storage in the optimizer. Finally, we propose scaling the parameter updates based on the scale of the parameters themselves."
    },
    {
        "paperId": "57fbd1841a7cf8582682da399d2811655f020c0a",
        "url": "https://www.semanticscholar.org/paper/57fbd1841a7cf8582682da399d2811655f020c0a",
        "title": "Attention-based Deep Multiple Instance Learning",
        "venue": "International Conference on Machine Learning",
        "year": 2018,
        "citationCount": 2240,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1802.04712, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "35186565",
                "name": "Maximilian Ilse"
            },
            {
                "authorId": "1849327",
                "name": "J. Tomczak"
            },
            {
                "authorId": "1678311",
                "name": "M. Welling"
            }
        ],
        "abstract": "Multiple instance learning (MIL) is a variation of supervised learning where a single class label is assigned to a bag of instances. In this paper, we state the MIL problem as learning the Bernoulli distribution of the bag label where the bag label probability is fully parameterized by neural networks. Furthermore, we propose a neural network-based permutation-invariant aggregation operator that corresponds to the attention mechanism. Notably, an application of the proposed attention-based operator provides insight into the contribution of each instance to the bag label. We show empirically that our approach achieves comparable performance to the best MIL methods on benchmark MIL datasets and it outperforms other methods on a MNIST-based MIL dataset and two real-life histopathology datasets without sacrificing interpretability."
    },
    {
        "paperId": "5a8f99c24bce7241a23c0941ab4be7e7b8faf49d",
        "url": "https://www.semanticscholar.org/paper/5a8f99c24bce7241a23c0941ab4be7e7b8faf49d",
        "title": "A Reductions Approach to Fair Classification",
        "venue": "International Conference on Machine Learning",
        "year": 2018,
        "citationCount": 1198,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1803.02453, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "40333747",
                "name": "Alekh Agarwal"
            },
            {
                "authorId": "2624289",
                "name": "A. Beygelzimer"
            },
            {
                "authorId": "144652072",
                "name": "Miroslav Dudk"
            },
            {
                "authorId": "144162125",
                "name": "J. Langford"
            },
            {
                "authorId": "1831395",
                "name": "Hanna M. Wallach"
            }
        ],
        "abstract": "We present a systematic approach for achieving fairness in a binary classification setting. While we focus on two well-known quantitative definitions of fairness, our approach encompasses many other previously studied definitions as special cases. The key idea is to reduce fair classification to a sequence of cost-sensitive classification problems, whose solutions yield a randomized classifier with the lowest (empirical) error subject to the desired constraints. We introduce two reductions that work for any representation of the cost-sensitive classifier and compare favorably to prior baselines on a variety of data sets, while overcoming several of their disadvantages."
    },
    {
        "paperId": "5ad1cfdc40f58c5ee078496312798f784fc80801",
        "url": "https://www.semanticscholar.org/paper/5ad1cfdc40f58c5ee078496312798f784fc80801",
        "title": "Byzantine-Robust Distributed Learning: Towards Optimal Statistical Rates",
        "venue": "International Conference on Machine Learning",
        "year": 2018,
        "citationCount": 1868,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1803.01498, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "50559902",
                "name": "Dong Yin"
            },
            {
                "authorId": "51310474",
                "name": "Yudong Chen"
            },
            {
                "authorId": "144161012",
                "name": "K. Ramchandran"
            },
            {
                "authorId": "1745169",
                "name": "P. Bartlett"
            }
        ],
        "abstract": "In large-scale distributed learning, security issues have become increasingly important. Particularly in a decentralized environment, some computing units may behave abnormally, or even exhibit Byzantine failures---arbitrary and potentially adversarial behavior. In this paper, we develop distributed learning algorithms that are provably robust against such failures, with a focus on achieving optimal statistical performance. A main result of this work is a sharp analysis of two robust distributed gradient descent algorithms based on median and trimmed mean operations, respectively. We prove statistical error rates for three kinds of population loss functions: strongly convex, non-strongly convex, and smooth non-convex. In particular, these algorithms are shown to achieve order-optimal statistical error rates for strongly convex losses. To achieve better communication efficiency, we further propose a median-based distributed algorithm that is provably robust, and uses only one communication round. For strongly convex quadratic loss, we show that this algorithm achieves the same optimal error rate as the robust distributed gradient descent algorithms."
    },
    {
        "paperId": "5aea95e1ae78a66474051a330ded374e199b658c",
        "url": "https://www.semanticscholar.org/paper/5aea95e1ae78a66474051a330ded374e199b658c",
        "title": "Representation Learning on Graphs with Jumping Knowledge Networks",
        "venue": "International Conference on Machine Learning",
        "year": 2018,
        "citationCount": 2231,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1806.03536, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3360632",
                "name": "Keyulu Xu"
            },
            {
                "authorId": "2047150637",
                "name": "Chengtao Li"
            },
            {
                "authorId": "2476765",
                "name": "Yonglong Tian"
            },
            {
                "authorId": "2811963",
                "name": "Tomohiro Sonobe"
            },
            {
                "authorId": "1743527",
                "name": "K. Kawarabayashi"
            },
            {
                "authorId": "2594093",
                "name": "S. Jegelka"
            }
        ],
        "abstract": "Recent deep learning approaches for representation learning on graphs follow a neighborhood aggregation procedure. We analyze some important properties of these models, and propose a strategy to overcome those. In particular, the range of \"neighboring\" nodes that a node's representation draws from strongly depends on the graph structure, analogous to the spread of a random walk. To adapt to local neighborhood properties and tasks, we explore an architecture -- jumping knowledge (JK) networks -- that flexibly leverages, for each node, different neighborhood ranges to enable better structure-aware representation. In a number of experiments on social, bioinformatics and citation networks, we demonstrate that our model achieves state-of-the-art performance. Furthermore, combining the JK framework with models like Graph Convolutional Networks, GraphSAGE and Graph Attention Networks consistently improves those models' performance."
    },
    {
        "paperId": "651adaa058f821a890f2c5d1053d69eb481a8352",
        "url": "https://www.semanticscholar.org/paper/651adaa058f821a890f2c5d1053d69eb481a8352",
        "title": "Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples",
        "venue": "International Conference on Machine Learning",
        "year": 2018,
        "citationCount": 3369,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1802.00420, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "38939786",
                "name": "Anish Athalye"
            },
            {
                "authorId": "2365115787",
                "name": "Nicholas Carlini"
            },
            {
                "authorId": "145394689",
                "name": "D. Wagner"
            }
        ],
        "abstract": "We identify obfuscated gradients, a kind of gradient masking, as a phenomenon that leads to a false sense of security in defenses against adversarial examples. While defenses that cause obfuscated gradients appear to defeat iterative optimization-based attacks, we find defenses relying on this effect can be circumvented. We describe characteristic behaviors of defenses exhibiting the effect, and for each of the three types of obfuscated gradients we discover, we develop attack techniques to overcome it. In a case study, examining non-certified white-box-secure defenses at ICLR 2018, we find obfuscated gradients are a common occurrence, with 7 of 9 defenses relying on obfuscated gradients. Our new attacks successfully circumvent 6 completely, and 1 partially, in the original threat model each paper considers."
    },
    {
        "paperId": "6af440915b8a0718c93be1cf61905e41e620484a",
        "url": "https://www.semanticscholar.org/paper/6af440915b8a0718c93be1cf61905e41e620484a",
        "title": "Deep One-Class Classification",
        "venue": "International Conference on Machine Learning",
        "year": 2018,
        "citationCount": 2334,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "51127632",
                "name": "Lukas Ruff"
            },
            {
                "authorId": "2948486",
                "name": "Nico Grnitz"
            },
            {
                "authorId": "39503505",
                "name": "Lucas Deecke"
            },
            {
                "authorId": "29005173",
                "name": "Shoaib Ahmed Siddiqui"
            },
            {
                "authorId": "49004415",
                "name": "Robert A. Vandermeulen"
            },
            {
                "authorId": "49345823",
                "name": "Alexander Binder"
            },
            {
                "authorId": "2067608246",
                "name": "Emmanuel Mller"
            },
            {
                "authorId": "2749512",
                "name": "M. Kloft"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "6b4ca249b3b28d3fee65f69714440c08d42cee64",
        "url": "https://www.semanticscholar.org/paper/6b4ca249b3b28d3fee65f69714440c08d42cee64",
        "title": "Which Training Methods for GANs do actually Converge?",
        "venue": "International Conference on Machine Learning",
        "year": 2018,
        "citationCount": 1582,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1801.04406, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "8226549",
                "name": "L. Mescheder"
            },
            {
                "authorId": "47237027",
                "name": "Andreas Geiger"
            },
            {
                "authorId": "2388416",
                "name": "Sebastian Nowozin"
            }
        ],
        "abstract": "Recent work has shown local convergence of GAN training for absolutely continuous data and generator distributions. In this paper, we show that the requirement of absolute continuity is necessary: we describe a simple yet prototypical counterexample showing that in the more realistic case of distributions that are not absolutely continuous, unregularized GAN training is not always convergent. Furthermore, we discuss regularization strategies that were recently proposed to stabilize GAN training. Our analysis shows that GAN training with instance noise or zero-centered gradient penalties converges. On the other hand, we show that Wasserstein-GANs and WGAN-GP with a finite number of discriminator updates per generator update do not always converge to the equilibrium point. We discuss these results, leading us to a new explanation for the stability problems of GAN training. Based on our analysis, we extend our convergence results to more general GANs and prove local convergence for simplified gradient penalties even if the generator and data distribution lie on lower dimensional manifolds. We find these penalties to work well in practice and use them to learn high-resolution generative image models for a variety of datasets with little hyperparameter tuning."
    },
    {
        "paperId": "6b73775f40467aed52784ff355b9bb7168e9078c",
        "url": "https://www.semanticscholar.org/paper/6b73775f40467aed52784ff355b9bb7168e9078c",
        "title": "Mutual Information Neural Estimation",
        "venue": "International Conference on Machine Learning",
        "year": 2018,
        "citationCount": 1465,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1801.04062, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "31619472",
                "name": "Mohamed Ishmael Belghazi"
            },
            {
                "authorId": "14398916",
                "name": "A. Baratin"
            },
            {
                "authorId": "2306961146",
                "name": "Sai Rajeswar"
            },
            {
                "authorId": "1955694",
                "name": "Sherjil Ozair"
            },
            {
                "authorId": "1751762",
                "name": "Yoshua Bengio"
            },
            {
                "authorId": "40482726",
                "name": "R. Devon Hjelm"
            },
            {
                "authorId": "1760871",
                "name": "Aaron C. Courville"
            }
        ],
        "abstract": "We argue that the estimation of mutual information between high dimensional continuous random variables can be achieved by gradient descent over neural networks. We present a Mutual Information Neural Estimator (MINE) that is linearly scalable in dimensionality as well as in sample size, trainable through back-prop, and strongly consistent. We present a handful of applications on which MINE can be used to minimize or maximize mutual information. We apply MINE to improve adversarially trained generative models. We also use MINE to implement Information Bottleneck, applying it to supervised classification; our results demonstrate substantial improvement in flexibility and performance in these settings."
    },
    {
        "paperId": "6c66108edb9af0533309055e7b2ecb8922db03d8",
        "url": "https://www.semanticscholar.org/paper/6c66108edb9af0533309055e7b2ecb8922db03d8",
        "title": "Analyzing Federated Learning through an Adversarial Lens",
        "venue": "International Conference on Machine Learning",
        "year": 2018,
        "citationCount": 1206,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1811.12470, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "10754103",
                "name": "A. Bhagoji"
            },
            {
                "authorId": "144387904",
                "name": "Supriyo Chakraborty"
            },
            {
                "authorId": "143615345",
                "name": "Prateek Mittal"
            },
            {
                "authorId": "2096811",
                "name": "S. Calo"
            }
        ],
        "abstract": "Federated learning distributes model training among a multitude of agents, who, guided by privacy concerns, perform training using their local data but share only model parameter updates, for iterative aggregation at the server. In this work, we explore the threat of model poisoning attacks on federated learning initiated by a single, non-colluding malicious agent where the adversarial objective is to cause the model to misclassify a set of chosen inputs with high confidence. We explore a number of strategies to carry out this attack, starting with simple boosting of the malicious agent's update to overcome the effects of other agents' updates. To increase attack stealth, we propose an alternating minimization strategy, which alternately optimizes for the training loss and the adversarial objective. We follow up by using parameter estimation for the benign agents' updates to improve on attack success. Finally, we use a suite of interpretability techniques to generate visual explanations of model decisions for both benign and malicious models and show that the explanations are nearly visually indistinguishable. Our results indicate that even a highly constrained adversary can carry out model poisoning attacks while simultaneously maintaining stealth, thus highlighting the vulnerability of the federated learning setting and the need to develop effective defense strategies."
    },
    {
        "paperId": "715a73290f260cf2196307e59fe0b6776841f170",
        "url": "https://www.semanticscholar.org/paper/715a73290f260cf2196307e59fe0b6776841f170",
        "title": "On the Spectral Bias of Neural Networks",
        "venue": "International Conference on Machine Learning",
        "year": 2018,
        "citationCount": 1854,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1806.08734, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "40050919",
                "name": "Nasim Rahaman"
            },
            {
                "authorId": "14398916",
                "name": "A. Baratin"
            },
            {
                "authorId": "2309967",
                "name": "Devansh Arpit"
            },
            {
                "authorId": "23152499",
                "name": "Felix Drxler"
            },
            {
                "authorId": "1491081747",
                "name": "Min Lin"
            },
            {
                "authorId": "1685187",
                "name": "F. Hamprecht"
            },
            {
                "authorId": "1751762",
                "name": "Yoshua Bengio"
            },
            {
                "authorId": "1760871",
                "name": "Aaron C. Courville"
            }
        ],
        "abstract": "Neural networks are known to be a class of highly expressive functions able to fit even random input-output mappings with $100\\%$ accuracy. In this work, we present properties of neural networks that complement this aspect of expressivity. By using tools from Fourier analysis, we show that deep ReLU networks are biased towards low frequency functions, meaning that they cannot have local fluctuations without affecting their global behavior. Intuitively, this property is in line with the observation that over-parameterized networks find simple patterns that generalize across data samples. We also investigate how the shape of the data manifold affects expressivity by showing evidence that learning high frequencies gets \\emph{easier} with increasing manifold complexity, and present a theoretical understanding of this behavior. Finally, we study the robustness of the frequency components with respect to parameter perturbation, to develop the intuition that the parameters must be finely tuned to express high frequency functions."
    },
    {
        "paperId": "80196cdfcd0c6ce2953bf65a7f019971e2026386",
        "url": "https://www.semanticscholar.org/paper/80196cdfcd0c6ce2953bf65a7f019971e2026386",
        "title": "IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures",
        "venue": "International Conference on Machine Learning",
        "year": 2018,
        "citationCount": 1732,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1802.01561, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2311318",
                "name": "L. Espeholt"
            },
            {
                "authorId": "2794457",
                "name": "Hubert Soyer"
            },
            {
                "authorId": "1708654",
                "name": "R. Munos"
            },
            {
                "authorId": "34838386",
                "name": "K. Simonyan"
            },
            {
                "authorId": "3255983",
                "name": "Volodymyr Mnih"
            },
            {
                "authorId": "2056968992",
                "name": "Tom Ward"
            },
            {
                "authorId": "2895238",
                "name": "Yotam Doron"
            },
            {
                "authorId": "9559485",
                "name": "Vlad Firoiu"
            },
            {
                "authorId": "3367786",
                "name": "Tim Harley"
            },
            {
                "authorId": "2768462",
                "name": "Iain Dunning"
            },
            {
                "authorId": "34313265",
                "name": "S. Legg"
            },
            {
                "authorId": "2645384",
                "name": "K. Kavukcuoglu"
            }
        ],
        "abstract": "In this work we aim to solve a large collection of tasks using a single reinforcement learning agent with a single set of parameters. A key challenge is to handle the increased amount of data and extended training time. We have developed a new distributed agent IMPALA (Importance Weighted Actor-Learner Architecture) that not only uses resources more efficiently in single-machine training but also scales to thousands of machines without sacrificing data efficiency or resource utilisation. We achieve stable learning at high throughput by combining decoupled acting and learning with a novel off-policy correction method called V-trace. We demonstrate the effectiveness of IMPALA for multi-task reinforcement learning on DMLab-30 (a set of 30 tasks from the DeepMind Lab environment (Beattie et al., 2016)) and Atari-57 (all available Atari games in Arcade Learning Environment (Bellemare et al., 2013a)). Our results show that IMPALA is able to achieve better performance than previous agents with less data, and crucially exhibits positive transfer between tasks as a result of its multi-task approach."
    },
    {
        "paperId": "811df72e210e20de99719539505da54762a11c6d",
        "url": "https://www.semanticscholar.org/paper/811df72e210e20de99719539505da54762a11c6d",
        "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
        "venue": "International Conference on Machine Learning",
        "year": 2018,
        "citationCount": 9989,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1801.01290, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2587648",
                "name": "Tuomas Haarnoja"
            },
            {
                "authorId": "35499972",
                "name": "Aurick Zhou"
            },
            {
                "authorId": "1689992",
                "name": "P. Abbeel"
            },
            {
                "authorId": "1736651",
                "name": "S. Levine"
            }
        ],
        "abstract": "Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds."
    },
    {
        "paperId": "93436a26d744e0417e21df10abdfce2cc74b1e58",
        "url": "https://www.semanticscholar.org/paper/93436a26d744e0417e21df10abdfce2cc74b1e58",
        "title": "BOHB: Robust and Efficient Hyperparameter Optimization at Scale",
        "venue": "International Conference on Machine Learning",
        "year": 2018,
        "citationCount": 1228,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1807.01774, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2154062",
                "name": "S. Falkner"
            },
            {
                "authorId": "145227684",
                "name": "Aaron Klein"
            },
            {
                "authorId": "144661829",
                "name": "F. Hutter"
            }
        ],
        "abstract": "Modern deep learning methods are very sensitive to many hyperparameters, and, due to the long training times of state-of-the-art models, vanilla Bayesian hyperparameter optimization is typically computationally infeasible. On the other hand, bandit-based configuration evaluation approaches based on random search lack guidance and do not converge to the best configurations as quickly. Here, we propose to combine the benefits of both Bayesian optimization and bandit-based methods, in order to achieve the best of both worlds: strong anytime performance and fast convergence to optimal configurations. We propose a new practical state-of-the-art hyperparameter optimization method, which consistently outperforms both Bayesian optimization and Hyperband on a wide range of problem types, including high-dimensional toy functions, support vector machines, feed-forward neural networks, Bayesian neural networks, deep reinforcement learning, and convolutional neural networks. Our method is robust and versatile, while at the same time being conceptually simple and easy to implement."
    },
    {
        "paperId": "97884ff15e0a4e83f534b7b13979e519d1c50a54",
        "url": "https://www.semanticscholar.org/paper/97884ff15e0a4e83f534b7b13979e519d1c50a54",
        "title": "signSGD: compressed optimisation for non-convex problems",
        "venue": "International Conference on Machine Learning",
        "year": 2018,
        "citationCount": 1169,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1802.04434, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2075339320",
                "name": "Jeremy Bernstein"
            },
            {
                "authorId": "2040617",
                "name": "Yu-Xiang Wang"
            },
            {
                "authorId": "3371922",
                "name": "K. Azizzadenesheli"
            },
            {
                "authorId": "2047844",
                "name": "Anima Anandkumar"
            }
        ],
        "abstract": "Training large neural networks requires distributing learning across multiple workers, where the cost of communicating gradients can be a significant bottleneck. signSGD alleviates this problem by transmitting just the sign of each minibatch stochastic gradient. We prove that it can get the best of both worlds: compressed gradients and SGD-level convergence rate. signSGD can exploit mismatches between L1 and L2 geometry: when noise and curvature are much sparser than the gradients, signSGD is expected to converge at the same rate or faster than full-precision SGD. Measurements of the L1 versus L2 geometry of real networks support our theoretical claims, and we find that the momentum counterpart of signSGD is able to match the accuracy and convergence speed of Adam on deep Imagenet models. We extend our theory to the distributed setting, where the parameter server uses majority vote to aggregate gradient signs from each worker enabling 1-bit compression of worker-server communication in both directions. Using a theorem by Gauss, we prove that the non-convex convergence rate of majority vote matches that of distributed SGD. Thus, there is great promise for sign-based optimisation schemes to achieve both communication efficiency and high accuracy."
    },
    {
        "paperId": "9c5c794094fbf5da8c48df5c3242615dc0b1d245",
        "url": "https://www.semanticscholar.org/paper/9c5c794094fbf5da8c48df5c3242615dc0b1d245",
        "title": "Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations",
        "venue": "International Conference on Machine Learning",
        "year": 2018,
        "citationCount": 1606,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1811.12359, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "9557137",
                "name": "Francesco Locatello"
            },
            {
                "authorId": "153125952",
                "name": "Stefan Bauer"
            },
            {
                "authorId": "34302129",
                "name": "Mario Lucic"
            },
            {
                "authorId": "1802148",
                "name": "S. Gelly"
            },
            {
                "authorId": "1707625",
                "name": "B. Scholkopf"
            },
            {
                "authorId": "1936951",
                "name": "Olivier Bachem"
            }
        ],
        "abstract": "The key idea behind the unsupervised learning of disentangled representations is that real-world data is generated by a few explanatory factors of variation which can be recovered by unsupervised learning algorithms. In this paper, we provide a sober look at recent progress in the field and challenge some common assumptions. We first theoretically show that the unsupervised learning of disentangled representations is fundamentally impossible without inductive biases on both the models and the data. Then, we train more than 12000 models covering most prominent methods and evaluation metrics in a reproducible large-scale experimental study on seven different data sets. We observe that while the different methods successfully enforce properties ``encouraged'' by the corresponding losses, well-disentangled models seemingly cannot be identified without supervision. Furthermore, increased disentanglement does not seem to lead to a decreased sample complexity of learning for downstream tasks. Our results suggest that future work on disentanglement learning should be explicit about the role of inductive biases and (implicit) supervision, investigate concrete benefits of enforcing disentanglement of the learned representations, and consider a reproducible experimental setup covering several data sets."
    },
    {
        "paperId": "a89f0a78f86077864e108a1bd2c4e670c85907f8",
        "url": "https://www.semanticscholar.org/paper/a89f0a78f86077864e108a1bd2c4e670c85907f8",
        "title": "GAIN: Missing Data Imputation using Generative Adversarial Nets",
        "venue": "International Conference on Machine Learning",
        "year": 2018,
        "citationCount": 1222,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1806.02920, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2144029",
                "name": "Jinsung Yoon"
            },
            {
                "authorId": "37996637",
                "name": "James Jordon"
            },
            {
                "authorId": "1729969",
                "name": "M. Schaar"
            }
        ],
        "abstract": "We propose a novel method for imputing missing data by adapting the well-known Generative Adversarial Nets (GAN) framework. Accordingly, we call our method Generative Adversarial Imputation Nets (GAIN). The generator (G) observes some components of a real data vector, imputes the missing components conditioned on what is actually observed, and outputs a completed vector. The discriminator (D) then takes a completed vector and attempts to determine which components were actually observed and which were imputed. To ensure that D forces G to learn the desired distribution, we provide D with some additional information in the form of a hint vector. The hint reveals to D partial information about the missingness of the original sample, which is used by D to focus its attention on the imputation quality of particular components. This hint ensures that G does in fact learn to generate according to the true data distribution. We tested our method on various datasets and found that GAIN significantly outperforms state-of-the-art imputation methods."
    },
    {
        "paperId": "a8f3dc53e321fbb2565f5925def4365b9f68d1af",
        "url": "https://www.semanticscholar.org/paper/a8f3dc53e321fbb2565f5925def4365b9f68d1af",
        "title": "Self-Attention Generative Adversarial Networks",
        "venue": "International Conference on Machine Learning",
        "year": 2018,
        "citationCount": 4010,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1805.08318, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "48213346",
                "name": "Han Zhang"
            },
            {
                "authorId": "153440022",
                "name": "I. Goodfellow"
            },
            {
                "authorId": "1711560",
                "name": "Dimitris N. Metaxas"
            },
            {
                "authorId": "2624088",
                "name": "Augustus Odena"
            }
        ],
        "abstract": "In this paper, we propose the Self-Attention Generative Adversarial Network (SAGAN) which allows attention-driven, long-range dependency modeling for image generation tasks. Traditional convolutional GANs generate high-resolution details as a function of only spatially local points in lower-resolution feature maps. In SAGAN, details can be generated using cues from all feature locations. Moreover, the discriminator can check that highly detailed features in distant portions of the image are consistent with each other. Furthermore, recent work has shown that generator conditioning affects GAN performance. Leveraging this insight, we apply spectral normalization to the GAN generator and find that this improves training dynamics. The proposed SAGAN achieves the state-of-the-art results, boosting the best published Inception score from 36.8 to 52.52 and reducing Frechet Inception distance from 27.62 to 18.65 on the challenging ImageNet dataset. Visualization of the attention layers shows that the generator leverages neighborhoods that correspond to object shapes rather than local regions of fixed shape."
    },
    {
        "paperId": "b3f83e8416010e9c3a705a0b6390d268e5ddf5c0",
        "url": "https://www.semanticscholar.org/paper/b3f83e8416010e9c3a705a0b6390d268e5ddf5c0",
        "title": "Black-box Adversarial Attacks with Limited Queries and Information",
        "venue": "International Conference on Machine Learning",
        "year": 2018,
        "citationCount": 1304,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1804.08598, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "34562927",
                "name": "Andrew Ilyas"
            },
            {
                "authorId": "39468283",
                "name": "Logan Engstrom"
            },
            {
                "authorId": "38939786",
                "name": "Anish Athalye"
            },
            {
                "authorId": "32815692",
                "name": "Jessy Lin"
            }
        ],
        "abstract": "Current neural network-based classifiers are susceptible to adversarial examples even in the black-box setting, where the attacker only has query access to the model. In practice, the threat model for real-world systems is often more restrictive than the typical black-box model where the adversary can observe the full output of the network on arbitrarily many chosen inputs. We define three realistic threat models that more accurately characterize many real-world classifiers: the query-limited setting, the partial-information setting, and the label-only setting. We develop new attacks that fool classifiers under these more restrictive threat models, where previous methods would be impractical or ineffective. We demonstrate that our methods are effective against an ImageNet classifier under our proposed threat models. We also demonstrate a targeted black-box attack against a commercial classifier, overcoming the challenges of limited query access, partial information, and other practical issues to break the Google Cloud Vision API."
    },
    {
        "paperId": "c5420ef59d7508d82e53671b0d623027eb58e6ed",
        "url": "https://www.semanticscholar.org/paper/c5420ef59d7508d82e53671b0d623027eb58e6ed",
        "title": "Learning to Reweight Examples for Robust Deep Learning",
        "venue": "International Conference on Machine Learning",
        "year": 2018,
        "citationCount": 1562,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1803.09050, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2540599",
                "name": "Mengye Ren"
            },
            {
                "authorId": "3468942",
                "name": "Wenyuan Zeng"
            },
            {
                "authorId": "49188662",
                "name": "Binh Yang"
            },
            {
                "authorId": "2422559",
                "name": "R. Urtasun"
            }
        ],
        "abstract": "Deep neural networks have been shown to be very powerful modeling tools for many supervised learning tasks involving complex input patterns. However, they can also easily overfit to training set biases and label noises. In addition to various regularizers, example reweighting algorithms are popular solutions to these problems, but they require careful tuning of additional hyperparameters, such as example mining schedules and regularization hyperparameters. In contrast to past reweighting methods, which typically consist of functions of the cost value of each example, in this work we propose a novel meta-learning algorithm that learns to assign weights to training examples based on their gradient directions. To determine the example weights, our method performs a meta gradient descent step on the current mini-batch example weights (which are initialized from zero) to minimize the loss on a clean unbiased validation set. Our proposed method can be easily implemented on any type of deep network, does not require any additional hyperparameter tuning, and achieves impressive performance on class imbalance and corrupted label problems where only a small amount of clean validation data is available."
    },
    {
        "paperId": "fd17bd9a5dc24a081ad9743570f50dd6750f54b2",
        "url": "https://www.semanticscholar.org/paper/fd17bd9a5dc24a081ad9743570f50dd6750f54b2",
        "title": "Junction Tree Variational Autoencoder for Molecular Graph Generation",
        "venue": "International Conference on Machine Learning",
        "year": 2018,
        "citationCount": 1522,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1802.04364, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2400119",
                "name": "Wengong Jin"
            },
            {
                "authorId": "1741283",
                "name": "R. Barzilay"
            },
            {
                "authorId": "35132120",
                "name": "T. Jaakkola"
            }
        ],
        "abstract": "We seek to automate the design of molecules based on specific chemical properties. In computational terms, this task involves continuous embedding and generation of molecular graphs. Our primary contribution is the direct realization of molecular graphs, a task previously approached by generating linear SMILES strings instead of graphs. Our junction tree variational autoencoder generates molecular graphs in two phases, by first generating a tree-structured scaffold over chemical substructures, and then combining them into a molecule with a graph message passing network. This approach allows us to incrementally expand molecules while maintaining chemical validity at every step. We evaluate our model on multiple tasks ranging from molecular generation to optimization. Across these tasks, our model outperforms previous state-of-the-art baselines by a significant margin."
    },
    {
        "paperId": "fe9b8aac9fa3bfd9724db5a881a578e471e612d7",
        "url": "https://www.semanticscholar.org/paper/fe9b8aac9fa3bfd9724db5a881a578e471e612d7",
        "title": "Efficient Neural Architecture Search via Parameter Sharing",
        "venue": "International Conference on Machine Learning",
        "year": 2018,
        "citationCount": 2913,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1802.03268, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "143950636",
                "name": "Hieu Pham"
            },
            {
                "authorId": "152565355",
                "name": "M. Guan"
            },
            {
                "authorId": "2368067",
                "name": "Barret Zoph"
            },
            {
                "authorId": "2827616",
                "name": "Quoc V. Le"
            },
            {
                "authorId": "48448318",
                "name": "J. Dean"
            }
        ],
        "abstract": "We propose Efficient Neural Architecture Search (ENAS), a fast and inexpensive approach for automatic model design. In ENAS, a controller learns to discover neural network architectures by searching for an optimal subgraph within a large computational graph. The controller is trained with policy gradient to select a subgraph that maximizes the expected reward on the validation set. Meanwhile the model corresponding to the selected subgraph is trained to minimize a canonical cross entropy loss. Thanks to parameter sharing between child models, ENAS is fast: it delivers strong empirical performances using much fewer GPU-hours than all existing automatic model design approaches, and notably, 1000x less expensive than standard Neural Architecture Search. On the Penn Treebank dataset, ENAS discovers a novel architecture that achieves a test perplexity of 55.8, establishing a new state-of-the-art among all methods without post-training processing. On the CIFAR-10 dataset, ENAS designs novel architectures that achieve a test error of 2.89%, which is on par with NASNet (Zoph et al., 2018), whose test error is 2.65%."
    },
    {
        "paperId": "fea3e63c97c7292dc6fbcb3ffe7131eb54053986",
        "url": "https://www.semanticscholar.org/paper/fea3e63c97c7292dc6fbcb3ffe7131eb54053986",
        "title": "Learning Latent Dynamics for Planning from Pixels",
        "venue": "International Conference on Machine Learning",
        "year": 2018,
        "citationCount": 1638,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1811.04551, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "35006479",
                "name": "Danijar Hafner"
            },
            {
                "authorId": "2542999",
                "name": "T. Lillicrap"
            },
            {
                "authorId": "33091759",
                "name": "Ian S. Fischer"
            },
            {
                "authorId": "144543406",
                "name": "Ruben Villegas"
            },
            {
                "authorId": "39810222",
                "name": "David R Ha"
            },
            {
                "authorId": "1697141",
                "name": "Honglak Lee"
            },
            {
                "authorId": "2068894907",
                "name": "James Davidson"
            }
        ],
        "abstract": "Planning has been very successful for control tasks with known environment dynamics. To leverage planning in unknown environments, the agent needs to learn the dynamics from interactions with the world. However, learning dynamics models that are accurate enough for planning has been a long-standing challenge, especially in image-based domains. We propose the Deep Planning Network (PlaNet), a purely model-based agent that learns the environment dynamics from images and chooses actions through fast online planning in latent space. To achieve high performance, the dynamics model must accurately predict the rewards ahead for multiple time steps. We approach this using a latent dynamics model with both deterministic and stochastic transition components. Moreover, we propose a multi-step variational inference objective that we name latent overshooting. Using only pixel observations, our agent solves continuous control tasks with contact dynamics, partial observability, and sparse rewards, which exceed the difficulty of tasks that were previously solved by planning with learned models. PlaNet uses substantially fewer episodes and reaches final performance close to and sometimes higher than strong model-free algorithms."
    },
    {
        "paperId": "ffc211476f2e40e79466ffc198c919a97da3bb76",
        "url": "https://www.semanticscholar.org/paper/ffc211476f2e40e79466ffc198c919a97da3bb76",
        "title": "QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning",
        "venue": "International Conference on Machine Learning",
        "year": 2018,
        "citationCount": 1858,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1803.11485, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "36054740",
                "name": "Tabish Rashid"
            },
            {
                "authorId": "49089678",
                "name": "Mikayel Samvelyan"
            },
            {
                "authorId": "47542438",
                "name": "C. S. D. Witt"
            },
            {
                "authorId": "38698094",
                "name": "Gregory Farquhar"
            },
            {
                "authorId": "145356667",
                "name": "Jakob N. Foerster"
            },
            {
                "authorId": "1766767",
                "name": "Shimon Whiteson"
            }
        ],
        "abstract": "In many real-world settings, a team of agents must coordinate their behaviour while acting in a decentralised way. At the same time, it is often possible to train the agents in a centralised fashion in a simulated or laboratory setting, where global state information is available and communication constraints are lifted. Learning joint action-values conditioned on extra state information is an attractive way to exploit centralised learning, but the best strategy for then extracting decentralised policies is unclear. Our solution is QMIX, a novel value-based method that can train decentralised policies in a centralised end-to-end fashion. QMIX employs a network that estimates joint action-values as a complex non-linear combination of per-agent values that condition only on local observations. We structurally enforce that the joint-action value is monotonic in the per-agent values, which allows tractable maximisation of the joint action-value in off-policy learning, and guarantees consistency between the centralised and decentralised policies. We evaluate QMIX on a challenging set of StarCraft II micromanagement tasks, and show that QMIX significantly outperforms existing value-based multi-agent reinforcement learning methods."
    },
    {
        "paperId": "14558cb69319eed0d5bfc5648aafcd09d882f443",
        "url": "https://www.semanticscholar.org/paper/14558cb69319eed0d5bfc5648aafcd09d882f443",
        "title": "Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks",
        "venue": "International Conference on Machine Learning",
        "year": 2019,
        "citationCount": 1026,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1901.08584, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "145563465",
                "name": "Sanjeev Arora"
            },
            {
                "authorId": "145697585",
                "name": "S. Du"
            },
            {
                "authorId": "1471043558",
                "name": "Wei Hu"
            },
            {
                "authorId": "46947755",
                "name": "Zhiyuan Li"
            },
            {
                "authorId": "2108693711",
                "name": "Ruosong Wang"
            }
        ],
        "abstract": "Recent works have cast some light on the mystery of why deep nets fit any data and generalize despite being very overparametrized. This paper analyzes training and generalization for a simple 2-layer ReLU net with random initialization, and provides the following improvements over recent works: \n(i) Using a tighter characterization of training speed than recent papers, an explanation for why training a neural net with random labels leads to slower training, as originally observed in [Zhang et al. ICLR'17]. \n(ii) Generalization bound independent of network size, using a data-dependent complexity measure. Our measure distinguishes clearly between random labels and true labels on MNIST and CIFAR, as shown by experiments. Moreover, recent papers require sample complexity to increase (slowly) with the size, while our sample complexity is completely independent of the network size. \n(iii) Learnability of a broad class of smooth functions by 2-layer ReLU nets trained via gradient descent. \nThe key idea is to track dynamics of training and generalization via properties of a related kernel."
    },
    {
        "paperId": "159395b0f7a2b9ea04f9a758d18887bcb970ee78",
        "url": "https://www.semanticscholar.org/paper/159395b0f7a2b9ea04f9a758d18887bcb970ee78",
        "title": "Agnostic Federated Learning",
        "venue": "International Conference on Machine Learning",
        "year": 2019,
        "citationCount": 1044,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1902.00146, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "81080659",
                "name": "M. Mohri"
            },
            {
                "authorId": "67086013",
                "name": "Gary Sivek"
            },
            {
                "authorId": "9486035",
                "name": "A. Suresh"
            }
        ],
        "abstract": "A key learning scenario in large-scale applications is that of federated learning, where a centralized model is trained based on data originating from a large number of clients. We argue that, with the existing training and inference, federated models can be biased towards different clients. Instead, we propose a new framework of agnostic federated learning, where the centralized model is optimized for any target distribution formed by a mixture of the client distributions. We further show that this framework naturally yields a notion of fairness. We present data-dependent Rademacher complexity guarantees for learning with this objective, which guide the definition of an algorithm for agnostic federated learning. We also give a fast stochastic optimization algorithm for solving the corresponding optimization problem, for which we prove convergence bounds, assuming a convex loss function and hypothesis set. We further empirically demonstrate the benefits of our approach in several datasets. Beyond federated learning, our framework and algorithm can be of interest to other learning scenarios such as cloud computing, domain adaptation, drifting, and other contexts where the training and test distributions do not coincide."
    },
    {
        "paperId": "1cae417456711c4da184f5efcd1b7464a7a0661a",
        "url": "https://www.semanticscholar.org/paper/1cae417456711c4da184f5efcd1b7464a7a0661a",
        "title": "Data-Efficient Image Recognition with Contrastive Predictive Coding",
        "venue": "International Conference on Machine Learning",
        "year": 2019,
        "citationCount": 1517,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1905.09272, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2856712",
                "name": "Olivier J. Hnaff"
            },
            {
                "authorId": "41207614",
                "name": "A. Srinivas"
            },
            {
                "authorId": "3364908",
                "name": "J. Fauw"
            },
            {
                "authorId": "143653164",
                "name": "Ali Razavi"
            },
            {
                "authorId": "2786693",
                "name": "Carl Doersch"
            },
            {
                "authorId": "143648071",
                "name": "S. Eslami"
            },
            {
                "authorId": "3422336",
                "name": "Aron van den Oord"
            }
        ],
        "abstract": "Human observers can learn to recognize new categories of images from a handful of examples, yet doing so with artificial ones remains an open challenge. We hypothesize that data-efficient recognition is enabled by representations which make the variability in natural signals more predictable. We therefore revisit and improve Contrastive Predictive Coding, an unsupervised objective for learning such representations. This new implementation produces features which support state-of-the-art linear classification accuracy on the ImageNet dataset. When used as input for non-linear classification with deep neural networks, this representation allows us to use 2-5x less labels than classifiers trained directly on image pixels. Finally, this unsupervised representation substantially improves transfer learning to object detection on the PASCAL VOC dataset, surpassing fully supervised pre-trained ImageNet classifiers."
    },
    {
        "paperId": "29ddc1f43f28af7c846515e32cc167bc66886d0c",
        "url": "https://www.semanticscholar.org/paper/29ddc1f43f28af7c846515e32cc167bc66886d0c",
        "title": "Parameter-Efficient Transfer Learning for NLP",
        "venue": "International Conference on Machine Learning",
        "year": 2019,
        "citationCount": 5596,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1902.00751, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2815290",
                "name": "N. Houlsby"
            },
            {
                "authorId": "1911881",
                "name": "A. Giurgiu"
            },
            {
                "authorId": "40569328",
                "name": "Stanislaw Jastrzebski"
            },
            {
                "authorId": "68973833",
                "name": "Bruna Morrone"
            },
            {
                "authorId": "51985388",
                "name": "Quentin de Laroussilhe"
            },
            {
                "authorId": "2813347",
                "name": "Andrea Gesmundo"
            },
            {
                "authorId": "2809991",
                "name": "Mona Attariyan"
            },
            {
                "authorId": "1802148",
                "name": "S. Gelly"
            }
        ],
        "abstract": "Fine-tuning large pre-trained models is an effective transfer mechanism in NLP. However, in the presence of many downstream tasks, fine-tuning is parameter inefficient: an entire new model is required for every task. As an alternative, we propose transfer with adapter modules. Adapter modules yield a compact and extensible model; they add only a few trainable parameters per task, and new tasks can be added without revisiting previous ones. The parameters of the original network remain fixed, yielding a high degree of parameter sharing. To demonstrate adapter's effectiveness, we transfer the recently proposed BERT Transformer model to 26 diverse text classification tasks, including the GLUE benchmark. Adapters attain near state-of-the-art performance, whilst adding only a few parameters per task. On GLUE, we attain within 0.4% of the performance of full fine-tuning, adding only 3.6% parameters per task. By contrast, fine-tuning trains 100% of the parameters per task."
    },
    {
        "paperId": "4e0bb8c1c683b43357c5d5216f6b74ff2cb32434",
        "url": "https://www.semanticscholar.org/paper/4e0bb8c1c683b43357c5d5216f6b74ff2cb32434",
        "title": "Do ImageNet Classifiers Generalize to ImageNet?",
        "venue": "International Conference on Machine Learning",
        "year": 2019,
        "citationCount": 1987,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1902.10811, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "9229182",
                "name": "B. Recht"
            },
            {
                "authorId": "40458654",
                "name": "R. Roelofs"
            },
            {
                "authorId": "152772922",
                "name": "Ludwig Schmidt"
            },
            {
                "authorId": "34961417",
                "name": "Vaishaal Shankar"
            }
        ],
        "abstract": "We build new test sets for the CIFAR-10 and ImageNet datasets. Both benchmarks have been the focus of intense research for almost a decade, raising the danger of overfitting to excessively re-used test sets. By closely following the original dataset creation processes, we test to what extent current classification models generalize to new data. We evaluate a broad range of models and find accuracy drops of 3% - 15% on CIFAR-10 and 11% - 14% on ImageNet. However, accuracy gains on the original test sets translate to larger gains on the new test sets. Our results suggest that the accuracy drops are not caused by adaptivity, but by the models' inability to generalize to slightly \"harder\" images than those found in the original test sets."
    },
    {
        "paperId": "4efb9a950f252138a30eeb942ed02663a3ea29d1",
        "url": "https://www.semanticscholar.org/paper/4efb9a950f252138a30eeb942ed02663a3ea29d1",
        "title": "MixHop: Higher-Order Graph Convolutional Architectures via Sparsified Neighborhood Mixing",
        "venue": "International Conference on Machine Learning",
        "year": 2019,
        "citationCount": 1060,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1905.00067, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1389570466",
                "name": "Sami Abu-El-Haija"
            },
            {
                "authorId": "2271808",
                "name": "Bryan Perozzi"
            },
            {
                "authorId": "46494353",
                "name": "Amol Kapoor"
            },
            {
                "authorId": "144787340",
                "name": "Hrayr Harutyunyan"
            },
            {
                "authorId": "10007732",
                "name": "N. Alipourfard"
            },
            {
                "authorId": "1782658",
                "name": "Kristina Lerman"
            },
            {
                "authorId": "1719898",
                "name": "G. V. Steeg"
            },
            {
                "authorId": "143728483",
                "name": "A. Galstyan"
            }
        ],
        "abstract": "Existing popular methods for semi-supervised learning with Graph Neural Networks (such as the Graph Convolutional Network) provably cannot learn a general class of neighborhood mixing relationships. To address this weakness, we propose a new model, MixHop, that can learn these relationships, including difference operators, by repeatedly mixing feature representations of neighbors at various distances. Mixhop requires no additional memory or computational complexity, and outperforms on challenging baselines. In addition, we propose sparsity regularization that allows us to visualize how the network prioritizes neighborhood information across different graph datasets. Our analysis of the learned architectures reveals that neighborhood mixing varies per datasets."
    },
    {
        "paperId": "4f2eda8077dc7a69bb2b4e0a1a086cf054adb3f9",
        "url": "https://www.semanticscholar.org/paper/4f2eda8077dc7a69bb2b4e0a1a086cf054adb3f9",
        "title": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks",
        "venue": "International Conference on Machine Learning",
        "year": 2019,
        "citationCount": 21490,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1905.11946, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "120805419",
                "name": "Mingxing Tan"
            },
            {
                "authorId": "2827616",
                "name": "Quoc V. Le"
            }
        ],
        "abstract": "Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet. \nTo go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.4% top-1 / 97.1% top-5 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7%), Flowers (98.8%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters. Source code is at this https URL."
    },
    {
        "paperId": "6c405d4b5dc41a86be05acd59c06ed19daf01d14",
        "url": "https://www.semanticscholar.org/paper/6c405d4b5dc41a86be05acd59c06ed19daf01d14",
        "title": "Theoretically Principled Trade-off between Robustness and Accuracy",
        "venue": "International Conference on Machine Learning",
        "year": 2019,
        "citationCount": 2837,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1901.08573, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "40975176",
                "name": "Hongyang Zhang"
            },
            {
                "authorId": "29001000",
                "name": "Yaodong Yu"
            },
            {
                "authorId": "2784735",
                "name": "Jiantao Jiao"
            },
            {
                "authorId": "143977260",
                "name": "E. Xing"
            },
            {
                "authorId": "1701847",
                "name": "L. Ghaoui"
            },
            {
                "authorId": "1694621",
                "name": "Michael I. Jordan"
            }
        ],
        "abstract": "We identify a trade-off between robustness and accuracy that serves as a guiding principle in the design of defenses against adversarial examples. Although this problem has been widely studied empirically, much remains unknown concerning the theory underlying this trade-off. In this work, we decompose the prediction error for adversarial examples (robust error) as the sum of the natural (classification) error and boundary error, and provide a differentiable upper bound using the theory of classification-calibrated loss, which is shown to be the tightest possible upper bound uniform over all probability distributions and measurable predictors. Inspired by our theoretical analysis, we also design a new defense method, TRADES, to trade adversarial robustness off against accuracy. Our proposed algorithm performs well experimentally in real-world datasets. The methodology is the foundation of our entry to the NeurIPS 2018 Adversarial Vision Challenge in which we won the 1st place out of ~2,000 submissions, surpassing the runner-up approach by $11.41\\%$ in terms of mean $\\ell_2$ perturbation distance."
    },
    {
        "paperId": "726320cdbd04804ffa8f3a78c095bd1b55a2a695",
        "url": "https://www.semanticscholar.org/paper/726320cdbd04804ffa8f3a78c095bd1b55a2a695",
        "title": "Similarity of Neural Network Representations Revisited",
        "venue": "International Conference on Machine Learning",
        "year": 2019,
        "citationCount": 1732,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1905.00414, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "40464924",
                "name": "Simon Kornblith"
            },
            {
                "authorId": "144739074",
                "name": "Mohammad Norouzi"
            },
            {
                "authorId": "1697141",
                "name": "Honglak Lee"
            },
            {
                "authorId": "1695689",
                "name": "Geoffrey E. Hinton"
            }
        ],
        "abstract": "Recent work has sought to understand the behavior of neural networks by comparing representations between layers and between different trained models. We examine methods for comparing neural network representations based on canonical correlation analysis (CCA). We show that CCA belongs to a family of statistics for measuring multivariate similarity, but that neither CCA nor any other statistic that is invariant to invertible linear transformation can measure meaningful similarities between representations of higher dimension than the number of data points. We introduce a similarity index that measures the relationship between representational similarity matrices and does not suffer from this limitation. This similarity index is equivalent to centered kernel alignment (CKA) and is also closely connected to CCA. Unlike CCA, CKA can reliably identify correspondences between representations in networks trained from different initializations."
    },
    {
        "paperId": "7e71eedb078181873a56f2adcfef9dddaeb95602",
        "url": "https://www.semanticscholar.org/paper/7e71eedb078181873a56f2adcfef9dddaeb95602",
        "title": "Simplifying Graph Convolutional Networks",
        "venue": "International Conference on Machine Learning",
        "year": 2019,
        "citationCount": 3618,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1902.07153, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "24277779",
                "name": "Felix Wu"
            },
            {
                "authorId": "123437034",
                "name": "Tianyi Zhang"
            },
            {
                "authorId": "3383481",
                "name": "A. Souza"
            },
            {
                "authorId": "2258547281",
                "name": "Christopher Fifty"
            },
            {
                "authorId": null,
                "name": "Tao Yu"
            },
            {
                "authorId": "7446832",
                "name": "Kilian Q. Weinberger"
            }
        ],
        "abstract": "Graph Convolutional Networks (GCNs) and their variants have experienced significant attention and have become the de facto methods for learning graph representations. GCNs derive inspiration primarily from recent deep learning approaches, and as a result, may inherit unnecessary complexity and redundant computation. In this paper, we reduce this excess complexity through successively removing nonlinearities and collapsing weight matrices between consecutive layers. We theoretically analyze the resulting linear model and show that it corresponds to a fixed low-pass filter followed by a linear classifier. Notably, our experimental evaluation demonstrates that these simplifications do not negatively impact accuracy in many downstream applications. Moreover, the resulting model scales to larger datasets, is naturally interpretable, and yields up to two orders of magnitude speedup over FastGCN."
    },
    {
        "paperId": "a436e41141a9d9c5dfc44d16fade67b9a3b76778",
        "url": "https://www.semanticscholar.org/paper/a436e41141a9d9c5dfc44d16fade67b9a3b76778",
        "title": "Test-Time Training with Self-Supervision for Generalization under Distribution Shifts",
        "venue": "International Conference on Machine Learning",
        "year": 2019,
        "citationCount": 1066,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "2117104165",
                "name": "Yu Sun"
            },
            {
                "authorId": "39849136",
                "name": "X. Wang"
            },
            {
                "authorId": "2109168016",
                "name": "Zhuang Liu"
            },
            {
                "authorId": "1489216270",
                "name": "John Miller"
            },
            {
                "authorId": "1763086",
                "name": "Alexei A. Efros"
            },
            {
                "authorId": "1775622",
                "name": "Moritz Hardt"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "ee8b1603c79a4f9c3bdc0d6633b595aa93ff3a0f",
        "url": "https://www.semanticscholar.org/paper/ee8b1603c79a4f9c3bdc0d6633b595aa93ff3a0f",
        "title": "Self-Attention Graph Pooling",
        "venue": "International Conference on Machine Learning",
        "year": 2019,
        "citationCount": 1251,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1904.08082, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2108454776",
                "name": "Junhyun Lee"
            },
            {
                "authorId": "2152634091",
                "name": "Inyeop Lee"
            },
            {
                "authorId": "144323862",
                "name": "Jaewoo Kang"
            }
        ],
        "abstract": "Advanced methods of applying deep learning to structured data such as graphs have been proposed in recent years. In particular, studies have focused on generalizing convolutional neural networks to graph data, which includes redefining the convolution and the downsampling (pooling) operations for graphs. The method of generalizing the convolution operation to graphs has been proven to improve performance and is widely used. However, the method of applying downsampling to graphs is still difficult to perform and has room for improvement. In this paper, we propose a graph pooling method based on self-attention. Self-attention using graph convolution allows our pooling method to consider both node features and graph topology. To ensure a fair comparison, the same training procedures and model architectures were used for the existing pooling methods and our method. The experimental results demonstrate that our method achieves superior graph classification performance on the benchmark datasets using a reasonable number of parameters."
    },
    {
        "paperId": "f4061bd225b3be5b3f5b18eb1a229ce991efefeb",
        "url": "https://www.semanticscholar.org/paper/f4061bd225b3be5b3f5b18eb1a229ce991efefeb",
        "title": "PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization",
        "venue": "International Conference on Machine Learning",
        "year": 2019,
        "citationCount": 2284,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1912.08777, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "47540100",
                "name": "Jingqing Zhang"
            },
            {
                "authorId": "2143397386",
                "name": "Yao Zhao"
            },
            {
                "authorId": "144413479",
                "name": "Mohammad Saleh"
            },
            {
                "authorId": "35025299",
                "name": "Peter J. Liu"
            }
        ],
        "abstract": "Recent work pre-training Transformers with self-supervised objectives on large text corpora has shown great success when fine-tuned on downstream NLP tasks including text summarization. However, pre-training objectives tailored for abstractive text summarization have not been explored. Furthermore there is a lack of systematic evaluation across diverse domains. In this work, we propose pre-training large Transformer-based encoder-decoder models on massive text corpora with a new self-supervised objective. In PEGASUS, important sentences are removed/masked from an input document and are generated together as one output sequence from the remaining sentences, similar to an extractive summary. We evaluated our best PEGASUS model on 12 downstream summarization tasks spanning news, science, stories, instructions, emails, patents, and legislative bills. Experiments demonstrate it achieves state-of-the-art performance on all 12 downstream datasets measured by ROUGE scores. Our model also shows surprising performance on low-resource summarization, surpassing previous state-of-the-art results on 6 datasets with only 1000 examples. Finally we validated our results using human evaluation and show that our model summaries achieve human performance on multiple datasets."
    },
    {
        "paperId": "f7f73185e3975bb62a3c42b2ba6bd4db57fee8ed",
        "url": "https://www.semanticscholar.org/paper/f7f73185e3975bb62a3c42b2ba6bd4db57fee8ed",
        "title": "Certified Adversarial Robustness via Randomized Smoothing",
        "venue": "International Conference on Machine Learning",
        "year": 2019,
        "citationCount": 2270,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1902.02918, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "39951470",
                "name": "Jeremy M. Cohen"
            },
            {
                "authorId": "49686853",
                "name": "Elan Rosenfeld"
            },
            {
                "authorId": "145116464",
                "name": "J. Z. Kolter"
            }
        ],
        "abstract": "We show how to turn any classifier that classifies well under Gaussian noise into a new classifier that is certifiably robust to adversarial perturbations under the $\\ell_2$ norm. This \"randomized smoothing\" technique has been proposed recently in the literature, but existing guarantees are loose. We prove a tight robustness guarantee in $\\ell_2$ norm for smoothing with Gaussian noise. We use randomized smoothing to obtain an ImageNet classifier with e.g. a certified top-1 accuracy of 49% under adversarial perturbations with $\\ell_2$ norm less than 0.5 (=127/255). No certified defense has been shown feasible on ImageNet except for smoothing. On smaller-scale datasets where competing approaches to certified $\\ell_2$ robustness are viable, smoothing delivers higher certified accuracies. Our strong empirical results suggest that randomized smoothing is a promising direction for future research into adversarially robust classification. Code and models are available at this http URL."
    },
    {
        "paperId": "fc7b1823bd8b59a590d0bc33bd7a145518fd71c5",
        "url": "https://www.semanticscholar.org/paper/fc7b1823bd8b59a590d0bc33bd7a145518fd71c5",
        "title": "SCAFFOLD: Stochastic Controlled Averaging for Federated Learning",
        "venue": "International Conference on Machine Learning",
        "year": 2019,
        "citationCount": 3401,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "40911632",
                "name": "Sai Praneeth Karimireddy"
            },
            {
                "authorId": "144055676",
                "name": "Satyen Kale"
            },
            {
                "authorId": "81080659",
                "name": "M. Mohri"
            },
            {
                "authorId": "1981186",
                "name": "Sashank J. Reddi"
            },
            {
                "authorId": "2127057",
                "name": "Sebastian U. Stich"
            },
            {
                "authorId": "9486035",
                "name": "A. Suresh"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "18939eadc9c4460c8385e0591cde214a1ead067b",
        "url": "https://www.semanticscholar.org/paper/18939eadc9c4460c8385e0591cde214a1ead067b",
        "title": "Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks",
        "venue": "International Conference on Machine Learning",
        "year": 2020,
        "citationCount": 2154,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2003.01690, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "39171784",
                "name": "Francesco Croce"
            },
            {
                "authorId": "143610806",
                "name": "Matthias Hein"
            }
        ],
        "abstract": "The field of defense strategies against adversarial attacks has significantly grown over the last years, but progress is hampered as the evaluation of adversarial defenses is often insufficient and thus gives a wrong impression of robustness. Many promising defenses could be broken later on, making it difficult to identify the state-of-the-art. Frequent pitfalls in the evaluation are improper tuning of hyperparameters of the attacks, gradient obfuscation or masking. In this paper we first propose two extensions of the PGD-attack overcoming failures due to suboptimal step size and problems of the objective function. We then combine our novel attacks with two complementary existing ones to form a parameter-free, computationally affordable and user-independent ensemble of attacks to test adversarial robustness. We apply our ensemble to over 50 models from papers published at recent top machine learning and computer vision venues. In all except one of the cases we achieve lower robust test accuracy than reported in these papers, often by more than $10\\%$, identifying several broken defenses."
    },
    {
        "paperId": "1d81e7f428fea2b2e15ee3a96fe843ca603acc4c",
        "url": "https://www.semanticscholar.org/paper/1d81e7f428fea2b2e15ee3a96fe843ca603acc4c",
        "title": "Simple and Deep Graph Convolutional Networks",
        "venue": "International Conference on Machine Learning",
        "year": 2020,
        "citationCount": 1780,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2007.02133, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2108633355",
                "name": "Ming Chen"
            },
            {
                "authorId": "12457830",
                "name": "Zhewei Wei"
            },
            {
                "authorId": "3251920",
                "name": "Zengfeng Huang"
            },
            {
                "authorId": "1696332",
                "name": "Bolin Ding"
            },
            {
                "authorId": "2110479359",
                "name": "Yaliang Li"
            }
        ],
        "abstract": "Graph convolutional networks (GCNs) are a powerful deep learning approach for graph-structured data. Recently, GCNs and subsequent variants have shown superior performance in various application areas on real-world datasets. Despite their success, most of the current GCN models are shallow, due to the {\\em over-smoothing} problem. In this paper, we study the problem of designing and analyzing deep graph convolutional networks. We propose the GCNII, an extension of the vanilla GCN model with two simple yet effective techniques: {\\em Initial residual} and {\\em Identity mapping}. We provide theoretical and empirical evidence that the two techniques effectively relieves the problem of over-smoothing. Our experiments show that the deep GCNII model outperforms the state-of-the-art methods on various semi- and full-supervised tasks. Code is available at this https URL ."
    },
    {
        "paperId": "1f3c381eedfe8914b81e93070bfdb00cf86ac943",
        "url": "https://www.semanticscholar.org/paper/1f3c381eedfe8914b81e93070bfdb00cf86ac943",
        "title": "Contrastive Multi-View Representation Learning on Graphs",
        "venue": "International Conference on Machine Learning",
        "year": 2020,
        "citationCount": 1559,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2006.05582, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2443055",
                "name": "Kaveh Hassani"
            },
            {
                "authorId": "1516916058",
                "name": "Amir Hosein Khas Ahmadi"
            }
        ],
        "abstract": "We introduce a self-supervised approach for learning node and graph level representations by contrasting structural views of graphs. We show that unlike visual representation learning, increasing the number of views to more than two or contrasting multi-scale encodings do not improve performance, and the best performance is achieved by contrasting encodings from first-order neighbors and a graph diffusion. We achieve new state-of-the-art results in self-supervised learning on 8 out of 8 node and graph classification benchmarks under the linear evaluation protocol. For example, on Cora (node) and Reddit-Binary (graph) classification benchmarks, we achieve 86.8% and 84.5% accuracy, which are 5.5% and 2.4% relative improvements over previous state-of-the-art. When compared to supervised baselines, our approach outperforms them in 4 out of 8 benchmarks. Source code is released at: this https URL"
    },
    {
        "paperId": "31949039a48961f939ac50440c3b4b8504fccceb",
        "url": "https://www.semanticscholar.org/paper/31949039a48961f939ac50440c3b4b8504fccceb",
        "title": "Ditto: Fair and Robust Federated Learning Through Personalization",
        "venue": "International Conference on Machine Learning",
        "year": 2020,
        "citationCount": 1145,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2012.04221, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2118910109",
                "name": "Tian Li"
            },
            {
                "authorId": "51305275",
                "name": "Shengyuan Hu"
            },
            {
                "authorId": "1791052",
                "name": "Ahmad Beirami"
            },
            {
                "authorId": "145260024",
                "name": "Virginia Smith"
            }
        ],
        "abstract": "Fairness and robustness are two important concerns for federated learning systems. In this work, we identify that robustness to data and model poisoning attacks and fairness, measured as the uniformity of performance across devices, are competing constraints in statistically heterogeneous networks. To address these constraints, we propose employing a simple, general framework for personalized federated learning, Ditto, that can inherently provide fairness and robustness benefits, and develop a scalable solver for it. Theoretically, we analyze the ability of Ditto to achieve fairness and robustness simultaneously on a class of linear problems. Empirically, across a suite of federated datasets, we show that Ditto not only achieves competitive performance relative to recent personalization methods, but also enables more accurate, robust, and fair models relative to state-of-the-art fair or robust baselines."
    },
    {
        "paperId": "3621fff4a1c791901ea4a1359c10575193ec712d",
        "url": "https://www.semanticscholar.org/paper/3621fff4a1c791901ea4a1359c10575193ec712d",
        "title": "Out-of-Distribution Generalization via Risk Extrapolation (REx)",
        "venue": "International Conference on Machine Learning",
        "year": 2020,
        "citationCount": 1082,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2003.00688, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "145055042",
                "name": "David Krueger"
            },
            {
                "authorId": "24130593",
                "name": "Ethan Caballero"
            },
            {
                "authorId": "51919565",
                "name": "J. Jacobsen"
            },
            {
                "authorId": "2111672235",
                "name": "Amy Zhang"
            },
            {
                "authorId": "1737610",
                "name": "Jonathan Binas"
            },
            {
                "authorId": "10712297",
                "name": "Rmi Le Priol"
            },
            {
                "authorId": "1760871",
                "name": "Aaron C. Courville"
            }
        ],
        "abstract": "Generalizing outside of the training distribution is an open challenge for current machine learning systems. A weak form of out-of-distribution (OoD) generalization is the ability to successfully interpolate between multiple observed distributions. One way to achieve this is through robust optimization, which seeks to minimize the worst-case risk over convex combinations of the training distributions. However, a much stronger form of OoD generalization is the ability of models to extrapolate beyond the distributions observed during training. In pursuit of strong OoD generalization, we introduce the principle of Risk Extrapolation (REx). REx can be viewed as encouraging robustness over affine combinations of training risks, by encouraging strict equality between training risks. We show conceptually how this principle enables extrapolation, and demonstrate the effectiveness and scalability of instantiations of REx on various OoD generalization tasks. Our code can be found at this https URL."
    },
    {
        "paperId": "3a24bfb77ed271fef948058e414850f89b0955a7",
        "url": "https://www.semanticscholar.org/paper/3a24bfb77ed271fef948058e414850f89b0955a7",
        "title": "Concept Bottleneck Models",
        "venue": "International Conference on Machine Learning",
        "year": 2020,
        "citationCount": 1048,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2007.04612, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2572525",
                "name": "Pang Wei Koh"
            },
            {
                "authorId": "2117824274",
                "name": "Thao Nguyen"
            },
            {
                "authorId": "103603264",
                "name": "Y. S. Tang"
            },
            {
                "authorId": "1776721",
                "name": "Stephen Mussmann"
            },
            {
                "authorId": "145192191",
                "name": "E. Pierson"
            },
            {
                "authorId": "3351164",
                "name": "Been Kim"
            },
            {
                "authorId": "145419642",
                "name": "Percy Liang"
            }
        ],
        "abstract": "We seek to learn models that we can interact with using high-level concepts: if the model did not think there was a bone spur in the x-ray, would it still predict severe arthritis? State-of-the-art models today do not typically support the manipulation of concepts like the existence of bone spurs, as they are trained end-to-end to go directly from raw input (e.g., pixels) to output (e.g., arthritis severity). We revisit the classic idea of first predicting concepts that are provided at training time, and then using these concepts to predict the label. By construction, we can intervene on these concept bottleneck models by editing their predicted concept values and propagating these changes to the final prediction. On x-ray grading and bird identification, concept bottleneck models achieve competitive accuracy with standard end-to-end models, while enabling interpretation in terms of high-level clinical concepts (bone spurs) or bird attributes (wing color). These models also allow for richer human-model interaction: accuracy improves significantly if we can correct model mistakes on concepts at test time."
    },
    {
        "paperId": "40848b41ed8c9c255ecd8a920006877691b52d03",
        "url": "https://www.semanticscholar.org/paper/40848b41ed8c9c255ecd8a920006877691b52d03",
        "title": "WILDS: A Benchmark of in-the-Wild Distribution Shifts",
        "venue": "International Conference on Machine Learning",
        "year": 2020,
        "citationCount": 1644,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2012.07421, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2276205042",
                "name": "Pang Wei Koh"
            },
            {
                "authorId": "2389237",
                "name": "Shiori Sagawa"
            },
            {
                "authorId": "66977188",
                "name": "H. Marklund"
            },
            {
                "authorId": "2114080615",
                "name": "Sang Michael Xie"
            },
            {
                "authorId": "2281037751",
                "name": "Marvin Zhang"
            },
            {
                "authorId": "1693411",
                "name": "Akshay Balsubramani"
            },
            {
                "authorId": "2146241852",
                "name": "Weihua Hu"
            },
            {
                "authorId": "19168196",
                "name": "Michihiro Yasunaga"
            },
            {
                "authorId": "2069956950",
                "name": "R. L. Phillips"
            },
            {
                "authorId": "2280985094",
                "name": "Irena Gao"
            },
            {
                "authorId": "2280998729",
                "name": "Tony Lee"
            },
            {
                "authorId": "2280985491",
                "name": "Etiene David"
            },
            {
                "authorId": "2351671",
                "name": "I. Stavness"
            },
            {
                "authorId": "2280992321",
                "name": "Wei Guo"
            },
            {
                "authorId": "3162326",
                "name": "Berton A. Earnshaw"
            },
            {
                "authorId": "1983211",
                "name": "I. Haque"
            },
            {
                "authorId": "2134791809",
                "name": "Sara Beery"
            },
            {
                "authorId": "2251205420",
                "name": "J. Leskovec"
            },
            {
                "authorId": "2844479",
                "name": "Anshul B Kundaje"
            },
            {
                "authorId": "2277459687",
                "name": "Emma Pierson"
            },
            {
                "authorId": "2249615151",
                "name": "Sergey Levine"
            },
            {
                "authorId": "46881670",
                "name": "Chelsea Finn"
            },
            {
                "authorId": "2249641250",
                "name": "Percy Liang"
            }
        ],
        "abstract": "Distribution shifts -- where the training distribution differs from the test distribution -- can substantially degrade the accuracy of machine learning (ML) systems deployed in the wild. Despite their ubiquity in the real-world deployments, these distribution shifts are under-represented in the datasets widely used in the ML community today. To address this gap, we present WILDS, a curated benchmark of 10 datasets reflecting a diverse range of distribution shifts that naturally arise in real-world applications, such as shifts across hospitals for tumor identification; across camera traps for wildlife monitoring; and across time and location in satellite imaging and poverty mapping. On each dataset, we show that standard training yields substantially lower out-of-distribution than in-distribution performance. This gap remains even with models trained by existing methods for tackling distribution shifts, underscoring the need for new methods for training models that are more robust to the types of distribution shifts that arise in practice. To facilitate method development, we provide an open-source package that automates dataset loading, contains default model architectures and hyperparameters, and standardizes evaluations. Code and leaderboards are available at https://wilds.stanford.edu."
    },
    {
        "paperId": "6e1bb490ae54b42f13d14d69b2012edda4664949",
        "url": "https://www.semanticscholar.org/paper/6e1bb490ae54b42f13d14d69b2012edda4664949",
        "title": "Do We Really Need to Access the Source Data? Source Hypothesis Transfer for Unsupervised Domain Adaptation",
        "venue": "International Conference on Machine Learning",
        "year": 2020,
        "citationCount": 1493,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2002.08546, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "143932869",
                "name": "Jian Liang"
            },
            {
                "authorId": "49592281",
                "name": "Dapeng Hu"
            },
            {
                "authorId": "33221685",
                "name": "Jiashi Feng"
            }
        ],
        "abstract": "Unsupervised domain adaptation (UDA) aims to leverage the knowledge learned from a labeled source dataset to solve similar tasks in a new unlabeled domain. Prior UDA methods typically require to access the source data when learning to adapt the model, making them risky and inefficient for decentralized private data. This work tackles a practical setting where only a trained source model is available and investigates how we can effectively utilize such a model without source data to solve UDA problems. We propose a simple yet generic representation learning framework, named \\emph{Source HypOthesis Transfer} (SHOT). SHOT freezes the classifier module (hypothesis) of the source model and learns the target-specific feature extraction module by exploiting both information maximization and self-supervised pseudo-labeling to implicitly align representations from the target domains to the source hypothesis. To verify its versatility, we evaluate SHOT in a variety of adaptation cases including closed-set, partial-set, and open-set domain adaptation. Experiments indicate that SHOT yields state-of-the-art results among multiple domain adaptation benchmarks."
    },
    {
        "paperId": "6f68e1bb253925d8431588555d3010419f322e04",
        "url": "https://www.semanticscholar.org/paper/6f68e1bb253925d8431588555d3010419f322e04",
        "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention",
        "venue": "International Conference on Machine Learning",
        "year": 2020,
        "citationCount": 2278,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2006.16236, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3493855",
                "name": "Angelos Katharopoulos"
            },
            {
                "authorId": "2992087",
                "name": "Apoorv Vyas"
            },
            {
                "authorId": "143958923",
                "name": "Nikolaos Pappas"
            },
            {
                "authorId": "116272138",
                "name": "Franccois Fleuret"
            }
        ],
        "abstract": "Transformers achieve remarkable performance in several tasks but due to their quadratic complexity, with respect to the input's length, they are prohibitively slow for very long sequences. To address this limitation, we express the self-attention as a linear dot-product of kernel feature maps and make use of the associativity property of matrix products to reduce the complexity from $\\mathcal{O}\\left(N^2\\right)$ to $\\mathcal{O}\\left(N\\right)$, where $N$ is the sequence length. We show that this formulation permits an iterative implementation that dramatically accelerates autoregressive transformers and reveals their relationship to recurrent neural networks. Our linear transformers achieve similar performance to vanilla transformers and they are up to 4000x faster on autoregressive prediction of very long sequences."
    },
    {
        "paperId": "7af72a461ed7cda180e7eab878efd5f35d79bbf4",
        "url": "https://www.semanticscholar.org/paper/7af72a461ed7cda180e7eab878efd5f35d79bbf4",
        "title": "A Simple Framework for Contrastive Learning of Visual Representations",
        "venue": "International Conference on Machine Learning",
        "year": 2020,
        "citationCount": 22189,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2002.05709, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "145358498",
                "name": "Ting Chen"
            },
            {
                "authorId": "40464924",
                "name": "Simon Kornblith"
            },
            {
                "authorId": "144739074",
                "name": "Mohammad Norouzi"
            },
            {
                "authorId": "1695689",
                "name": "Geoffrey E. Hinton"
            }
        ],
        "abstract": "This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy, outperforming AlexNet with 100X fewer labels."
    },
    {
        "paperId": "832fff14d2ed50eb7969c4c4b976c35776548f56",
        "url": "https://www.semanticscholar.org/paper/832fff14d2ed50eb7969c4c4b976c35776548f56",
        "title": "REALM: Retrieval-Augmented Language Model Pre-Training",
        "venue": "International Conference on Machine Learning",
        "year": 2020,
        "citationCount": 2561,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2002.08909, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2091768",
                "name": "Kelvin Guu"
            },
            {
                "authorId": "2544107",
                "name": "Kenton Lee"
            },
            {
                "authorId": "9941702",
                "name": "Zora Tung"
            },
            {
                "authorId": "2616463",
                "name": "Panupong Pasupat"
            },
            {
                "authorId": "1744179",
                "name": "Ming-Wei Chang"
            }
        ],
        "abstract": "Language model pre-training has been shown to capture a surprising amount of world knowledge, crucial for NLP tasks such as question answering. However, this knowledge is stored implicitly in the parameters of a neural network, requiring ever-larger networks to cover more facts. \nTo capture knowledge in a more modular and interpretable way, we augment language model pre-training with a latent knowledge retriever, which allows the model to retrieve and attend over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference. For the first time, we show how to pre-train such a knowledge retriever in an unsupervised manner, using masked language modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents. \nWe demonstrate the effectiveness of Retrieval-Augmented Language Model pre-training (REALM) by fine-tuning on the challenging task of Open-domain Question Answering (Open-QA). We compare against state-of-the-art models for both explicit and implicit knowledge storage on three popular Open-QA benchmarks, and find that we outperform all previous methods by a significant margin (4-16% absolute accuracy), while also providing qualitative benefits such as interpretability and modularity."
    },
    {
        "paperId": "9a56ab8b1aba50dc2fea3cf4b531d30891a88ba9",
        "url": "https://www.semanticscholar.org/paper/9a56ab8b1aba50dc2fea3cf4b531d30891a88ba9",
        "title": "Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere",
        "venue": "International Conference on Machine Learning",
        "year": 2020,
        "citationCount": 2163,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2005.10242, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3978031",
                "name": "Tongzhou Wang"
            },
            {
                "authorId": "2094770",
                "name": "Phillip Isola"
            }
        ],
        "abstract": "Contrastive representation learning has been outstandingly successful in practice. In this work, we identify two key properties related to the contrastive loss: (1) alignment (closeness) of features from positive pairs, and (2) uniformity of the induced distribution of the (normalized) features on the hypersphere. We prove that, asymptotically, the contrastive loss optimizes these properties, and analyze their positive effects on downstream tasks. Empirically, we introduce an optimizable metric to quantify each property. Extensive experiments on standard vision and language datasets confirm the strong agreement between both metrics and downstream task performance. Remarkably, directly optimizing for these two metrics leads to representations with comparable or better performance at downstream tasks than contrastive learning. \nProject Page: this https URL \nCode: this https URL , this https URL"
    },
    {
        "paperId": "9efb64f20ab1f157ca9f4050d4aaacf6c3f9b2b2",
        "url": "https://www.semanticscholar.org/paper/9efb64f20ab1f157ca9f4050d4aaacf6c3f9b2b2",
        "title": "CURL: Contrastive Unsupervised Representations for Reinforcement Learning",
        "venue": "International Conference on Machine Learning",
        "year": 2020,
        "citationCount": 1199,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2004.04136, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "41207614",
                "name": "A. Srinivas"
            },
            {
                "authorId": "51093256",
                "name": "M. Laskin"
            },
            {
                "authorId": "1689992",
                "name": "P. Abbeel"
            }
        ],
        "abstract": "We present CURL: Contrastive Unsupervised Representations for Reinforcement Learning. CURL extracts high-level features from raw pixels using contrastive learning and performs off-policy control on top of the extracted features. CURL outperforms prior pixel-based methods, both model-based and model-free, on complex tasks in the DeepMind Control Suite and Atari Games showing 1.9x and 1.2x performance gains at the 100K environment and interaction steps benchmarks respectively. On the DeepMind Control Suite, CURL is the first image-based algorithm to nearly match the sample-efficiency of methods that use state-based features. Our code is open-sourced and available at this https URL."
    },
    {
        "paperId": "ad7ddcc14984caae308c397f1a589aae75d4ab71",
        "url": "https://www.semanticscholar.org/paper/ad7ddcc14984caae308c397f1a589aae75d4ab71",
        "title": "Training data-efficient image transformers & distillation through attention",
        "venue": "International Conference on Machine Learning",
        "year": 2020,
        "citationCount": 8183,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2012.12877, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2113243762",
                "name": "Hugo Touvron"
            },
            {
                "authorId": "51021910",
                "name": "M. Cord"
            },
            {
                "authorId": "3271933",
                "name": "Matthijs Douze"
            },
            {
                "authorId": "1403239967",
                "name": "Francisco Massa"
            },
            {
                "authorId": "3469062",
                "name": "Alexandre Sablayrolles"
            },
            {
                "authorId": "2065248680",
                "name": "Herv'e J'egou"
            }
        ],
        "abstract": "Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. However, these visual transformers are pre-trained with hundreds of millions of images using an expensive infrastructure, thereby limiting their adoption. In this work, we produce a competitive convolution-free transformer by training on Imagenet only. We train them on a single computer in less than 3 days. Our reference vision transformer (86M parameters) achieves top-1 accuracy of 83.1% (single-crop evaluation) on ImageNet with no external data. More importantly, we introduce a teacher-student strategy specific to transformers. It relies on a distillation token ensuring that the student learns from the teacher through attention. We show the interest of this token-based distillation, especially when using a convnet as a teacher. This leads us to report results competitive with convnets for both Imagenet (where we obtain up to 85.2% accuracy) and when transferring to other tasks. We share our code and models."
    },
    {
        "paperId": "b45d656ac8cc2e940609580cf291ee76ffcac20a",
        "url": "https://www.semanticscholar.org/paper/b45d656ac8cc2e940609580cf291ee76ffcac20a",
        "title": "On Layer Normalization in the Transformer Architecture",
        "venue": "International Conference on Machine Learning",
        "year": 2020,
        "citationCount": 1215,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2002.04745, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "51130380",
                "name": "Ruibin Xiong"
            },
            {
                "authorId": "79327071",
                "name": "Yunchang Yang"
            },
            {
                "authorId": "1391126980",
                "name": "Di He"
            },
            {
                "authorId": null,
                "name": "Kai Zheng"
            },
            {
                "authorId": "150311931",
                "name": "Shuxin Zheng"
            },
            {
                "authorId": "50461046",
                "name": "Chen Xing"
            },
            {
                "authorId": "2973831",
                "name": "Huishuai Zhang"
            },
            {
                "authorId": "37510256",
                "name": "Yanyan Lan"
            },
            {
                "authorId": "24952249",
                "name": "Liwei Wang"
            },
            {
                "authorId": "2110264337",
                "name": "Tie-Yan Liu"
            }
        ],
        "abstract": "The Transformer is widely used in natural language processing tasks. To train a Transformer however, one usually needs a carefully designed learning rate warm-up stage, which is shown to be crucial to the final performance but will slow down the optimization and bring more hyperparameter tunings. In this paper, we first study theoretically why the learning rate warm-up stage is essential and show that the location of layer normalization matters. Specifically, we prove with mean field theory that at initialization, for the original-designed Post-LN Transformer, which places the layer normalization between the residual blocks, the expected gradients of the parameters near the output layer are large. Therefore, using a large learning rate on those gradients makes the training unstable. The warm-up stage is practically helpful for avoiding this problem. On the other hand, our theory also shows that if the layer normalization is put inside the residual blocks (recently proposed as Pre-LN Transformer), the gradients are well-behaved at initialization. This motivates us to remove the warm-up stage for the training of Pre-LN Transformers. We show in our experiments that Pre-LN Transformers without the warm-up stage can reach comparable results with baselines while requiring significantly less training time and hyper-parameter tuning on a wide range of applications."
    },
    {
        "paperId": "ba4a34680e09e77984624c95f5245d91b54373f6",
        "url": "https://www.semanticscholar.org/paper/ba4a34680e09e77984624c95f5245d91b54373f6",
        "title": "XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalization",
        "venue": "International Conference on Machine Learning",
        "year": 2020,
        "citationCount": 1065,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2003.11080, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2149221827",
                "name": "Junjie Hu"
            },
            {
                "authorId": "2884561",
                "name": "Sebastian Ruder"
            },
            {
                "authorId": "9356387",
                "name": "Aditya Siddhant"
            },
            {
                "authorId": "1700325",
                "name": "Graham Neubig"
            },
            {
                "authorId": "2345617",
                "name": "Orhan Firat"
            },
            {
                "authorId": "145657834",
                "name": "Melvin Johnson"
            }
        ],
        "abstract": "Much recent progress in applications of machine learning models to NLP has been driven by benchmarks that evaluate models across a wide variety of tasks. However, these broad-coverage benchmarks have been mostly limited to English, and despite an increasing interest in multilingual models, a benchmark that enables the comprehensive evaluation of such methods on a diverse range of languages and tasks is still missing. To this end, we introduce the Cross-lingual TRansfer Evaluation of Multilingual Encoders XTREME benchmark, a multi-task benchmark for evaluating the cross-lingual generalization capabilities of multilingual representations across 40 languages and 9 tasks. We demonstrate that while models tested on English reach human performance on many tasks, there is still a sizable gap in the performance of cross-lingually transferred models, particularly on syntactic and sentence retrieval tasks. There is also a wide spread of results across languages. We release the benchmark to encourage research on cross-lingual learning methods that transfer linguistic knowledge across a diverse and representative set of languages and tasks."
    },
    {
        "paperId": "bc022dbb37b1bbf3905a7404d19c03ccbf6b81a8",
        "url": "https://www.semanticscholar.org/paper/bc022dbb37b1bbf3905a7404d19c03ccbf6b81a8",
        "title": "Generative Pretraining From Pixels",
        "venue": "International Conference on Machine Learning",
        "year": 2020,
        "citationCount": 1696,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "2108828435",
                "name": "Mark Chen"
            },
            {
                "authorId": "38909097",
                "name": "Alec Radford"
            },
            {
                "authorId": "49387725",
                "name": "Jeff Wu"
            },
            {
                "authorId": "35450887",
                "name": "Heewoo Jun"
            },
            {
                "authorId": "6515819",
                "name": "Prafulla Dhariwal"
            },
            {
                "authorId": "150970919",
                "name": "D. Luan"
            },
            {
                "authorId": "1701686",
                "name": "I. Sutskever"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "c529f5b08675f787cdcc094ee495239592339f82",
        "url": "https://www.semanticscholar.org/paper/c529f5b08675f787cdcc094ee495239592339f82",
        "title": "Learning to Simulate Complex Physics with Graph Networks",
        "venue": "International Conference on Machine Learning",
        "year": 2020,
        "citationCount": 1304,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2002.09405, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1398105826",
                "name": "Alvaro Sanchez-Gonzalez"
            },
            {
                "authorId": "2069002234",
                "name": "Jonathan Godwin"
            },
            {
                "authorId": "2054956",
                "name": "T. Pfaff"
            },
            {
                "authorId": "83539859",
                "name": "Rex Ying"
            },
            {
                "authorId": "1702139",
                "name": "J. Leskovec"
            },
            {
                "authorId": "2019153",
                "name": "P. Battaglia"
            }
        ],
        "abstract": "Here we present a machine learning framework and model implementation that can learn to simulate a wide variety of challenging physical domains, involving fluids, rigid solids, and deformable materials interacting with one another. Our framework---which we term \"Graph Network-based Simulators\" (GNS)---represents the state of a physical system with particles, expressed as nodes in a graph, and computes dynamics via learned message-passing. Our results show that our model can generalize from single-timestep predictions with thousands of particles during training, to different initial conditions, thousands of timesteps, and at least an order of magnitude more particles at test time. Our model was robust to hyperparameter choices across various evaluation metrics: the main determinants of long-term performance were the number of message-passing steps, and mitigating the accumulation of error by corrupting the training data with noise. Our GNS framework advances the state-of-the-art in learned physical simulation, and holds promise for solving a wide range of complex forward and inverse problems."
    },
    {
        "paperId": "002c256d30d6be4b23d365a8de8ae0e67e4c9641",
        "url": "https://www.semanticscholar.org/paper/002c256d30d6be4b23d365a8de8ae0e67e4c9641",
        "title": "Improving language models by retrieving from trillions of tokens",
        "venue": "International Conference on Machine Learning",
        "year": 2021,
        "citationCount": 1402,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2112.04426, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "148016269",
                "name": "Sebastian Borgeaud"
            },
            {
                "authorId": "1697879",
                "name": "A. Mensch"
            },
            {
                "authorId": "46616544",
                "name": "Jordan Hoffmann"
            },
            {
                "authorId": "2072572294",
                "name": "Trevor Cai"
            },
            {
                "authorId": "2143538252",
                "name": "Eliza Rutherford"
            },
            {
                "authorId": "2143434227",
                "name": "Katie Millican"
            },
            {
                "authorId": "47568983",
                "name": "George van den Driessche"
            },
            {
                "authorId": "143783339",
                "name": "Jean-Baptiste Lespiau"
            },
            {
                "authorId": "2143374656",
                "name": "Bogdan Damoc"
            },
            {
                "authorId": "31993415",
                "name": "Aidan Clark"
            },
            {
                "authorId": "40550616",
                "name": "Diego de Las Casas"
            },
            {
                "authorId": "40895205",
                "name": "Aurelia Guy"
            },
            {
                "authorId": "10698483",
                "name": "Jacob Menick"
            },
            {
                "authorId": "81387328",
                "name": "Roman Ring"
            },
            {
                "authorId": "4629007",
                "name": "T. Hennigan"
            },
            {
                "authorId": "2148653469",
                "name": "Saffron Huang"
            },
            {
                "authorId": "108173905",
                "name": "Lorenzo Maggiore"
            },
            {
                "authorId": "2115601070",
                "name": "Chris Jones"
            },
            {
                "authorId": "51042571",
                "name": "Albin Cassirer"
            },
            {
                "authorId": "2065040422",
                "name": "Andy Brock"
            },
            {
                "authorId": "35550664",
                "name": "Michela Paganini"
            },
            {
                "authorId": "2060655766",
                "name": "G. Irving"
            },
            {
                "authorId": "1689108",
                "name": "O. Vinyals"
            },
            {
                "authorId": "2217144",
                "name": "Simon Osindero"
            },
            {
                "authorId": "34838386",
                "name": "K. Simonyan"
            },
            {
                "authorId": "34269227",
                "name": "Jack W. Rae"
            },
            {
                "authorId": "152585800",
                "name": "Erich Elsen"
            },
            {
                "authorId": "2175946",
                "name": "L. Sifre"
            }
        ],
        "abstract": "We enhance auto-regressive language models by conditioning on document chunks retrieved from a large corpus, based on local similarity with preceding tokens. With a $2$ trillion token database, our Retrieval-Enhanced Transformer (RETRO) obtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite using 25$\\times$ fewer parameters. After fine-tuning, RETRO performance translates to downstream knowledge-intensive tasks such as question answering. RETRO combines a frozen Bert retriever, a differentiable encoder and a chunked cross-attention mechanism to predict tokens based on an order of magnitude more data than what is typically consumed during training. We typically train RETRO from scratch, yet can also rapidly RETROfit pre-trained transformers with retrieval and still achieve good performance. Our work opens up new avenues for improving language models through explicit memory at unprecedented scale."
    },
    {
        "paperId": "0839722fb5369c0abaff8515bfc08299efc790a1",
        "url": "https://www.semanticscholar.org/paper/0839722fb5369c0abaff8515bfc08299efc790a1",
        "title": "ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision",
        "venue": "International Conference on Machine Learning",
        "year": 2021,
        "citationCount": 2089,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2102.03334, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2382193",
                "name": "Wonjae Kim"
            },
            {
                "authorId": "65842988",
                "name": "Bokyung Son"
            },
            {
                "authorId": "14972026",
                "name": "Ildoo Kim"
            }
        ],
        "abstract": "Vision-and-Language Pre-training (VLP) has improved performance on various joint vision-and-language downstream tasks. Current approaches to VLP heavily rely on image feature extraction processes, most of which involve region supervision (e.g., object detection) and the convolutional architecture (e.g., ResNet). Although disregarded in the literature, we find it problematic in terms of both (1) efficiency/speed, that simply extracting input features requires much more computation than the multimodal interaction steps; and (2) expressive power, as it is upper bounded to the expressive power of the visual embedder and its predefined visual vocabulary. In this paper, we present a minimal VLP model, Vision-and-Language Transformer (ViLT), monolithic in the sense that the processing of visual inputs is drastically simplified to just the same convolution-free manner that we process textual inputs. We show that ViLT is up to tens of times faster than previous VLP models, yet with competitive or better downstream task performance. Our code and pre-trained weights are available at https://github.com/dandelin/vilt."
    },
    {
        "paperId": "141a5033d9994242b18bb3b217e79582f1ee9306",
        "url": "https://www.semanticscholar.org/paper/141a5033d9994242b18bb3b217e79582f1ee9306",
        "title": "Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision",
        "venue": "International Conference on Machine Learning",
        "year": 2021,
        "citationCount": 4840,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2102.05918, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2082436672",
                "name": "Chao Jia"
            },
            {
                "authorId": "2781059",
                "name": "Yinfei Yang"
            },
            {
                "authorId": "2327118351",
                "name": "Ye Xia"
            },
            {
                "authorId": "2109060286",
                "name": "Yi-Ting Chen"
            },
            {
                "authorId": "27456119",
                "name": "Zarana Parekh"
            },
            {
                "authorId": "143950636",
                "name": "Hieu Pham"
            },
            {
                "authorId": "2827616",
                "name": "Quoc V. Le"
            },
            {
                "authorId": "2305450",
                "name": "Yun-Hsuan Sung"
            },
            {
                "authorId": "2110121852",
                "name": "Zhen Li"
            },
            {
                "authorId": "2066508193",
                "name": "Tom Duerig"
            }
        ],
        "abstract": "Pre-trained representations are becoming crucial for many NLP and perception tasks. While representation learning in NLP has transitioned to training on raw text without human annotations, visual and vision-language representations still rely heavily on curated training datasets that are expensive or require expert knowledge. For vision applications, representations are mostly learned using datasets with explicit class labels such as ImageNet or OpenImages. For vision-language, popular datasets like Conceptual Captions, MSCOCO, or CLIP all involve a non-trivial data collection (and cleaning) process. This costly curation process limits the size of datasets and hence hinders the scaling of trained models. In this paper, we leverage a noisy dataset of over one billion image alt-text pairs, obtained without expensive filtering or post-processing steps in the Conceptual Captions dataset. A simple dual-encoder architecture learns to align visual and language representations of the image and text pairs using a contrastive loss. We show that the scale of our corpus can make up for its noise and leads to state-of-the-art representations even with such a simple learning scheme. Our visual representation achieves strong performance when transferred to classification tasks such as ImageNet and VTAB. The aligned visual and language representations enables zero-shot image classification and also set new state-of-the-art results on Flickr30K and MSCOCO image-text retrieval benchmarks, even when compared with more sophisticated cross-attention models. The representations also enable cross-modality search with complex text and text + image queries."
    },
    {
        "paperId": "2cd605106b88c85d7d8b865b1ef0f8c8293debf1",
        "url": "https://www.semanticscholar.org/paper/2cd605106b88c85d7d8b865b1ef0f8c8293debf1",
        "title": "Zero-Shot Text-to-Image Generation",
        "venue": "International Conference on Machine Learning",
        "year": 2021,
        "citationCount": 5931,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2102.12092, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1992922591",
                "name": "A. Ramesh"
            },
            {
                "authorId": "2068123790",
                "name": "Mikhail Pavlov"
            },
            {
                "authorId": "40087786",
                "name": "Gabriel Goh"
            },
            {
                "authorId": "145565184",
                "name": "Scott Gray"
            },
            {
                "authorId": "153387869",
                "name": "Chelsea Voss"
            },
            {
                "authorId": "38909097",
                "name": "Alec Radford"
            },
            {
                "authorId": "2108828435",
                "name": "Mark Chen"
            },
            {
                "authorId": "1701686",
                "name": "I. Sutskever"
            }
        ],
        "abstract": "Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. We describe a simple approach for this task based on a transformer that autoregressively models the text and image tokens as a single stream of data. With sufficient data and scale, our approach is competitive with previous domain-specific models when evaluated in a zero-shot fashion."
    },
    {
        "paperId": "3a173a2042b3d18d21f59187f1e5d7d83e791635",
        "url": "https://www.semanticscholar.org/paper/3a173a2042b3d18d21f59187f1e5d7d83e791635",
        "title": "SimAM: A Simple, Parameter-Free Attention Module for Convolutional Neural Networks",
        "venue": "International Conference on Machine Learning",
        "year": 2021,
        "citationCount": 1390,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "49576171",
                "name": "Lingxiao Yang"
            },
            {
                "authorId": "2238393356",
                "name": "Ru-Yuan Zhang"
            },
            {
                "authorId": "2107914265",
                "name": "Lida Li"
            },
            {
                "authorId": "46397019",
                "name": "Xiaohua Xie"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4",
        "url": "https://www.semanticscholar.org/paper/6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4",
        "title": "Learning Transferable Visual Models From Natural Language Supervision",
        "venue": "International Conference on Machine Learning",
        "year": 2021,
        "citationCount": 40671,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2103.00020, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "38909097",
                "name": "Alec Radford"
            },
            {
                "authorId": "2110935237",
                "name": "Jong Wook Kim"
            },
            {
                "authorId": "2004021329",
                "name": "Chris Hallacy"
            },
            {
                "authorId": "1992922591",
                "name": "A. Ramesh"
            },
            {
                "authorId": "40087786",
                "name": "Gabriel Goh"
            },
            {
                "authorId": "144517868",
                "name": "Sandhini Agarwal"
            },
            {
                "authorId": "144864359",
                "name": "Girish Sastry"
            },
            {
                "authorId": "119609682",
                "name": "Amanda Askell"
            },
            {
                "authorId": "2051714782",
                "name": "Pamela Mishkin"
            },
            {
                "authorId": "2115193883",
                "name": "Jack Clark"
            },
            {
                "authorId": "2064404342",
                "name": "Gretchen Krueger"
            },
            {
                "authorId": "1701686",
                "name": "I. Sutskever"
            }
        ],
        "abstract": "State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP."
    },
    {
        "paperId": "7002ae048e4b8c9133a55428441e8066070995cb",
        "url": "https://www.semanticscholar.org/paper/7002ae048e4b8c9133a55428441e8066070995cb",
        "title": "GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models",
        "venue": "International Conference on Machine Learning",
        "year": 2021,
        "citationCount": 4338,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2112.10741, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "38967461",
                "name": "Alex Nichol"
            },
            {
                "authorId": "6515819",
                "name": "Prafulla Dhariwal"
            },
            {
                "authorId": "1992922591",
                "name": "A. Ramesh"
            },
            {
                "authorId": "67311962",
                "name": "Pranav Shyam"
            },
            {
                "authorId": "2051714782",
                "name": "Pamela Mishkin"
            },
            {
                "authorId": "39593364",
                "name": "Bob McGrew"
            },
            {
                "authorId": "1701686",
                "name": "I. Sutskever"
            },
            {
                "authorId": "2108828435",
                "name": "Mark Chen"
            }
        ],
        "abstract": "Diffusion models have recently been shown to generate high-quality synthetic images, especially when paired with a guidance technique to trade off diversity for fidelity. We explore diffusion models for the problem of text-conditional image synthesis and compare two different guidance strategies: CLIP guidance and classifier-free guidance. We find that the latter is preferred by human evaluators for both photorealism and caption similarity, and often produces photorealistic samples. Samples from a 3.5 billion parameter text-conditional diffusion model using classifier-free guidance are favored by human evaluators to those from DALL-E, even when the latter uses expensive CLIP reranking. Additionally, we find that our models can be fine-tuned to perform image inpainting, enabling powerful text-driven image editing. We train a smaller model on a filtered dataset and release the code and weights at https://github.com/openai/glide-text2im."
    },
    {
        "paperId": "80d0116d77beeded0c23cf48946d9d10d4faee14",
        "url": "https://www.semanticscholar.org/paper/80d0116d77beeded0c23cf48946d9d10d4faee14",
        "title": "GLaM: Efficient Scaling of Language Models with Mixture-of-Experts",
        "venue": "International Conference on Machine Learning",
        "year": 2021,
        "citationCount": 1035,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2112.06905, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2140321952",
                "name": "Nan Du"
            },
            {
                "authorId": "2145438541",
                "name": "Yanping Huang"
            },
            {
                "authorId": "2555924",
                "name": "Andrew M. Dai"
            },
            {
                "authorId": "2058177533",
                "name": "Simon Tong"
            },
            {
                "authorId": "150077954",
                "name": "Dmitry Lepikhin"
            },
            {
                "authorId": "2145139570",
                "name": "Yuanzhong Xu"
            },
            {
                "authorId": "2048712",
                "name": "M. Krikun"
            },
            {
                "authorId": "2389316",
                "name": "Yanqi Zhou"
            },
            {
                "authorId": "40625240",
                "name": "Adams Wei Yu"
            },
            {
                "authorId": "2345617",
                "name": "Orhan Firat"
            },
            {
                "authorId": "2368067",
                "name": "Barret Zoph"
            },
            {
                "authorId": "2096916416",
                "name": "L. Fedus"
            },
            {
                "authorId": "40377863",
                "name": "Maarten Bosma"
            },
            {
                "authorId": "1389392654",
                "name": "Zongwei Zhou"
            },
            {
                "authorId": null,
                "name": "Tao Wang"
            },
            {
                "authorId": "2153608756",
                "name": "Yu Emma Wang"
            },
            {
                "authorId": "20825661",
                "name": "Kellie Webster"
            },
            {
                "authorId": "97905921",
                "name": "Marie Pellat"
            },
            {
                "authorId": "2148473059",
                "name": "Kevin Robinson"
            },
            {
                "authorId": "1398655031",
                "name": "K. Meier-Hellstern"
            },
            {
                "authorId": "2145151992",
                "name": "Toju Duke"
            },
            {
                "authorId": "2065639113",
                "name": "Lucas Dixon"
            },
            {
                "authorId": "1556095165",
                "name": "Kun Zhang"
            },
            {
                "authorId": "2827616",
                "name": "Quoc V. Le"
            },
            {
                "authorId": "48607963",
                "name": "Yonghui Wu"
            },
            {
                "authorId": "2545358",
                "name": "Z. Chen"
            },
            {
                "authorId": "2052275005",
                "name": "Claire Cui"
            }
        ],
        "abstract": "Scaling language models with more data, compute and parameters has driven significant progress in natural language processing. For example, thanks to scaling, GPT-3 was able to achieve strong results on in-context learning tasks. However, training these large dense models requires significant amounts of computing resources. In this paper, we propose and develop a family of language models named GLaM (Generalist Language Model), which uses a sparsely activated mixture-of-experts architecture to scale the model capacity while also incurring substantially less training cost compared to dense variants. The largest GLaM has 1.2 trillion parameters, which is approximately 7x larger than GPT-3. It consumes only 1/3 of the energy used to train GPT-3 and requires half of the computation flops for inference, while still achieving better overall zero-shot and one-shot performance across 29 NLP tasks."
    },
    {
        "paperId": "8a9d84d86ac0d76e63914802f9738325c3bece9c",
        "url": "https://www.semanticscholar.org/paper/8a9d84d86ac0d76e63914802f9738325c3bece9c",
        "title": "Barlow Twins: Self-Supervised Learning via Redundancy Reduction",
        "venue": "International Conference on Machine Learning",
        "year": 2021,
        "citationCount": 2731,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2103.03230, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3105120",
                "name": "Jure Zbontar"
            },
            {
                "authorId": "50780902",
                "name": "Li Jing"
            },
            {
                "authorId": "1806773",
                "name": "Ishan Misra"
            },
            {
                "authorId": "1688882",
                "name": "Yann LeCun"
            },
            {
                "authorId": "4789914",
                "name": "Stphane Deny"
            }
        ],
        "abstract": "Self-supervised learning (SSL) is rapidly closing the gap with supervised methods on large computer vision benchmarks. A successful approach to SSL is to learn embeddings which are invariant to distortions of the input sample. However, a recurring issue with this approach is the existence of trivial constant solutions. Most current methods avoid such solutions by careful implementation details. We propose an objective function that naturally avoids collapse by measuring the cross-correlation matrix between the outputs of two identical networks fed with distorted versions of a sample, and making it as close to the identity matrix as possible. This causes the embedding vectors of distorted versions of a sample to be similar, while minimizing the redundancy between the components of these vectors. The method is called Barlow Twins, owing to neuroscientist H. Barlow's redundancy-reduction principle applied to a pair of identical networks. Barlow Twins does not require large batches nor asymmetry between the network twins such as a predictor network, gradient stopping, or a moving average on the weight updates. Intriguingly it benefits from very high-dimensional output vectors. Barlow Twins outperforms previous methods on ImageNet for semi-supervised classification in the low-data regime, and is on par with current state of the art for ImageNet classification with a linear classifier head, and for transfer tasks of classification and object detection."
    },
    {
        "paperId": "8ea9cb53779a8c1bb0e53764f88669bd7edf38f0",
        "url": "https://www.semanticscholar.org/paper/8ea9cb53779a8c1bb0e53764f88669bd7edf38f0",
        "title": "E(n) Equivariant Graph Neural Networks",
        "venue": "International Conference on Machine Learning",
        "year": 2021,
        "citationCount": 1285,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2102.09844, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "73240341",
                "name": "Victor Garcia Satorras"
            },
            {
                "authorId": "65928943",
                "name": "Emiel Hoogeboom"
            },
            {
                "authorId": "1678311",
                "name": "M. Welling"
            }
        ],
        "abstract": "This paper introduces a new model to learn graph neural networks equivariant to rotations, translations, reflections and permutations called E(n)-Equivariant Graph Neural Networks (EGNNs). In contrast with existing methods, our work does not require computationally expensive higher-order representations in intermediate layers while it still achieves competitive or better performance. In addition, whereas existing methods are limited to equivariance on 3 dimensional spaces, our model is easily scaled to higher-dimensional spaces. We demonstrate the effectiveness of our method on dynamical systems modelling, representation learning in graph autoencoders and predicting molecular properties."
    },
    {
        "paperId": "8f8f73f0f208302546c825ed474432389ed63be4",
        "url": "https://www.semanticscholar.org/paper/8f8f73f0f208302546c825ed474432389ed63be4",
        "title": "EfficientNetV2: Smaller Models and Faster Training",
        "venue": "International Conference on Machine Learning",
        "year": 2021,
        "citationCount": 3664,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2104.00298, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "120805419",
                "name": "Mingxing Tan"
            },
            {
                "authorId": "1397917613",
                "name": "Quoc V. Le"
            }
        ],
        "abstract": "This paper introduces EfficientNetV2, a new family of convolutional networks that have faster training speed and better parameter efficiency than previous models. To develop this family of models, we use a combination of training-aware neural architecture search and scaling, to jointly optimize training speed and parameter efficiency. The models were searched from the search space enriched with new ops such as Fused-MBConv. Our experiments show that EfficientNetV2 models train much faster than state-of-the-art models while being up to 6.8x smaller. Our training can be further sped up by progressively increasing the image size during training, but it often causes a drop in accuracy. To compensate for this accuracy drop, we propose to adaptively adjust regularization (e.g., dropout and data augmentation) as well, such that we can achieve both fast training and good accuracy. With progressive learning, our EfficientNetV2 significantly outperforms previous models on ImageNet and CIFAR/Cars/Flowers datasets. By pretraining on the same ImageNet21k, our EfficientNetV2 achieves 87.3% top-1 accuracy on ImageNet ILSVRC2012, outperforming the recent ViT by 2.0% accuracy while training 5x-11x faster using the same computing resources. Code will be available at https://github.com/google/automl/tree/master/efficientnetv2."
    },
    {
        "paperId": "92bf1c069747374fbc3efb55e7a916a3e2d736da",
        "url": "https://www.semanticscholar.org/paper/92bf1c069747374fbc3efb55e7a916a3e2d736da",
        "title": "Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech",
        "venue": "International Conference on Machine Learning",
        "year": 2021,
        "citationCount": 1141,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2106.06103, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "94862572",
                "name": "Jaehyeon Kim"
            },
            {
                "authorId": "1712238254",
                "name": "Jungil Kong"
            },
            {
                "authorId": "72192294",
                "name": "Juhee Son"
            }
        ],
        "abstract": "Several recent end-to-end text-to-speech (TTS) models enabling single-stage training and parallel sampling have been proposed, but their sample quality does not match that of two-stage TTS systems. In this work, we present a parallel end-to-end TTS method that generates more natural sounding audio than current two-stage models. Our method adopts variational inference augmented with normalizing flows and an adversarial training process, which improves the expressive power of generative modeling. We also propose a stochastic duration predictor to synthesize speech with diverse rhythms from input text. With the uncertainty modeling over latent variables and the stochastic duration predictor, our method expresses the natural one-to-many relationship in which a text input can be spoken in multiple ways with different pitches and rhythms. A subjective human evaluation (mean opinion score, or MOS) on the LJ Speech, a single speaker dataset, shows that our method outperforms the best publicly available TTS systems and achieves a MOS comparable to ground truth."
    },
    {
        "paperId": "b3bf9fe13195e9aa70e1dac04e01fcff7008e812",
        "url": "https://www.semanticscholar.org/paper/b3bf9fe13195e9aa70e1dac04e01fcff7008e812",
        "title": "Perceiver: General Perception with Iterative Attention",
        "venue": "International Conference on Machine Learning",
        "year": 2021,
        "citationCount": 1240,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2103.03206, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2689633",
                "name": "Andrew Jaegle"
            },
            {
                "authorId": "49423009",
                "name": "Felix Gimeno"
            },
            {
                "authorId": "2065040247",
                "name": "Andrew Brock"
            },
            {
                "authorId": "1688869",
                "name": "Andrew Zisserman"
            },
            {
                "authorId": "1689108",
                "name": "O. Vinyals"
            },
            {
                "authorId": "35681810",
                "name": "Joo Carreira"
            }
        ],
        "abstract": "Biological systems perceive the world by simultaneously processing high-dimensional inputs from modalities as diverse as vision, audition, touch, proprioception, etc. The perception models used in deep learning on the other hand are designed for individual modalities, often relying on domain-specific assumptions such as the local grid structures exploited by virtually all existing vision models. These priors introduce helpful inductive biases, but also lock models to individual modalities. In this paper we introduce the Perceiver - a model that builds upon Transformers and hence makes few architectural assumptions about the relationship between its inputs, but that also scales to hundreds of thousands of inputs, like ConvNets. The model leverages an asymmetric attention mechanism to iteratively distill inputs into a tight latent bottleneck, allowing it to scale to handle very large inputs. We show that this architecture is competitive with or outperforms strong, specialized models on classification tasks across various modalities: images, point clouds, audio, video, and video+audio. The Perceiver obtains performance comparable to ResNet-50 and ViT on ImageNet without 2D convolutions by directly attending to 50,000 pixels. It is also competitive in all modalities in AudioSet."
    },
    {
        "paperId": "d7ac65d335b5d847f4f5826313a8732bc7abc7a8",
        "url": "https://www.semanticscholar.org/paper/d7ac65d335b5d847f4f5826313a8732bc7abc7a8",
        "title": "Calibrate Before Use: Improving Few-Shot Performance of Language Models",
        "venue": "International Conference on Machine Learning",
        "year": 2021,
        "citationCount": 1664,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2102.09690, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "145914976",
                "name": "Tony Zhao"
            },
            {
                "authorId": "145217343",
                "name": "Eric Wallace"
            },
            {
                "authorId": "2113511266",
                "name": "Shi Feng"
            },
            {
                "authorId": "38666915",
                "name": "D. Klein"
            },
            {
                "authorId": "34650964",
                "name": "Sameer Singh"
            }
        ],
        "abstract": "GPT-3 can perform numerous tasks when provided a natural language prompt that contains a few training examples. We show that this type of few-shot learning can be unstable: the choice of prompt format, training examples, and even the order of the training examples can cause accuracy to vary from near chance to near state-of-the-art. We demonstrate that this instability arises from the bias of language models towards predicting certain answers, e.g., those that are placed near the end of the prompt or are common in the pre-training data. To mitigate this, we first estimate the model's bias towards each answer by asking for its prediction when given the training prompt and a content-free test input such as \"N/A\". We then fit calibration parameters that cause the prediction for this input to be uniform across answers. On a diverse set of tasks, this contextual calibration procedure substantially improves GPT-3 and GPT-2's average accuracy (up to 30.0% absolute) and reduces variance across different choices of the prompt."
    },
    {
        "paperId": "de18baa4964804cf471d85a5a090498242d2e79f",
        "url": "https://www.semanticscholar.org/paper/de18baa4964804cf471d85a5a090498242d2e79f",
        "title": "Improved Denoising Diffusion Probabilistic Models",
        "venue": "International Conference on Machine Learning",
        "year": 2021,
        "citationCount": 4671,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2102.09672, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "38967461",
                "name": "Alex Nichol"
            },
            {
                "authorId": "6515819",
                "name": "Prafulla Dhariwal"
            }
        ],
        "abstract": "Denoising diffusion probabilistic models (DDPM) are a class of generative models which have recently been shown to produce excellent samples. We show that with a few simple modifications, DDPMs can also achieve competitive log-likelihoods while maintaining high sample quality. Additionally, we find that learning variances of the reverse diffusion process allows sampling with an order of magnitude fewer forward passes with a negligible difference in sample quality, which is important for the practical deployment of these models. We additionally use precision and recall to compare how well DDPMs and GANs cover the target distribution. Finally, we show that the sample quality and likelihood of these models scale smoothly with model capacity and training compute, making them easily scalable. We release our code at https://github.com/openai/improved-diffusion"
    },
    {
        "paperId": "fa08b41ccdfc5d8771adfbc34c176fa237d4646c",
        "url": "https://www.semanticscholar.org/paper/fa08b41ccdfc5d8771adfbc34c176fa237d4646c",
        "title": "Is Space-Time Attention All You Need for Video Understanding?",
        "venue": "International Conference on Machine Learning",
        "year": 2021,
        "citationCount": 2610,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2102.05095, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3313330",
                "name": "Gedas Bertasius"
            },
            {
                "authorId": "46506697",
                "name": "Heng Wang"
            },
            {
                "authorId": "1732879",
                "name": "L. Torresani"
            }
        ],
        "abstract": "We present a convolution-free approach to video classification built exclusively on self-attention over space and time. Our method, named\"TimeSformer,\"adapts the standard Transformer architecture to video by enabling spatiotemporal feature learning directly from a sequence of frame-level patches. Our experimental study compares different self-attention schemes and suggests that\"divided attention,\"where temporal attention and spatial attention are separately applied within each block, leads to the best video classification accuracy among the design choices considered. Despite the radically new design, TimeSformer achieves state-of-the-art results on several action recognition benchmarks, including the best reported accuracy on Kinetics-400 and Kinetics-600. Finally, compared to 3D convolutional networks, our model is faster to train, it can achieve dramatically higher test efficiency (at a small drop in accuracy), and it can also be applied to much longer video clips (over one minute long). Code and models are available at: https://github.com/facebookresearch/TimeSformer."
    },
    {
        "paperId": "1bfa62ddfa3f6691e0e40c06f8ead594b6449cfa",
        "url": "https://www.semanticscholar.org/paper/1bfa62ddfa3f6691e0e40c06f8ead594b6449cfa",
        "title": "OFA: Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework",
        "venue": "International Conference on Machine Learning",
        "year": 2022,
        "citationCount": 1001,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2202.03052, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2155302144",
                "name": "Peng Wang"
            },
            {
                "authorId": null,
                "name": "An Yang"
            },
            {
                "authorId": "47447639",
                "name": "Rui Men"
            },
            {
                "authorId": "35996608",
                "name": "Junyang Lin"
            },
            {
                "authorId": "3768186",
                "name": "Shuai Bai"
            },
            {
                "authorId": "2109657047",
                "name": "Zhikang Li"
            },
            {
                "authorId": "47793076",
                "name": "Jianxin Ma"
            },
            {
                "authorId": "144161025",
                "name": "Chang Zhou"
            },
            {
                "authorId": "1709595",
                "name": "Jingren Zhou"
            },
            {
                "authorId": "38385080",
                "name": "Hongxia Yang"
            }
        ],
        "abstract": "In this work, we pursue a unified paradigm for multimodal pretraining to break the scaffolds of complex task/modality-specific customization. We propose OFA, a Task-Agnostic and Modality-Agnostic framework that supports Task Comprehensiveness. OFA unifies a diverse set of cross-modal and unimodal tasks, including image generation, visual grounding, image captioning, image classification, language modeling, etc., in a simple sequence-to-sequence learning framework. OFA follows the instruction-based learning in both pretraining and finetuning stages, requiring no extra task-specific layers for downstream tasks. In comparison with the recent state-of-the-art vision&language models that rely on extremely large cross-modal datasets, OFA is pretrained on only 20M publicly available image-text pairs. Despite its simplicity and relatively small-scale training data, OFA achieves new SOTAs in a series of cross-modal tasks while attaining highly competitive performances on uni-modal tasks. Our further analysis indicates that OFA can also effectively transfer to unseen tasks and unseen domains. Our code and models are publicly available at https://github.com/OFA-Sys/OFA."
    },
    {
        "paperId": "2c994fadbb84fb960d8306ee138dbeef41a5b323",
        "url": "https://www.semanticscholar.org/paper/2c994fadbb84fb960d8306ee138dbeef41a5b323",
        "title": "SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models",
        "venue": "International Conference on Machine Learning",
        "year": 2022,
        "citationCount": 1175,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2211.10438",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2211.10438, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2046958974",
                "name": "Guangxuan Xiao"
            },
            {
                "authorId": "46698300",
                "name": "Ji Lin"
            },
            {
                "authorId": "66890356",
                "name": "Mickael Seznec"
            },
            {
                "authorId": "32604218",
                "name": "Julien Demouth"
            },
            {
                "authorId": "143840275",
                "name": "Song Han"
            }
        ],
        "abstract": "Large language models (LLMs) show excellent performance but are compute- and memory-intensive. Quantization can reduce memory and accelerate inference. However, existing methods cannot maintain accuracy and hardware efficiency at the same time. We propose SmoothQuant, a training-free, accuracy-preserving, and general-purpose post-training quantization (PTQ) solution to enable 8-bit weight, 8-bit activation (W8A8) quantization for LLMs. Based on the fact that weights are easy to quantize while activations are not, SmoothQuant smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation. SmoothQuant enables an INT8 quantization of both weights and activations for all the matrix multiplications in LLMs, including OPT, BLOOM, GLM, MT-NLG, Llama-1/2, Falcon, Mistral, and Mixtral models. We demonstrate up to 1.56x speedup and 2x memory reduction for LLMs with negligible loss in accuracy. SmoothQuant enables serving 530B LLM within a single node. Our work offers a turn-key solution that reduces hardware costs and democratizes LLMs. Code is available at https://github.com/mit-han-lab/smoothquant."
    },
    {
        "paperId": "54020e5fe48ebb250f27d744e20a63cac2988a84",
        "url": "https://www.semanticscholar.org/paper/54020e5fe48ebb250f27d744e20a63cac2988a84",
        "title": "Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time",
        "venue": "International Conference on Machine Learning",
        "year": 2022,
        "citationCount": 1270,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2203.05482",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2203.05482, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "52193502",
                "name": "Mitchell Wortsman"
            },
            {
                "authorId": "1387994137",
                "name": "Gabriel Ilharco"
            },
            {
                "authorId": "1387466862",
                "name": "S. Gadre"
            },
            {
                "authorId": "40458654",
                "name": "R. Roelofs"
            },
            {
                "authorId": "2158366935",
                "name": "Raphael Gontijo-Lopes"
            },
            {
                "authorId": "4690624",
                "name": "Ari S. Morcos"
            },
            {
                "authorId": "40281109",
                "name": "Hongseok Namkoong"
            },
            {
                "authorId": "143787583",
                "name": "Ali Farhadi"
            },
            {
                "authorId": "2444742",
                "name": "Y. Carmon"
            },
            {
                "authorId": "40464924",
                "name": "Simon Kornblith"
            },
            {
                "authorId": "152772922",
                "name": "Ludwig Schmidt"
            }
        ],
        "abstract": "The conventional recipe for maximizing model accuracy is to (1) train multiple models with various hyperparameters and (2) pick the individual model which performs best on a held-out validation set, discarding the remainder. In this paper, we revisit the second step of this procedure in the context of fine-tuning large pre-trained models, where fine-tuned models often appear to lie in a single low error basin. We show that averaging the weights of multiple models fine-tuned with different hyperparameter configurations often improves accuracy and robustness. Unlike a conventional ensemble, we may average many models without incurring any additional inference or memory costs -- we call the results\"model soups.\"When fine-tuning large pre-trained models such as CLIP, ALIGN, and a ViT-G pre-trained on JFT, our soup recipe provides significant improvements over the best model in a hyperparameter sweep on ImageNet. The resulting ViT-G model, which attains 90.94% top-1 accuracy on ImageNet, achieved a new state of the art. Furthermore, we show that the model soup approach extends to multiple image classification and natural language processing tasks, improves out-of-distribution performance, and improves zero-shot performance on new downstream tasks. Finally, we analytically relate the performance similarity of weight-averaging and logit-ensembling to flatness of the loss and confidence of the predictions, and validate this relation empirically. Code is available at https://github.com/mlfoundations/model-soups."
    },
    {
        "paperId": "563bac1c5cdd5096e9dbf8d4f3d5b3c4f7284e06",
        "url": "https://www.semanticscholar.org/paper/563bac1c5cdd5096e9dbf8d4f3d5b3c4f7284e06",
        "title": "FEDformer: Frequency Enhanced Decomposed Transformer for Long-term Series Forecasting",
        "venue": "International Conference on Machine Learning",
        "year": 2022,
        "citationCount": 2349,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2201.12740, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "50018188",
                "name": "Tian Zhou"
            },
            {
                "authorId": "1387898803",
                "name": "Ziqing Ma"
            },
            {
                "authorId": "3308963",
                "name": "Qingsong Wen"
            },
            {
                "authorId": "2118294665",
                "name": "Xue Wang"
            },
            {
                "authorId": "2110940896",
                "name": "Liang Sun"
            },
            {
                "authorId": "2152101871",
                "name": "Rong Jin"
            }
        ],
        "abstract": "Although Transformer-based methods have significantly improved state-of-the-art results for long-term series forecasting, they are not only computationally expensive but more importantly, are unable to capture the global view of time series (e.g. overall trend). To address these problems, we propose to combine Transformer with the seasonal-trend decomposition method, in which the decomposition method captures the global profile of time series while Transformers capture more detailed structures. To further enhance the performance of Transformer for long-term prediction, we exploit the fact that most time series tend to have a sparse representation in well-known basis such as Fourier transform, and develop a frequency enhanced Transformer. Besides being more effective, the proposed method, termed as Frequency Enhanced Decomposed Transformer ({\\bf FEDformer}), is more efficient than standard Transformer with a linear complexity to the sequence length. Our empirical studies with six benchmark datasets show that compared with state-of-the-art methods, FEDformer can reduce prediction error by $14.8\\%$ and $22.6\\%$ for multivariate and univariate time series, respectively. Code is publicly available at https://github.com/MAZiqing/FEDformer."
    },
    {
        "paperId": "8f2bca9d684005675e294b33c26481e36f528cdb",
        "url": "https://www.semanticscholar.org/paper/8f2bca9d684005675e294b33c26481e36f528cdb",
        "title": "data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language",
        "venue": "International Conference on Machine Learning",
        "year": 2022,
        "citationCount": 1025,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2202.03555, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "14667698",
                "name": "Alexei Baevski"
            },
            {
                "authorId": "2957796",
                "name": "Wei-Ning Hsu"
            },
            {
                "authorId": "2149106061",
                "name": "Qiantong Xu"
            },
            {
                "authorId": "2078281119",
                "name": "Arun Babu"
            },
            {
                "authorId": "3016273",
                "name": "Jiatao Gu"
            },
            {
                "authorId": "2325985",
                "name": "Michael Auli"
            }
        ],
        "abstract": "While the general idea of self-supervised learning is identical across modalities, the actual algorithms and objectives differ widely because they were developed with a single modality in mind. To get us closer to general self-supervised learning, we present data2vec, a framework that uses the same learning method for either speech, NLP or computer vision. The core idea is to predict latent representations of the full input data based on a masked view of the input in a self-distillation setup using a standard Transformer architecture. Instead of predicting modality-specific targets such as words, visual tokens or units of human speech which are local in nature, data2vec predicts contextualized latent representations that contain information from the entire input. Experiments on the major benchmarks of speech recognition, image classification, and natural language understanding demonstrate a new state of the art or competitive performance to predominant approaches."
    },
    {
        "paperId": "92a8f7f09f3705cb5a6009a42220a6f01ea084e8",
        "url": "https://www.semanticscholar.org/paper/92a8f7f09f3705cb5a6009a42220a6f01ea084e8",
        "title": "Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents",
        "venue": "International Conference on Machine Learning",
        "year": 2022,
        "citationCount": 1376,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2201.07207, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2158105356",
                "name": "Wenlong Huang"
            },
            {
                "authorId": "1689992",
                "name": "P. Abbeel"
            },
            {
                "authorId": "2004879394",
                "name": "Deepak Pathak"
            },
            {
                "authorId": "2080746",
                "name": "Igor Mordatch"
            }
        ],
        "abstract": "Can world knowledge learned by large language models (LLMs) be used to act in interactive environments? In this paper, we investigate the possibility of grounding high-level tasks, expressed in natural language (e.g.\"make breakfast\"), to a chosen set of actionable steps (e.g.\"open fridge\"). While prior work focused on learning from explicit step-by-step examples of how to act, we surprisingly find that if pre-trained LMs are large enough and prompted appropriately, they can effectively decompose high-level tasks into mid-level plans without any further training. However, the plans produced naively by LLMs often cannot map precisely to admissible actions. We propose a procedure that conditions on existing demonstrations and semantically translates the plans to admissible actions. Our evaluation in the recent VirtualHome environment shows that the resulting method substantially improves executability over the LLM baseline. The conducted human evaluation reveals a trade-off between executability and correctness but shows a promising sign towards extracting actionable knowledge from language models. Website at https://huangwl18.github.io/language-planner"
    },
    {
        "paperId": "a02fbaf22237a1aedacb1320b6007cd70c1fe6ec",
        "url": "https://www.semanticscholar.org/paper/a02fbaf22237a1aedacb1320b6007cd70c1fe6ec",
        "title": "Robust Speech Recognition via Large-Scale Weak Supervision",
        "venue": "International Conference on Machine Learning",
        "year": 2022,
        "citationCount": 5640,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2212.04356, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "38909097",
                "name": "Alec Radford"
            },
            {
                "authorId": "2110935237",
                "name": "Jong Wook Kim"
            },
            {
                "authorId": "2118717067",
                "name": "Tao Xu"
            },
            {
                "authorId": "2065151121",
                "name": "Greg Brockman"
            },
            {
                "authorId": "3028785",
                "name": "Christine McLeavey"
            },
            {
                "authorId": "1701686",
                "name": "I. Sutskever"
            }
        ],
        "abstract": "We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results but in a zero-shot transfer setting without the need for any fine-tuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing."
    },
    {
        "paperId": "a3b42a83669998f65df60d7c065a70d07ca95e99",
        "url": "https://www.semanticscholar.org/paper/a3b42a83669998f65df60d7c065a70d07ca95e99",
        "title": "BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation",
        "venue": "International Conference on Machine Learning",
        "year": 2022,
        "citationCount": 5660,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2201.12086, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "49299019",
                "name": "Junnan Li"
            },
            {
                "authorId": "2981509",
                "name": "Dongxu Li"
            },
            {
                "authorId": "2054594326",
                "name": "Caiming Xiong"
            },
            {
                "authorId": "1741126",
                "name": "S. Hoi"
            }
        ],
        "abstract": "Vision-Language Pre-training (VLP) has advanced the performance for many vision-language tasks. However, most existing pre-trained models only excel in either understanding-based tasks or generation-based tasks. Furthermore, performance improvement has been largely achieved by scaling up the dataset with noisy image-text pairs collected from the web, which is a suboptimal source of supervision. In this paper, we propose BLIP, a new VLP framework which transfers flexibly to both vision-language understanding and generation tasks. BLIP effectively utilizes the noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. We achieve state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval (+2.7% in average recall@1), image captioning (+2.8% in CIDEr), and VQA (+1.6% in VQA score). BLIP also demonstrates strong generalization ability when directly transferred to video-language tasks in a zero-shot manner. Code, models, and datasets are released at https://github.com/salesforce/BLIP."
    },
    {
        "paperId": "d8e9f8c8a37cb4cd26b92ad0d942d641cd512644",
        "url": "https://www.semanticscholar.org/paper/d8e9f8c8a37cb4cd26b92ad0d942d641cd512644",
        "title": "Fast Inference from Transformers via Speculative Decoding",
        "venue": "International Conference on Machine Learning",
        "year": 2022,
        "citationCount": 1125,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2211.17192",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2211.17192, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2188067081",
                "name": "Yaniv Leviathan"
            },
            {
                "authorId": "21605105",
                "name": "Matan Kalman"
            },
            {
                "authorId": "1745572",
                "name": "Yossi Matias"
            }
        ],
        "abstract": "Inference from large autoregressive models like Transformers is slow - decoding K tokens takes K serial runs of the model. In this work we introduce speculative decoding - an algorithm to sample from autoregressive models faster without any changes to the outputs, by computing several tokens in parallel. At the heart of our approach lie the observations that (1) hard language-modeling tasks often include easier subtasks that can be approximated well by more efficient models, and (2) using speculative execution and a novel sampling method, we can make exact decoding from the large models faster, by running them in parallel on the outputs of the approximation models, potentially generating several tokens concurrently, and without changing the distribution. Our method can accelerate existing off-the-shelf models without retraining or architecture changes. We demonstrate it on T5-XXL and show a 2X-3X acceleration compared to the standard T5X implementation, with identical outputs."
    },
    {
        "paperId": "db2e818f0670e07324884d28392edd3343a37da0",
        "url": "https://www.semanticscholar.org/paper/db2e818f0670e07324884d28392edd3343a37da0",
        "title": "Constrained Efficient Global Optimization of Expensive Black-box Functions",
        "venue": "International Conference on Machine Learning",
        "year": 2022,
        "citationCount": 1951,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2211.00162, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2118096690",
                "name": "Donald R. Jones"
            },
            {
                "authorId": "1815604",
                "name": "Matthias Schonlau"
            },
            {
                "authorId": "145228938",
                "name": "W. Welch"
            }
        ],
        "abstract": "We study the problem of constrained efficient global optimization, where both the objective and constraints are expensive black-box functions that can be learned with Gaussian processes. We propose CONFIG (CONstrained efFIcient Global Optimization), a simple and effective algorithm to solve it. Under certain regularity assumptions, we show that our algorithm enjoys the same cumulative regret bound as that in the unconstrained case and similar cumulative constraint violation upper bounds. For commonly used Matern and Squared Exponential kernels, our bounds are sublinear and allow us to derive a convergence rate to the optimal solution of the original constrained problem. In addition, our method naturally provides a scheme to declare infeasibility when the original black-box optimization problem is infeasible. Numerical experiments on sampled instances from the Gaussian process, artificial numerical problems, and a black-box building controller tuning problem all demonstrate the competitive performance of our algorithm. Compared to the other state-of-the-art methods, our algorithm significantly improves the theoretical guarantees, while achieving competitive empirical performance."
    },
    {
        "paperId": "38fe8f324d2162e63a967a9ac6648974fc4c66f3",
        "url": "https://www.semanticscholar.org/paper/38fe8f324d2162e63a967a9ac6648974fc4c66f3",
        "title": "PaLM-E: An Embodied Multimodal Language Model",
        "venue": "International Conference on Machine Learning",
        "year": 2023,
        "citationCount": 2203,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2303.03378",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2303.03378, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2283848260",
                "name": "Danny Driess"
            },
            {
                "authorId": "144956443",
                "name": "F. Xia"
            },
            {
                "authorId": "2283034",
                "name": "Mehdi S. M. Sajjadi"
            },
            {
                "authorId": "32245472",
                "name": "Corey Lynch"
            },
            {
                "authorId": "2841893",
                "name": "A. Chowdhery"
            },
            {
                "authorId": "2704814",
                "name": "Brian Ichter"
            },
            {
                "authorId": "88728227",
                "name": "Ayzaan Wahid"
            },
            {
                "authorId": "2704494",
                "name": "Jonathan Tompson"
            },
            {
                "authorId": "144579461",
                "name": "Q. Vuong"
            },
            {
                "authorId": "10909315",
                "name": "Tianhe Yu"
            },
            {
                "authorId": "2158105356",
                "name": "Wenlong Huang"
            },
            {
                "authorId": "2527420",
                "name": "Yevgen Chebotar"
            },
            {
                "authorId": "3142556",
                "name": "P. Sermanet"
            },
            {
                "authorId": "40620532",
                "name": "Daniel Duckworth"
            },
            {
                "authorId": "1736651",
                "name": "S. Levine"
            },
            {
                "authorId": "2657155",
                "name": "Vincent Vanhoucke"
            },
            {
                "authorId": "1944801",
                "name": "Karol Hausman"
            },
            {
                "authorId": "144918851",
                "name": "Marc Toussaint"
            },
            {
                "authorId": "3035541",
                "name": "Klaus Greff"
            },
            {
                "authorId": "38591293",
                "name": "Andy Zeng"
            },
            {
                "authorId": "2080746",
                "name": "Igor Mordatch"
            },
            {
                "authorId": "47686265",
                "name": "Peter R. Florence"
            }
        ],
        "abstract": "Large language models excel at a wide range of complex tasks. However, enabling general inference in the real world, e.g., for robotics problems, raises the challenge of grounding. We propose embodied language models to directly incorporate real-world continuous sensor modalities into language models and thereby establish the link between words and percepts. Input to our embodied language model are multi-modal sentences that interleave visual, continuous state estimation, and textual input encodings. We train these encodings end-to-end, in conjunction with a pre-trained large language model, for multiple embodied tasks including sequential robotic manipulation planning, visual question answering, and captioning. Our evaluations show that PaLM-E, a single large embodied multimodal model, can address a variety of embodied reasoning tasks, from a variety of observation modalities, on multiple embodiments, and further, exhibits positive transfer: the model benefits from diverse joint training across internet-scale language, vision, and visual-language domains. Our largest model, PaLM-E-562B with 562B parameters, in addition to being trained on robotics tasks, is a visual-language generalist with state-of-the-art performance on OK-VQA, and retains generalist language capabilities with increasing scale."
    },
    {
        "paperId": "3f5b31c4f7350dc88002c121aecbdc82f86eb5bb",
        "url": "https://www.semanticscholar.org/paper/3f5b31c4f7350dc88002c121aecbdc82f86eb5bb",
        "title": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",
        "venue": "International Conference on Machine Learning",
        "year": 2023,
        "citationCount": 6560,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2301.12597",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2301.12597, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "49299019",
                "name": "Junnan Li"
            },
            {
                "authorId": "2981509",
                "name": "Dongxu Li"
            },
            {
                "authorId": "1702137",
                "name": "S. Savarese"
            },
            {
                "authorId": "2184854289",
                "name": "Steven C. H. Hoi"
            }
        ],
        "abstract": "The cost of vision-and-language pre-training has become increasingly prohibitive due to end-to-end training of large-scale models. This paper proposes BLIP-2, a generic and efficient pre-training strategy that bootstraps vision-language pre-training from off-the-shelf frozen pre-trained image encoders and frozen large language models. BLIP-2 bridges the modality gap with a lightweight Querying Transformer, which is pre-trained in two stages. The first stage bootstraps vision-language representation learning from a frozen image encoder. The second stage bootstraps vision-to-language generative learning from a frozen language model. BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods. For example, our model outperforms Flamingo80B by 8.7% on zero-shot VQAv2 with 54x fewer trainable parameters. We also demonstrate the model's emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions."
    },
    {
        "paperId": "4780d0a027c5c5a8e01d7cf697f6296880ffc945",
        "url": "https://www.semanticscholar.org/paper/4780d0a027c5c5a8e01d7cf697f6296880ffc945",
        "title": "Improving Factuality and Reasoning in Language Models through Multiagent Debate",
        "venue": "International Conference on Machine Learning",
        "year": 2023,
        "citationCount": 1153,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2305.14325",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.14325, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "15394275",
                "name": "Yilun Du"
            },
            {
                "authorId": "145015904",
                "name": "Shuang Li"
            },
            {
                "authorId": "143805211",
                "name": "A. Torralba"
            },
            {
                "authorId": "1763295",
                "name": "J. Tenenbaum"
            },
            {
                "authorId": "2316241372",
                "name": "Igor Mordatch"
            }
        ],
        "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in language generation, understanding, and few-shot learning in recent years. An extensive body of work has explored how their performance may be further improved through the tools of prompting, ranging from verification, self-consistency, or intermediate scratchpads. In this paper, we present a complementary approach to improve language responses where multiple language model instances propose and debate their individual responses and reasoning processes over multiple rounds to arrive at a common final answer. Our findings indicate that this approach significantly enhances mathematical and strategic reasoning across a number of tasks. We also demonstrate that our approach improves the factual validity of generated content, reducing fallacious answers and hallucinations that contemporary models are prone to. Our approach may be directly applied to existing black-box models and uses identical procedure and prompts for all tasks we investigate. Overall, our findings suggest that such\"society of minds\"approach has the potential to significantly advance the capabilities of LLMs and pave the way for further breakthroughs in language generation and understanding."
    },
    {
        "paperId": "909ad57ce8caa6b390a65ae09db352d27d8f3996",
        "url": "https://www.semanticscholar.org/paper/909ad57ce8caa6b390a65ae09db352d27d8f3996",
        "title": "SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot",
        "venue": "International Conference on Machine Learning",
        "year": 2023,
        "citationCount": 1020,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2301.00774",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2301.00774, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1502248377",
                "name": "Elias Frantar"
            },
            {
                "authorId": "3311387",
                "name": "Dan Alistarh"
            }
        ],
        "abstract": "We show for the first time that large-scale generative pretrained transformer (GPT) family models can be pruned to at least 50% sparsity in one-shot, without any retraining, at minimal loss of accuracy. This is achieved via a new pruning method called SparseGPT, specifically designed to work efficiently and accurately on massive GPT-family models. We can execute SparseGPT on the largest available open-source models, OPT-175B and BLOOM-176B, in under 4.5 hours, and can reach 60% unstructured sparsity with negligible increase in perplexity: remarkably, more than 100 billion weights from these models can be ignored at inference time. SparseGPT generalizes to semi-structured (2:4 and 4:8) patterns, and is compatible with weight quantization approaches. The code is available at: https://github.com/IST-DASLab/sparsegpt."
    },
    {
        "paperId": "94972e30504017156ef5b5debc419bf6edc67384",
        "url": "https://www.semanticscholar.org/paper/94972e30504017156ef5b5debc419bf6edc67384",
        "title": "MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities",
        "venue": "International Conference on Machine Learning",
        "year": 2023,
        "citationCount": 1015,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2308.02490",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2308.02490, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "23476952",
                "name": "Weihao Yu"
            },
            {
                "authorId": "2149231840",
                "name": "Zhengyuan Yang"
            },
            {
                "authorId": "50703697",
                "name": "Linjie Li"
            },
            {
                "authorId": "2124948371",
                "name": "Jianfeng Wang"
            },
            {
                "authorId": "143786724",
                "name": "Kevin Lin"
            },
            {
                "authorId": "2145253136",
                "name": "Zicheng Liu"
            },
            {
                "authorId": "48631088",
                "name": "Xinchao Wang"
            },
            {
                "authorId": "29957038",
                "name": "Lijuan Wang"
            }
        ],
        "abstract": "We propose MM-Vet, an evaluation benchmark that examines large multimodal models (LMMs) on complicated multimodal tasks. Recent LMMs have shown various intriguing abilities, such as solving math problems written on the blackboard, reasoning about events and celebrities in news images, and explaining visual jokes. Rapid model advancements pose challenges to evaluation benchmark development. Problems include: (1) How to systematically structure and evaluate the complicated multimodal tasks; (2) How to design evaluation metrics that work well across question and answer types; and (3) How to give model insights beyond a simple performance ranking. To this end, we present MM-Vet, designed based on the insight that the intriguing ability to solve complicated tasks is often achieved by a generalist model being able to integrate different core vision-language (VL) capabilities. MM-Vet defines 6 core VL capabilities and examines the 16 integrations of interest derived from the capability combination. For evaluation metrics, we propose an LLM-based evaluator for open-ended outputs. The evaluator enables the evaluation across different question types and answer styles, resulting in a unified scoring metric. We evaluate representative LMMs on MM-Vet, providing insights into the capabilities of different LMM system paradigms and models."
    },
    {
        "paperId": "ac974291d7e3a152067382675524f3e3c2ded11b",
        "url": "https://www.semanticscholar.org/paper/ac974291d7e3a152067382675524f3e3c2ded11b",
        "title": "Consistency Models",
        "venue": "International Conference on Machine Learning",
        "year": 2023,
        "citationCount": 1411,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2303.01469, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2157995251",
                "name": "Yang Song"
            },
            {
                "authorId": "6515819",
                "name": "Prafulla Dhariwal"
            },
            {
                "authorId": "2108828435",
                "name": "Mark Chen"
            },
            {
                "authorId": "1701686",
                "name": "I. Sutskever"
            }
        ],
        "abstract": "Diffusion models have significantly advanced the fields of image, audio, and video generation, but they depend on an iterative sampling process that causes slow generation. To overcome this limitation, we propose consistency models, a new family of models that generate high quality samples by directly mapping noise to data. They support fast one-step generation by design, while still allowing multistep sampling to trade compute for sample quality. They also support zero-shot data editing, such as image inpainting, colorization, and super-resolution, without requiring explicit training on these tasks. Consistency models can be trained either by distilling pre-trained diffusion models, or as standalone generative models altogether. Through extensive experiments, we demonstrate that they outperform existing distillation techniques for diffusion models in one- and few-step sampling, achieving the new state-of-the-art FID of 3.55 on CIFAR-10 and 6.20 on ImageNet 64x64 for one-step generation. When trained in isolation, consistency models become a new family of generative models that can outperform existing one-step, non-adversarial generative models on standard benchmarks such as CIFAR-10, ImageNet 64x64 and LSUN 256x256."
    },
    {
        "paperId": "be55e8ec4213868db08f2c3168ae666001bea4b8",
        "url": "https://www.semanticscholar.org/paper/be55e8ec4213868db08f2c3168ae666001bea4b8",
        "title": "Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling",
        "venue": "International Conference on Machine Learning",
        "year": 2023,
        "citationCount": 1613,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2304.01373",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2304.01373, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "103476203",
                "name": "Stella Biderman"
            },
            {
                "authorId": "2184031883",
                "name": "Hailey Schoelkopf"
            },
            {
                "authorId": "2260401329",
                "name": "Quentin Anthony"
            },
            {
                "authorId": "2070768742",
                "name": "Herbie Bradley"
            },
            {
                "authorId": "2212970046",
                "name": "Kyle O'Brien"
            },
            {
                "authorId": "2162462983",
                "name": "Eric Hallahan"
            },
            {
                "authorId": "2168771748",
                "name": "Mohammad Aflah Khan"
            },
            {
                "authorId": "2162467233",
                "name": "Shivanshu Purohit"
            },
            {
                "authorId": "2162462141",
                "name": "USVSN Sai Prashanth"
            },
            {
                "authorId": "34885007",
                "name": "Edward Raff"
            },
            {
                "authorId": "2213349418",
                "name": "Aviya Skowron"
            },
            {
                "authorId": "35566806",
                "name": "Lintang Sutawika"
            },
            {
                "authorId": "1986356851",
                "name": "Oskar van der Wal"
            }
        ],
        "abstract": "How do large language models (LLMs) develop and evolve over the course of training? How do these patterns change as models scale? To answer these questions, we introduce \\textit{Pythia}, a suite of 16 LLMs all trained on public data seen in the exact same order and ranging in size from 70M to 12B parameters. We provide public access to 154 checkpoints for each one of the 16 models, alongside tools to download and reconstruct their exact training dataloaders for further study. We intend \\textit{Pythia} to facilitate research in many areas, and we present several case studies including novel results in memorization, term frequency effects on few-shot performance, and reducing gender bias. We demonstrate that this highly controlled setup can be used to yield novel insights toward LLMs and their training dynamics. Trained models, analysis code, training code, and training data can be found at \\url{https://github.com/EleutherAI/pythia}."
    },
    {
        "paperId": "38c48a1cd296d16dc9c56717495d6e44cc354444",
        "url": "https://www.semanticscholar.org/paper/38c48a1cd296d16dc9c56717495d6e44cc354444",
        "title": "Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model",
        "venue": "International Conference on Machine Learning",
        "year": 2024,
        "citationCount": 1333,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2401.09417, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2261901203",
                "name": "Lianghui Zhu"
            },
            {
                "authorId": "2060439659",
                "name": "Bencheng Liao"
            },
            {
                "authorId": "2261816376",
                "name": "Qian Zhang"
            },
            {
                "authorId": "2261792454",
                "name": "Xinlong Wang"
            },
            {
                "authorId": "2257432695",
                "name": "Wenyu Liu"
            },
            {
                "authorId": "2261738789",
                "name": "Xinggang Wang"
            }
        ],
        "abstract": "Recently the state space models (SSMs) with efficient hardware-aware designs, i.e., the Mamba deep learning model, have shown great potential for long sequence modeling. Meanwhile building efficient and generic vision backbones purely upon SSMs is an appealing direction. However, representing visual data is challenging for SSMs due to the position-sensitivity of visual data and the requirement of global context for visual understanding. In this paper, we show that the reliance on self-attention for visual representation learning is not necessary and propose a new generic vision backbone with bidirectional Mamba blocks (Vim), which marks the image sequences with position embeddings and compresses the visual representation with bidirectional state space models. On ImageNet classification, COCO object detection, and ADE20k semantic segmentation tasks, Vim achieves higher performance compared to well-established vision transformers like DeiT, while also demonstrating significantly improved computation&memory efficiency. For example, Vim is 2.8$\\times$ faster than DeiT and saves 86.8% GPU memory when performing batch inference to extract features on images with a resolution of 1248$\\times$1248. The results demonstrate that Vim is capable of overcoming the computation&memory constraints on performing Transformer-style understanding for high-resolution images and it has great potential to be the next-generation backbone for vision foundation models. Code is available at https://github.com/hustvl/Vim."
    },
    {
        "paperId": "41a66997ce0a366bba3becf7c3f37c9aebb13fbd",
        "url": "https://www.semanticscholar.org/paper/41a66997ce0a366bba3becf7c3f37c9aebb13fbd",
        "title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis",
        "venue": "International Conference on Machine Learning",
        "year": 2024,
        "citationCount": 2671,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.03206, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "35175531",
                "name": "Patrick Esser"
            },
            {
                "authorId": "3411322",
                "name": "Sumith Kulal"
            },
            {
                "authorId": "119843260",
                "name": "A. Blattmann"
            },
            {
                "authorId": "2316859494",
                "name": "Rahim Entezari"
            },
            {
                "authorId": "2188737195",
                "name": "Jonas Muller"
            },
            {
                "authorId": "2289994508",
                "name": "Harry Saini"
            },
            {
                "authorId": "2290013499",
                "name": "Yam Levi"
            },
            {
                "authorId": "2053482699",
                "name": "Dominik Lorenz"
            },
            {
                "authorId": "40562186",
                "name": "Axel Sauer"
            },
            {
                "authorId": "2290014125",
                "name": "Frederic Boesel"
            },
            {
                "authorId": "2221125727",
                "name": "Dustin Podell"
            },
            {
                "authorId": "102541178",
                "name": "Tim Dockhorn"
            },
            {
                "authorId": "2221127565",
                "name": "Zion English"
            },
            {
                "authorId": "2221126982",
                "name": "Kyle Lacey"
            },
            {
                "authorId": "2290014122",
                "name": "Alex Goodwin"
            },
            {
                "authorId": "2290014387",
                "name": "Yannik Marek"
            },
            {
                "authorId": "1660819540",
                "name": "Robin Rombach"
            }
        ],
        "abstract": "Diffusion models create data from noise by inverting the forward paths of data towards noise and have emerged as a powerful generative modeling technique for high-dimensional, perceptual data such as images and videos. Rectified flow is a recent generative model formulation that connects data and noise in a straight line. Despite its better theoretical properties and conceptual simplicity, it is not yet decisively established as standard practice. In this work, we improve existing noise sampling techniques for training rectified flow models by biasing them towards perceptually relevant scales. Through a large-scale study, we demonstrate the superior performance of this approach compared to established diffusion formulations for high-resolution text-to-image synthesis. Additionally, we present a novel transformer-based architecture for text-to-image generation that uses separate weights for the two modalities and enables a bidirectional flow of information between image and text tokens, improving text comprehension, typography, and human preference ratings. We demonstrate that this architecture follows predictable scaling trends and correlates lower validation loss to improved text-to-image synthesis as measured by various metrics and human evaluations. Our largest models outperform state-of-the-art models, and we will make our experimental data, code, and model weights publicly available."
    },
    {
        "paperId": "ca9f5b3bf0f54ad97513e6175b30497873670fed",
        "url": "https://www.semanticscholar.org/paper/ca9f5b3bf0f54ad97513e6175b30497873670fed",
        "title": "Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality",
        "venue": "International Conference on Machine Learning",
        "year": 2024,
        "citationCount": 1015,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.21060, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2269146652",
                "name": "Tri Dao"
            },
            {
                "authorId": "2269161650",
                "name": "Albert Gu"
            }
        ],
        "abstract": "While Transformers have been the main architecture behind deep learning's success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices. Our state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling."
    },
    {
        "paperId": "38c48a1cd296d16dc9c56717495d6e44cc354444",
        "url": "https://www.semanticscholar.org/paper/38c48a1cd296d16dc9c56717495d6e44cc354444",
        "title": "Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model",
        "venue": "International Conference on Machine Learning",
        "year": 2024,
        "citationCount": 1333,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2401.09417, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2261901203",
                "name": "Lianghui Zhu"
            },
            {
                "authorId": "2060439659",
                "name": "Bencheng Liao"
            },
            {
                "authorId": "2261816376",
                "name": "Qian Zhang"
            },
            {
                "authorId": "2261792454",
                "name": "Xinlong Wang"
            },
            {
                "authorId": "2257432695",
                "name": "Wenyu Liu"
            },
            {
                "authorId": "2261738789",
                "name": "Xinggang Wang"
            }
        ],
        "abstract": "Recently the state space models (SSMs) with efficient hardware-aware designs, i.e., the Mamba deep learning model, have shown great potential for long sequence modeling. Meanwhile building efficient and generic vision backbones purely upon SSMs is an appealing direction. However, representing visual data is challenging for SSMs due to the position-sensitivity of visual data and the requirement of global context for visual understanding. In this paper, we show that the reliance on self-attention for visual representation learning is not necessary and propose a new generic vision backbone with bidirectional Mamba blocks (Vim), which marks the image sequences with position embeddings and compresses the visual representation with bidirectional state space models. On ImageNet classification, COCO object detection, and ADE20k semantic segmentation tasks, Vim achieves higher performance compared to well-established vision transformers like DeiT, while also demonstrating significantly improved computation&memory efficiency. For example, Vim is 2.8$\\times$ faster than DeiT and saves 86.8% GPU memory when performing batch inference to extract features on images with a resolution of 1248$\\times$1248. The results demonstrate that Vim is capable of overcoming the computation&memory constraints on performing Transformer-style understanding for high-resolution images and it has great potential to be the next-generation backbone for vision foundation models. Code is available at https://github.com/hustvl/Vim."
    },
    {
        "paperId": "41a66997ce0a366bba3becf7c3f37c9aebb13fbd",
        "url": "https://www.semanticscholar.org/paper/41a66997ce0a366bba3becf7c3f37c9aebb13fbd",
        "title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis",
        "venue": "International Conference on Machine Learning",
        "year": 2024,
        "citationCount": 2671,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.03206, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "35175531",
                "name": "Patrick Esser"
            },
            {
                "authorId": "3411322",
                "name": "Sumith Kulal"
            },
            {
                "authorId": "119843260",
                "name": "A. Blattmann"
            },
            {
                "authorId": "2316859494",
                "name": "Rahim Entezari"
            },
            {
                "authorId": "2188737195",
                "name": "Jonas Muller"
            },
            {
                "authorId": "2289994508",
                "name": "Harry Saini"
            },
            {
                "authorId": "2290013499",
                "name": "Yam Levi"
            },
            {
                "authorId": "2053482699",
                "name": "Dominik Lorenz"
            },
            {
                "authorId": "40562186",
                "name": "Axel Sauer"
            },
            {
                "authorId": "2290014125",
                "name": "Frederic Boesel"
            },
            {
                "authorId": "2221125727",
                "name": "Dustin Podell"
            },
            {
                "authorId": "102541178",
                "name": "Tim Dockhorn"
            },
            {
                "authorId": "2221127565",
                "name": "Zion English"
            },
            {
                "authorId": "2221126982",
                "name": "Kyle Lacey"
            },
            {
                "authorId": "2290014122",
                "name": "Alex Goodwin"
            },
            {
                "authorId": "2290014387",
                "name": "Yannik Marek"
            },
            {
                "authorId": "1660819540",
                "name": "Robin Rombach"
            }
        ],
        "abstract": "Diffusion models create data from noise by inverting the forward paths of data towards noise and have emerged as a powerful generative modeling technique for high-dimensional, perceptual data such as images and videos. Rectified flow is a recent generative model formulation that connects data and noise in a straight line. Despite its better theoretical properties and conceptual simplicity, it is not yet decisively established as standard practice. In this work, we improve existing noise sampling techniques for training rectified flow models by biasing them towards perceptually relevant scales. Through a large-scale study, we demonstrate the superior performance of this approach compared to established diffusion formulations for high-resolution text-to-image synthesis. Additionally, we present a novel transformer-based architecture for text-to-image generation that uses separate weights for the two modalities and enables a bidirectional flow of information between image and text tokens, improving text comprehension, typography, and human preference ratings. We demonstrate that this architecture follows predictable scaling trends and correlates lower validation loss to improved text-to-image synthesis as measured by various metrics and human evaluations. Our largest models outperform state-of-the-art models, and we will make our experimental data, code, and model weights publicly available."
    },
    {
        "paperId": "ca9f5b3bf0f54ad97513e6175b30497873670fed",
        "url": "https://www.semanticscholar.org/paper/ca9f5b3bf0f54ad97513e6175b30497873670fed",
        "title": "Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality",
        "venue": "International Conference on Machine Learning",
        "year": 2024,
        "citationCount": 1015,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.21060, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2269146652",
                "name": "Tri Dao"
            },
            {
                "authorId": "2269161650",
                "name": "Albert Gu"
            }
        ],
        "abstract": "While Transformers have been the main architecture behind deep learning's success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices. Our state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling."
    },
    {
        "paperId": "eb7cfabee095cd9392ba2ff0f4d4ccfad0108427",
        "url": "https://www.semanticscholar.org/paper/eb7cfabee095cd9392ba2ff0f4d4ccfad0108427",
        "title": "Pushing the EL Envelope",
        "venue": "International Joint Conference on Artificial Intelligence",
        "year": 2005,
        "citationCount": 1287,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.25368/2022.144?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.25368/2022.144, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1710233",
                "name": "F. Baader"
            },
            {
                "authorId": "144105939",
                "name": "S. Brandt"
            },
            {
                "authorId": "144350265",
                "name": "C. Lutz"
            }
        ],
        "abstract": "Recently, it has been shown that the small DL EL, which allows for conjunction and existential restrictions, has better algorithmic properties than its counterpart FL, which allows for conjunction and value restrictions. Whereas the subsumption problem in FL becomes already intractable in the presence of aclyc TBoxes, it remains tractable in EL even w.r.t. general concept inclusion axioms (GCIs). On the one hand, we will extend the positive result for EL by identifying a set of expressive means that can be added to EL without sacrificing tractability. On the other hand, we will show that basically all other additions of typical DL constructors to EL with GCIs make subsumption intractable, and in most cases even EXPTIME-complete. In addition, we will show that subsumption in FL with GCIs is EXPTIME-complete."
    },
    {
        "paperId": "a9fee459ed211f53bfadef22e3ab774d0e927358",
        "url": "https://www.semanticscholar.org/paper/a9fee459ed211f53bfadef22e3ab774d0e927358",
        "title": "Computing Semantic Relatedness Using Wikipedia-based Explicit Semantic Analysis",
        "venue": "International Joint Conference on Artificial Intelligence",
        "year": 2007,
        "citationCount": 2384,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "1718798",
                "name": "E. Gabrilovich"
            },
            {
                "authorId": "2309269",
                "name": "Shaul Markovitch"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "4dfe43ddfcfbe00dd663a4d70b0df9dcc8c92184",
        "url": "https://www.semanticscholar.org/paper/4dfe43ddfcfbe00dd663a4d70b0df9dcc8c92184",
        "title": "YAGO2: A Spatially and Temporally Enhanced Knowledge Base from Wikipedia: Extended Abstract",
        "venue": "International Joint Conference on Artificial Intelligence",
        "year": 2013,
        "citationCount": 1310,
        "openAccessPdf": {
            "url": "https://doi.org/10.1016/j.artint.2012.06.001",
            "status": "BRONZE",
            "license": "publisher-specific-oa",
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.artint.2012.06.001?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.artint.2012.06.001, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1727527",
                "name": "Johannes Hoffart"
            },
            {
                "authorId": "1679784",
                "name": "Fabian M. Suchanek"
            },
            {
                "authorId": "1806397",
                "name": "K. Berberich"
            },
            {
                "authorId": "1751591",
                "name": "G. Weikum"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "39a1abcbb87d8ff48ee47d446411a3455451f25b",
        "url": "https://www.semanticscholar.org/paper/39a1abcbb87d8ff48ee47d446411a3455451f25b",
        "title": "Deep Convolutional Neural Networks on Multichannel Time Series for Human Activity Recognition",
        "venue": "International Joint Conference on Artificial Intelligence",
        "year": 2015,
        "citationCount": 1134,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "2109612299",
                "name": "Jianbo Yang"
            },
            {
                "authorId": "2623209",
                "name": "M. N. Nguyen"
            },
            {
                "authorId": "2307813",
                "name": "P. P. San"
            },
            {
                "authorId": "39952499",
                "name": "Xiaoli Li"
            },
            {
                "authorId": "1781256",
                "name": "S. Krishnaswamy"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "fce14c6aa64e888456256ac6796744683165a0ff",
        "url": "https://www.semanticscholar.org/paper/fce14c6aa64e888456256ac6796744683165a0ff",
        "title": "Network Representation Learning with Rich Text Information",
        "venue": "International Joint Conference on Artificial Intelligence",
        "year": 2015,
        "citationCount": 1090,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "3443627",
                "name": "Cheng Yang"
            },
            {
                "authorId": "49293587",
                "name": "Zhiyuan Liu"
            },
            {
                "authorId": "1678783",
                "name": "Deli Zhao"
            },
            {
                "authorId": "1753344",
                "name": "Maosong Sun"
            },
            {
                "authorId": "152536227",
                "name": "Edward Y. Chang"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "d9595f9acec303c7ad06eeae8a054743cbaf21ce",
        "url": "https://www.semanticscholar.org/paper/d9595f9acec303c7ad06eeae8a054743cbaf21ce",
        "title": "Detecting Rumors from Microblogs with Recurrent Neural Networks",
        "venue": "International Joint Conference on Artificial Intelligence",
        "year": 2016,
        "citationCount": 1231,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "2157403695",
                "name": "Jing Ma"
            },
            {
                "authorId": "145816335",
                "name": "Wei Gao"
            },
            {
                "authorId": "143930195",
                "name": "P. Mitra"
            },
            {
                "authorId": "2399803",
                "name": "Sejeong Kwon"
            },
            {
                "authorId": "144715575",
                "name": "B. Jansen"
            },
            {
                "authorId": "1784988",
                "name": "Kam-Fai Wong"
            },
            {
                "authorId": "1775511",
                "name": "Meeyoung Cha"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "e28ab7c3b994dd4e30baac1eb67c7f87e40c2b7b",
        "url": "https://www.semanticscholar.org/paper/e28ab7c3b994dd4e30baac1eb67c7f87e40c2b7b",
        "title": "Recurrent Neural Network for Text Classification with Multi-Task Learning",
        "venue": "International Joint Conference on Artificial Intelligence",
        "year": 2016,
        "citationCount": 1336,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1605.05101, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "144118452",
                "name": "Pengfei Liu"
            },
            {
                "authorId": "1767521",
                "name": "Xipeng Qiu"
            },
            {
                "authorId": "1790227",
                "name": "Xuanjing Huang"
            }
        ],
        "abstract": "Neural network based methods have obtained great progress on a variety of natural language processing tasks. However, in most previous works, the models are learned based on single-task supervised objectives, which often suffer from insufficient training data. In this paper, we use the multi-task learning framework to jointly learn across multiple related tasks. Based on recurrent neural network, we propose three different mechanisms of sharing information to model text with task-specific and shared layers. The entire network is trained jointly on all these tasks. Experiments on four benchmark text classification tasks show that our proposed models can improve the performance of a task with the help of other related tasks."
    },
    {
        "paperId": "1d122a074c936fcfd95faf44608e377a9d1799c8",
        "url": "https://www.semanticscholar.org/paper/1d122a074c936fcfd95faf44608e377a9d1799c8",
        "title": "DeepFM: A Factorization-Machine based Neural Network for CTR Prediction",
        "venue": "International Joint Conference on Artificial Intelligence",
        "year": 2017,
        "citationCount": 2903,
        "openAccessPdf": {
            "url": "https://www.ijcai.org/proceedings/2017/0239.pdf",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1703.04247, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3339005",
                "name": "Huifeng Guo"
            },
            {
                "authorId": "2824766",
                "name": "Ruiming Tang"
            },
            {
                "authorId": "144782498",
                "name": "Yunming Ye"
            },
            {
                "authorId": "7718952",
                "name": "Zhenguo Li"
            },
            {
                "authorId": "1996703",
                "name": "Xiuqiang He"
            }
        ],
        "abstract": "Learning sophisticated feature interactions behind user behaviors is critical in maximizing CTR for recommender systems. Despite great progress, existing methods seem to have a strong bias towards low- or high-order interactions, or require expertise feature engineering. In this paper, we show that it is possible to derive an end-to-end learning model that emphasizes both low- and high-order feature interactions. The proposed model, DeepFM, combines the power of factorization machines for recommendation and deep learning for feature learning in a new neural network architecture. Compared to the latest Wide & Deep model from Google, DeepFM has a shared input to its \"wide\" and \"deep\" parts, with no need of feature engineering besides raw features. Comprehensive experiments are conducted to demonstrate the effectiveness and efficiency of DeepFM over the existing models for CTR prediction, on both benchmark data and commercial data."
    },
    {
        "paperId": "5e40be36d11483923cb12260bed6e8f7ed355ff3",
        "url": "https://www.semanticscholar.org/paper/5e40be36d11483923cb12260bed6e8f7ed355ff3",
        "title": "Interactive Attention Networks for Aspect-Level Sentiment Classification",
        "venue": "International Joint Conference on Artificial Intelligence",
        "year": 2017,
        "citationCount": 1060,
        "openAccessPdf": {
            "url": "https://www.ijcai.org/proceedings/2017/0568.pdf",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1709.00893, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2963245",
                "name": "Dehong Ma"
            },
            {
                "authorId": "1695451",
                "name": "Sujian Li"
            },
            {
                "authorId": "1686594",
                "name": "Xiaodong Zhang"
            },
            {
                "authorId": "1781885",
                "name": "Houfeng Wang"
            }
        ],
        "abstract": "Aspect-level sentiment classification aims at identifying the sentiment polarity of specific target in its context. Previous approaches have realized the importance of targets in sentiment classification and developed various methods with the goal of precisely modeling their contexts via generating target-specific representations. However, these studies always ignore the separate modeling of targets. In this paper, we argue that both targets and contexts deserve special treatment and need to be learned their own representations via interactive learning. Then, we propose the interactive attention networks (IAN) to interactively learn attentions in the contexts and targets, and generate the representations for targets and contexts separately. With this design, the IAN model can well represent a target and its collocative context, which is helpful to sentiment classification. Experimental results on SemEval 2014 Datasets demonstrate the effectiveness of our model."
    },
    {
        "paperId": "72edcb3788f9c141a3ed28e6d36f75ca4977d27e",
        "url": "https://www.semanticscholar.org/paper/72edcb3788f9c141a3ed28e6d36f75ca4977d27e",
        "title": "Spatio-temporal Graph Convolutional Neural Network: A Deep Learning Framework for Traffic Forecasting",
        "venue": "International Joint Conference on Artificial Intelligence",
        "year": 2017,
        "citationCount": 4418,
        "openAccessPdf": {
            "url": "https://www.ijcai.org/proceedings/2018/0505.pdf",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1709.04875, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "46806278",
                "name": "Ting Yu"
            },
            {
                "authorId": "26379330",
                "name": "Haoteng Yin"
            },
            {
                "authorId": "1703952",
                "name": "Zhanxing Zhu"
            }
        ],
        "abstract": "Timely accurate traffic forecast is crucial for urban traffic control and guidance. Due to the high nonlinearity and complexity of traffic flow, traditional methods cannot satisfy the requirements of mid-and-long term prediction tasks and often neglect spatial and temporal dependencies. In this paper, we propose a novel deep learning framework, Spatio-Temporal Graph Convolutional Networks (STGCN), to tackle the time series prediction problem in traffic domain. Instead of applying regular convolutional and recurrent units, we formulate the problem on graphs and build the model with complete convolutional structures, which enable much faster training speed with fewer parameters. Experiments show that our model STGCN effectively captures comprehensive spatio-temporal correlations through modeling multi-scale traffic networks and consistently outperforms state-of-the-art baselines on various real-world traffic datasets."
    },
    {
        "paperId": "76624f8ff1391e942c3313b79ed08a335aa5077a",
        "url": "https://www.semanticscholar.org/paper/76624f8ff1391e942c3313b79ed08a335aa5077a",
        "title": "A Dual-Stage Attention-Based Recurrent Neural Network for Time Series Prediction",
        "venue": "International Joint Conference on Artificial Intelligence",
        "year": 2017,
        "citationCount": 1410,
        "openAccessPdf": {
            "url": "https://www.ijcai.org/proceedings/2017/0366.pdf",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1704.02971, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "145592705",
                "name": "Yao Qin"
            },
            {
                "authorId": "2451800",
                "name": "Dongjin Song"
            },
            {
                "authorId": "1825678210",
                "name": "Haifeng Chen"
            },
            {
                "authorId": "145859270",
                "name": "Wei Cheng"
            },
            {
                "authorId": "1791767",
                "name": "Guofei Jiang"
            },
            {
                "authorId": "48524582",
                "name": "G. Cottrell"
            }
        ],
        "abstract": "The Nonlinear autoregressive exogenous (NARX) model, which predicts the current value of a time series based upon its previous values as well as the current and past values of multiple driving (exogenous) series, has been studied for decades. Despite the fact that various NARX models have been developed, few of them can capture the long-term temporal dependencies appropriately and select the relevant driving series to make predictions. In this paper, we propose a dual-stage attention-based recurrent neural network (DA-RNN) to address these two issues. In the first stage, we introduce an input attention mechanism to adaptively extract relevant driving series (a.k.a., input features) at each time step by referring to the previous encoder hidden state. In the second stage, we use a temporal attention mechanism to select relevant encoder hidden states across all time steps. With this dual-stage attention scheme, our model can not only make predictions effectively, but can also be easily interpreted. Thorough empirical studies based upon the SML 2010 dataset and the NASDAQ 100 Stock dataset demonstrate that the DA-RNN can outperform state-of-the-art methods for time series prediction."
    },
    {
        "paperId": "7ea35b35392c6ef5738635cec7d17b24fe3e4f04",
        "url": "https://www.semanticscholar.org/paper/7ea35b35392c6ef5738635cec7d17b24fe3e4f04",
        "title": "Deep forest",
        "venue": "International Joint Conference on Artificial Intelligence",
        "year": 2017,
        "citationCount": 1077,
        "openAccessPdf": {
            "url": "https://academic.oup.com/nsr/article-pdf/6/1/74/27981411/nwy108.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1702.08835, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "145624000",
                "name": "Zhi-Hua Zhou"
            },
            {
                "authorId": "2108993600",
                "name": "Ji Feng"
            }
        ],
        "abstract": "Abstract Current deep-learning models are mostly built upon neural networks, i.e. multiple layers of parameterized differentiable non-linear modules that can be trained by backpropagation. In this paper, we explore the possibility of building deep models based on non-differentiable modules such as decision trees. After a discussion about the mystery behind deep neural networks, particularly by contrasting them with shallow neural networks and traditional machine-learning techniques such as decision trees and boosting machines, we conjecture that the success of deep neural networks owes much to three characteristics, i.e. layer-by-layer processing, in-model feature transformation and sufficient model complexity. On one hand, our conjecture may offer inspiration for theoretical understanding of deep learning; on the other hand, to verify the conjecture, we propose an approach that generates deep forest holding these characteristics. This is a decision-tree ensemble approach, with fewer hyper-parameters than deep neural networks, and its model complexity can be automatically determined in a data-dependent way. Experiments show that its performance is quite robust to hyper-parameter settings, such that in most cases, even across different data from different domains, it is able to achieve excellent performance by using the same default setting. This study opens the door to deep learning based on non-differentiable modules without gradient-based adjustment, and exhibits the possibility of constructing deep models without backpropagation."
    },
    {
        "paperId": "52ff452c2c38d082c07eb434996e07a8c242a692",
        "url": "https://www.semanticscholar.org/paper/52ff452c2c38d082c07eb434996e07a8c242a692",
        "title": "Soft Filter Pruning for Accelerating Deep Convolutional Neural Networks",
        "venue": "International Joint Conference on Artificial Intelligence",
        "year": 2018,
        "citationCount": 1025,
        "openAccessPdf": {
            "url": "https://www.ijcai.org/proceedings/2018/0309.pdf",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1808.06866, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1743616",
                "name": "Yang He"
            },
            {
                "authorId": "3374337",
                "name": "Guoliang Kang"
            },
            {
                "authorId": "145808756",
                "name": "Xuanyi Dong"
            },
            {
                "authorId": "35782003",
                "name": "Yanwei Fu"
            },
            {
                "authorId": "2048438762",
                "name": "Yi Yang"
            }
        ],
        "abstract": "This paper proposed a Soft Filter Pruning (SFP) method to accelerate the inference procedure of deep Convolutional Neural Networks (CNNs). Specifically, the proposed SFP enables the pruned filters to be updated when training the model after pruning. SFP has two advantages over previous works: (1) Larger model capacity. Updating previously pruned filters provides our approach with larger optimization space than fixing the filters to zero. Therefore, the network trained by our method has a larger model capacity to learn from the training data. (2) Less dependence on the pretrained model. Large capacity enables SFP to train from scratch and prune the model simultaneously. In contrast, previous filter pruning methods should be conducted on the basis of the pre-trained model to guarantee their performance. Empirically, SFP from scratch outperforms the previous filter pruning methods. Moreover, our approach has been demonstrated effective for many advanced CNN architectures. Notably, on ILSCRC-2012, SFP reduces more than 42% FLOPs on ResNet-101 with even 0.2% top-5 accuracy improvement, which has advanced the state-of-the-art. Code is publicly available on GitHub: https://github.com/he-y/softfilter-pruning"
    },
    {
        "paperId": "d1e5942d8cb764e58f4c38a0c254ca56782a98c3",
        "url": "https://www.semanticscholar.org/paper/d1e5942d8cb764e58f4c38a0c254ca56782a98c3",
        "title": "Enhanced-alignment Measure for Binary Foreground Map Evaluation",
        "venue": "International Joint Conference on Artificial Intelligence",
        "year": 2018,
        "citationCount": 1433,
        "openAccessPdf": {
            "url": "https://www.ijcai.org/proceedings/2018/0097.pdf",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1805.10421, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "23999143",
                "name": "Deng-Ping Fan"
            },
            {
                "authorId": "2057185367",
                "name": "Cheng Gong"
            },
            {
                "authorId": "145871531",
                "name": "Yang Cao"
            },
            {
                "authorId": "144651368",
                "name": "Bo Ren"
            },
            {
                "authorId": "37535930",
                "name": "Ming-Ming Cheng"
            },
            {
                "authorId": "3177797",
                "name": "A. Borji"
            }
        ],
        "abstract": "The existing binary foreground map (FM) measures address various types of errors in either pixel-wise or structural ways. These measures consider pixel-level match or image-level information independently, while cognitive vision studies have shown that human vision is highly sensitive to both global information and local details in scenes. In this paper, we take a detailed look at current binary FM evaluation measures and propose a novel and effective E-measure (Enhanced-alignment measure). Our measure combines local pixel values with the image-level mean value in one term, jointly capturing image-level statistics and local pixel matching information. We demonstrate the superiority of our measure over the available measures on 4 popular datasets via 5 meta-measures, including ranking models for applications, demoting generic, random Gaussian noise maps, ground-truth switch, as well as human judgments. We find large improvements in almost all the meta-measures. For instance, in terms of application ranking, we observe improvement ranging from 9.08% to 19.65% compared with other popular measures."
    },
    {
        "paperId": "fae129338c0899576524506008427f64477d3967",
        "url": "https://www.semanticscholar.org/paper/fae129338c0899576524506008427f64477d3967",
        "title": "Graph WaveNet for Deep Spatial-Temporal Graph Modeling",
        "venue": "International Joint Conference on Artificial Intelligence",
        "year": 2019,
        "citationCount": 2711,
        "openAccessPdf": {
            "url": "https://www.ijcai.org/proceedings/2019/0264.pdf",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1906.00121, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2109557884",
                "name": "Zonghan Wu"
            },
            {
                "authorId": "2585415",
                "name": "Shirui Pan"
            },
            {
                "authorId": "2062835",
                "name": "Guodong Long"
            },
            {
                "authorId": "1746594",
                "name": "Jing Jiang"
            },
            {
                "authorId": "48934799",
                "name": "Chengqi Zhang"
            }
        ],
        "abstract": "Spatial-temporal graph modeling is an important task to analyze the spatial relations and temporal trends of components in a system. Existing approaches mostly capture the spatial dependency on a fixed graph structure, assuming that the underlying relation between entities is pre-determined. However, the explicit graph structure (relation) does not necessarily reflect the true dependency and genuine relation may be missing due to the incomplete connections in the data. Furthermore, existing methods are ineffective to capture the temporal trends as the RNNs or CNNs employed in these methods cannot capture long-range temporal sequences. To overcome these limitations, we propose in this paper a novel graph neural network architecture, {Graph WaveNet}, for spatial-temporal graph modeling. By developing a novel adaptive dependency matrix and learn it through node embedding, our model can precisely capture the hidden spatial dependency in the data. With a stacked dilated 1D convolution component whose receptive field grows exponentially as the number of layers increases, Graph WaveNet is able to handle very long sequences. These two components are integrated seamlessly in a unified framework and the whole framework is learned in an end-to-end manner. Experimental results on two public traffic network datasets, METR-LA and PEMS-BAY, demonstrate the superior performance of our algorithm."
    },
    {
        "paperId": "ed4087f6e8d77452810979f58246c5b2ad846cf8",
        "url": "https://www.semanticscholar.org/paper/ed4087f6e8d77452810979f58246c5b2ad846cf8",
        "title": "Transformers in Time Series: A Survey",
        "venue": "International Joint Conference on Artificial Intelligence",
        "year": 2022,
        "citationCount": 1170,
        "openAccessPdf": {
            "url": "https://www.ijcai.org/proceedings/2023/0759.pdf",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2202.07125, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3308963",
                "name": "Qingsong Wen"
            },
            {
                "authorId": "50018188",
                "name": "Tian Zhou"
            },
            {
                "authorId": "144677904",
                "name": "Chao Zhang"
            },
            {
                "authorId": "2109780096",
                "name": "Weiqiu Chen"
            },
            {
                "authorId": "1387898803",
                "name": "Ziqing Ma"
            },
            {
                "authorId": "3063894",
                "name": "Junchi Yan"
            },
            {
                "authorId": "2110940896",
                "name": "Liang Sun"
            }
        ],
        "abstract": "Transformers have achieved superior performances in many tasks in natural language processing and computer vision, which also triggered great interest in the time series community. Among multiple advantages of Transformers, the ability to capture long-range dependencies and interactions is especially attractive for time series modeling, leading to exciting progress in various time series applications. In this paper, we systematically review Transformer schemes for time series modeling by highlighting their strengths as well as limitations. In particular, we examine the development of time series Transformers in two perspectives. From the perspective of network structure, we summarize the adaptations and modifications that have been made to Transformers in order to accommodate the challenges in time series analysis. From the perspective of applications, we categorize time series Transformers based on common tasks including forecasting, anomaly detection, and classification. Empirically, we perform robust analysis, model size analysis, and seasonal-trend decomposition analysis to study how Transformers perform in time series. Finally, we discuss and suggest future directions to provide useful research guidance."
    },
    {
        "paperId": "0407b605b8f55db72e2545586bfe8e946b691b70",
        "url": "https://www.semanticscholar.org/paper/0407b605b8f55db72e2545586bfe8e946b691b70",
        "title": "An Empirical Investigation of Catastrophic Forgeting in Gradient-Based Neural Networks",
        "venue": "International Conference on Learning Representations",
        "year": 2013,
        "citationCount": 1570,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1312.6211, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "153440022",
                "name": "I. Goodfellow"
            },
            {
                "authorId": "153583218",
                "name": "Mehdi Mirza"
            },
            {
                "authorId": "2058614620",
                "name": "Xia Da"
            },
            {
                "authorId": "1760871",
                "name": "Aaron C. Courville"
            },
            {
                "authorId": "1751762",
                "name": "Yoshua Bengio"
            }
        ],
        "abstract": "Catastrophic forgetting is a problem faced by many machine learning models and algorithms. When trained on one task, then trained on a second task, many machine learning models \"forget\" how to perform the first task. This is widely believed to be a serious problem for neural networks. Here, we investigate the extent to which the catastrophic forgetting problem occurs for modern neural networks, comparing both established and recent gradient-based training algorithms and activation functions. We also examine the effect of the relationship between the first task and the second task on catastrophic forgetting. We find that it is always best to train using the dropout algorithm--the dropout algorithm is consistently best at adapting to the new task, remembering the old task, and has the best tradeoff curve between these two extremes. We find that different tasks and relationships between tasks result in very different rankings of activation function performance. This suggests the choice of activation function should always be cross-validated."
    },
    {
        "paperId": "0abb49fe138e8fb7332c26b148a48d0db39724fc",
        "url": "https://www.semanticscholar.org/paper/0abb49fe138e8fb7332c26b148a48d0db39724fc",
        "title": "Stochastic Pooling for Regularization of Deep Convolutional Neural Networks",
        "venue": "International Conference on Learning Representations",
        "year": 2013,
        "citationCount": 1015,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1301.3557, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "48799969",
                "name": "Matthew D. Zeiler"
            },
            {
                "authorId": "2276554",
                "name": "R. Fergus"
            }
        ],
        "abstract": "We introduce a simple and effective method for regularizing large convolutional neural networks. We replace the conventional deterministic pooling operations with a stochastic procedure, randomly picking the activation within each pooling region according to a multinomial distribution, given by the activities within the pooling region. The approach is hyper-parameter free and can be combined with other regularization approaches, such as dropout and data augmentation. We achieve state-of-the-art performance on four image datasets, relative to other approaches that do not utilize data augmentation."
    },
    {
        "paperId": "1109b663453e78a59e4f66446d71720ac58cec25",
        "url": "https://www.semanticscholar.org/paper/1109b663453e78a59e4f66446d71720ac58cec25",
        "title": "OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks",
        "venue": "International Conference on Learning Representations",
        "year": 2013,
        "citationCount": 5098,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1312.6229, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3142556",
                "name": "P. Sermanet"
            },
            {
                "authorId": "2060028",
                "name": "D. Eigen"
            },
            {
                "authorId": null,
                "name": "Xiang Zhang"
            },
            {
                "authorId": "143949035",
                "name": "Michal Mathieu"
            },
            {
                "authorId": "2276554",
                "name": "R. Fergus"
            },
            {
                "authorId": "1688882",
                "name": "Yann LeCun"
            }
        ],
        "abstract": "We present an integrated framework for using Convolutional Networks for classification, localization and detection. We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object boundaries. Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence. We show that different tasks can be learned simultaneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks. In post-competition work, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat."
    },
    {
        "paperId": "533ee188324b833e059cb59b654e6160776d5812",
        "url": "https://www.semanticscholar.org/paper/533ee188324b833e059cb59b654e6160776d5812",
        "title": "How to Construct Deep Recurrent Neural Networks",
        "venue": "International Conference on Learning Representations",
        "year": 2013,
        "citationCount": 1052,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1312.6026, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1996134",
                "name": "Razvan Pascanu"
            },
            {
                "authorId": "1854385",
                "name": "aglar Glehre"
            },
            {
                "authorId": "1979489",
                "name": "Kyunghyun Cho"
            },
            {
                "authorId": "1751762",
                "name": "Yoshua Bengio"
            }
        ],
        "abstract": "In this paper, we explore different ways to extend a recurrent neural network (RNN) to a \\textit{deep} RNN. We start by arguing that the concept of depth in an RNN is not as clear as it is in feedforward neural networks. By carefully analyzing and understanding the architecture of an RNN, however, we find three points of an RNN which may be made deeper; (1) input-to-hidden function, (2) hidden-to-hidden transition and (3) hidden-to-output function. Based on this observation, we propose two novel architectures of a deep RNN which are orthogonal to an earlier attempt of stacking multiple recurrent layers to build a deep RNN (Schmidhuber, 1992; El Hihi and Bengio, 1996). We provide an alternative interpretation of these deep RNNs using a novel framework based on neural operators. The proposed deep RNNs are empirically evaluated on the tasks of polyphonic music prediction and language modeling. The experimental result supports our claim that the proposed deep RNNs benefit from the depth and outperform the conventional, shallow RNNs."
    },
    {
        "paperId": "5e83ab70d0cbc003471e87ec306d27d9c80ecb16",
        "url": "https://www.semanticscholar.org/paper/5e83ab70d0cbc003471e87ec306d27d9c80ecb16",
        "title": "Network In Network",
        "venue": "International Conference on Learning Representations",
        "year": 2013,
        "citationCount": 6556,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1312.4400, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2115913164",
                "name": "Min Lin"
            },
            {
                "authorId": "35370244",
                "name": "Qiang Chen"
            },
            {
                "authorId": "143653681",
                "name": "Shuicheng Yan"
            }
        ],
        "abstract": "We propose a novel deep network structure called \"Network In Network\" (NIN) to enhance model discriminability for local patches within the receptive field. The conventional convolutional layer uses linear filters followed by a nonlinear activation function to scan the input. Instead, we build micro neural networks with more complex structures to abstract the data within the receptive field. We instantiate the micro neural network with a multilayer perceptron, which is a potent function approximator. The feature maps are obtained by sliding the micro networks over the input in a similar manner as CNN; they are then fed into the next layer. Deep NIN can be implemented by stacking mutiple of the above described structure. With enhanced local modeling via the micro network, we are able to utilize global average pooling over feature maps in the classification layer, which is easier to interpret and less prone to overfitting than traditional fully connected layers. We demonstrated the state-of-the-art classification performances with NIN on CIFAR-10 and CIFAR-100, and reasonable performances on SVHN and MNIST datasets."
    },
    {
        "paperId": "5e925a9f1e20df61d1e860a7aa71894b35a1c186",
        "url": "https://www.semanticscholar.org/paper/5e925a9f1e20df61d1e860a7aa71894b35a1c186",
        "title": "Spectral Networks and Locally Connected Networks on Graphs",
        "venue": "International Conference on Learning Representations",
        "year": 2013,
        "citationCount": 5229,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1312.6203, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "143627859",
                "name": "Joan Bruna"
            },
            {
                "authorId": "2563432",
                "name": "Wojciech Zaremba"
            },
            {
                "authorId": "3149531",
                "name": "Arthur Szlam"
            },
            {
                "authorId": "1688882",
                "name": "Yann LeCun"
            }
        ],
        "abstract": "Convolutional Neural Networks are extremely efficient architectures in image and audio recognition tasks, thanks to their ability to exploit the local translational invariance of signal classes over their domain. In this paper we consider possible generalizations of CNNs to signals defined on more general domains without the action of a translation group. In particular, we propose two constructions, one based upon a hierarchical clustering of the domain, and another based on the spectrum of the graph Laplacian. We show through experiments that for low-dimensional graphs it is possible to learn convolutional layers with a number of parameters independent of the input size, resulting in efficient deep architectures."
    },
    {
        "paperId": "5f5dc5b9a2ba710937e2c413b37b053cd673df02",
        "url": "https://www.semanticscholar.org/paper/5f5dc5b9a2ba710937e2c413b37b053cd673df02",
        "title": "Auto-Encoding Variational Bayes",
        "venue": "International Conference on Learning Representations",
        "year": 2013,
        "citationCount": 17251,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1312.6114, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1726807",
                "name": "Diederik P. Kingma"
            },
            {
                "authorId": "1678311",
                "name": "M. Welling"
            }
        ],
        "abstract": "Abstract: How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results."
    },
    {
        "paperId": "99c970348b8f70ce23d6641e201904ea49266b6e",
        "url": "https://www.semanticscholar.org/paper/99c970348b8f70ce23d6641e201904ea49266b6e",
        "title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks",
        "venue": "International Conference on Learning Representations",
        "year": 2013,
        "citationCount": 1965,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1312.6120, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "34927843",
                "name": "Andrew M. Saxe"
            },
            {
                "authorId": "1701656",
                "name": "James L. McClelland"
            },
            {
                "authorId": "25769960",
                "name": "S. Ganguli"
            }
        ],
        "abstract": "Despite the widespread practical success of deep learning methods, our theoretical understanding of the dynamics of learning in deep neural networks remains quite sparse. We attempt to bridge the gap between the theory and practice of deep learning by systematically analyzing learning dynamics for the restricted case of deep linear neural networks. Despite the linearity of their input-output map, such networks have nonlinear gradient descent dynamics on weights that change with the addition of each new hidden layer. We show that deep linear networks exhibit nonlinear learning phenomena similar to those seen in simulations of nonlinear networks, including long plateaus followed by rapid transitions to lower error solutions, and faster convergence from greedy unsupervised pretraining initial conditions than from random initial conditions. We provide an analytical description of these phenomena by finding new exact solutions to the nonlinear dynamics of deep learning. Our theoretical analysis also reveals the surprising finding that as the depth of a network approaches infinity, learning speed can nevertheless remain finite: for a special class of initial conditions on the weights, very deep networks incur only a finite, depth independent, delay in learning speed relative to shallow networks. We show that, under certain conditions on the training data, unsupervised pretraining can find this special class of initial conditions, while scaled random Gaussian initializations cannot. We further exhibit a new class of random orthogonal initial conditions on weights that, like unsupervised pre-training, enjoys depth independent learning times. We further show that these initial conditions also lead to faithful propagation of gradients even in deep nonlinear networks, as long as they operate in a special regime known as the edge of chaos."
    },
    {
        "paperId": "d891dc72cbd40ffaeefdc79f2e7afe1e530a23ad",
        "url": "https://www.semanticscholar.org/paper/d891dc72cbd40ffaeefdc79f2e7afe1e530a23ad",
        "title": "Intriguing properties of neural networks",
        "venue": "International Conference on Learning Representations",
        "year": 2013,
        "citationCount": 15952,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1312.6199, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2574060",
                "name": "Christian Szegedy"
            },
            {
                "authorId": "2563432",
                "name": "Wojciech Zaremba"
            },
            {
                "authorId": "1701686",
                "name": "I. Sutskever"
            },
            {
                "authorId": "143627859",
                "name": "Joan Bruna"
            },
            {
                "authorId": "1761978",
                "name": "D. Erhan"
            },
            {
                "authorId": "153440022",
                "name": "I. Goodfellow"
            },
            {
                "authorId": "2276554",
                "name": "R. Fergus"
            }
        ],
        "abstract": "Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. \nFirst, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks. \nSecond, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. We can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input."
    },
    {
        "paperId": "dc6ac3437f0a6e64e4404b1b9d188394f8a3bf71",
        "url": "https://www.semanticscholar.org/paper/dc6ac3437f0a6e64e4404b1b9d188394f8a3bf71",
        "title": "Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps",
        "venue": "International Conference on Learning Representations",
        "year": 2013,
        "citationCount": 7886,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1312.6034, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "34838386",
                "name": "K. Simonyan"
            },
            {
                "authorId": "1687524",
                "name": "A. Vedaldi"
            },
            {
                "authorId": "1688869",
                "name": "Andrew Zisserman"
            }
        ],
        "abstract": "This paper addresses the visualisation of image classification models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The first one generates an image, which maximises the class score [Erhan et al., 2009], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, specific to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classification ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks [Zeiler et al., 2013]."
    },
    {
        "paperId": "f6b51c8753a871dc94ff32152c00c01e94f90f09",
        "url": "https://www.semanticscholar.org/paper/f6b51c8753a871dc94ff32152c00c01e94f90f09",
        "title": "Efficient Estimation of Word Representations in Vector Space",
        "venue": "International Conference on Learning Representations",
        "year": 2013,
        "citationCount": 33352,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1301.3781, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2047446108",
                "name": "Tomas Mikolov"
            },
            {
                "authorId": "2118440152",
                "name": "Kai Chen"
            },
            {
                "authorId": "32131713",
                "name": "G. Corrado"
            },
            {
                "authorId": "49959210",
                "name": "J. Dean"
            }
        ],
        "abstract": "We propose two novel model architectures for computing continuous vector\nrepresentations of words from very large data sets. The quality of these\nrepresentations is measured in a word similarity task, and the results are\ncompared to the previously best performing techniques based on different types\nof neural networks. We observe large improvements in accuracy at much lower\ncomputational cost, i.e. it takes less than a day to learn high quality word\nvectors from a 1.6 billion words data set. Furthermore, we show that these\nvectors provide state-of-the-art performance on our test set for measuring\nsyntactic and semantic word similarities."
    },
    {
        "paperId": "33af9298e5399269a12d4b9901492fe406af62b4",
        "url": "https://www.semanticscholar.org/paper/33af9298e5399269a12d4b9901492fe406af62b4",
        "title": "Striving for Simplicity: The All Convolutional Net",
        "venue": "International Conference on Learning Representations",
        "year": 2014,
        "citationCount": 4899,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1412.6806, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2060551",
                "name": "Jost Tobias Springenberg"
            },
            {
                "authorId": "2841331",
                "name": "Alexey Dosovitskiy"
            },
            {
                "authorId": "1710872",
                "name": "T. Brox"
            },
            {
                "authorId": "3137672",
                "name": "Martin A. Riedmiller"
            }
        ],
        "abstract": "Most modern convolutional neural networks (CNNs) used for object recognition are built using the same principles: Alternating convolution and max-pooling layers followed by a small number of fully connected layers. We re-evaluate the state of the art for object recognition from small images with convolutional networks, questioning the necessity of different components in the pipeline. We find that max-pooling can simply be replaced by a convolutional layer with increased stride without loss in accuracy on several image recognition benchmarks. Following this finding -- and building on other recent work for finding simple network structures -- we propose a new architecture that consists solely of convolutional layers and yields competitive or state of the art performance on several object recognition datasets (CIFAR-10, CIFAR-100, ImageNet). To analyze the network we introduce a new variant of the \"deconvolution approach\" for visualizing features learned by CNNs, which can be applied to a broader range of network structures than existing approaches."
    },
    {
        "paperId": "39ad6c911f3351a3b390130a6e4265355b4d593b",
        "url": "https://www.semanticscholar.org/paper/39ad6c911f3351a3b390130a6e4265355b4d593b",
        "title": "Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs",
        "venue": "International Conference on Learning Representations",
        "year": 2014,
        "citationCount": 5148,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1412.7062, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "34192119",
                "name": "Liang-Chieh Chen"
            },
            {
                "authorId": "2776496",
                "name": "G. Papandreou"
            },
            {
                "authorId": "2010660",
                "name": "Iasonas Kokkinos"
            },
            {
                "authorId": "1702318",
                "name": "K. Murphy"
            },
            {
                "authorId": "145081362",
                "name": "A. Yuille"
            }
        ],
        "abstract": "Deep Convolutional Neural Networks (DCNNs) have recently shown state of the art performance in high level vision tasks, such as image classification and object detection. This work brings together methods from DCNNs and probabilistic graphical models for addressing the task of pixel-level classification (also called \"semantic image segmentation\"). We show that responses at the final layer of DCNNs are not sufficiently localized for accurate object segmentation. This is due to the very invariance properties that make DCNNs good for high level tasks. We overcome this poor localization property of deep networks by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF). Qualitatively, our \"DeepLab\" system is able to localize segment boundaries at a level of accuracy which is beyond previous methods. Quantitatively, our method sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 71.6% IOU accuracy in the test set. We show how these results can be obtained efficiently: Careful network re-purposing and a novel application of the 'hole' algorithm from the wavelet community allow dense computation of neural net responses at 8 frames per second on a modern GPU."
    },
    {
        "paperId": "401192b00b650adfa5ac49de59b720e1c81f1410",
        "url": "https://www.semanticscholar.org/paper/401192b00b650adfa5ac49de59b720e1c81f1410",
        "title": "Object Detectors Emerge in Deep Scene CNNs",
        "venue": "International Conference on Learning Representations",
        "year": 2014,
        "citationCount": 1311,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1412.6856, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "145291669",
                "name": "Bolei Zhou"
            },
            {
                "authorId": "2556428",
                "name": "A. Khosla"
            },
            {
                "authorId": "2677488",
                "name": "gata Lapedriza"
            },
            {
                "authorId": "143868587",
                "name": "A. Oliva"
            },
            {
                "authorId": "143805211",
                "name": "A. Torralba"
            }
        ],
        "abstract": "With the success of new computational architectures for visual processing, such as convolutional neural networks (CNN) and access to image databases with millions of labeled examples (e.g., ImageNet, Places), the state of the art in computer vision is advancing rapidly. One important factor for continued progress is to understand the representations that are learned by the inner layers of these deep architectures. Here we show that object detectors emerge from training CNNs to perform scene classification. As scenes are composed of objects, the CNN for scene classification automatically discovers meaningful objects detectors, representative of the learned scene categories. With object detectors emerging as a result of learning to recognize scenes, our work demonstrates that the same network can perform both scene recognition and object localization in a single forward-pass, without ever having been explicitly taught the notion of objects."
    },
    {
        "paperId": "54b2b6f35f1b5704dddfaa3a137a2f4ad3dfe745",
        "url": "https://www.semanticscholar.org/paper/54b2b6f35f1b5704dddfaa3a137a2f4ad3dfe745",
        "title": "Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN)",
        "venue": "International Conference on Learning Representations",
        "year": 2014,
        "citationCount": 1272,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1412.6632, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "36010601",
                "name": "Junhua Mao"
            },
            {
                "authorId": "145738410",
                "name": "W. Xu"
            },
            {
                "authorId": "2143686417",
                "name": "Yi Yang"
            },
            {
                "authorId": "152924487",
                "name": "Jiang Wang"
            },
            {
                "authorId": "145081362",
                "name": "A. Yuille"
            }
        ],
        "abstract": "In this paper, we present a multimodal Recurrent Neural Network (m-RNN) model for generating novel image captions. It directly models the probability distribution of generating a word given previous words and an image. Image captions are generated by sampling from this distribution. The model consists of two sub-networks: a deep recurrent neural network for sentences and a deep convolutional network for images. These two sub-networks interact with each other in a multimodal layer to form the whole m-RNN model. The effectiveness of our model is validated on four benchmark datasets (IAPR TC-12, Flickr 8K, Flickr 30K and MS COCO). Our model outperforms the state-of-the-art methods. In addition, we apply the m-RNN model to retrieval tasks for retrieving images or sentences, and achieves significant performance improvement over the state-of-the-art methods which directly optimize the ranking objective function for retrieval. The project page of this work is: www.stat.ucla.edu/~junhua.mao/m-RNN.html ."
    },
    {
        "paperId": "71ae756c75ac89e2d731c9c79649562b5768ff39",
        "url": "https://www.semanticscholar.org/paper/71ae756c75ac89e2d731c9c79649562b5768ff39",
        "title": "Memory Networks",
        "venue": "International Conference on Learning Representations",
        "year": 2014,
        "citationCount": 1793,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1410.3916, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "145183709",
                "name": "J. Weston"
            },
            {
                "authorId": "3295092",
                "name": "S. Chopra"
            },
            {
                "authorId": "1713934",
                "name": "Antoine Bordes"
            }
        ],
        "abstract": "Abstract: We describe a new class of learning models called memory networks. Memory networks reason with inference components combined with a long-term memory component; they learn how to use these jointly. The long-term memory can be read and written to, with the goal of using it for prediction. We investigate these models in the context of question answering (QA) where the long-term memory effectively acts as a (dynamic) knowledge base, and the output is a textual response. We evaluate them on a large-scale QA task, and a smaller, but more complex, toy task generated from a simulated world. In the latter, we show the reasoning power of such models by chaining multiple supporting sentences to answer questions that require understanding the intension of verbs."
    },
    {
        "paperId": "77e3c48aa10535276e7f570a3af594ba63de7d65",
        "url": "https://www.semanticscholar.org/paper/77e3c48aa10535276e7f570a3af594ba63de7d65",
        "title": "Training Deep Neural Networks on Noisy Labels with Bootstrapping",
        "venue": "International Conference on Learning Representations",
        "year": 2014,
        "citationCount": 1065,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1412.6596, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "144828948",
                "name": "Scott E. Reed"
            },
            {
                "authorId": "1697141",
                "name": "Honglak Lee"
            },
            {
                "authorId": "1838674",
                "name": "Dragomir Anguelov"
            },
            {
                "authorId": "2574060",
                "name": "Christian Szegedy"
            },
            {
                "authorId": "1761978",
                "name": "D. Erhan"
            },
            {
                "authorId": "39863668",
                "name": "Andrew Rabinovich"
            }
        ],
        "abstract": "Current state-of-the-art deep learning systems for visual object recognition and detection use purely supervised training with regularization such as dropout to avoid overfitting. The performance depends critically on the amount of labeled examples, and in current practice the labels are assumed to be unambiguous and accurate. However, this assumption often does not hold; e.g. in recognition, class labels may be missing; in detection, objects in the image may not be localized; and in general, the labeling may be subjective. In this work we propose a generic way to handle noisy and incomplete labeling by augmenting the prediction objective with a notion of consistency. We consider a prediction consistent if the same prediction is made given similar percepts, where the notion of similarity is between deep network features computed from the input data. In experiments we demonstrate that our approach yields substantial robustness to label noise on several datasets. On MNIST handwritten digits, we show that our model is robust to label corruption. On the Toronto Face Database, we show that our model handles well the case of subjective labels in emotion recognition, achieving state-of-the- art results, and can also benefit from unlabeled face images with no modification to our method. On the ILSVRC2014 detection challenge data, we show that our approach extends to very deep networks, high resolution images and structured outputs, and results in improved scalable detection."
    },
    {
        "paperId": "8604f376633af8b347e31d84c6150a93b11e34c2",
        "url": "https://www.semanticscholar.org/paper/8604f376633af8b347e31d84c6150a93b11e34c2",
        "title": "FitNets: Hints for Thin Deep Nets",
        "venue": "International Conference on Learning Representations",
        "year": 2014,
        "citationCount": 4336,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1412.6550, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2069136633",
                "name": "Adriana Romero"
            },
            {
                "authorId": "2482072",
                "name": "Nicolas Ballas"
            },
            {
                "authorId": "3127597",
                "name": "Samira Ebrahimi Kahou"
            },
            {
                "authorId": "3186079",
                "name": "Antoine Chassang"
            },
            {
                "authorId": "143706039",
                "name": "C. Gatta"
            },
            {
                "authorId": "1751762",
                "name": "Yoshua Bengio"
            }
        ],
        "abstract": "While depth tends to improve network performances, it also makes gradient-based training more difficult since deeper networks tend to be more non-linear. The recently proposed knowledge distillation approach is aimed at obtaining small and fast-to-execute models, and it has shown that a student network could imitate the soft output of a larger teacher network or ensemble of networks. In this paper, we extend this idea to allow the training of a student that is deeper and thinner than the teacher, using not only the outputs but also the intermediate representations learned by the teacher as hints to improve the training process and final performance of the student. Because the student intermediate hidden layer will generally be smaller than the teacher's intermediate hidden layer, additional parameters are introduced to map the student hidden layer to the prediction of the teacher hidden layer. This allows one to train deeper students that can generalize better or run faster, a trade-off that is controlled by the chosen student capacity. For example, on CIFAR-10, a deep student network with almost 10.4 times less parameters outperforms a larger, state-of-the-art teacher network."
    },
    {
        "paperId": "86412306b777ee35aba71d4795b02915cb8a04c3",
        "url": "https://www.semanticscholar.org/paper/86412306b777ee35aba71d4795b02915cb8a04c3",
        "title": "Embedding Entities and Relations for Learning and Inference in Knowledge Bases",
        "venue": "International Conference on Learning Representations",
        "year": 2014,
        "citationCount": 3551,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1412.6575, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "7324641",
                "name": "Bishan Yang"
            },
            {
                "authorId": "144105277",
                "name": "Wen-tau Yih"
            },
            {
                "authorId": "144137069",
                "name": "Xiaodong He"
            },
            {
                "authorId": "1800422",
                "name": "Jianfeng Gao"
            },
            {
                "authorId": "144718788",
                "name": "L. Deng"
            }
        ],
        "abstract": "Abstract: We consider learning representations of entities and relations in KBs using the neural-embedding approach. We show that most existing models, including NTN (Socher et al., 2013) and TransE (Bordes et al., 2013b), can be generalized under a unified learning framework, where entities are low-dimensional vectors learned from a neural network and relations are bilinear and/or linear mapping functions. Under this framework, we compare a variety of embedding models on the link prediction task. We show that a simple bilinear formulation achieves new state-of-the-art results for the task (achieving a top-10 accuracy of 73.2% vs. 54.7% by TransE on Freebase). Furthermore, we introduce a novel approach that utilizes the learned relation embeddings to mine logical rules such as \"BornInCity(a,b) and CityInCountry(b,c) => Nationality(a,c)\". We find that embeddings learned from the bilinear objective are particularly good at capturing relational semantics and that the composition of relations is characterized by matrix multiplication. More interestingly, we demonstrate that our embedding-based rule extraction approach successfully outperforms a state-of-the-art confidence-based rule mining approach in mining Horn rules that involve compositional reasoning."
    },
    {
        "paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8",
        "url": "https://www.semanticscholar.org/paper/a6cb366736791bcccc5c8639de5a8f9636bf87e8",
        "title": "Adam: A Method for Stochastic Optimization",
        "venue": "International Conference on Learning Representations",
        "year": 2014,
        "citationCount": 160974,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1412.6980, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1726807",
                "name": "Diederik P. Kingma"
            },
            {
                "authorId": "2503659",
                "name": "Jimmy Ba"
            }
        ],
        "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm."
    },
    {
        "paperId": "bee044c8e8903fb67523c1f8c105ab4718600cdb",
        "url": "https://www.semanticscholar.org/paper/bee044c8e8903fb67523c1f8c105ab4718600cdb",
        "title": "Explaining and Harnessing Adversarial Examples",
        "venue": "International Conference on Learning Representations",
        "year": 2014,
        "citationCount": 20930,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1412.6572, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "153440022",
                "name": "I. Goodfellow"
            },
            {
                "authorId": "1789737",
                "name": "Jonathon Shlens"
            },
            {
                "authorId": "2574060",
                "name": "Christian Szegedy"
            }
        ],
        "abstract": "Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset."
    },
    {
        "paperId": "dc8301b67f98accbb331190dd7bd987952a692af",
        "url": "https://www.semanticscholar.org/paper/dc8301b67f98accbb331190dd7bd987952a692af",
        "title": "NICE: Non-linear Independent Components Estimation",
        "venue": "International Conference on Learning Representations",
        "year": 2014,
        "citationCount": 2475,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1410.8516, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "46573521",
                "name": "Laurent Dinh"
            },
            {
                "authorId": "145055042",
                "name": "David Krueger"
            },
            {
                "authorId": "1751762",
                "name": "Yoshua Bengio"
            }
        ],
        "abstract": "We propose a deep learning framework for modeling complex high-dimensional densities called Non-linear Independent Component Estimation (NICE). It is based on the idea that a good representation is one in which the data has a distribution that is easy to model. For this purpose, a non-linear deterministic transformation of the data is learned that maps it to a latent space so as to make the transformed data conform to a factorized distribution, i.e., resulting in independent latent variables. We parametrize this transformation so that computing the Jacobian determinant and inverse transform is trivial, yet we maintain the ability to learn complex non-linear transformations, via a composition of simple building blocks, each based on a deep neural network. The training criterion is simply the exact log-likelihood, which is tractable. Unbiased ancestral sampling is also easy. We show that this approach yields good generative models on four image datasets and can be used for inpainting."
    },
    {
        "paperId": "eb42cf88027de515750f230b23b1a057dc782108",
        "url": "https://www.semanticscholar.org/paper/eb42cf88027de515750f230b23b1a057dc782108",
        "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
        "venue": "International Conference on Learning Representations",
        "year": 2014,
        "citationCount": 107964,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1409.1556, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "34838386",
                "name": "K. Simonyan"
            },
            {
                "authorId": "1688869",
                "name": "Andrew Zisserman"
            }
        ],
        "abstract": "In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision."
    },
    {
        "paperId": "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5",
        "url": "https://www.semanticscholar.org/paper/fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5",
        "title": "Neural Machine Translation by Jointly Learning to Align and Translate",
        "venue": "International Conference on Learning Representations",
        "year": 2014,
        "citationCount": 28606,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1409.0473, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3335364",
                "name": "Dzmitry Bahdanau"
            },
            {
                "authorId": "1979489",
                "name": "Kyunghyun Cho"
            },
            {
                "authorId": "1751762",
                "name": "Yoshua Bengio"
            }
        ],
        "abstract": "Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition."
    },
    {
        "paperId": "024006d4c2a89f7acacc6e4438d156525b60a98f",
        "url": "https://www.semanticscholar.org/paper/024006d4c2a89f7acacc6e4438d156525b60a98f",
        "title": "Continuous control with deep reinforcement learning",
        "venue": "International Conference on Learning Representations",
        "year": 2015,
        "citationCount": 14621,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1509.02971, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2542999",
                "name": "T. Lillicrap"
            },
            {
                "authorId": "2323922",
                "name": "Jonathan J. Hunt"
            },
            {
                "authorId": "1863250",
                "name": "A. Pritzel"
            },
            {
                "authorId": "2801204",
                "name": "N. Heess"
            },
            {
                "authorId": "1968210",
                "name": "Tom Erez"
            },
            {
                "authorId": "2109481",
                "name": "Yuval Tassa"
            },
            {
                "authorId": "145824029",
                "name": "David Silver"
            },
            {
                "authorId": "1688276",
                "name": "Daan Wierstra"
            }
        ],
        "abstract": "We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs."
    },
    {
        "paperId": "17fa1c2a24ba8f731c8b21f1244463bc4b465681",
        "url": "https://www.semanticscholar.org/paper/17fa1c2a24ba8f731c8b21f1244463bc4b465681",
        "title": "Deep multi-scale video prediction beyond mean square error",
        "venue": "International Conference on Learning Representations",
        "year": 2015,
        "citationCount": 1951,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1511.05440, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "143949035",
                "name": "Michal Mathieu"
            },
            {
                "authorId": "2341378",
                "name": "C. Couprie"
            },
            {
                "authorId": "1688882",
                "name": "Yann LeCun"
            }
        ],
        "abstract": "Learning to predict future images from a video sequence involves the construction of an internal representation that models the image evolution accurately, and therefore, to some degree, its content and dynamics. This is why pixel-space video prediction may be viewed as a promising avenue for unsupervised feature learning. In addition, while optical flow has been a very studied problem in computer vision for a long time, future frame prediction is rarely approached. Still, many vision applications could benefit from the knowledge of the next frames of videos, that does not require the complexity of tracking every pixel trajectories. In this work, we train a convolutional network to generate future frames given an input sequence. To deal with the inherently blurry predictions obtained from the standard Mean Squared Error (MSE) loss function, we propose three different and complementary feature learning strategies: a multi-scale architecture, an adversarial training method, and an image gradient difference loss function. We compare our predictions to different published results based on recurrent neural networks on the UCF101 dataset"
    },
    {
        "paperId": "39e0c341351f8f4a39ac890b96217c7f4bde5369",
        "url": "https://www.semanticscholar.org/paper/39e0c341351f8f4a39ac890b96217c7f4bde5369",
        "title": "A note on the evaluation of generative models",
        "venue": "International Conference on Learning Representations",
        "year": 2015,
        "citationCount": 1184,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1511.01844, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2073063",
                "name": "Lucas Theis"
            },
            {
                "authorId": "3422336",
                "name": "Aron van den Oord"
            },
            {
                "authorId": "1731199",
                "name": "M. Bethge"
            }
        ],
        "abstract": "Probabilistic generative models can be used for compression, denoising, inpainting, texture synthesis, semi-supervised learning, unsupervised feature learning, and other tasks. Given this wide range of applications, it is not surprising that a lot of heterogeneity exists in the way these models are formulated, trained, and evaluated. As a consequence, direct comparison between models is often difficult. This article reviews mostly known but often underappreciated properties relating to the evaluation and interpretation of generative models with a focus on image models. In particular, we show that three of the currently most commonly used criteria---average log-likelihood, Parzen window estimates, and visual fidelity of samples---are largely independent of each other when the data is high-dimensional. Good performance with respect to one criterion therefore need not imply good performance with respect to the other criteria. Our results show that extrapolation from one criterion to another is not warranted and generative models need to be evaluated directly with respect to the application(s) they were intended for. In addition, we provide examples demonstrating that Parzen window estimates should generally be avoided."
    },
    {
        "paperId": "3e47c4c2dd98c49b7771c7228812d5fd9eee56a3",
        "url": "https://www.semanticscholar.org/paper/3e47c4c2dd98c49b7771c7228812d5fd9eee56a3",
        "title": "Importance Weighted Autoencoders",
        "venue": "International Conference on Learning Representations",
        "year": 2015,
        "citationCount": 1303,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1509.00519, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3080409",
                "name": "Yuri Burda"
            },
            {
                "authorId": "1785346",
                "name": "R. Grosse"
            },
            {
                "authorId": "145124475",
                "name": "R. Salakhutdinov"
            }
        ],
        "abstract": "The variational autoencoder (VAE; Kingma, Welling (2014)) is a recently proposed generative model pairing a top-down generative network with a bottom-up recognition network which approximates posterior inference. It typically makes strong assumptions about posterior inference, for instance that the posterior distribution is approximately factorial, and that its parameters can be approximated with nonlinear regression from the observations. As we show empirically, the VAE objective can lead to overly simplified representations which fail to use the network's entire modeling capacity. We present the importance weighted autoencoder (IWAE), a generative model with the same architecture as the VAE, but which uses a strictly tighter log-likelihood lower bound derived from importance weighting. In the IWAE, the recognition network uses multiple samples to approximate the posterior, giving it increased flexibility to model complex posteriors which do not fit the VAE modeling assumptions. We show empirically that IWAEs learn richer latent space representations than VAEs, leading to improved test log-likelihood on density estimation benchmarks."
    },
    {
        "paperId": "492f57ee9ceb61fb5a47ad7aebfec1121887a175",
        "url": "https://www.semanticscholar.org/paper/492f57ee9ceb61fb5a47ad7aebfec1121887a175",
        "title": "Gated Graph Sequence Neural Networks",
        "venue": "International Conference on Learning Representations",
        "year": 2015,
        "citationCount": 3506,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1511.05493, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "47002813",
                "name": "Yujia Li"
            },
            {
                "authorId": "1725299",
                "name": "Daniel Tarlow"
            },
            {
                "authorId": "2107692",
                "name": "Marc Brockschmidt"
            },
            {
                "authorId": "1804104",
                "name": "R. Zemel"
            }
        ],
        "abstract": "Abstract: Graph-structured data appears frequently in domains including chemistry, natural language semantics, social networks, and knowledge bases. In this work, we study feature learning techniques for graph-structured inputs. Our starting point is previous work on Graph Neural Networks (Scarselli et al., 2009), which we modify to use gated recurrent units and modern optimization techniques and then extend to output sequences. The result is a flexible and broadly useful class of neural network models that has favorable inductive biases relative to purely sequence-based models (e.g., LSTMs) when the problem is graph-structured. We demonstrate the capabilities on some simple AI (bAbI) and graph algorithm learning tasks. We then show it achieves state-of-the-art performance on a problem from program verification, in which subgraphs need to be matched to abstract data structures."
    },
    {
        "paperId": "55a3c3bcf0827f5f750a105fd676ee33e86803fb",
        "url": "https://www.semanticscholar.org/paper/55a3c3bcf0827f5f750a105fd676ee33e86803fb",
        "title": "Particular object retrieval with integral max-pooling of CNN activations",
        "venue": "International Conference on Learning Representations",
        "year": 2015,
        "citationCount": 1005,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1511.05879, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1706195",
                "name": "Giorgos Tolias"
            },
            {
                "authorId": "1993738",
                "name": "R. Sicre"
            },
            {
                "authorId": "1681054",
                "name": "H. Jgou"
            }
        ],
        "abstract": "Recently, image representation built upon Convolutional Neural Network (CNN) has been shown to provide effective descriptors for image search, outperforming pre-CNN features as short-vector representations. Yet such models are not compatible with geometry-aware re-ranking methods and still outperformed, on some particular object retrieval benchmarks, by traditional image search systems relying on precise descriptor matching, geometric re-ranking, or query expansion. This work revisits both retrieval stages, namely initial search and re-ranking, by employing the same primitive information derived from the CNN. We build compact feature vectors that encode several image regions without the need to feed multiple inputs to the network. Furthermore, we extend integral images to handle max-pooling on convolutional layer activations, allowing us to efficiently localize matching objects. The resulting bounding box is finally used for image re-ranking. As a result, this paper significantly improves existing CNN-based recognition pipeline: We report for the first time results competing with traditional methods on the challenging Oxford5k and Paris6k datasets."
    },
    {
        "paperId": "642d0f49b7826adcf986616f4af77e736229990f",
        "url": "https://www.semanticscholar.org/paper/642d0f49b7826adcf986616f4af77e736229990f",
        "title": "Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding",
        "venue": "International Conference on Learning Representations",
        "year": 2015,
        "citationCount": 9524,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1510.00149, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "143840275",
                "name": "Song Han"
            },
            {
                "authorId": "3123774",
                "name": "Huizi Mao"
            },
            {
                "authorId": "80724002",
                "name": "W. Dally"
            }
        ],
        "abstract": "Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems with limited hardware resources. To address this limitation, we introduce \"deep compression\", a three stage pipeline: pruning, trained quantization and Huffman coding, that work together to reduce the storage requirement of neural networks by 35x to 49x without affecting their accuracy. Our method first prunes the network by learning only the important connections. Next, we quantize the weights to enforce weight sharing, finally, we apply Huffman coding. After the first two steps we retrain the network to fine tune the remaining connections and the quantized centroids. Pruning, reduces the number of connections by 9x to 13x; Quantization then reduces the number of bits that represent each connection from 32 to 5. On the ImageNet dataset, our method reduced the storage required by AlexNet by 35x, from 240MB to 6.9MB, without loss of accuracy. Our method reduced the size of VGG-16 by 49x from 552MB to 11.3MB, again with no loss of accuracy. This allows fitting the model into on-chip SRAM cache rather than off-chip DRAM memory. Our compression method also facilitates the use of complex neural networks in mobile applications where application size and download bandwidth are constrained. Benchmarked on CPU, GPU and mobile GPU, compressed network has 3x to 4x layerwise speedup and 3x to 7x better energy efficiency."
    },
    {
        "paperId": "7f5fc84819c0cf94b771fe15141f65b123f7b8ec",
        "url": "https://www.semanticscholar.org/paper/7f5fc84819c0cf94b771fe15141f65b123f7b8ec",
        "title": "Multi-Scale Context Aggregation by Dilated Convolutions",
        "venue": "International Conference on Learning Representations",
        "year": 2015,
        "citationCount": 9070,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1511.07122, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1807197",
                "name": "F. Yu"
            },
            {
                "authorId": "145231047",
                "name": "V. Koltun"
            }
        ],
        "abstract": "State-of-the-art models for semantic segmentation are based on adaptations of convolutional networks that had originally been designed for image classification. However, dense prediction and image classification are structurally different. In this work, we develop a new convolutional network module that is specifically designed for dense prediction. The presented module uses dilated convolutions to systematically aggregate multi-scale contextual information without losing resolution. The architecture is based on the fact that dilated convolutions support exponential expansion of the receptive field without loss of resolution or coverage. We show that the presented context module increases the accuracy of state-of-the-art semantic segmentation systems. In addition, we examine the adaptation of image classification networks to dense prediction and show that simplifying the adapted network can increase accuracy."
    },
    {
        "paperId": "8388f1be26329fa45e5807e968a641ce170ea078",
        "url": "https://www.semanticscholar.org/paper/8388f1be26329fa45e5807e968a641ce170ea078",
        "title": "Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks",
        "venue": "International Conference on Learning Representations",
        "year": 2015,
        "citationCount": 14775,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1511.06434, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "38909097",
                "name": "Alec Radford"
            },
            {
                "authorId": "2096458",
                "name": "Luke Metz"
            },
            {
                "authorId": "2127604",
                "name": "Soumith Chintala"
            }
        ],
        "abstract": "In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations."
    },
    {
        "paperId": "8f2182846d5d4cfbc216b5e4c00411e021dc4776",
        "url": "https://www.semanticscholar.org/paper/8f2182846d5d4cfbc216b5e4c00411e021dc4776",
        "title": "Learning to Diagnose with LSTM Recurrent Neural Networks",
        "venue": "International Conference on Learning Representations",
        "year": 2015,
        "citationCount": 1162,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1511.03677, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "32219137",
                "name": "Zachary Chase Lipton"
            },
            {
                "authorId": "2107807",
                "name": "David C. Kale"
            },
            {
                "authorId": "1722831",
                "name": "C. Elkan"
            },
            {
                "authorId": "144616817",
                "name": "R. Wetzel"
            }
        ],
        "abstract": "Clinical medical data, especially in the intensive care unit (ICU), consist of multivariate time series of observations. For each patient visit (or episode), sensor data and lab test results are recorded in the patient's Electronic Health Record (EHR). While potentially containing a wealth of insights, the data is difficult to mine effectively, owing to varying length, irregular sampling and missing data. Recurrent Neural Networks (RNNs), particularly those using Long Short-Term Memory (LSTM) hidden units, are powerful and increasingly popular models for learning from sequence data. They effectively model varying length sequences and capture long range dependencies. We present the first study to empirically evaluate the ability of LSTMs to recognize patterns in multivariate time series of clinical measurements. Specifically, we consider multilabel classification of diagnoses, training a model to classify 128 diagnoses given 13 frequently but irregularly sampled clinical measurements. First, we establish the effectiveness of a simple LSTM network for modeling clinical data. Then we demonstrate a straightforward and effective training strategy in which we replicate targets at each sequence step. Trained only on raw time series, our models outperform several strong baselines, including a multilayer perceptron trained on hand-engineered features."
    },
    {
        "paperId": "abb33d75dc297993fcc3fb75e0f4498f413eb4f6",
        "url": "https://www.semanticscholar.org/paper/abb33d75dc297993fcc3fb75e0f4498f413eb4f6",
        "title": "Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks",
        "venue": "International Conference on Learning Representations",
        "year": 2015,
        "citationCount": 1224,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1502.05698, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "145183709",
                "name": "J. Weston"
            },
            {
                "authorId": "1713934",
                "name": "Antoine Bordes"
            },
            {
                "authorId": "3295092",
                "name": "S. Chopra"
            },
            {
                "authorId": "2047446108",
                "name": "Tomas Mikolov"
            }
        ],
        "abstract": "One long-term goal of machine learning research is to produce methods that are applicable to reasoning and natural language, in particular building an intelligent dialogue agent. To measure progress towards that goal, we argue for the usefulness of a set of proxy tasks that evaluate reading comprehension via question answering. Our tasks measure understanding in several ways: whether a system is able to answer questions via chaining facts, simple induction, deduction and many more. The tasks are designed to be prerequisites for any system that aims to be capable of conversing with a human. We believe many existing learning systems can currently not solve them, and hence our aim is to classify these tasks into skill sets, so that researchers can identify (and then rectify) the failings of their systems. We also extend and improve the recently introduced Memory Networks model, and show it is able to solve some, but not all, of the tasks."
    },
    {
        "paperId": "b7aee9dfb027d6061c6a653684c0fa9a9bba750d",
        "url": "https://www.semanticscholar.org/paper/b7aee9dfb027d6061c6a653684c0fa9a9bba750d",
        "title": "Sequence Level Training with Recurrent Neural Networks",
        "venue": "International Conference on Learning Representations",
        "year": 2015,
        "citationCount": 1706,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1511.06732, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1706809",
                "name": "Marc'Aurelio Ranzato"
            },
            {
                "authorId": "3295092",
                "name": "S. Chopra"
            },
            {
                "authorId": "2325985",
                "name": "Michael Auli"
            },
            {
                "authorId": "2563432",
                "name": "Wojciech Zaremba"
            }
        ],
        "abstract": "Many natural language processing applications use language models to generate text. These models are typically trained to predict the next word in a sequence, given the previous words and some context such as an image. However, at test time the model is expected to generate the entire sequence from scratch. This discrepancy makes generation brittle, as errors may accumulate along the way. We address this issue by proposing a novel sequence level training algorithm that directly optimizes the metric used at test time, such as BLEU or ROUGE. On three different tasks, our approach outperforms several strong baselines for greedy generation. The method is also competitive when these baselines employ beam search, while being several times faster."
    },
    {
        "paperId": "c6170fa90d3b2efede5a2e1660cb23e1c824f2ca",
        "url": "https://www.semanticscholar.org/paper/c6170fa90d3b2efede5a2e1660cb23e1c824f2ca",
        "title": "Prioritized Experience Replay",
        "venue": "International Conference on Learning Representations",
        "year": 2015,
        "citationCount": 4149,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1511.05952, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1725157",
                "name": "T. Schaul"
            },
            {
                "authorId": "34660073",
                "name": "John Quan"
            },
            {
                "authorId": "2460849",
                "name": "Ioannis Antonoglou"
            },
            {
                "authorId": "145824029",
                "name": "David Silver"
            }
        ],
        "abstract": "Experience replay lets online reinforcement learning agents remember and reuse experiences from the past. In prior work, experience transitions were uniformly sampled from a replay memory. However, this approach simply replays transitions at the same frequency that they were originally experienced, regardless of their significance. In this paper we develop a framework for prioritizing experience, so as to replay important transitions more frequently, and therefore learn more efficiently. We use prioritized experience replay in Deep Q-Networks (DQN), a reinforcement learning algorithm that achieved human-level performance across many Atari games. DQN with prioritized experience replay achieves a new state-of-the-art, outperforming DQN with uniform replay on 41 out of 49 games."
    },
    {
        "paperId": "d316c82c12cf4c45f9e85211ef3d1fa62497bff8",
        "url": "https://www.semanticscholar.org/paper/d316c82c12cf4c45f9e85211ef3d1fa62497bff8",
        "title": "High-Dimensional Continuous Control Using Generalized Advantage Estimation",
        "venue": "International Conference on Learning Representations",
        "year": 2015,
        "citationCount": 3989,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1506.02438, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "47971768",
                "name": "John Schulman"
            },
            {
                "authorId": "29912342",
                "name": "Philipp Moritz"
            },
            {
                "authorId": "1736651",
                "name": "S. Levine"
            },
            {
                "authorId": "1694621",
                "name": "Michael I. Jordan"
            },
            {
                "authorId": "1689992",
                "name": "P. Abbeel"
            }
        ],
        "abstract": "Policy gradient methods are an appealing approach in reinforcement learning because they directly optimize the cumulative reward and can straightforwardly be used with nonlinear function approximators such as neural networks. The two main challenges are the large number of samples typically required, and the difficulty of obtaining stable and steady improvement despite the nonstationarity of the incoming data. We address the first challenge by using value functions to substantially reduce the variance of policy gradient estimates at the cost of some bias, with an exponentially-weighted estimator of the advantage function that is analogous to TD(lambda). We address the second challenge by using trust region optimization procedure for both the policy and the value function, which are represented by neural networks. \nOur approach yields strong empirical results on highly challenging 3D locomotion tasks, learning running gaits for bipedal and quadrupedal simulated robots, and learning a policy for getting the biped to stand up from starting out lying on the ground. In contrast to a body of prior work that uses hand-crafted policy representations, our neural network policies map directly from raw kinematics to joint torques. Our algorithm is fully model-free, and the amount of simulated experience required for the learning tasks on 3D bipeds corresponds to 1-2 weeks of real time."
    },
    {
        "paperId": "e0021d61c2ab1334bc725852edd44597f4c65dff",
        "url": "https://www.semanticscholar.org/paper/e0021d61c2ab1334bc725852edd44597f4c65dff",
        "title": "Session-based Recommendations with Recurrent Neural Networks",
        "venue": "International Conference on Learning Representations",
        "year": 2015,
        "citationCount": 2557,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1511.06939, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2507883",
                "name": "Balzs Hidasi"
            },
            {
                "authorId": "1713164",
                "name": "Alexandros Karatzoglou"
            },
            {
                "authorId": "2666397",
                "name": "L. Baltrunas"
            },
            {
                "authorId": "1754164",
                "name": "D. Tikk"
            }
        ],
        "abstract": "We apply recurrent neural networks (RNN) on a new domain, namely recommender systems. Real-life recommender systems often face the problem of having to base recommendations only on short session-based data (e.g. a small sportsware website) instead of long user histories (as in the case of Netflix). In this situation the frequently praised matrix factorization approaches are not accurate. This problem is usually overcome in practice by resorting to item-to-item recommendations, i.e. recommending similar items. We argue that by modeling the whole session, more accurate recommendations can be provided. We therefore propose an RNN-based approach for session-based recommendations. Our approach also considers practical aspects of the task and introduces several modifications to classic RNNs such as a ranking loss function that make it more viable for this specific problem. Experimental results on two data-sets show marked improvements over widely used approaches."
    },
    {
        "paperId": "f63e917638553414526a0cc8550de4ad2d83fe7a",
        "url": "https://www.semanticscholar.org/paper/f63e917638553414526a0cc8550de4ad2d83fe7a",
        "title": "Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)",
        "venue": "International Conference on Learning Representations",
        "year": 2015,
        "citationCount": 5881,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1511.07289, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "34917892",
                "name": "Djork-Arn Clevert"
            },
            {
                "authorId": "2465270",
                "name": "Thomas Unterthiner"
            },
            {
                "authorId": "3308557",
                "name": "Sepp Hochreiter"
            }
        ],
        "abstract": "We introduce the \"exponential linear unit\" (ELU) which speeds up learning in deep neural networks and leads to higher classification accuracies. Like rectified linear units (ReLUs), leaky ReLUs (LReLUs) and parametrized ReLUs (PReLUs), ELUs alleviate the vanishing gradient problem via the identity for positive values. However, ELUs have improved learning characteristics compared to the units with other activation functions. In contrast to ReLUs, ELUs have negative values which allows them to push mean unit activations closer to zero like batch normalization but with lower computational complexity. Mean shifts toward zero speed up learning by bringing the normal gradient closer to the unit natural gradient because of a reduced bias shift effect. While LReLUs and PReLUs have negative values, too, they do not ensure a noise-robust deactivation state. ELUs saturate to a negative value with smaller inputs and thereby decrease the forward propagated variation and information. Therefore, ELUs code the degree of presence of particular phenomena in the input, while they do not quantitatively model the degree of their absence. In experiments, ELUs lead not only to faster learning, but also to significantly better generalization performance than ReLUs and LReLUs on networks with more than 5 layers. On CIFAR-100 ELUs networks significantly outperform ReLU networks with batch normalization while batch normalization does not improve ELU networks. ELU networks are among the top 10 reported CIFAR-10 results and yield the best published result on CIFAR-100, without resorting to multi-view evaluation or model averaging. On ImageNet, ELU networks considerably speed up learning compared to a ReLU network with the same architecture, obtaining less than 10% classification error for a single crop, single model network."
    },
    {
        "paperId": "04bd2907111855b9fde9413bb25b9788a4c03f26",
        "url": "https://www.semanticscholar.org/paper/04bd2907111855b9fde9413bb25b9788a4c03f26",
        "title": "Unsupervised Cross-Domain Image Generation",
        "venue": "International Conference on Learning Representations",
        "year": 2016,
        "citationCount": 1029,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1611.02200, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2188620",
                "name": "Yaniv Taigman"
            },
            {
                "authorId": "33964593",
                "name": "Adam Polyak"
            },
            {
                "authorId": "145128145",
                "name": "Lior Wolf"
            }
        ],
        "abstract": "We study the problem of transferring a sample in one domain to an analog sample in another domain. Given two related domains, S and T, we would like to learn a generative function G that maps an input sample from S to the domain T, such that the output of a given function f, which accepts inputs in either domains, would remain unchanged. Other than the function f, the training data is unsupervised and consist of a set of samples from each domain. The Domain Transfer Network (DTN) we present employs a compound loss function that includes a multiclass GAN loss, an f-constancy component, and a regularizing component that encourages G to map samples from T to themselves. We apply our method to visual domains including digits and face images and demonstrate its ability to generate convincing novel images of previously unseen entities, while preserving their identity."
    },
    {
        "paperId": "09879f7956dddc2a9328f5c1472feeb8402bcbcf",
        "url": "https://www.semanticscholar.org/paper/09879f7956dddc2a9328f5c1472feeb8402bcbcf",
        "title": "Density estimation using Real NVP",
        "venue": "International Conference on Learning Representations",
        "year": 2016,
        "citationCount": 4091,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1605.08803, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "46573521",
                "name": "Laurent Dinh"
            },
            {
                "authorId": "1407546424",
                "name": "Jascha Narain Sohl-Dickstein"
            },
            {
                "authorId": "1751569",
                "name": "Samy Bengio"
            }
        ],
        "abstract": "Unsupervised learning of probabilistic models is a central yet challenging problem in machine learning. Specifically, designing models with tractable learning, sampling, inference and evaluation is crucial in solving this task. We extend the space of such models using real-valued non-volume preserving (real NVP) transformations, a set of powerful invertible and learnable transformations, resulting in an unsupervised learning algorithm with exact log-likelihood computation, exact sampling, exact inference of latent variables, and an interpretable latent space. We demonstrate its ability to model natural images on four datasets through sampling, log-likelihood evaluation and latent variable manipulations."
    },
    {
        "paperId": "1db6e3078597386ac4222ba6c3f4f61b61f53539",
        "url": "https://www.semanticscholar.org/paper/1db6e3078597386ac4222ba6c3f4f61b61f53539",
        "title": "Adversarial Feature Learning",
        "venue": "International Conference on Learning Representations",
        "year": 2016,
        "citationCount": 1880,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1605.09782, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "7408951",
                "name": "Jeff Donahue"
            },
            {
                "authorId": "2562966",
                "name": "Philipp Krhenbhl"
            },
            {
                "authorId": "1753210",
                "name": "Trevor Darrell"
            }
        ],
        "abstract": "The ability of the Generative Adversarial Networks (GANs) framework to learn generative models mapping from simple latent distributions to arbitrarily complex data distributions has been demonstrated empirically, with compelling results showing that the latent space of such generators captures semantic variation in the data distribution. Intuitively, models trained to predict these semantic latent representations given data may serve as useful feature representations for auxiliary problems where semantics are relevant. However, in their existing form, GANs have no means of learning the inverse mapping -- projecting data back into the latent space. We propose Bidirectional Generative Adversarial Networks (BiGANs) as a means of learning this inverse mapping, and demonstrate that the resulting learned feature representation is useful for auxiliary supervised discrimination tasks, competitive with contemporary approaches to unsupervised and self-supervised feature learning."
    },
    {
        "paperId": "232148b97bd0543613ffd98fb4edcff79434ce1a",
        "url": "https://www.semanticscholar.org/paper/232148b97bd0543613ffd98fb4edcff79434ce1a",
        "title": "End-to-end Optimized Image Compression",
        "venue": "International Conference on Learning Representations",
        "year": 2016,
        "citationCount": 1906,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1611.01704, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "144517934",
                "name": "J. Ball"
            },
            {
                "authorId": "2732577",
                "name": "Valero Laparra"
            },
            {
                "authorId": "1689350",
                "name": "Eero P. Simoncelli"
            }
        ],
        "abstract": "We describe an image compression method, consisting of a nonlinear analysis transformation, a uniform quantizer, and a nonlinear synthesis transformation. The transforms are constructed in three successive stages of convolutional linear filters and nonlinear activation functions. Unlike most convolutional neural networks, the joint nonlinearity is chosen to implement a form of local gain control, inspired by those used to model biological neurons. Using a variant of stochastic gradient descent, we jointly optimize the entire model for rate-distortion performance over a database of training images, introducing a continuous proxy for the discontinuous loss function arising from the quantizer. Under certain conditions, the relaxed loss function may be interpreted as the log likelihood of a generative model, as implemented by a variational autoencoder. Unlike these models, however, the compression model must operate at any given point along the rate-distortion curve, as specified by a trade-off parameter. Across an independent set of test images, we find that the optimized method generally exhibits better rate-distortion performance than the standard JPEG and JPEG 2000 compression methods. More importantly, we observe a dramatic improvement in visual quality for all images at all bit rates, which is supported by objective quality estimates using MS-SSIM."
    },
    {
        "paperId": "29c887794eed2ca9462638ff853e6fe1ab91d5d8",
        "url": "https://www.semanticscholar.org/paper/29c887794eed2ca9462638ff853e6fe1ab91d5d8",
        "title": "Optimization as a Model for Few-Shot Learning",
        "venue": "International Conference on Learning Representations",
        "year": 2016,
        "citationCount": 3601,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "49517463",
                "name": "S. Ravi"
            },
            {
                "authorId": "1777528",
                "name": "H. Larochelle"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "29e944711a354c396fad71936f536e83025b6ce0",
        "url": "https://www.semanticscholar.org/paper/29e944711a354c396fad71936f536e83025b6ce0",
        "title": "Categorical Reparameterization with Gumbel-Softmax",
        "venue": "International Conference on Learning Representations",
        "year": 2016,
        "citationCount": 5896,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1611.01144, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "145116380",
                "name": "Eric Jang"
            },
            {
                "authorId": "2046135",
                "name": "S. Gu"
            },
            {
                "authorId": "16443937",
                "name": "Ben Poole"
            }
        ],
        "abstract": "Categorical variables are a natural choice for representing discrete structure in the world. However, stochastic neural networks rarely use categorical latent variables due to the inability to backpropagate through samples. In this work, we present an efficient gradient estimator that replaces the non-differentiable sample from a categorical distribution with a differentiable sample from a novel Gumbel-Softmax distribution. This distribution has the essential property that it can be smoothly annealed into a categorical distribution. We show that our Gumbel-Softmax estimator outperforms state-of-the-art gradient estimators on structured output prediction and unsupervised generative modeling tasks with categorical latent variables, and enables large speedups on semi-supervised classification."
    },
    {
        "paperId": "2ba23d9b46027e47b4483243871760e315213ffe",
        "url": "https://www.semanticscholar.org/paper/2ba23d9b46027e47b4483243871760e315213ffe",
        "title": "Energy-based Generative Adversarial Network",
        "venue": "International Conference on Learning Representations",
        "year": 2016,
        "citationCount": 1134,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1609.03126, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "7818229",
                "name": "J. Zhao"
            },
            {
                "authorId": "143949035",
                "name": "Michal Mathieu"
            },
            {
                "authorId": "1688882",
                "name": "Yann LeCun"
            }
        ],
        "abstract": "We introduce the \"Energy-based Generative Adversarial Network\" model (EBGAN) which views the discriminator as an energy function that attributes low energies to the regions near the data manifold and higher energies to other regions. Similar to the probabilistic GANs, a generator is seen as being trained to produce contrastive samples with minimal energies, while the discriminator is trained to assign high energies to these generated samples. Viewing the discriminator as an energy function allows to use a wide variety of architectures and loss functionals in addition to the usual binary classifier with logistic output. Among them, we show one instantiation of EBGAN framework as using an auto-encoder architecture, with the energy being the reconstruction error, in place of the discriminator. We show that this form of EBGAN exhibits more stable behavior than regular GANs during training. We also show that a single-scale architecture can be trained to generate high-resolution images."
    },
    {
        "paperId": "2cd55ded95d5d13430edfa223ba591b514ebe8a5",
        "url": "https://www.semanticscholar.org/paper/2cd55ded95d5d13430edfa223ba591b514ebe8a5",
        "title": "Adversarial Training Methods for Semi-Supervised Text Classification",
        "venue": "International Conference on Learning Representations",
        "year": 2016,
        "citationCount": 1113,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1605.07725, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3213400",
                "name": "Takeru Miyato"
            },
            {
                "authorId": "2555924",
                "name": "Andrew M. Dai"
            },
            {
                "authorId": "153440022",
                "name": "I. Goodfellow"
            }
        ],
        "abstract": "Adversarial training provides a means of regularizing supervised learning algorithms while virtual adversarial training is able to extend supervised learning algorithms to the semi-supervised setting. However, both methods require making small perturbations to numerous entries of the input vector, which is inappropriate for sparse high-dimensional inputs such as one-hot word representations. We extend adversarial and virtual adversarial training to the text domain by applying perturbations to the word embeddings in a recurrent neural network rather than to the original input itself. The proposed method achieves state of the art results on multiple benchmark semi-supervised and purely supervised tasks. We provide visualizations and analysis showing that the learned word embeddings have improved in quality and that while training, the model is less prone to overfitting."
    },
    {
        "paperId": "36eff562f65125511b5dfab68ce7f7a943c27478",
        "url": "https://www.semanticscholar.org/paper/36eff562f65125511b5dfab68ce7f7a943c27478",
        "title": "Semi-Supervised Classification with Graph Convolutional Networks",
        "venue": "International Conference on Learning Representations",
        "year": 2016,
        "citationCount": 32722,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1609.02907, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "41016725",
                "name": "Thomas Kipf"
            },
            {
                "authorId": "1678311",
                "name": "M. Welling"
            }
        ],
        "abstract": "We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin."
    },
    {
        "paperId": "3a7b63b50c64f4ec3358477790e84cbd6be2a0b4",
        "url": "https://www.semanticscholar.org/paper/3a7b63b50c64f4ec3358477790e84cbd6be2a0b4",
        "title": "Bidirectional Attention Flow for Machine Comprehension",
        "venue": "International Conference on Learning Representations",
        "year": 2016,
        "citationCount": 2121,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1611.01603, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "4418074",
                "name": "Minjoon Seo"
            },
            {
                "authorId": "2684226",
                "name": "Aniruddha Kembhavi"
            },
            {
                "authorId": "143787583",
                "name": "Ali Farhadi"
            },
            {
                "authorId": "2548384",
                "name": "Hannaneh Hajishirzi"
            }
        ],
        "abstract": "Machine comprehension (MC), answering a query about a given context paragraph, requires modeling complex interactions between the context and the query. Recently, attention mechanisms have been successfully extended to MC. Typically these methods use attention to focus on a small portion of the context and summarize it with a fixed-size vector, couple attentions temporally, and/or often form a uni-directional attention. In this paper we introduce the Bi-Directional Attention Flow (BIDAF) network, a multi-stage hierarchical process that represents the context at different levels of granularity and uses bi-directional attention flow mechanism to obtain a query-aware context representation without early summarization. Our experimental evaluations show that our model achieves the state-of-the-art results in Stanford Question Answering Dataset (SQuAD) and CNN/DailyMail cloze test."
    },
    {
        "paperId": "3db8730c203f88d7f08a6a99e8c02a077dc9b011",
        "url": "https://www.semanticscholar.org/paper/3db8730c203f88d7f08a6a99e8c02a077dc9b011",
        "title": "Pruning Convolutional Neural Networks for Resource Efficient Inference",
        "venue": "International Conference on Learning Representations",
        "year": 2016,
        "citationCount": 2142,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "2824500",
                "name": "Pavlo Molchanov"
            },
            {
                "authorId": "2342481",
                "name": "Stephen Tyree"
            },
            {
                "authorId": "2976930",
                "name": "Tero Karras"
            },
            {
                "authorId": "1761103",
                "name": "Timo Aila"
            },
            {
                "authorId": "2273651410",
                "name": "Jan Kautz"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "488bb25e0b1777847f04c943e6dbc4f84415b712",
        "url": "https://www.semanticscholar.org/paper/488bb25e0b1777847f04c943e6dbc4f84415b712",
        "title": "Unrolled Generative Adversarial Networks",
        "venue": "International Conference on Learning Representations",
        "year": 2016,
        "citationCount": 1054,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1611.02163, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2096458",
                "name": "Luke Metz"
            },
            {
                "authorId": "16443937",
                "name": "Ben Poole"
            },
            {
                "authorId": "144846367",
                "name": "David Pfau"
            },
            {
                "authorId": "1407546424",
                "name": "Jascha Narain Sohl-Dickstein"
            }
        ],
        "abstract": "We introduce a method to stabilize Generative Adversarial Networks (GANs) by defining the generator objective with respect to an unrolled optimization of the discriminator. This allows training to be adjusted between using the optimal discriminator in the generator's objective, which is ideal but infeasible in practice, and using the current value of the discriminator, which is often unstable and leads to poor solutions. We show how this technique solves the common problem of mode collapse, stabilizes training of GANs with complex recurrent generators, and increases diversity and coverage of the data distribution by the generator."
    },
    {
        "paperId": "515a21e90117941150923e559729c59f5fdade1c",
        "url": "https://www.semanticscholar.org/paper/515a21e90117941150923e559729c59f5fdade1c",
        "title": "The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables",
        "venue": "International Conference on Learning Representations",
        "year": 2016,
        "citationCount": 2689,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1611.00712, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2772217",
                "name": "Chris J. Maddison"
            },
            {
                "authorId": "1714004",
                "name": "A. Mnih"
            },
            {
                "authorId": "1725303",
                "name": "Y. Teh"
            }
        ],
        "abstract": "The reparameterization trick enables optimizing large scale stochastic computation graphs via gradient descent. The essence of the trick is to refactor each stochastic node into a differentiable function of its parameters and a random variable with fixed distribution. After refactoring, the gradients of the loss propagated by the chain rule through the graph are low variance unbiased estimators of the gradients of the expected loss. While many continuous random variables have such reparameterizations, discrete random variables lack useful reparameterizations due to the discontinuous nature of discrete states. In this work we introduce Concrete random variables---continuous relaxations of discrete random variables. The Concrete distribution is a new family of distributions with closed form densities and a simple reparameterization. Whenever a discrete stochastic node of a computation graph can be refactored into a one-hot bit representation that is treated continuously, Concrete stochastic nodes can be used with automatic differentiation to produce low-variance biased gradients of objectives (including objectives that depend on the log-probability of latent stochastic nodes) on the corresponding discrete graph. We demonstrate the effectiveness of Concrete relaxations on density estimation and structured prediction tasks using neural networks."
    },
    {
        "paperId": "54ddb00fa691728944fd8becea90a373d21597cf",
        "url": "https://www.semanticscholar.org/paper/54ddb00fa691728944fd8becea90a373d21597cf",
        "title": "Understanding deep learning requires rethinking generalization",
        "venue": "International Conference on Learning Representations",
        "year": 2016,
        "citationCount": 4900,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1611.03530, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "151505981",
                "name": "Chiyuan Zhang"
            },
            {
                "authorId": "1751569",
                "name": "Samy Bengio"
            },
            {
                "authorId": "1775622",
                "name": "Moritz Hardt"
            },
            {
                "authorId": "9229182",
                "name": "B. Recht"
            },
            {
                "authorId": "1689108",
                "name": "O. Vinyals"
            }
        ],
        "abstract": "Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training. \nThrough extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. \nWe interpret our experimental findings by comparison with traditional models."
    },
    {
        "paperId": "563783de03452683a9206e85fe6d661714436686",
        "url": "https://www.semanticscholar.org/paper/563783de03452683a9206e85fe6d661714436686",
        "title": "HyperNetworks",
        "venue": "International Conference on Learning Representations",
        "year": 2016,
        "citationCount": 1785,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1609.09106, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1389041357",
                "name": "David Ha"
            },
            {
                "authorId": "2555924",
                "name": "Andrew M. Dai"
            },
            {
                "authorId": "2827616",
                "name": "Quoc V. Le"
            }
        ],
        "abstract": "This work explores hypernetworks: an approach of using one network, also known as a hypernetwork, to generate the weights for another network. We apply hypernetworks to generate adaptive weights for recurrent networks. In this case, hypernetworks can be viewed as a relaxed form of weight-sharing across layers. In our implementation, hypernetworks are are trained jointly with the main network in an end-to-end fashion. Our main result is that hypernetworks can generate non-shared weights for LSTM and achieve state-of-the-art results on a variety of sequence modelling tasks including character-level language modelling, handwriting generation and neural machine translation, challenging the weight-sharing paradigm for recurrent networks."
    },
    {
        "paperId": "5e23a28063b395bdaf784dc548a046885cb90cf2",
        "url": "https://www.semanticscholar.org/paper/5e23a28063b395bdaf784dc548a046885cb90cf2",
        "title": "Understanding intermediate layers using linear classifier probes",
        "venue": "International Conference on Learning Representations",
        "year": 2016,
        "citationCount": 1160,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1610.01644, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1815021",
                "name": "Guillaume Alain"
            },
            {
                "authorId": "1751762",
                "name": "Yoshua Bengio"
            }
        ],
        "abstract": "Neural network models have a reputation for being black boxes. We propose to monitor the features at every layer of a model and measure how suitable they are for classification. We use linear classifiers, which we refer to as \"probes\", trained entirely independently of the model itself. \nThis helps us better understand the roles and dynamics of the intermediate layers. We demonstrate how this can be used to develop a better intuition about models and to diagnose potential problems. \nWe apply this technique to the popular models Inception v3 and Resnet-50. Among other things, we observe experimentally that the linear separability of features increase monotonically along the depth of the model."
    },
    {
        "paperId": "67d968c7450878190e45ac7886746de867bf673d",
        "url": "https://www.semanticscholar.org/paper/67d968c7450878190e45ac7886746de867bf673d",
        "title": "Neural Architecture Search with Reinforcement Learning",
        "venue": "International Conference on Learning Representations",
        "year": 2016,
        "citationCount": 5689,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1611.01578, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2368067",
                "name": "Barret Zoph"
            },
            {
                "authorId": "2827616",
                "name": "Quoc V. Le"
            }
        ],
        "abstract": "Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines. Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214."
    },
    {
        "paperId": "6cd5dfccd9f52538b19a415e00031d0ee4e5b181",
        "url": "https://www.semanticscholar.org/paper/6cd5dfccd9f52538b19a415e00031d0ee4e5b181",
        "title": "Designing Neural Network Architectures using Reinforcement Learning",
        "venue": "International Conference on Learning Representations",
        "year": 2016,
        "citationCount": 1529,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1611.02167, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "40566201",
                "name": "Bowen Baker"
            },
            {
                "authorId": "145418561",
                "name": "O. Gupta"
            },
            {
                "authorId": "48685423",
                "name": "Nikhil Naik"
            },
            {
                "authorId": "145711633",
                "name": "R. Raskar"
            }
        ],
        "abstract": "At present, designing convolutional neural network (CNN) architectures requires both human expertise and labor. New architectures are handcrafted by careful experimentation or modified from a handful of existing networks. We introduce MetaQNN, a meta-modeling algorithm based on reinforcement learning to automatically generate high-performing CNN architectures for a given learning task. The learning agent is trained to sequentially choose CNN layers using $Q$-learning with an $\\epsilon$-greedy exploration strategy and experience replay. The agent explores a large but finite space of possible architectures and iteratively discovers designs with improved performance on the learning task. On image classification benchmarks, the agent-designed networks (consisting of only standard convolution, pooling, and fully-connected layers) beat existing networks designed with the same layer types and are competitive against the state-of-the-art methods that use more complex layer types. We also outperform existing meta-modeling approaches for network design on image classification tasks."
    },
    {
        "paperId": "6ff2a434578ff2746b9283e45abf296887f48a2d",
        "url": "https://www.semanticscholar.org/paper/6ff2a434578ff2746b9283e45abf296887f48a2d",
        "title": "A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks",
        "venue": "International Conference on Learning Representations",
        "year": 2016,
        "citationCount": 3896,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1610.02136, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3422872",
                "name": "Dan Hendrycks"
            },
            {
                "authorId": "1700980",
                "name": "Kevin Gimpel"
            }
        ],
        "abstract": "We consider the two related problems of detecting if an example is misclassified or out-of-distribution. We present a simple baseline that utilizes probabilities from softmax distributions. Correctly classified examples tend to have greater maximum softmax probabilities than erroneously classified and out-of-distribution examples, allowing for their detection. We assess performance by defining several tasks in computer vision, natural language processing, and automatic speech recognition, showing the effectiveness of this baseline across all. We then show the baseline can sometimes be surpassed, demonstrating the room for future research on these underexplored detection tasks."
    },
    {
        "paperId": "8cbef23c9ee2ae7c35cc691a0c1d713a6377c9f2",
        "url": "https://www.semanticscholar.org/paper/8cbef23c9ee2ae7c35cc691a0c1d713a6377c9f2",
        "title": "Deep Biaffine Attention for Neural Dependency Parsing",
        "venue": "International Conference on Learning Representations",
        "year": 2016,
        "citationCount": 1280,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1611.01734, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2277385",
                "name": "Timothy Dozat"
            },
            {
                "authorId": "144783904",
                "name": "Christopher D. Manning"
            }
        ],
        "abstract": "This paper builds off recent work from Kiperwasser & Goldberg (2016) using neural attention in a simple graph-based dependency parser. We use a larger but more thoroughly regularized parser than other recent BiLSTM-based approaches, with biaffine classifiers to predict arcs and labels. Our parser gets state of the art or near state of the art performance on standard treebanks for six different languages, achieving 95.7% UAS and 94.1% LAS on the most popular English PTB dataset. This makes it the highest-performing graph-based parser on this benchmark---outperforming Kiperwasser Goldberg (2016) by 1.8% and 2.2%---and comparable to the highest performing transition-based parser (Kuncoro et al., 2016), which achieves 95.8% UAS and 94.6% LAS. We also show which hyperparameter choices had a significant effect on parsing accuracy, allowing us to achieve large gains over other graph-based approaches."
    },
    {
        "paperId": "8ec5896b4490c6e127d1718ffc36a3439d84cb81",
        "url": "https://www.semanticscholar.org/paper/8ec5896b4490c6e127d1718ffc36a3439d84cb81",
        "title": "On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima",
        "venue": "International Conference on Learning Representations",
        "year": 2016,
        "citationCount": 3216,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1609.04836, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2844898",
                "name": "N. Keskar"
            },
            {
                "authorId": "2205699",
                "name": "Dheevatsa Mudigere"
            },
            {
                "authorId": "2784955",
                "name": "J. Nocedal"
            },
            {
                "authorId": "1711231",
                "name": "M. Smelyanskiy"
            },
            {
                "authorId": "144669504",
                "name": "P. T. P. Tang"
            }
        ],
        "abstract": "The stochastic gradient descent (SGD) method and its variants are algorithms of choice for many Deep Learning tasks. These methods operate in a small-batch regime wherein a fraction of the training data, say $32$-$512$ data points, is sampled to compute an approximation to the gradient. It has been observed in practice that when using a larger batch there is a degradation in the quality of the model, as measured by its ability to generalize. We investigate the cause for this generalization drop in the large-batch regime and present numerical evidence that supports the view that large-batch methods tend to converge to sharp minimizers of the training and testing functions - and as is well known, sharp minima lead to poorer generalization. In contrast, small-batch methods consistently converge to flat minimizers, and our experiments support a commonly held view that this is due to the inherent noise in the gradient estimation. We discuss several strategies to attempt to help large-batch methods eliminate this generalization gap."
    },
    {
        "paperId": "99542f614d7e4146cad17196e76c997e57a69e4d",
        "url": "https://www.semanticscholar.org/paper/99542f614d7e4146cad17196e76c997e57a69e4d",
        "title": "A Learned Representation For Artistic Style",
        "venue": "International Conference on Learning Representations",
        "year": 2016,
        "citationCount": 1232,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1610.07629, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3074927",
                "name": "Vincent Dumoulin"
            },
            {
                "authorId": "1789737",
                "name": "Jonathon Shlens"
            },
            {
                "authorId": "1942300",
                "name": "M. Kudlur"
            }
        ],
        "abstract": "The diversity of painting styles represents a rich visual vocabulary for the construction of an image. The degree to which one may learn and parsimoniously capture this visual vocabulary measures our understanding of the higher level features of paintings, if not images in general. In this work we investigate the construction of a single, scalable deep network that can parsimoniously capture the artistic style of a diversity of paintings. We demonstrate that such a network generalizes across a diversity of artistic styles by reducing a painting to a point in an embedding space. Importantly, this model permits a user to explore new painting styles by arbitrarily combining the styles learned from individual paintings. We hope that this work provides a useful step towards building rich models of paintings and offers a window on to the structure of the learned representation of artistic style."
    },
    {
        "paperId": "99e5a8c10cf92749d4a7c2949691c3a6046e499a",
        "url": "https://www.semanticscholar.org/paper/99e5a8c10cf92749d4a7c2949691c3a6046e499a",
        "title": "Delving into Transferable Adversarial Examples and Black-box Attacks",
        "venue": "International Conference on Learning Representations",
        "year": 2016,
        "citationCount": 1865,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1611.02770, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2108162815",
                "name": "Yanpei Liu"
            },
            {
                "authorId": "1425082935",
                "name": "Xinyun Chen"
            },
            {
                "authorId": "2118484320",
                "name": "Chang Liu"
            },
            {
                "authorId": "143711382",
                "name": "D. Song"
            }
        ],
        "abstract": "An intriguing property of deep neural networks is the existence of adversarial examples, which can transfer among different architectures. These transferable adversarial examples may severely hinder deep neural network-based applications. Previous works mostly study the transferability using small scale datasets. In this work, we are the first to conduct an extensive study of the transferability over large models and a large scale dataset, and we are also the first to study the transferability of targeted adversarial examples with their target labels. We study both non-targeted and targeted adversarial examples, and show that while transferable non-targeted adversarial examples are easy to find, targeted adversarial examples generated using existing approaches almost never transfer with their target labels. Therefore, we propose novel ensemble-based approaches to generating transferable adversarial examples. Using such approaches, we observe a large proportion of targeted adversarial examples that are able to transfer with their target labels for the first time. We also present some geometric studies to help understanding the transferable adversarial examples. Finally, we show that the adversarial examples generated using ensemble-based approaches can successfully attack this http URL, which is a black-box image classification system."
    },
    {
        "paperId": "a90226c41b79f8b06007609f39f82757073641e2",
        "url": "https://www.semanticscholar.org/paper/a90226c41b79f8b06007609f39f82757073641e2",
        "title": "beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework",
        "venue": "International Conference on Learning Representations",
        "year": 2016,
        "citationCount": 5523,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "39051054",
                "name": "I. Higgins"
            },
            {
                "authorId": "2367480",
                "name": "L. Matthey"
            },
            {
                "authorId": "3422676",
                "name": "Arka Pal"
            },
            {
                "authorId": "145463968",
                "name": "Christopher P. Burgess"
            },
            {
                "authorId": "3119801",
                "name": "Xavier Glorot"
            },
            {
                "authorId": "46378362",
                "name": "M. Botvinick"
            },
            {
                "authorId": "14594344",
                "name": "S. Mohamed"
            },
            {
                "authorId": "2289726",
                "name": "Alexander Lerchner"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "b022f2a277a4bf5f42382e86e4380b96340b9e86",
        "url": "https://www.semanticscholar.org/paper/b022f2a277a4bf5f42382e86e4380b96340b9e86",
        "title": "SGDR: Stochastic Gradient Descent with Warm Restarts",
        "venue": "International Conference on Learning Representations",
        "year": 2016,
        "citationCount": 9491,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1608.03983, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1678656",
                "name": "I. Loshchilov"
            },
            {
                "authorId": "144661829",
                "name": "F. Hutter"
            }
        ],
        "abstract": "Restart techniques are common in gradient-free optimization to deal with multimodal functions. Partial warm restarts are also gaining popularity in gradient-based optimization to improve the rate of convergence in accelerated gradient schemes to deal with ill-conditioned functions. In this paper, we propose a simple warm restart technique for stochastic gradient descent to improve its anytime performance when training deep neural networks. We empirically study its performance on the CIFAR-10 and CIFAR-100 datasets, where we demonstrate new state-of-the-art results at 3.14% and 16.21%, respectively. We also demonstrate its advantages on a dataset of EEG recordings and on a downsampled version of the ImageNet dataset. Our source code is available at this https URL"
    },
    {
        "paperId": "b544ca32b66b4c9c69bcfa00d63ee4b799d8ab6b",
        "url": "https://www.semanticscholar.org/paper/b544ca32b66b4c9c69bcfa00d63ee4b799d8ab6b",
        "title": "Adversarial examples in the physical world",
        "venue": "International Conference on Learning Representations",
        "year": 2016,
        "citationCount": 6416,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1607.02533",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1607.02533, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "145714153",
                "name": "Alexey Kurakin"
            },
            {
                "authorId": "153440022",
                "name": "I. Goodfellow"
            },
            {
                "authorId": "1751569",
                "name": "Samy Bengio"
            }
        ],
        "abstract": "Most existing machine learning classifiers are highly vulnerable to adversarial examples. An adversarial example is a sample of input data which has been modified very slightly in a way that is intended to cause a machine learning classifier to misclassify it. In many cases, these modifications can be so subtle that a human observer does not even notice the modification at all, yet the classifier still makes a mistake. Adversarial examples pose security concerns because they could be used to perform an attack on machine learning systems, even if the adversary has no access to the underlying model. Up to now, all previous work have assumed a threat model in which the adversary can feed data directly into the machine learning classifier. This is not always the case for systems operating in the physical world, for example those which are using signals from cameras and other sensors as an input. This paper shows that even in such physical world scenarios, machine learning systems are vulnerable to adversarial examples. We demonstrate this by feeding adversarial images obtained from cell-phone camera to an ImageNet Inception classifier and measuring the classification accuracy of the system. We find that a large fraction of adversarial examples are classified incorrectly even when perceived through the camera."
    },
    {
        "paperId": "c2a1cb1612ba21e067a5c3ba478a8d73b796b77a",
        "url": "https://www.semanticscholar.org/paper/c2a1cb1612ba21e067a5c3ba478a8d73b796b77a",
        "title": "Pruning Filters for Efficient ConvNets",
        "venue": "International Conference on Learning Representations",
        "year": 2016,
        "citationCount": 3934,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1608.08710, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1706574",
                "name": "Hao Li"
            },
            {
                "authorId": "2293919",
                "name": "Asim Kadav"
            },
            {
                "authorId": "1844230",
                "name": "Igor Durdanovic"
            },
            {
                "authorId": "1719385",
                "name": "H. Samet"
            },
            {
                "authorId": "1775043",
                "name": "H. Graf"
            }
        ],
        "abstract": "The success of CNNs in various applications is accompanied by a significant increase in the computation and parameter storage costs. Recent efforts toward reducing these overheads involve pruning and compressing the weights of various layers without hurting original accuracy. However, magnitude-based pruning of weights reduces a significant number of parameters from the fully connected layers and may not adequately reduce the computation costs in the convolutional layers due to irregular sparsity in the pruned networks. We present an acceleration method for CNNs, where we prune filters from CNNs that are identified as having a small effect on the output accuracy. By removing whole filters in the network together with their connecting feature maps, the computation costs are reduced significantly. In contrast to pruning weights, this approach does not result in sparse connectivity patterns. Hence, it does not need the support of sparse convolution libraries and can work with existing efficient BLAS libraries for dense matrix multiplications. We show that even simple filter pruning techniques can reduce inference costs for VGG-16 by up to 34% and ResNet-110 by up to 38% on CIFAR10 while regaining close to the original accuracy by retraining the networks."
    },
    {
        "paperId": "d0156126edbfc524c8d108bdc0cf811cfe3129aa",
        "url": "https://www.semanticscholar.org/paper/d0156126edbfc524c8d108bdc0cf811cfe3129aa",
        "title": "FractalNet: Ultra-Deep Neural Networks without Residuals",
        "venue": "International Conference on Learning Representations",
        "year": 2016,
        "citationCount": 1012,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1605.07648, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "34652567",
                "name": "Gustav Larsson"
            },
            {
                "authorId": "145854440",
                "name": "M. Maire"
            },
            {
                "authorId": "2490189",
                "name": "Gregory Shakhnarovich"
            }
        ],
        "abstract": "We introduce a design strategy for neural network macro-architecture based on self-similarity. Repeated application of a simple expansion rule generates deep networks whose structural layouts are precisely truncated fractals. These networks contain interacting subpaths of different lengths, but do not include any pass-through or residual connections; every internal signal is transformed by a filter and nonlinearity before being seen by subsequent layers. In experiments, fractal networks match the excellent performance of standard residual networks on both CIFAR and ImageNet classification tasks, thereby demonstrating that residual representations may not be fundamental to the success of extremely deep convolutional neural networks. Rather, the key may be the ability to transition, during training, from effectively shallow to deep. We note similarities with student-teacher behavior and develop drop-path, a natural extension of dropout, to regularize co-adaptation of subpaths in fractal architectures. Such regularization allows extraction of high-performance fixed-depth subnetworks. Additionally, fractal networks exhibit an anytime property: shallow subnetworks provide a quick answer, while deeper subnetworks, with higher latency, provide a more accurate answer."
    },
    {
        "paperId": "d2e4587744a89bad95fea69e08842cad6c8ff0dd",
        "url": "https://www.semanticscholar.org/paper/d2e4587744a89bad95fea69e08842cad6c8ff0dd",
        "title": "Temporal Ensembling for Semi-Supervised Learning",
        "venue": "International Conference on Learning Representations",
        "year": 2016,
        "citationCount": 2750,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1610.02242, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "36436218",
                "name": "S. Laine"
            },
            {
                "authorId": "1761103",
                "name": "Timo Aila"
            }
        ],
        "abstract": "In this paper, we present a simple and efficient method for training deep neural networks in a semi-supervised setting where only a small portion of training data is labeled. We introduce self-ensembling, where we form a consensus prediction of the unknown labels using the outputs of the network-in-training on different epochs, and most importantly, under different regularization and input augmentation conditions. This ensemble prediction can be expected to be a better predictor for the unknown labels than the output of the network at the most recent training epoch, and can thus be used as a target for training. Using our method, we set new records for two standard semi-supervised learning benchmarks, reducing the (non-augmented) classification error rate from 18.44% to 7.05% in SVHN with 500 labels and from 18.63% to 16.55% in CIFAR-10 with 4000 labels, and further to 5.12% and 12.16% by enabling the standard augmentations. We additionally obtain a clear improvement in CIFAR-100 classification accuracy by using random images from the Tiny Images dataset as unlabeled extra inputs during training. Finally, we demonstrate good tolerance to incorrect labels."
    },
    {
        "paperId": "d418295cd3027c43eccc5592ae5b8303ba8192be",
        "url": "https://www.semanticscholar.org/paper/d418295cd3027c43eccc5592ae5b8303ba8192be",
        "title": "Trained Ternary Quantization",
        "venue": "International Conference on Learning Representations",
        "year": 2016,
        "citationCount": 1066,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1612.01064, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "7883636",
                "name": "Chenzhuo Zhu"
            },
            {
                "authorId": "143840275",
                "name": "Song Han"
            },
            {
                "authorId": "3123774",
                "name": "Huizi Mao"
            },
            {
                "authorId": "80724002",
                "name": "W. Dally"
            }
        ],
        "abstract": "Deep neural networks are widely used in machine learning applications. However, the deployment of large neural networks models can be difficult to deploy on mobile devices with limited power budgets. To solve this problem, we propose Trained Ternary Quantization (TTQ), a method that can reduce the precision of weights in neural networks to ternary values. This method has very little accuracy degradation and can even improve the accuracy of some models (32, 44, 56-layer ResNet) on CIFAR-10 and AlexNet on ImageNet. And our AlexNet model is trained from scratch, which means it's as easy as to train normal full precision model. We highlight our trained quantization method that can learn both ternary values and ternary assignment. During inference, only ternary values (2-bit weights) and scaling factors are needed, therefore our models are nearly 16x smaller than full-precision models. Our ternary models can also be viewed as sparse binary weight networks, which can potentially be accelerated with custom circuit. Experiments on CIFAR-10 show that the ternary models obtained by trained quantization method outperform full-precision models of ResNet-32,44,56 by 0.04%, 0.16%, 0.36%, respectively. On ImageNet, our model outperforms full-precision AlexNet model by 0.3% of Top-1 accuracy and outperforms previous ternary models by 3%."
    },
    {
        "paperId": "d7878c2044fb699e0ce0cad83e411824b1499dc8",
        "url": "https://www.semanticscholar.org/paper/d7878c2044fb699e0ce0cad83e411824b1499dc8",
        "title": "Neural Combinatorial Optimization with Reinforcement Learning",
        "venue": "International Conference on Learning Representations",
        "year": 2016,
        "citationCount": 1666,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1611.09940, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "4689792",
                "name": "Irwan Bello"
            },
            {
                "authorId": "143950636",
                "name": "Hieu Pham"
            },
            {
                "authorId": "2827616",
                "name": "Quoc V. Le"
            },
            {
                "authorId": "144739074",
                "name": "Mohammad Norouzi"
            },
            {
                "authorId": "1751569",
                "name": "Samy Bengio"
            }
        ],
        "abstract": "This paper presents a framework to tackle combinatorial optimization problems using neural networks and reinforcement learning. We focus on the traveling salesman problem (TSP) and train a recurrent network that, given a set of city coordinates, predicts a distribution over different city permutations. Using negative tour length as the reward signal, we optimize the parameters of the recurrent network using a policy gradient method. We compare learning the network parameters on a set of training graphs against learning them on individual test graphs. Despite the computational expense, without much engineering and heuristic designing, Neural Combinatorial Optimization achieves close to optimal results on 2D Euclidean graphs with up to 100 nodes. Applied to the KnapSack, another NP-hard problem, the same method obtains optimal solutions for instances with up to 200 items."
    },
    {
        "paperId": "d7bd6e3addd8bc8e2e154048300eea15f030ed33",
        "url": "https://www.semanticscholar.org/paper/d7bd6e3addd8bc8e2e154048300eea15f030ed33",
        "title": "Reinforcement Learning with Unsupervised Auxiliary Tasks",
        "venue": "International Conference on Learning Representations",
        "year": 2016,
        "citationCount": 1275,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1611.05397, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3093886",
                "name": "Max Jaderberg"
            },
            {
                "authorId": "3255983",
                "name": "Volodymyr Mnih"
            },
            {
                "authorId": "144792148",
                "name": "Wojciech M. Czarnecki"
            },
            {
                "authorId": "1725157",
                "name": "T. Schaul"
            },
            {
                "authorId": "1700356",
                "name": "Joel Z. Leibo"
            },
            {
                "authorId": "145824029",
                "name": "David Silver"
            },
            {
                "authorId": "2645384",
                "name": "K. Kavukcuoglu"
            }
        ],
        "abstract": "Deep reinforcement learning agents have achieved state-of-the-art results by directly maximising cumulative reward. However, environments contain a much wider variety of possible training signals. In this paper, we introduce an agent that also maximises many other pseudo-reward functions simultaneously by reinforcement learning. All of these tasks share a common representation that, like unsupervised learning, continues to develop in the absence of extrinsic rewards. We also introduce a novel mechanism for focusing this representation upon extrinsic rewards, so that learning can rapidly adapt to the most relevant aspects of the actual task. Our agent significantly outperforms the previous state-of-the-art on Atari, averaging 880\\% expert human performance, and a challenging suite of first-person, three-dimensional \\emph{Labyrinth} tasks leading to a mean speedup in learning of 10$\\times$ and averaging 87\\% expert human performance on Labyrinth."
    },
    {
        "paperId": "e2a85a6766b982ff7c8980e57ca6342d22493827",
        "url": "https://www.semanticscholar.org/paper/e2a85a6766b982ff7c8980e57ca6342d22493827",
        "title": "Adversarial Machine Learning at Scale",
        "venue": "International Conference on Learning Representations",
        "year": 2016,
        "citationCount": 3316,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1611.01236, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "145714153",
                "name": "Alexey Kurakin"
            },
            {
                "authorId": "153440022",
                "name": "I. Goodfellow"
            },
            {
                "authorId": "1751569",
                "name": "Samy Bengio"
            }
        ],
        "abstract": "Adversarial examples are malicious inputs designed to fool machine learning models. They often transfer from one model to another, allowing attackers to mount black box attacks without knowledge of the target model's parameters. Adversarial training is the process of explicitly training a model on adversarial examples, in order to make it more robust to attack or to reduce its test error on clean inputs. So far, adversarial training has primarily been applied to small problems. In this research, we apply adversarial training to ImageNet. Our contributions include: (1) recommendations for how to succesfully scale adversarial training to large models and datasets, (2) the observation that adversarial training confers robustness to single-step attack methods, (3) the finding that multi-step attack methods are somewhat less transferable than single-step attack methods, so single-step attacks are the best for mounting black-box attacks, and (4) resolution of a \"label leaking\" effect that causes adversarially trained models to perform better on adversarial examples than on clean examples, because the adversarial example construction process uses the true label and the model can learn to exploit regularities in the construction process."
    },
    {
        "paperId": "e70b9a38fcf8373865dd6e7b45e45cca7ff2eaa9",
        "url": "https://www.semanticscholar.org/paper/e70b9a38fcf8373865dd6e7b45e45cca7ff2eaa9",
        "title": "Semi-supervised Knowledge Transfer for Deep Learning from Private Training Data",
        "venue": "International Conference on Learning Representations",
        "year": 2016,
        "citationCount": 1086,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1610.05755, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2367796356",
                "name": "Nicolas Papernot"
            },
            {
                "authorId": "2057642721",
                "name": "Martn Abadi"
            },
            {
                "authorId": "1758110",
                "name": ". Erlingsson"
            },
            {
                "authorId": "153440022",
                "name": "I. Goodfellow"
            },
            {
                "authorId": "35210462",
                "name": "Kunal Talwar"
            }
        ],
        "abstract": "Some machine learning applications involve training data that is sensitive, such as the medical histories of patients in a clinical trial. A model may inadvertently and implicitly store some of its training data; careful analysis of the model may therefore reveal sensitive information. \nTo address this problem, we demonstrate a generally applicable approach to providing strong privacy guarantees for training data: Private Aggregation of Teacher Ensembles (PATE). The approach combines, in a black-box fashion, multiple models trained with disjoint datasets, such as records from different subsets of users. Because they rely directly on sensitive data, these models are not published, but instead used as \"teachers\" for a \"student\" model. The student learns to predict an output chosen by noisy voting among all of the teachers, and cannot directly access an individual teacher or the underlying data or parameters. The student's privacy properties can be understood both intuitively (since no single teacher and thus no single dataset dictates the student's training) and formally, in terms of differential privacy. These properties hold even if an adversary can not only query the student but also inspect its internal workings. \nCompared with previous work, the approach imposes only weak assumptions on how teachers are trained: it applies to any model, including non-convex models like DNNs. We achieve state-of-the-art privacy/utility trade-offs on MNIST and SVHN thanks to an improved privacy analysis and semi-supervised learning."
    },
    {
        "paperId": "efbd381493bb9636f489b965a2034d529cd56bcd",
        "url": "https://www.semanticscholar.org/paper/efbd381493bb9636f489b965a2034d529cd56bcd",
        "title": "Pointer Sentinel Mixture Models",
        "venue": "International Conference on Learning Representations",
        "year": 2016,
        "citationCount": 3470,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1609.07843, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3375440",
                "name": "Stephen Merity"
            },
            {
                "authorId": "2228109",
                "name": "Caiming Xiong"
            },
            {
                "authorId": "40518045",
                "name": "James Bradbury"
            },
            {
                "authorId": "2166511",
                "name": "R. Socher"
            }
        ],
        "abstract": "Recent neural network sequence models with softmax classifiers have achieved their best language modeling performance only with very large hidden states and large vocabularies. Even then they struggle to predict rare or unseen words even if the context makes the prediction unambiguous. We introduce the pointer sentinel mixture architecture for neural sequence models which has the ability to either reproduce a word from the recent context or produce a word from a standard softmax classifier. Our pointer sentinel-LSTM model achieves state of the art language modeling performance on the Penn Treebank (70.9 perplexity) while using far fewer parameters than a standard softmax LSTM. In order to evaluate how well language models can exploit longer contexts and deal with more realistic vocabularies and larger corpora we also introduce the freely available WikiText corpus."
    },
    {
        "paperId": "f7b032a4df721d4ed2bab97f6acd33d62477b7a5",
        "url": "https://www.semanticscholar.org/paper/f7b032a4df721d4ed2bab97f6acd33d62477b7a5",
        "title": "Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer",
        "venue": "International Conference on Learning Representations",
        "year": 2016,
        "citationCount": 2877,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1612.03928, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2134433",
                "name": "Sergey Zagoruyko"
            },
            {
                "authorId": "2505902",
                "name": "N. Komodakis"
            }
        ],
        "abstract": "Attention plays a critical role in human visual experience. Furthermore, it has recently been demonstrated that attention can also play an important role in the context of applying artificial neural networks to a variety of tasks from fields such as computer vision and NLP. In this work we show that, by properly defining attention for convolutional neural networks, we can actually use this type of information in order to significantly improve the performance of a student CNN network by forcing it to mimic the attention maps of a powerful teacher network. To that end, we propose several novel methods of transferring attention, showing consistent improvement across a variety of datasets and convolutional neural network architectures. Code and models for our experiments are available at this https URL"
    },
    {
        "paperId": "fcf43325529c8b1cc26aeb52fd5d7e532abb0a40",
        "url": "https://www.semanticscholar.org/paper/fcf43325529c8b1cc26aeb52fd5d7e532abb0a40",
        "title": "Adversarially Learned Inference",
        "venue": "International Conference on Learning Representations",
        "year": 2016,
        "citationCount": 1334,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1606.00704, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3074927",
                "name": "Vincent Dumoulin"
            },
            {
                "authorId": "3422357",
                "name": "Ishmael Belghazi"
            },
            {
                "authorId": "16443937",
                "name": "Ben Poole"
            },
            {
                "authorId": "49071560",
                "name": "Alex Lamb"
            },
            {
                "authorId": "2877311",
                "name": "Martn Arjovsky"
            },
            {
                "authorId": "3422889",
                "name": "Olivier Mastropietro"
            },
            {
                "authorId": "1760871",
                "name": "Aaron C. Courville"
            }
        ],
        "abstract": "We introduce the adversarially learned inference (ALI) model, which jointly learns a generation network and an inference network using an adversarial process. The generation network maps samples from stochastic latent variables to the data space while the inference network maps training examples in data space to the space of latent variables. An adversarial game is cast between these two networks and a discriminative network is trained to distinguish between joint latent/data-space samples from the generative network and joint samples from the inference network. We illustrate the ability of the model to learn mutually coherent inference and generation networks through the inspections of model samples and reconstructions and confirm the usefulness of the learned representations by obtaining a performance competitive with state-of-the-art on the semi-supervised SVHN and CIFAR10 tasks."
    },
    {
        "paperId": "032274e57f7d8b456bd255fe76b909b2c1d7458e",
        "url": "https://www.semanticscholar.org/paper/032274e57f7d8b456bd255fe76b909b2c1d7458e",
        "title": "A Deep Reinforced Model for Abstractive Summarization",
        "venue": "International Conference on Learning Representations",
        "year": 2017,
        "citationCount": 1619,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1705.04304, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2896063",
                "name": "Romain Paulus"
            },
            {
                "authorId": "2228109",
                "name": "Caiming Xiong"
            },
            {
                "authorId": "2166511",
                "name": "R. Socher"
            }
        ],
        "abstract": "Attentional, RNN-based encoder-decoder models for abstractive summarization have achieved good performance on short input and output sequences. For longer documents and summaries however these models often include repetitive and incoherent phrases. We introduce a neural network model with a novel intra-attention that attends over the input and continuously generated output separately, and a new training method that combines standard supervised word prediction and reinforcement learning (RL). Models trained only with supervised learning often exhibit \"exposure bias\" - they assume ground truth is provided at each step during training. However, when standard word prediction is combined with the global sequence prediction training of RL the resulting summaries become more readable. We evaluate this model on the CNN/Daily Mail and New York Times datasets. Our model obtains a 41.16 ROUGE-1 score on the CNN/Daily Mail dataset, an improvement over previous state-of-the-art models. Human evaluation also shows that our model produces higher quality summaries."
    },
    {
        "paperId": "061fef7e31c2b6ae59e49b8cf3dfb9c449aebc0a",
        "url": "https://www.semanticscholar.org/paper/061fef7e31c2b6ae59e49b8cf3dfb9c449aebc0a",
        "title": "On Detecting Adversarial Perturbations",
        "venue": "International Conference on Learning Representations",
        "year": 2017,
        "citationCount": 1000,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1702.04267, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2708564",
                "name": "J. Metzen"
            },
            {
                "authorId": "3081854",
                "name": "Tim Genewein"
            },
            {
                "authorId": "47092548",
                "name": "Volker Fischer"
            },
            {
                "authorId": "3452473",
                "name": "Bastian Bischoff"
            }
        ],
        "abstract": "Machine learning and deep learning in particular has advanced tremendously on perceptual tasks in recent years. However, it remains vulnerable against adversarial perturbations of the input that have been crafted specifically to fool the system while being quasi-imperceptible to a human. In this work, we propose to augment deep neural networks with a small \"detector\" subnetwork which is trained on the binary classification task of distinguishing genuine data from data containing adversarial perturbations. Our method is orthogonal to prior work on addressing adversarial perturbations, which has mostly focused on making the classification network itself more robust. We show empirically that adversarial perturbations can be detected surprisingly well even though they are quasi-imperceptible to humans. Moreover, while the detectors have been trained to detect only a specific adversary, they generalize to similar and weaker adversaries. In addition, we propose an adversarial attack that fools both the classifier and the detector and a novel training procedure for the detector that counteracts this attack."
    },
    {
        "paperId": "075556dd42900a6bc4552a2f2531ba21b9b7b4c0",
        "url": "https://www.semanticscholar.org/paper/075556dd42900a6bc4552a2f2531ba21b9b7b4c0",
        "title": "Deep Neural Networks as Gaussian Processes",
        "venue": "International Conference on Learning Representations",
        "year": 2017,
        "citationCount": 1178,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1711.00165, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "49685832",
                "name": "Jaehoon Lee"
            },
            {
                "authorId": "12383244",
                "name": "Yasaman Bahri"
            },
            {
                "authorId": "39068839",
                "name": "Roman Novak"
            },
            {
                "authorId": "2601641",
                "name": "S. Schoenholz"
            },
            {
                "authorId": "143845796",
                "name": "Jeffrey Pennington"
            },
            {
                "authorId": "1407546424",
                "name": "Jascha Narain Sohl-Dickstein"
            }
        ],
        "abstract": "It has long been known that a single-layer fully-connected neural network with an i.i.d. prior over its parameters is equivalent to a Gaussian process (GP), in the limit of infinite network width. This correspondence enables exact Bayesian inference for infinite width neural networks on regression tasks by means of evaluating the corresponding GP. Recently, kernel functions which mimic multi-layer random neural networks have been developed, but only outside of a Bayesian framework. As such, previous work has not identified that these kernels can be used as covariance functions for GPs and allow fully Bayesian prediction with a deep neural network. \nIn this work, we derive the exact equivalence between infinitely wide deep networks and GPs. We further develop a computationally efficient pipeline to compute the covariance function for these GPs. We then use the resulting GPs to perform Bayesian inference for wide deep neural networks on MNIST and CIFAR-10. We observe that trained neural network accuracy approaches that of the corresponding GP with increasing layer width, and that the GP uncertainty is strongly correlated with trained network prediction error. We further find that test performance increases as finite-width trained networks are made wider and more similar to a GP, and thus that GP predictions typically outperform those of finite-width networks. Finally we connect the performance of these GPs to the recent theory of signal propagation in random neural networks."
    },
    {
        "paperId": "136dee73f203df2f4831994bf4f0c0a4ad2e764e",
        "url": "https://www.semanticscholar.org/paper/136dee73f203df2f4831994bf4f0c0a4ad2e764e",
        "title": "Ensemble Adversarial Training: Attacks and Defenses",
        "venue": "International Conference on Learning Representations",
        "year": 2017,
        "citationCount": 2927,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1705.07204, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2444919",
                "name": "Florian Tramr"
            },
            {
                "authorId": "145714153",
                "name": "Alexey Kurakin"
            },
            {
                "authorId": "2367796356",
                "name": "Nicolas Papernot"
            },
            {
                "authorId": "1752788",
                "name": "D. Boneh"
            },
            {
                "authorId": "144061974",
                "name": "P. Mcdaniel"
            }
        ],
        "abstract": "Adversarial examples are perturbed inputs designed to fool machine learning models. Adversarial training injects such examples into training data to increase robustness. To scale this technique to large datasets, perturbations are crafted using fast single-step methods that maximize a linear approximation of the model's loss. We show that this form of adversarial training converges to a degenerate global minimum, wherein small curvature artifacts near the data points obfuscate a linear approximation of the loss. The model thus learns to generate weak perturbations, rather than defend against strong ones. As a result, we find that adversarial training remains vulnerable to black-box attacks, where we transfer perturbations computed on undefended models, as well as to a powerful novel single-step attack that escapes the non-smooth vicinity of the input data via a small random step. We further introduce Ensemble Adversarial Training, a technique that augments training data with perturbations transferred from other models. On ImageNet, Ensemble Adversarial Training yields models with strong robustness to black-box attacks. In particular, our most robust model won the first round of the NIPS 2017 competition on Defenses against Adversarial Attacks. However, subsequent work found that more elaborate black-box attacks could significantly enhance transferability and reduce the accuracy of our models."
    },
    {
        "paperId": "1b225474e7a5794f98cdfbde8b12ccbc56799409",
        "url": "https://www.semanticscholar.org/paper/1b225474e7a5794f98cdfbde8b12ccbc56799409",
        "title": "Decision-Based Adversarial Attacks: Reliable Attacks Against Black-Box Machine Learning Models",
        "venue": "International Conference on Learning Representations",
        "year": 2017,
        "citationCount": 1462,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1712.04248, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "40634590",
                "name": "Wieland Brendel"
            },
            {
                "authorId": "19237612",
                "name": "Jonas Rauber"
            },
            {
                "authorId": "1731199",
                "name": "M. Bethge"
            }
        ],
        "abstract": "Many machine learning algorithms are vulnerable to almost imperceptible perturbations of their inputs. So far it was unclear how much risk adversarial perturbations carry for the safety of real-world machine learning applications because most methods used to generate such perturbations rely either on detailed model information (gradient-based attacks) or on confidence scores such as class probabilities (score-based attacks), neither of which are available in most real-world scenarios. In many such cases one currently needs to retreat to transfer-based attacks which rely on cumbersome substitute models, need access to the training data and can be defended against. Here we emphasise the importance of attacks which solely rely on the final model decision. Such decision-based attacks are (1) applicable to real-world black-box models such as autonomous cars, (2) need less knowledge and are easier to apply than transfer-based attacks and (3) are more robust to simple defences than gradient- or score-based attacks. Previous attacks in this category were limited to simple models or simple datasets. Here we introduce the Boundary Attack, a decision-based attack that starts from a large adversarial perturbation and then seeks to reduce the perturbation while staying adversarial. The attack is conceptually simple, requires close to no hyperparameter tuning, does not rely on substitute models and is competitive with the best gradient-based attacks in standard computer vision tasks like ImageNet. We apply the attack on two black-box algorithms from Clarifai.com. The Boundary Attack in particular and the class of decision-based attacks in general open new avenues to study the robustness of machine learning models and raise new questions regarding the safety of deployed machine learning systems. An implementation of the attack is available as part of Foolbox at this https URL ."
    },
    {
        "paperId": "204a4a70428f3938d2c538a4d74c7ae0416306d8",
        "url": "https://www.semanticscholar.org/paper/204a4a70428f3938d2c538a4d74c7ae0416306d8",
        "title": "A Structured Self-attentive Sentence Embedding",
        "venue": "International Conference on Learning Representations",
        "year": 2017,
        "citationCount": 2246,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1703.03130, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3146592",
                "name": "Zhouhan Lin"
            },
            {
                "authorId": "2521552",
                "name": "Minwei Feng"
            },
            {
                "authorId": "1790831",
                "name": "C. D. Santos"
            },
            {
                "authorId": "2482533",
                "name": "Mo Yu"
            },
            {
                "authorId": "144028698",
                "name": "Bing Xiang"
            },
            {
                "authorId": "145218984",
                "name": "Bowen Zhou"
            },
            {
                "authorId": "1751762",
                "name": "Yoshua Bengio"
            }
        ],
        "abstract": "This paper proposes a new model for extracting an interpretable sentence embedding by introducing self-attention. Instead of using a vector, we use a 2-D matrix to represent the embedding, with each row of the matrix attending on a different part of the sentence. We also propose a self-attention mechanism and a special regularization term for the model. As a side effect, the embedding comes with an easy way of visualizing what specific parts of the sentence are encoded into the embedding. We evaluate our model on 3 different tasks: author profiling, sentiment classification, and textual entailment. Results show that our model yields a significant performance gain compared to other sentence embedding methods in all of the 3 tasks."
    },
    {
        "paperId": "2ec7156913117949ab933f27f492d0149bc0031f",
        "url": "https://www.semanticscholar.org/paper/2ec7156913117949ab933f27f492d0149bc0031f",
        "title": "Learning Sparse Neural Networks through L0 Regularization",
        "venue": "International Conference on Learning Representations",
        "year": 2017,
        "citationCount": 1232,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1712.01312, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2075783",
                "name": "Christos Louizos"
            },
            {
                "authorId": "1678311",
                "name": "M. Welling"
            },
            {
                "authorId": "1726807",
                "name": "Diederik P. Kingma"
            }
        ],
        "abstract": "We propose a practical method for $L_0$ norm regularization for neural networks: pruning the network during training by encouraging weights to become exactly zero. Such regularization is interesting since (1) it can greatly speed up training and inference, and (2) it can improve generalization. AIC and BIC, well-known model selection criteria, are special cases of $L_0$ regularization. However, since the $L_0$ norm of weights is non-differentiable, we cannot incorporate it directly as a regularization term in the objective function. We propose a solution through the inclusion of a collection of non-negative stochastic gates, which collectively determine which weights to set to zero. We show that, somewhat surprisingly, for certain distributions over the gates, the expected $L_0$ norm of the resulting gated weights is differentiable with respect to the distribution parameters. We further propose the \\emph{hard concrete} distribution for the gates, which is obtained by \"stretching\" a binary concrete distribution and then transforming its samples with a hard-sigmoid. The parameters of the distribution over the gates can then be jointly optimized with the original network parameters. As a result our method allows for straightforward and efficient learning of model structures with stochastic gradient descent and allows for conditional computation in a principled way. We perform various experiments to demonstrate the effectiveness of the resulting approach and regularizer."
    },
    {
        "paperId": "3299aee7a354877e43339d06abb967af2be8b872",
        "url": "https://www.semanticscholar.org/paper/3299aee7a354877e43339d06abb967af2be8b872",
        "title": "Don't Decay the Learning Rate, Increase the Batch Size",
        "venue": "International Conference on Learning Representations",
        "year": 2017,
        "citationCount": 1078,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1711.00489, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2157770601",
                "name": "Samuel L. Smith"
            },
            {
                "authorId": "2113697",
                "name": "Pieter-Jan Kindermans"
            },
            {
                "authorId": "2827616",
                "name": "Quoc V. Le"
            }
        ],
        "abstract": "It is common practice to decay the learning rate. Here we show one can usually obtain the same learning curve on both training and test sets by instead increasing the batch size during training. This procedure is successful for stochastic gradient descent (SGD), SGD with momentum, Nesterov momentum, and Adam. It reaches equivalent test accuracies after the same number of training epochs, but with fewer parameter updates, leading to greater parallelism and shorter training times. We can further reduce the number of parameter updates by increasing the learning rate $\\epsilon$ and scaling the batch size $B \\propto \\epsilon$. Finally, one can increase the momentum coefficient $m$ and scale $B \\propto 1/(1-m)$, although this tends to slightly reduce the test accuracy. Crucially, our techniques allow us to repurpose existing training schedules for large batch training with no hyper-parameter tuning. We train ResNet-50 on ImageNet to $76.1\\%$ validation accuracy in under 30 minutes."
    },
    {
        "paperId": "33998aff64ce51df8dee45989cdca4b6b1329ec4",
        "url": "https://www.semanticscholar.org/paper/33998aff64ce51df8dee45989cdca4b6b1329ec4",
        "title": "Graph Attention Networks",
        "venue": "International Conference on Learning Representations",
        "year": 2017,
        "citationCount": 23897,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1710.10903, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3444569",
                "name": "Petar Velickovic"
            },
            {
                "authorId": "7153363",
                "name": "Guillem Cucurull"
            },
            {
                "authorId": "8742492",
                "name": "Arantxa Casanova"
            },
            {
                "authorId": "144290131",
                "name": "Adriana Romero"
            },
            {
                "authorId": "2392269716",
                "name": "Pietro Li"
            },
            {
                "authorId": "1751762",
                "name": "Yoshua Bengio"
            }
        ],
        "abstract": "We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs remain unseen during training)."
    },
    {
        "paperId": "3b4d671a8c7018c0b42673ba581e5ff3ae762d6c",
        "url": "https://www.semanticscholar.org/paper/3b4d671a8c7018c0b42673ba581e5ff3ae762d6c",
        "title": "To prune, or not to prune: exploring the efficacy of pruning for model compression",
        "venue": "International Conference on Learning Representations",
        "year": 2017,
        "citationCount": 1402,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1710.01878, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2152183723",
                "name": "Michael Zhu"
            },
            {
                "authorId": "2116011472",
                "name": "Suyog Gupta"
            }
        ],
        "abstract": "Model pruning seeks to induce sparsity in a deep neural network's various connection matrices, thereby reducing the number of nonzero-valued parameters in the model. Recent reports (Han et al., 2015; Narang et al., 2017) prune deep networks at the cost of only a marginal loss in accuracy and achieve a sizable reduction in model size. This hints at the possibility that the baseline models in these experiments are perhaps severely over-parameterized at the outset and a viable alternative for model compression might be to simply reduce the number of hidden units while maintaining the model's dense connection structure, exposing a similar trade-off in model size and accuracy. We investigate these two distinct paths for model compression within the context of energy-efficient inference in resource-constrained environments and propose a new gradual pruning technique that is simple and straightforward to apply across a variety of models/datasets with minimal tuning and can be seamlessly incorporated within the training process. We compare the accuracy of large, but pruned models (large-sparse) and their smaller, but dense (small-dense) counterparts with identical memory footprint. Across a broad range of neural network architectures (deep CNNs, stacked LSTM, and seq2seq LSTM models), we find large-sparse models to consistently outperform small-dense models and achieve up to 10x reduction in number of non-zero parameters with minimal loss in accuracy."
    },
    {
        "paperId": "3f1802d3f4f5f6d66875dac09112f978f12e1e1e",
        "url": "https://www.semanticscholar.org/paper/3f1802d3f4f5f6d66875dac09112f978f12e1e1e",
        "title": "A Simple but Tough-to-Beat Baseline for Sentence Embeddings",
        "venue": "International Conference on Learning Representations",
        "year": 2017,
        "citationCount": 1377,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "145563465",
                "name": "Sanjeev Arora"
            },
            {
                "authorId": "40609253",
                "name": "Yingyu Liang"
            },
            {
                "authorId": "1901958",
                "name": "Tengyu Ma"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "407ead18083e68626e82e07db1a9289ff0b7e862",
        "url": "https://www.semanticscholar.org/paper/407ead18083e68626e82e07db1a9289ff0b7e862",
        "title": "Incremental Network Quantization: Towards Lossless CNNs with Low-Precision Weights",
        "venue": "International Conference on Learning Representations",
        "year": 2017,
        "citationCount": 1101,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1702.03044, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "9548994",
                "name": "Aojun Zhou"
            },
            {
                "authorId": "2021251",
                "name": "Anbang Yao"
            },
            {
                "authorId": "2527106",
                "name": "Yiwen Guo"
            },
            {
                "authorId": "2152895776",
                "name": "Lin Xu"
            },
            {
                "authorId": "2109184871",
                "name": "Yurong Chen"
            }
        ],
        "abstract": "This paper presents incremental network quantization (INQ), a novel method, targeting to efficiently convert any pre-trained full-precision convolutional neural network (CNN) model into a low-precision version whose weights are constrained to be either powers of two or zero. Unlike existing methods which are struggled in noticeable accuracy loss, our INQ has the potential to resolve this issue, as benefiting from two innovations. On one hand, we introduce three interdependent operations, namely weight partition, group-wise quantization and re-training. A well-proven measure is employed to divide the weights in each layer of a pre-trained CNN model into two disjoint groups. The weights in the first group are responsible to form a low-precision base, thus they are quantized by a variable-length encoding method. The weights in the other group are responsible to compensate for the accuracy loss from the quantization, thus they are the ones to be re-trained. On the other hand, these three operations are repeated on the latest re-trained group in an iterative manner until all the weights are converted into low-precision ones, acting as an incremental network quantization and accuracy enhancement procedure. Extensive experiments on the ImageNet classification task using almost all known deep CNN architectures including AlexNet, VGG-16, GoogleNet and ResNets well testify the efficacy of the proposed method. Specifically, at 5-bit quantization, our models have improved accuracy than the 32-bit floating-point references. Taking ResNet-18 as an example, we further show that our quantized models with 4-bit, 3-bit and 2-bit ternary weights have improved or very similar accuracy against its 32-bit floating-point baseline. Besides, impressive results with the combination of network pruning and INQ are also reported. The code is available at this https URL."
    },
    {
        "paperId": "4feef0fd284feb1233399b400eb897f59ec92755",
        "url": "https://www.semanticscholar.org/paper/4feef0fd284feb1233399b400eb897f59ec92755",
        "title": "mixup: Beyond Empirical Risk Minimization",
        "venue": "International Conference on Learning Representations",
        "year": 2017,
        "citationCount": 11054,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1710.09412, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2108880169",
                "name": "Hongyi Zhang"
            },
            {
                "authorId": "5723508",
                "name": "Moustapha Ciss"
            },
            {
                "authorId": "2921469",
                "name": "Yann Dauphin"
            },
            {
                "authorId": "1401804750",
                "name": "David Lopez-Paz"
            }
        ],
        "abstract": "Large deep neural networks are powerful, but exhibit undesirable behaviors such as memorization and sensitivity to adversarial examples. In this work, we propose mixup, a simple learning principle to alleviate these issues. In essence, mixup trains a neural network on convex combinations of pairs of examples and their labels. By doing so, mixup regularizes the neural network to favor simple linear behavior in-between training examples. Our experiments on the ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show that mixup improves the generalization of state-of-the-art neural network architectures. We also find that mixup reduces the memorization of corrupt labels, increases the robustness to adversarial examples, and stabilizes the training of generative adversarial networks."
    },
    {
        "paperId": "510e26733aaff585d65701b9f1be7ca9d5afc586",
        "url": "https://www.semanticscholar.org/paper/510e26733aaff585d65701b9f1be7ca9d5afc586",
        "title": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer",
        "venue": "International Conference on Learning Representations",
        "year": 2017,
        "citationCount": 3635,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1701.06538, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1846258",
                "name": "Noam Shazeer"
            },
            {
                "authorId": "1861312",
                "name": "Azalia Mirhoseini"
            },
            {
                "authorId": "2275364713",
                "name": "Krzysztof Maziarz"
            },
            {
                "authorId": "36347083",
                "name": "Andy Davis"
            },
            {
                "authorId": "2827616",
                "name": "Quoc V. Le"
            },
            {
                "authorId": "1695689",
                "name": "Geoffrey E. Hinton"
            },
            {
                "authorId": "48448318",
                "name": "J. Dean"
            }
        ],
        "abstract": "The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. In practice, however, there are significant algorithmic and performance challenges. In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters. We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example. We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora. We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost."
    },
    {
        "paperId": "547c854985629cfa9404a5ba8ca29367b5f8c25f",
        "url": "https://www.semanticscholar.org/paper/547c854985629cfa9404a5ba8ca29367b5f8c25f",
        "title": "Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks",
        "venue": "International Conference on Learning Representations",
        "year": 2017,
        "citationCount": 2303,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1706.02690, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3075943",
                "name": "Shiyu Liang"
            },
            {
                "authorId": "1527103472",
                "name": "Yixuan Li"
            },
            {
                "authorId": "143808204",
                "name": "R. Srikant"
            }
        ],
        "abstract": "We consider the problem of detecting out-of-distribution images in neural networks. We propose ODIN, a simple and effective method that does not require any change to a pre-trained neural network. Our method is based on the observation that using temperature scaling and adding small perturbations to the input can separate the softmax score distributions between in- and out-of-distribution images, allowing for more effective detection. We show in a series of experiments that ODIN is compatible with diverse network architectures and datasets. It consistently outperforms the baseline approach by a large margin, establishing a new state-of-the-art performance on this task. For example, ODIN reduces the false positive rate from the baseline 34.7% to 4.3% on the DenseNet (applied to CIFAR-10) when the true positive rate is 95%."
    },
    {
        "paperId": "562c09c112df56c5696c010d90a815d6018a86c8",
        "url": "https://www.semanticscholar.org/paper/562c09c112df56c5696c010d90a815d6018a86c8",
        "title": "Word Translation Without Parallel Data",
        "venue": "International Conference on Learning Representations",
        "year": 2017,
        "citationCount": 1721,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1710.04087, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2480903",
                "name": "Alexis Conneau"
            },
            {
                "authorId": "1830914",
                "name": "Guillaume Lample"
            },
            {
                "authorId": "1706809",
                "name": "Marc'Aurelio Ranzato"
            },
            {
                "authorId": "8905591",
                "name": "Ludovic Denoyer"
            },
            {
                "authorId": "2065248680",
                "name": "Herv'e J'egou"
            }
        ],
        "abstract": "State-of-the-art methods for learning cross-lingual word embeddings have relied on bilingual dictionaries or parallel corpora. Recent studies showed that the need for parallel data supervision can be alleviated with character-level information. While these methods showed encouraging results, they are not on par with their supervised counterparts and are limited to pairs of languages sharing a common alphabet. In this work, we show that we can build a bilingual dictionary between two languages without using any parallel corpora, by aligning monolingual word embedding spaces in an unsupervised way. Without using any character information, our model even outperforms existing supervised methods on cross-lingual tasks for some language pairs. Our experiments demonstrate that our method works very well also for distant language pairs, like English-Russian or English-Chinese. We finally describe experiments on the English-Esperanto low-resource language pair, on which there only exists a limited amount of parallel data, to show the potential impact of our method in fully unsupervised machine translation. Our code, embeddings and dictionaries are publicly available."
    },
    {
        "paperId": "572a1f77306e160c3893299c18f3ed862fb5f6d9",
        "url": "https://www.semanticscholar.org/paper/572a1f77306e160c3893299c18f3ed862fb5f6d9",
        "title": "Few-Shot Learning with Graph Neural Networks",
        "venue": "International Conference on Learning Representations",
        "year": 2017,
        "citationCount": 1325,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1711.04043, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "73240341",
                "name": "Victor Garcia Satorras"
            },
            {
                "authorId": "143627859",
                "name": "Joan Bruna"
            }
        ],
        "abstract": "We propose to study the problem of few-shot learning with the prism of inference on a partially observed graphical model, constructed from a collection of input images whose label can be either observed or not. By assimilating generic message-passing inference algorithms with their neural-network counterparts, we define a graph neural network architecture that generalizes several of the recently proposed few-shot learning models. Besides providing improved numerical performance, our framework is easily extended to variants of few-shot learning, such as semi-supervised or active learning, demonstrating the ability of graph-based models to operate well on 'relational' tasks."
    },
    {
        "paperId": "58c6f890a1ae372958b7decf56132fe258152722",
        "url": "https://www.semanticscholar.org/paper/58c6f890a1ae372958b7decf56132fe258152722",
        "title": "Regularizing and Optimizing LSTM Language Models",
        "venue": "International Conference on Learning Representations",
        "year": 2017,
        "citationCount": 1140,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1708.02182, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3375440",
                "name": "Stephen Merity"
            },
            {
                "authorId": "2844898",
                "name": "N. Keskar"
            },
            {
                "authorId": "2166511",
                "name": "R. Socher"
            }
        ],
        "abstract": "Recurrent neural networks (RNNs), such as long short-term memory networks (LSTMs), serve as a fundamental building block for many sequence learning tasks, including machine translation, language modeling, and question answering. In this paper, we consider the specific problem of word-level language modeling and investigate strategies for regularizing and optimizing LSTM-based models. We propose the weight-dropped LSTM which uses DropConnect on hidden-to-hidden weights as a form of recurrent regularization. Further, we introduce NT-ASGD, a variant of the averaged stochastic gradient method, wherein the averaging trigger is determined using a non-monotonic condition as opposed to being tuned by the user. Using these and other regularization strategies, we achieve state-of-the-art word level perplexities on two data sets: 57.3 on Penn Treebank and 65.8 on WikiText-2. In exploring the effectiveness of a neural cache in conjunction with our proposed model, we achieve an even lower state-of-the-art perplexity of 52.8 on Penn Treebank and 52.0 on WikiText-2."
    },
    {
        "paperId": "6745c95b88ff9b12401a9ba6f4007f036be591a0",
        "url": "https://www.semanticscholar.org/paper/6745c95b88ff9b12401a9ba6f4007f036be591a0",
        "title": "Wasserstein Auto-Encoders",
        "venue": "International Conference on Learning Representations",
        "year": 2017,
        "citationCount": 1107,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1711.01558, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2065186595",
                "name": "Ilya O. Tolstikhin"
            },
            {
                "authorId": "2067142304",
                "name": "Olivier Bousquet"
            },
            {
                "authorId": "2075275631",
                "name": "Sylvain Gelly"
            },
            {
                "authorId": "90555751",
                "name": "B. Schlkopf"
            }
        ],
        "abstract": "We propose the Wasserstein Auto-Encoder (WAE)---a new algorithm for building a generative model of the data distribution. WAE minimizes a penalized form of the Wasserstein distance between the model distribution and the target distribution, which leads to a different regularizer than the one used by the Variational Auto-Encoder (VAE). This regularizer encourages the encoded training distribution to match the prior. We compare our algorithm with several other techniques and show that it is a generalization of adversarial auto-encoders (AAE). Our experiments show that WAE shares many of the properties of VAEs (stable training, encoder-decoder architecture, nice latent manifold structure) while generating samples of better quality, as measured by the FID score."
    },
    {
        "paperId": "6ce1922802169f757bbafc6e087cc274a867c763",
        "url": "https://www.semanticscholar.org/paper/6ce1922802169f757bbafc6e087cc274a867c763",
        "title": "Regularizing Neural Networks by Penalizing Confident Output Distributions",
        "venue": "International Conference on Learning Representations",
        "year": 2017,
        "citationCount": 1213,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1701.06548, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2064737506",
                "name": "Gabriel Pereyra"
            },
            {
                "authorId": "145499435",
                "name": "G. Tucker"
            },
            {
                "authorId": "2292403",
                "name": "J. Chorowski"
            },
            {
                "authorId": "40527594",
                "name": "Lukasz Kaiser"
            },
            {
                "authorId": "1695689",
                "name": "Geoffrey E. Hinton"
            }
        ],
        "abstract": "We systematically explore regularizing neural networks by penalizing low entropy output distributions. We show that penalizing low entropy output distributions, which has been shown to improve exploration in reinforcement learning, acts as a strong regularizer in supervised learning. Furthermore, we connect a maximum entropy based confidence penalty to label smoothing through the direction of the KL divergence. We exhaustively evaluate the proposed confidence penalty and label smoothing on 6 common benchmarks: image classification (MNIST and Cifar-10), language modeling (Penn Treebank), machine translation (WMT'14 English-to-German), and speech recognition (TIMIT and WSJ). We find that both label smoothing and the confidence penalty improve state-of-the-art models across benchmarks without modifying existing hyperparameters, suggesting the wide applicability of these regularizers."
    },
    {
        "paperId": "744fe47157477235032f7bb3777800f9f2f45e52",
        "url": "https://www.semanticscholar.org/paper/744fe47157477235032f7bb3777800f9f2f45e52",
        "title": "Progressive Growing of GANs for Improved Quality, Stability, and Variation",
        "venue": "International Conference on Learning Representations",
        "year": 2017,
        "citationCount": 8107,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1710.10196, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2976930",
                "name": "Tero Karras"
            },
            {
                "authorId": "1761103",
                "name": "Timo Aila"
            },
            {
                "authorId": "36436218",
                "name": "S. Laine"
            },
            {
                "authorId": "49244945",
                "name": "J. Lehtinen"
            }
        ],
        "abstract": "We describe a new training methodology for generative adversarial networks. The key idea is to grow both the generator and discriminator progressively: starting from a low resolution, we add new layers that model increasingly fine details as training progresses. This both speeds the training up and greatly stabilizes it, allowing us to produce images of unprecedented quality, e.g., CelebA images at 1024^2. We also propose a simple way to increase the variation in generated images, and achieve a record inception score of 8.80 in unsupervised CIFAR10. Additionally, we describe several implementation details that are important for discouraging unhealthy competition between the generator and discriminator. Finally, we suggest a new metric for evaluating GAN results, both in terms of image quality and variation. As an additional contribution, we construct a higher-quality version of the CelebA dataset."
    },
    {
        "paperId": "7aa38b85fa8cba64d6a4010543f6695dbf5f1386",
        "url": "https://www.semanticscholar.org/paper/7aa38b85fa8cba64d6a4010543f6695dbf5f1386",
        "title": "Towards Deep Learning Models Resistant to Adversarial Attacks",
        "venue": "International Conference on Learning Representations",
        "year": 2017,
        "citationCount": 13667,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1706.06083, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "143826246",
                "name": "A. Madry"
            },
            {
                "authorId": "17775913",
                "name": "Aleksandar Makelov"
            },
            {
                "authorId": "152772922",
                "name": "Ludwig Schmidt"
            },
            {
                "authorId": "2754804",
                "name": "Dimitris Tsipras"
            },
            {
                "authorId": "2869958",
                "name": "Adrian Vladu"
            }
        ],
        "abstract": "Recent work has demonstrated that deep neural networks are vulnerable to adversarial examples---inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. Code and pre-trained models are available at this https URL and this https URL."
    },
    {
        "paperId": "7e9c1e0d247b20a0683f4797d9ea248c3b53d424",
        "url": "https://www.semanticscholar.org/paper/7e9c1e0d247b20a0683f4797d9ea248c3b53d424",
        "title": "A Simple Neural Attentive Meta-Learner",
        "venue": "International Conference on Learning Representations",
        "year": 2017,
        "citationCount": 1358,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "3414570",
                "name": "Nikhil Mishra"
            },
            {
                "authorId": "22222033",
                "name": "Mostafa Rohaninejad"
            },
            {
                "authorId": "41192764",
                "name": "Xi Chen"
            },
            {
                "authorId": "1689992",
                "name": "P. Abbeel"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "92495abbac86394cb759bec15a763dbf49a8e590",
        "url": "https://www.semanticscholar.org/paper/92495abbac86394cb759bec15a763dbf49a8e590",
        "title": "Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training",
        "venue": "International Conference on Learning Representations",
        "year": 2017,
        "citationCount": 1532,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1712.01887, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "49417466",
                "name": "Yujun Lin"
            },
            {
                "authorId": "143840275",
                "name": "Song Han"
            },
            {
                "authorId": "3123774",
                "name": "Huizi Mao"
            },
            {
                "authorId": "40987227",
                "name": "Yu Wang"
            },
            {
                "authorId": "80724002",
                "name": "W. Dally"
            }
        ],
        "abstract": "Large-scale distributed training requires significant communication bandwidth for gradient exchange that limits the scalability of multi-node training, and requires expensive high-bandwidth network infrastructure. The situation gets even worse with distributed training on mobile devices (federated learning), which suffers from higher latency, lower throughput, and intermittent poor connections. In this paper, we find 99.9% of the gradient exchange in distributed SGD is redundant, and propose Deep Gradient Compression (DGC) to greatly reduce the communication bandwidth. To preserve accuracy during compression, DGC employs four methods: momentum correction, local gradient clipping, momentum factor masking, and warm-up training. We have applied Deep Gradient Compression to image classification, speech recognition, and language modeling with multiple datasets including Cifar10, ImageNet, Penn Treebank, and Librispeech Corpus. On these scenarios, Deep Gradient Compression achieves a gradient compression ratio from 270x to 600x without losing accuracy, cutting the gradient size of ResNet-50 from 97MB to 0.35MB, and for DeepSpeech from 488MB to 0.74MB. Deep gradient compression enables large-scale distributed training on inexpensive commodity 1Gbps Ethernet and facilitates distributed training on mobile."
    },
    {
        "paperId": "977560251c2bd4c28a6c7c707c29f4091c5e6247",
        "url": "https://www.semanticscholar.org/paper/977560251c2bd4c28a6c7c707c29f4091c5e6247",
        "title": "Lossy Image Compression with Compressive Autoencoders",
        "venue": "International Conference on Learning Representations",
        "year": 2017,
        "citationCount": 1110,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1703.00395, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2073063",
                "name": "Lucas Theis"
            },
            {
                "authorId": "46810836",
                "name": "Wenzhe Shi"
            },
            {
                "authorId": "2067984410",
                "name": "A. Cunningham"
            },
            {
                "authorId": "3108066",
                "name": "Ferenc Huszr"
            }
        ],
        "abstract": "We propose a new approach to the problem of optimizing autoencoders for lossy image compression. New media formats, changing hardware technology, as well as diverse requirements and content types create a need for compression algorithms which are more flexible than existing codecs. Autoencoders have the potential to address this need, but are difficult to optimize directly due to the inherent non-differentiabilty of the compression loss. We here show that minimal changes to the loss are sufficient to train deep autoencoders competitive with JPEG 2000 and outperforming recently proposed approaches based on RNNs. Our network is furthermore computationally efficient thanks to a sub-pixel architecture, which makes it suitable for high-resolution images. This is in contrast to previous work on autoencoders for compression using coarser approximations, shallower architectures, computationally expensive methods, or focusing on small images."
    },
    {
        "paperId": "9a089c56eec68df722b2a5a52727143aacdc2532",
        "url": "https://www.semanticscholar.org/paper/9a089c56eec68df722b2a5a52727143aacdc2532",
        "title": "Mitigating adversarial effects through randomization",
        "venue": "International Conference on Learning Representations",
        "year": 2017,
        "citationCount": 1158,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1711.01991, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3011497",
                "name": "Cihang Xie"
            },
            {
                "authorId": null,
                "name": "Jianyu Wang"
            },
            {
                "authorId": "2852303",
                "name": "Zhishuai Zhang"
            },
            {
                "authorId": "145888238",
                "name": "Zhou Ren"
            },
            {
                "authorId": "145081362",
                "name": "A. Yuille"
            }
        ],
        "abstract": "Convolutional neural networks have demonstrated high accuracy on various tasks in recent years. However, they are extremely vulnerable to adversarial examples. For example, imperceptible perturbations added to clean images can cause convolutional neural networks to fail. In this paper, we propose to utilize randomization at inference time to mitigate adversarial effects. Specifically, we use two randomization operations: random resizing, which resizes the input images to a random size, and random padding, which pads zeros around the input images in a random manner. Extensive experiments demonstrate that the proposed randomization method is very effective at defending against both single-step and iterative attacks. Our method provides the following advantages: 1) no additional training or fine-tuning, 2) very few additional computations, 3) compatible with other adversarial defense methods. By combining the proposed randomization method with an adversarially trained model, it achieves a normalized score of 0.924 (ranked No.2 among 107 defense teams) in the NIPS 2017 adversarial examples defense challenge, which is far better than using adversarial training alone with a normalized score of 0.773 (ranked No.56). The code is public available at this https URL."
    },
    {
        "paperId": "9a700c7a7e7468e436f00c34551fbe3e0f70e42f",
        "url": "https://www.semanticscholar.org/paper/9a700c7a7e7468e436f00c34551fbe3e0f70e42f",
        "title": "Towards Principled Methods for Training Generative Adversarial Networks",
        "venue": "International Conference on Learning Representations",
        "year": 2017,
        "citationCount": 2213,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1701.04862, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2877311",
                "name": "Martn Arjovsky"
            },
            {
                "authorId": "52184096",
                "name": "L. Bottou"
            }
        ],
        "abstract": "The goal of this paper is not to introduce a single algorithm or method, but to make theoretical steps towards fully understanding the training dynamics of generative adversarial networks. In order to substantiate our theoretical analysis, we perform targeted experiments to verify our assumptions, illustrate our claims, and quantify the phenomena. This paper is divided into three sections. The first section introduces the problem at hand. The second section is dedicated to studying and proving rigorously the problems including instability and saturation that arize when training generative adversarial networks. The third section examines a practical and theoretically grounded direction towards solving these problems, while introducing new tools to study them."
    },
    {
        "paperId": "9ba0186ed40656329c421f55ada7313293e13f17",
        "url": "https://www.semanticscholar.org/paper/9ba0186ed40656329c421f55ada7313293e13f17",
        "title": "Diffusion Convolutional Recurrent Neural Network: Data-Driven Traffic Forecasting",
        "venue": "International Conference on Learning Representations",
        "year": 2017,
        "citationCount": 3756,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "1720837956",
                "name": "Yaguang Li"
            },
            {
                "authorId": "2023052",
                "name": "Rose Yu"
            },
            {
                "authorId": "1773086",
                "name": "C. Shahabi"
            },
            {
                "authorId": "47909587",
                "name": "Yan Liu"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "a181fb5a42ad8fe2cc27b5542fa40384e9a8d72c",
        "url": "https://www.semanticscholar.org/paper/a181fb5a42ad8fe2cc27b5542fa40384e9a8d72c",
        "title": "Deep Variational Information Bottleneck",
        "venue": "International Conference on Learning Representations",
        "year": 2017,
        "citationCount": 1965,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1612.00410, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "122113652",
                "name": "Alexander A. Alemi"
            },
            {
                "authorId": "2057616290",
                "name": "Ian Fischer"
            },
            {
                "authorId": "2403637",
                "name": "Joshua V. Dillon"
            }
        ],
        "abstract": "We present a variational approximation to the information bottleneck of Tishby et al. (1999). This variational approach allows us to parameterize the information bottleneck model using a neural network and leverage the reparameterization trick for efficient training. We call this method \"Deep Variational Information Bottleneck\", or Deep VIB. We show that models trained with the VIB objective outperform those that are trained with other forms of regularization, in terms of generalization performance and robustness to adversarial attack."
    },
    {
        "paperId": "b134d0911e2e13ac169ffa5f478a39e6ef77869a",
        "url": "https://www.semanticscholar.org/paper/b134d0911e2e13ac169ffa5f478a39e6ef77869a",
        "title": "Snapshot Ensembles: Train 1, get M for free",
        "venue": "International Conference on Learning Representations",
        "year": 2017,
        "citationCount": 1030,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1704.00109, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "143983679",
                "name": "Gao Huang"
            },
            {
                "authorId": "1527103472",
                "name": "Yixuan Li"
            },
            {
                "authorId": "10804137",
                "name": "Geoff Pleiss"
            },
            {
                "authorId": "2109168016",
                "name": "Zhuang Liu"
            },
            {
                "authorId": "1706504",
                "name": "J. Hopcroft"
            },
            {
                "authorId": "7446832",
                "name": "Kilian Q. Weinberger"
            }
        ],
        "abstract": "Ensembles of neural networks are known to be much more robust and accurate than individual networks. However, training multiple deep networks for model averaging is computationally expensive. In this paper, we propose a method to obtain the seemingly contradictory goal of ensembling multiple neural networks at no additional training cost. We achieve this goal by training a single neural network, converging to several local minima along its optimization path and saving the model parameters. To obtain repeated rapid convergence, we leverage recent work on cyclic learning rate schedules. The resulting technique, which we refer to as Snapshot Ensembling, is simple, yet surprisingly effective. We show in a series of experiments that our approach is compatible with diverse network architectures and learning tasks. It consistently yields lower error rates than state-of-the-art single models at no additional training cost, and compares favorably with traditional network ensembles. On CIFAR-10 and CIFAR-100 our DenseNet Snapshot Ensembles obtain error rates of 3.4% and 17.4% respectively."
    },
    {
        "paperId": "c342c71cb23199f112d0bc644fcce56a7306bf94",
        "url": "https://www.semanticscholar.org/paper/c342c71cb23199f112d0bc644fcce56a7306bf94",
        "title": "Active Learning for Convolutional Neural Networks: A Core-Set Approach",
        "venue": "International Conference on Learning Representations",
        "year": 2017,
        "citationCount": 2282,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "3114252",
                "name": "Ozan Sener"
            },
            {
                "authorId": "1702137",
                "name": "S. Savarese"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "d07284a6811f1b2745d91bdb06b040b57f226882",
        "url": "https://www.semanticscholar.org/paper/d07284a6811f1b2745d91bdb06b040b57f226882",
        "title": "Decoupled Weight Decay Regularization",
        "venue": "International Conference on Learning Representations",
        "year": 2017,
        "citationCount": 29140,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "1678656",
                "name": "I. Loshchilov"
            },
            {
                "authorId": "144661829",
                "name": "F. Hutter"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "df1769afbbf3904877629dd7e785f195361ec531",
        "url": "https://www.semanticscholar.org/paper/df1769afbbf3904877629dd7e785f195361ec531",
        "title": "Lifelong Learning with Dynamically Expandable Networks",
        "venue": "International Conference on Learning Representations",
        "year": 2017,
        "citationCount": 1342,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1708.01547, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "13563486",
                "name": "Jaehong Yoon"
            },
            {
                "authorId": "1720494",
                "name": "Eunho Yang"
            },
            {
                "authorId": "2045514606",
                "name": "Jeongtae Lee"
            },
            {
                "authorId": "35788904",
                "name": "Sung Ju Hwang"
            }
        ],
        "abstract": "We propose a novel deep network architecture for lifelong learning which we refer to as Dynamically Expandable Network (DEN), that can dynamically decide its network capacity as it trains on a sequence of tasks, to learn a compact overlapping knowledge sharing structure among tasks. DEN is efficiently trained in an online manner by performing selective retraining, dynamically expands network capacity upon arrival of each task with only the necessary number of units, and effectively prevents semantic drift by splitting/duplicating units and timestamping them. We validate DEN on multiple public datasets under lifelong learning scenarios, on which it not only significantly outperforms existing lifelong learning methods for deep networks, but also achieves the same level of performance as the batch counterparts with substantially fewer number of parameters. Further, the obtained network fine-tuned on all tasks obtained significantly better performance over the batch models, which shows that it can be used to estimate the optimal network structure even when all tasks are available in the first place."
    },
    {
        "paperId": "e3d772986d176057aca2f5e3eb783da53b559134",
        "url": "https://www.semanticscholar.org/paper/e3d772986d176057aca2f5e3eb783da53b559134",
        "title": "Unsupervised Machine Translation Using Monolingual Corpora Only",
        "venue": "International Conference on Learning Representations",
        "year": 2017,
        "citationCount": 1127,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1711.00043, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1830914",
                "name": "Guillaume Lample"
            },
            {
                "authorId": "8905591",
                "name": "Ludovic Denoyer"
            },
            {
                "authorId": "1706809",
                "name": "Marc'Aurelio Ranzato"
            }
        ],
        "abstract": "Machine translation has recently achieved impressive performance thanks to recent advances in deep learning and the availability of large-scale parallel corpora. There have been numerous attempts to extend these successes to low-resource language pairs, yet requiring tens of thousands of parallel sentences. In this work, we take this research direction to the extreme and investigate whether it is possible to learn to translate even without any parallel data. We propose a model that takes sentences from monolingual corpora in two different languages and maps them into the same latent space. By learning to reconstruct in both languages from this shared feature space, the model effectively learns to translate without using any labeled data. We demonstrate our model on two widely used datasets and two language pairs, reporting BLEU scores of 32.8 and 15.1 on the Multi30k and WMT English-French datasets, without using even a single parallel sentence at training time."
    },
    {
        "paperId": "e7fd6848cb29ca221a7e17d823e06fb566f1f135",
        "url": "https://www.semanticscholar.org/paper/e7fd6848cb29ca221a7e17d823e06fb566f1f135",
        "title": "Mixed Precision Training",
        "venue": "International Conference on Learning Representations",
        "year": 2017,
        "citationCount": 2121,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1710.03740, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1802359",
                "name": "P. Micikevicius"
            },
            {
                "authorId": "46617804",
                "name": "Sharan Narang"
            },
            {
                "authorId": "67038137",
                "name": "Jonah Alben"
            },
            {
                "authorId": "2040049",
                "name": "G. Diamos"
            },
            {
                "authorId": "152585800",
                "name": "Erich Elsen"
            },
            {
                "authorId": "2082313059",
                "name": "David Garca"
            },
            {
                "authorId": "31963005",
                "name": "Boris Ginsburg"
            },
            {
                "authorId": "122523478",
                "name": "Michael Houston"
            },
            {
                "authorId": "2787022",
                "name": "Oleksii Kuchaiev"
            },
            {
                "authorId": "145595812",
                "name": "Ganesh Venkatesh"
            },
            {
                "authorId": "1491232360",
                "name": "Hao Wu"
            }
        ],
        "abstract": "Deep neural networks have enabled progress in a wide variety of applications. Growing the size of the neural network typically results in improved accuracy. As model sizes grow, the memory and compute requirements for training these models also increases. We introduce a technique to train deep neural networks using half precision floating point numbers. In our technique, weights, activations and gradients are stored in IEEE half-precision format. Half-precision floating numbers have limited numerical range compared to single-precision numbers. We propose two techniques to handle this loss of information. Firstly, we recommend maintaining a single-precision copy of the weights that accumulates the gradients after each optimizer step. This single-precision copy is rounded to half-precision format during training. Secondly, we propose scaling the loss appropriately to handle the loss of information with half-precision gradients. We demonstrate that this approach works for a wide variety of models including convolution neural networks, recurrent neural networks and generative adversarial networks. This technique works for large scale models with more than 100 million parameters trained on large datasets. Using this approach, we can reduce the memory consumption of deep learning models by nearly 2x. In future processors, we can also expect a significant computation speedup using half-precision hardware units."
    },
    {
        "paperId": "ed46493d568030b42f0154d9e5bf39bbd07962b3",
        "url": "https://www.semanticscholar.org/paper/ed46493d568030b42f0154d9e5bf39bbd07962b3",
        "title": "Learning Differentially Private Recurrent Language Models",
        "venue": "International Conference on Learning Representations",
        "year": 2017,
        "citationCount": 1406,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "145057514",
                "name": "H. B. McMahan"
            },
            {
                "authorId": "1878835",
                "name": "Daniel Ramage"
            },
            {
                "authorId": "35210462",
                "name": "Kunal Talwar"
            },
            {
                "authorId": "2152832173",
                "name": "Li Zhang"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "fe9cd683c3b8ebdfd8efd1109a857cdbf9edc364",
        "url": "https://www.semanticscholar.org/paper/fe9cd683c3b8ebdfd8efd1109a857cdbf9edc364",
        "title": "Data Augmentation Generative Adversarial Networks",
        "venue": "International Conference on Learning Representations",
        "year": 2017,
        "citationCount": 1139,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1711.04340, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2063959275",
                "name": "Antreas Antoniou"
            },
            {
                "authorId": "1728216",
                "name": "A. Storkey"
            },
            {
                "authorId": "144632352",
                "name": "Harrison Edwards"
            }
        ],
        "abstract": "Effective training of neural networks requires much data. In the low-data regime, parameters are underdetermined, and learnt networks generalise poorly. Data Augmentation alleviates this by using existing data more effectively. However standard data augmentation produces only limited plausible alternative data. Given there is potential to generate a much broader set of augmentations, we design and train a generative model to do data augmentation. The model, based on image conditional Generative Adversarial Networks, takes data from a source domain and learns to take any data item and generalise it to generate other within-class data items. As this generative process does not depend on the classes themselves, it can be applied to novel unseen classes of data. We show that a Data Augmentation Generative Adversarial Network (DAGAN) augments standard vanilla classifiers well. We also show a DAGAN can enhance few-shot learning systems such as Matching Networks. We demonstrate these approaches on Omniglot, on EMNIST having learnt the DAGAN on Omniglot, and VGG-Face data. In our experiments we can see over 13% increase in accuracy in the low-data regime experiments in Omniglot (from 69% to 82%), EMNIST (73.9% to 76%) and VGG-Face (4.5% to 12%); in Matching Networks for Omniglot we observe an increase of 0.5% (from 96.9% to 97.4%) and an increase of 1.8% in EMNIST (from 59.5% to 61.3%)."
    },
    {
        "paperId": "04f739a0c29b75877243731aeead512bf0ed1dff",
        "url": "https://www.semanticscholar.org/paper/04f739a0c29b75877243731aeead512bf0ed1dff",
        "title": "Meta-Learning with Latent Embedding Optimization",
        "venue": "International Conference on Learning Representations",
        "year": 2018,
        "citationCount": 1464,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1807.05960, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2228824",
                "name": "Andrei A. Rusu"
            },
            {
                "authorId": "143668237",
                "name": "Dushyant Rao"
            },
            {
                "authorId": "3407592",
                "name": "Jakub Sygnowski"
            },
            {
                "authorId": "1689108",
                "name": "O. Vinyals"
            },
            {
                "authorId": "1996134",
                "name": "Razvan Pascanu"
            },
            {
                "authorId": "2217144",
                "name": "Simon Osindero"
            },
            {
                "authorId": "2315504",
                "name": "R. Hadsell"
            }
        ],
        "abstract": "Gradient-based meta-learning techniques are both widely applicable and proficient at solving challenging few-shot learning and fast adaptation problems. However, they have practical difficulties when operating on high-dimensional parameter spaces in extreme low-data regimes. We show that it is possible to bypass these limitations by learning a data-dependent latent generative representation of model parameters, and performing gradient-based meta-learning in this low-dimensional latent space. The resulting approach, latent embedding optimization (LEO), decouples the gradient-based adaptation procedure from the underlying high-dimensional space of model parameters. Our evaluation shows that LEO can achieve state-of-the-art performance on the competitive miniImageNet and tieredImageNet few-shot classification tasks. Further analysis indicates LEO is able to capture uncertainty in the data, and can perform adaptation more effectively by optimizing in latent space."
    },
    {
        "paperId": "0f50b7483f1b200ebf88c4dd7698de986399a0f3",
        "url": "https://www.semanticscholar.org/paper/0f50b7483f1b200ebf88c4dd7698de986399a0f3",
        "title": "ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness",
        "venue": "International Conference on Learning Representations",
        "year": 2018,
        "citationCount": 2930,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1811.12231, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1949747",
                "name": "Robert Geirhos"
            },
            {
                "authorId": "52096618",
                "name": "Patricia Rubisch"
            },
            {
                "authorId": "40899528",
                "name": "Claudio Michaelis"
            },
            {
                "authorId": "1731199",
                "name": "M. Bethge"
            },
            {
                "authorId": "1924112",
                "name": "Felix Wichmann"
            },
            {
                "authorId": "40634590",
                "name": "Wieland Brendel"
            }
        ],
        "abstract": "Convolutional Neural Networks (CNNs) are commonly thought to recognise objects by learning increasingly complex representations of object shapes. Some recent studies suggest a more important role of image textures. We here put these conflicting hypotheses to a quantitative test by evaluating CNNs and human observers on images with a texture-shape cue conflict. We show that ImageNet-trained CNNs are strongly biased towards recognising textures rather than shapes, which is in stark contrast to human behavioural evidence and reveals fundamentally different classification strategies. We then demonstrate that the same standard architecture (ResNet-50) that learns a texture-based representation on ImageNet is able to learn a shape-based representation instead when trained on \"Stylized-ImageNet\", a stylized version of ImageNet. This provides a much better fit for human behavioural performance in our well-controlled psychophysical lab setting (nine experiments totalling 48,560 psychophysical trials across 97 observers) and comes with a number of unexpected emergent benefits such as improved object detection performance and previously unseen robustness towards a wide range of image distortions, highlighting advantages of a shape-based representation."
    },
    {
        "paperId": "1b9c6022598085dd892f360122c0fa4c630b3f18",
        "url": "https://www.semanticscholar.org/paper/1b9c6022598085dd892f360122c0fa4c630b3f18",
        "title": "Robustness May Be at Odds with Accuracy",
        "venue": "International Conference on Learning Representations",
        "year": 2018,
        "citationCount": 1873,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1805.12152, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2754804",
                "name": "Dimitris Tsipras"
            },
            {
                "authorId": "2852106",
                "name": "Shibani Santurkar"
            },
            {
                "authorId": "39468283",
                "name": "Logan Engstrom"
            },
            {
                "authorId": "2065454962",
                "name": "Alexander Turner"
            },
            {
                "authorId": "143826246",
                "name": "A. Madry"
            }
        ],
        "abstract": "We show that there may exist an inherent tension between the goal of adversarial robustness and that of standard generalization. Specifically, training robust models may not only be more resource-consuming, but also lead to a reduction of standard accuracy. We demonstrate that this trade-off between the standard accuracy of a model and its robustness to adversarial perturbations provably exists in a fairly simple and natural setting. These findings also corroborate a similar phenomenon observed empirically in more complex settings. Further, we argue that this phenomenon is a consequence of robust classifiers learning fundamentally different feature representations than standard classifiers. These differences, in particular, seem to result in unexpected benefits: the representations learned by robust models tend to align better with salient data characteristics and human perception."
    },
    {
        "paperId": "208cd4b25768f0096fb2e80e7690473da0e2a563",
        "url": "https://www.semanticscholar.org/paper/208cd4b25768f0096fb2e80e7690473da0e2a563",
        "title": "Meta-learning with differentiable closed-form solvers",
        "venue": "International Conference on Learning Representations",
        "year": 2018,
        "citationCount": 1014,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1805.08136, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2271057",
                "name": "Luca Bertinetto"
            },
            {
                "authorId": "143848064",
                "name": "Joo F. Henriques"
            },
            {
                "authorId": "143635540",
                "name": "Philip H. S. Torr"
            },
            {
                "authorId": "1687524",
                "name": "A. Vedaldi"
            }
        ],
        "abstract": "Adapting deep networks to new concepts from a few examples is challenging, due to the high computational requirements of standard fine-tuning procedures. Most work on few-shot learning has thus focused on simple learning techniques for adaptation, such as nearest neighbours or gradient descent. Nonetheless, the machine learning literature contains a wealth of methods that learn non-deep models very efficiently. In this paper, we propose to use these fast convergent methods as the main adaptation mechanism for few-shot learning. The main idea is to teach a deep network to use standard machine learning tools, such as ridge regression, as part of its own internal model, enabling it to quickly adapt to novel data. This requires back-propagating errors through the solver steps. While normally the cost of the matrix operations involved in such a process would be significant, by using the Woodbury identity we can make the small number of examples work to our advantage. We propose both closed-form and iterative solvers, based on ridge regression and logistic regression components. Our methods constitute a simple and novel approach to the problem of few-shot learning and achieve performance competitive with or superior to the state of the art on three benchmarks."
    },
    {
        "paperId": "21937ecd9d66567184b83eca3d3e09eb4e6fbd60",
        "url": "https://www.semanticscholar.org/paper/21937ecd9d66567184b83eca3d3e09eb4e6fbd60",
        "title": "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks",
        "venue": "International Conference on Learning Representations",
        "year": 2018,
        "citationCount": 3894,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1803.03635, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "25581960",
                "name": "Jonathan Frankle"
            },
            {
                "authorId": "1701041",
                "name": "Michael Carbin"
            }
        ],
        "abstract": "Neural network pruning techniques can reduce the parameter counts of trained networks by over 90%, decreasing storage requirements and improving computational performance of inference without compromising accuracy. However, contemporary experience is that the sparse architectures produced by pruning are difficult to train from the start, which would similarly improve training performance. \nWe find that a standard pruning technique naturally uncovers subnetworks whose initializations made them capable of training effectively. Based on these results, we articulate the \"lottery ticket hypothesis:\" dense, randomly-initialized, feed-forward networks contain subnetworks (\"winning tickets\") that - when trained in isolation - reach test accuracy comparable to the original network in a similar number of iterations. The winning tickets we find have won the initialization lottery: their connections have initial weights that make training particularly effective. \nWe present an algorithm to identify winning tickets and a series of experiments that support the lottery ticket hypothesis and the importance of these fortuitous initializations. We consistently find winning tickets that are less than 10-20% of the size of several fully-connected and convolutional feed-forward architectures for MNIST and CIFAR10. Above this size, the winning tickets that we find learn faster than the original network and reach higher test accuracy."
    },
    {
        "paperId": "22aab110058ebbd198edb1f1e7b4f69fb13c0613",
        "url": "https://www.semanticscholar.org/paper/22aab110058ebbd198edb1f1e7b4f69fb13c0613",
        "title": "Large Scale GAN Training for High Fidelity Natural Image Synthesis",
        "venue": "International Conference on Learning Representations",
        "year": 2018,
        "citationCount": 5877,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1809.11096, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2065040247",
                "name": "Andrew Brock"
            },
            {
                "authorId": "7408951",
                "name": "Jeff Donahue"
            },
            {
                "authorId": "34838386",
                "name": "K. Simonyan"
            }
        ],
        "abstract": "Despite recent progress in generative image modeling, successfully generating high-resolution, diverse samples from complex datasets such as ImageNet remains an elusive goal. To this end, we train Generative Adversarial Networks at the largest scale yet attempted, and study the instabilities specific to such scale. We find that applying orthogonal regularization to the generator renders it amenable to a simple \"truncation trick,\" allowing fine control over the trade-off between sample fidelity and variety by reducing the variance of the Generator's input. Our modifications lead to models which set the new state of the art in class-conditional image synthesis. When trained on ImageNet at 128x128 resolution, our models (BigGANs) achieve an Inception Score (IS) of 166.5 and Frechet Inception Distance (FID) of 7.4, improving over the previous best IS of 52.52 and FID of 18.6."
    },
    {
        "paperId": "2503dff90685857ce7295e37d0045e2eef41c8b8",
        "url": "https://www.semanticscholar.org/paper/2503dff90685857ce7295e37d0045e2eef41c8b8",
        "title": "FastGCN: Fast Learning with Graph Convolutional Networks via Importance Sampling",
        "venue": "International Conference on Learning Representations",
        "year": 2018,
        "citationCount": 1649,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1801.10247, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": null,
                "name": "Jie Chen"
            },
            {
                "authorId": "40411766",
                "name": "Tengfei Ma"
            },
            {
                "authorId": "145781464",
                "name": "Cao Xiao"
            }
        ],
        "abstract": "The graph convolutional networks (GCN) recently proposed by Kipf and Welling are an effective graph model for semi-supervised learning. This model, however, was originally designed to be learned with the presence of both training and test data. Moreover, the recursive neighborhood expansion across layers poses time and memory challenges for training with large, dense graphs. To relax the requirement of simultaneous availability of test data, we interpret graph convolutions as integral transforms of embedding functions under probability measures. Such an interpretation allows for the use of Monte Carlo approaches to consistently estimate the integrals, which in turn leads to a batched training scheme as we propose in this work---FastGCN. Enhanced with importance sampling, FastGCN not only is efficient for training but also generalizes well for inference. We show a comprehensive set of experiments to demonstrate its effectiveness compared with GCN and related models. In particular, training is orders of magnitude more efficient while predictions remain comparably accurate."
    },
    {
        "paperId": "2d8c97db4bae00ff243d122b957091a236a697a7",
        "url": "https://www.semanticscholar.org/paper/2d8c97db4bae00ff243d122b957091a236a697a7",
        "title": "Deep Anomaly Detection with Outlier Exposure",
        "venue": "International Conference on Learning Representations",
        "year": 2018,
        "citationCount": 1640,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1812.04606, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3422872",
                "name": "Dan Hendrycks"
            },
            {
                "authorId": "16787428",
                "name": "Mantas Mazeika"
            },
            {
                "authorId": "144299726",
                "name": "Thomas G. Dietterich"
            }
        ],
        "abstract": "It is important to detect anomalous inputs when deploying machine learning systems. The use of larger and more complex inputs in deep learning magnifies the difficulty of distinguishing between anomalous and in-distribution examples. At the same time, diverse image and text data are available in enormous quantities. We propose leveraging these data to improve deep anomaly detection by training anomaly detectors against an auxiliary dataset of outliers, an approach we call Outlier Exposure (OE). This enables anomaly detectors to generalize and detect unseen anomalies. In extensive experiments on natural language processing and small- and large-scale vision tasks, we find that Outlier Exposure significantly improves detection performance. We also observe that cutting-edge generative models trained on CIFAR-10 may assign higher likelihoods to SVHN images than to CIFAR-10 images; we use OE to mitigate this issue. We also analyze the flexibility and robustness of Outlier Exposure, and identify characteristics of the auxiliary dataset that improve performance."
    },
    {
        "paperId": "4a1004ecd34118116344633c7cdcc34493c423ee",
        "url": "https://www.semanticscholar.org/paper/4a1004ecd34118116344633c7cdcc34493c423ee",
        "title": "Rethinking the Value of Network Pruning",
        "venue": "International Conference on Learning Representations",
        "year": 2018,
        "citationCount": 1603,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1810.05270, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2109168016",
                "name": "Zhuang Liu"
            },
            {
                "authorId": "2984183",
                "name": "Mingjie Sun"
            },
            {
                "authorId": "1822702",
                "name": "Tinghui Zhou"
            },
            {
                "authorId": "143983679",
                "name": "Gao Huang"
            },
            {
                "authorId": "1753210",
                "name": "Trevor Darrell"
            }
        ],
        "abstract": "Network pruning is widely used for reducing the heavy inference cost of deep models in low-resource settings. A typical pruning algorithm is a three-stage pipeline, i.e., training (a large model), pruning and fine-tuning. During pruning, according to a certain criterion, redundant weights are pruned and important weights are kept to best preserve the accuracy. In this work, we make several surprising observations which contradict common beliefs. For all state-of-the-art structured pruning algorithms we examined, fine-tuning a pruned model only gives comparable or worse performance than training that model with randomly initialized weights. For pruning algorithms which assume a predefined target network architecture, one can get rid of the full pipeline and directly train the target network from scratch. Our observations are consistent for multiple network architectures, datasets, and tasks, which imply that: 1) training a large, over-parameterized model is often not necessary to obtain an efficient final model, 2) learned \"important\" weights of the large model are typically not useful for the small pruned model, 3) the pruned architecture itself, rather than a set of inherited \"important\" weights, is more crucial to the efficiency in the final model, which suggests that in some cases pruning can be useful as an architecture search paradigm. Our results suggest the need for more careful baseline evaluations in future research on structured pruning methods. We also compare with the \"Lottery Ticket Hypothesis\" (Frankle & Carbin 2019), and find that with optimal learning rate, the \"winning ticket\" initialization as used in Frankle & Carbin (2019) does not bring improvement over random initialization."
    },
    {
        "paperId": "4a954b3e72a61968ab235076bcc242aca3a05520",
        "url": "https://www.semanticscholar.org/paper/4a954b3e72a61968ab235076bcc242aca3a05520",
        "title": "Efficient Lifelong Learning with A-GEM",
        "venue": "International Conference on Learning Representations",
        "year": 2018,
        "citationCount": 1627,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1812.00420, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "22235380",
                "name": "Arslan Chaudhry"
            },
            {
                "authorId": "1706809",
                "name": "Marc'Aurelio Ranzato"
            },
            {
                "authorId": "34849128",
                "name": "Marcus Rohrbach"
            },
            {
                "authorId": "2066380705",
                "name": "Mohamed Elhoseiny"
            }
        ],
        "abstract": "In lifelong learning, the learner is presented with a sequence of tasks, incrementally building a data-driven prior which may be leveraged to speed up learning of a new task. In this work, we investigate the efficiency of current lifelong approaches, in terms of sample complexity, computational and memory cost. Towards this end, we first introduce a new and a more realistic evaluation protocol, whereby learners observe each example only once and hyper-parameter selection is done on a small and disjoint set of tasks, which is not used for the actual learning experience and evaluation. Second, we introduce a new metric measuring how quickly a learner acquires a new skill. Third, we propose an improved version of GEM (Lopez-Paz & Ranzato, 2017), dubbed Averaged GEM (A-GEM), which enjoys the same or even better performance as GEM, while being almost as computationally and memory efficient as EWC (Kirkpatrick et al., 2016) and other regularization-based methods. Finally, we show that all algorithms including A-GEM can learn even more quickly if they are provided with task descriptors specifying the classification tasks under consideration. Our experiments on several standard lifelong learning benchmarks demonstrate that A-GEM has the best trade-off between accuracy and efficiency."
    },
    {
        "paperId": "4cb3fd057949624aa4f0bbe7a6dcc8777ff04758",
        "url": "https://www.semanticscholar.org/paper/4cb3fd057949624aa4f0bbe7a6dcc8777ff04758",
        "title": "Exploration by Random Network Distillation",
        "venue": "International Conference on Learning Representations",
        "year": 2018,
        "citationCount": 1512,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1810.12894, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3080409",
                "name": "Yuri Burda"
            },
            {
                "authorId": "144632352",
                "name": "Harrison Edwards"
            },
            {
                "authorId": "1728216",
                "name": "A. Storkey"
            },
            {
                "authorId": "2067138712",
                "name": "Oleg Klimov"
            }
        ],
        "abstract": "We introduce an exploration bonus for deep reinforcement learning methods that is easy to implement and adds minimal overhead to the computation performed. The bonus is the error of a neural network predicting features of the observations given by a fixed randomly initialized neural network. We also introduce a method to flexibly combine intrinsic and extrinsic rewards. We find that the random network distillation (RND) bonus combined with this increased flexibility enables significant progress on several hard exploration Atari games. In particular we establish state of the art performance on Montezuma's Revenge, a game famously difficult for deep reinforcement learning methods. To the best of our knowledge, this is the first method that achieves better than average human performance on this game without using demonstrations or having access to the underlying state of the game, and occasionally completes the first level."
    },
    {
        "paperId": "5b01eaef54a653ba03ddd5a978690380fbc19bfc",
        "url": "https://www.semanticscholar.org/paper/5b01eaef54a653ba03ddd5a978690380fbc19bfc",
        "title": "Diversity is All You Need: Learning Skills without a Reward Function",
        "venue": "International Conference on Learning Representations",
        "year": 2018,
        "citationCount": 1197,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1802.06070, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "8140754",
                "name": "Benjamin Eysenbach"
            },
            {
                "authorId": "2129458064",
                "name": "Abhishek Gupta"
            },
            {
                "authorId": "46920727",
                "name": "Julian Ibarz"
            },
            {
                "authorId": "1736651",
                "name": "S. Levine"
            }
        ],
        "abstract": "Intelligent creatures can explore their environments and learn useful skills without supervision. In this paper, we propose DIAYN (\"Diversity is All You Need\"), a method for learning useful skills without a reward function. Our proposed method learns skills by maximizing an information theoretic objective using a maximum entropy policy. On a variety of simulated robotic tasks, we show that this simple objective results in the unsupervised emergence of diverse skills, such as walking and jumping. In a number of reinforcement learning benchmark environments, our method is able to learn a skill that solves the benchmark task despite never receiving the true task reward. In these environments, some of the learned skills correspond to solving the task, and each skill that solves the task does so in a distinct manner. Our results suggest that unsupervised discovery of skills can serve as an effective pretraining mechanism for overcoming challenges of exploration and data efficiency in reinforcement learning"
    },
    {
        "paperId": "603caed9430283db6c7f43169555c8d18e97a281",
        "url": "https://www.semanticscholar.org/paper/603caed9430283db6c7f43169555c8d18e97a281",
        "title": "Matrix capsules with EM routing",
        "venue": "International Conference on Learning Representations",
        "year": 2018,
        "citationCount": 1112,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "1695689",
                "name": "Geoffrey E. Hinton"
            },
            {
                "authorId": "143752292",
                "name": "S. Sabour"
            },
            {
                "authorId": "27737461",
                "name": "Nicholas Frosst"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "62ed9bf1d83c8db1f9cbf92ea2f57ea90ef683d9",
        "url": "https://www.semanticscholar.org/paper/62ed9bf1d83c8db1f9cbf92ea2f57ea90ef683d9",
        "title": "How Powerful are Graph Neural Networks?",
        "venue": "International Conference on Learning Representations",
        "year": 2018,
        "citationCount": 8961,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1810.00826, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3360632",
                "name": "Keyulu Xu"
            },
            {
                "authorId": "48594758",
                "name": "Weihua Hu"
            },
            {
                "authorId": "1702139",
                "name": "J. Leskovec"
            },
            {
                "authorId": "2594093",
                "name": "S. Jegelka"
            }
        ],
        "abstract": "Graph Neural Networks (GNNs) are an effective framework for representation learning of graphs. GNNs follow a neighborhood aggregation scheme, where the representation vector of a node is computed by recursively aggregating and transforming representation vectors of its neighboring nodes. Many GNN variants have been proposed and have achieved state-of-the-art results on both node and graph classification tasks. However, despite GNNs revolutionizing graph representation learning, there is limited understanding of their representational properties and limitations. Here, we present a theoretical framework for analyzing the expressive power of GNNs to capture different graph structures. Our results characterize the discriminative power of popular GNN variants, such as Graph Convolutional Networks and GraphSAGE, and show that they cannot learn to distinguish certain simple graph structures. We then develop a simple architecture that is provably the most expressive among the class of GNNs and is as powerful as the Weisfeiler-Lehman graph isomorphism test. We empirically validate our theoretical findings on a number of graph classification benchmarks, and demonstrate that our model achieves state-of-the-art performance."
    },
    {
        "paperId": "678c5b1771e7c15e86b454662dec8cd45d3d30bf",
        "url": "https://www.semanticscholar.org/paper/678c5b1771e7c15e86b454662dec8cd45d3d30bf",
        "title": "Variational image compression with a scale hyperprior",
        "venue": "International Conference on Learning Representations",
        "year": 2018,
        "citationCount": 2087,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1802.01436, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "144517934",
                "name": "J. Ball"
            },
            {
                "authorId": "3144223",
                "name": "David C. Minnen"
            },
            {
                "authorId": "2108498897",
                "name": "Saurabh Singh"
            },
            {
                "authorId": "2110796151",
                "name": "S. Hwang"
            },
            {
                "authorId": "49678398",
                "name": "Nick Johnston"
            }
        ],
        "abstract": "We describe an end-to-end trainable model for image compression based on variational autoencoders. The model incorporates a hyperprior to effectively capture spatial dependencies in the latent representation. This hyperprior relates to side information, a concept universal to virtually all modern image codecs, but largely unexplored in image compression using artificial neural networks (ANNs). Unlike existing autoencoder compression methods, our model trains a complex prior jointly with the underlying autoencoder. We demonstrate that this model leads to state-of-the-art image compression when measuring visual quality using the popular MS-SSIM index, and yields rate-distortion performance surpassing published ANN-based methods when evaluated using a more traditional metric based on squared error (PSNR). Furthermore, we provide a qualitative comparison of models trained for different distortion metrics."
    },
    {
        "paperId": "7cfa76a82be96c74b2eff514265b7fd271a179cd",
        "url": "https://www.semanticscholar.org/paper/7cfa76a82be96c74b2eff514265b7fd271a179cd",
        "title": "Local SGD Converges Fast and Communicates Little",
        "venue": "International Conference on Learning Representations",
        "year": 2018,
        "citationCount": 1178,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1805.09767, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2127057",
                "name": "Sebastian U. Stich"
            }
        ],
        "abstract": "Mini-batch stochastic gradient descent (SGD) is state of the art in large scale distributed training. The scheme can reach a linear speedup with respect to the number of workers, but this is rarely seen in practice as the scheme often suffers from large network delays and bandwidth limits. To overcome this communication bottleneck recent works propose to reduce the communication frequency. An algorithm of this type is local SGD that runs SGD independently in parallel on different workers and averages the sequences only once in a while. \nThis scheme shows promising results in practice, but eluded thorough theoretical analysis. We prove concise convergence rates for local SGD on convex problems and show that it converges at the same rate as mini-batch SGD in terms of number of evaluated gradients, that is, the scheme achieves linear speedup in the number of workers and mini-batch size. The number of communication rounds can be reduced up to a factor of T^{1/2}---where T denotes the number of total steps---compared to mini-batch SGD. This also holds for asynchronous implementations. Local SGD can also be used for large scale training of deep learning models. \nThe results shown here aim serving as a guideline to further explore the theoretical and practical aspects of local SGD in these applications."
    },
    {
        "paperId": "84de7d27e2f6160f634a483e8548c499a2cda7fa",
        "url": "https://www.semanticscholar.org/paper/84de7d27e2f6160f634a483e8548c499a2cda7fa",
        "title": "Spectral Normalization for Generative Adversarial Networks",
        "venue": "International Conference on Learning Representations",
        "year": 2018,
        "citationCount": 4744,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1802.05957, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3213400",
                "name": "Takeru Miyato"
            },
            {
                "authorId": "2056971870",
                "name": "Toshiki Kataoka"
            },
            {
                "authorId": "2877296",
                "name": "Masanori Koyama"
            },
            {
                "authorId": "51462146",
                "name": "Yuichi Yoshida"
            }
        ],
        "abstract": "One of the challenges in the study of generative adversarial networks is the instability of its training. In this paper, we propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator. Our new normalization technique is computationally light and easy to incorporate into existing implementations. We tested the efficacy of spectral normalization on CIFAR10, STL-10, and ILSVRC2012 dataset, and we experimentally confirmed that spectrally normalized GANs (SN-GANs) is capable of generating images of better or equal quality relative to the previous training stabilization techniques."
    },
    {
        "paperId": "8c1b00128e74f1cd92aede3959690615695d5101",
        "url": "https://www.semanticscholar.org/paper/8c1b00128e74f1cd92aede3959690615695d5101",
        "title": "QANet: Combining Local Convolution with Global Self-Attention for Reading Comprehension",
        "venue": "International Conference on Learning Representations",
        "year": 2018,
        "citationCount": 1123,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1804.09541, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "40625240",
                "name": "Adams Wei Yu"
            },
            {
                "authorId": "35363891",
                "name": "David Dohan"
            },
            {
                "authorId": "1707242",
                "name": "Minh-Thang Luong"
            },
            {
                "authorId": "2114012077",
                "name": "Rui Zhao"
            },
            {
                "authorId": "2118440152",
                "name": "Kai Chen"
            },
            {
                "authorId": "144739074",
                "name": "Mohammad Norouzi"
            },
            {
                "authorId": "2827616",
                "name": "Quoc V. Le"
            }
        ],
        "abstract": "Current end-to-end machine reading and question answering (Q\\&A) models are primarily based on recurrent neural networks (RNNs) with attention. Despite their success, these models are often slow for both training and inference due to the sequential nature of RNNs. We propose a new Q\\&A architecture called QANet, which does not require recurrent networks: Its encoder consists exclusively of convolution and self-attention, where convolution models local interactions and self-attention models global interactions. On the SQuAD dataset, our model is 3x to 13x faster in training and 4x to 9x faster in inference, while achieving equivalent accuracy to recurrent models. The speed-up gain allows us to train the model with much more data. We hence combine our model with data generated by backtranslation from a neural machine translation model. On the SQuAD dataset, our single model, trained with augmented data, achieves 84.6 F1 score on the test set, which is significantly better than the best published F1 score of 81.8."
    },
    {
        "paperId": "8f096071a09701012c9c279aee2a88143a295935",
        "url": "https://www.semanticscholar.org/paper/8f096071a09701012c9c279aee2a88143a295935",
        "title": "RotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space",
        "venue": "International Conference on Learning Representations",
        "year": 2018,
        "citationCount": 2493,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1902.10197, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "48064856",
                "name": "Zhiqing Sun"
            },
            {
                "authorId": "123580511",
                "name": "Zhihong Deng"
            },
            {
                "authorId": "143619007",
                "name": "Jian-Yun Nie"
            },
            {
                "authorId": "152226504",
                "name": "Jian Tang"
            }
        ],
        "abstract": "We study the problem of learning representations of entities and relations in knowledge graphs for predicting missing links. The success of such a task heavily relies on the ability of modeling and inferring the patterns of (or between) the relations. In this paper, we present a new approach for knowledge graph embedding called RotatE, which is able to model and infer various relation patterns including: symmetry/antisymmetry, inversion, and composition. Specifically, the RotatE model defines each relation as a rotation from the source entity to the target entity in the complex vector space. In addition, we propose a novel self-adversarial negative sampling technique for efficiently and effectively training the RotatE model. Experimental results on multiple benchmark knowledge graphs show that the proposed RotatE model is not only scalable, but also able to infer and model various relation patterns and significantly outperform existing state-of-the-art models for link prediction."
    },
    {
        "paperId": "967a21a111757d6af7f7a25ca7ea2bdf6d505098",
        "url": "https://www.semanticscholar.org/paper/967a21a111757d6af7f7a25ca7ea2bdf6d505098",
        "title": "Deep Graph Infomax",
        "venue": "International Conference on Learning Representations",
        "year": 2018,
        "citationCount": 2762,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1809.10341, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3444569",
                "name": "Petar Velickovic"
            },
            {
                "authorId": "26958176",
                "name": "W. Fedus"
            },
            {
                "authorId": "49437682",
                "name": "William L. Hamilton"
            },
            {
                "authorId": "2392269716",
                "name": "Pietro Li"
            },
            {
                "authorId": "1751762",
                "name": "Yoshua Bengio"
            },
            {
                "authorId": "40482726",
                "name": "R. Devon Hjelm"
            }
        ],
        "abstract": "We present Deep Graph Infomax (DGI), a general approach for learning node representations within graph-structured data in an unsupervised manner. DGI relies on maximizing mutual information between patch representations and corresponding high-level summaries of graphs---both derived using established graph convolutional network architectures. The learnt patch representations summarize subgraphs centered around nodes of interest, and can thus be reused for downstream node-wise learning tasks. In contrast to most prior approaches to unsupervised learning with GCNs, DGI does not rely on random walk objectives, and is readily applicable to both transductive and inductive learning setups. We demonstrate competitive performance on a variety of node classification benchmarks, which at times even exceeds the performance of supervised learning."
    },
    {
        "paperId": "9723066a5587e6267d8abfd7feefd0637a5a211c",
        "url": "https://www.semanticscholar.org/paper/9723066a5587e6267d8abfd7feefd0637a5a211c",
        "title": "Demystifying MMD GANs",
        "venue": "International Conference on Learning Representations",
        "year": 2018,
        "citationCount": 1787,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1801.01401, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "9961753",
                "name": "Mikolaj Binkowski"
            },
            {
                "authorId": "36326783",
                "name": "Danica J. Sutherland"
            },
            {
                "authorId": "1975860",
                "name": "M. Arbel"
            },
            {
                "authorId": "1708497",
                "name": "A. Gretton"
            }
        ],
        "abstract": "We investigate the training and performance of generative adversarial networks using the Maximum Mean Discrepancy (MMD) as critic, termed MMD GANs. As our main theoretical contribution, we clarify the situation with bias in GAN loss functions raised by recent work: we show that gradient estimators used in the optimization process for both MMD GANs and Wasserstein GANs are unbiased, but learning a discriminator based on samples leads to biased gradients for the generator parameters. We also discuss the issue of kernel choice for the MMD critic, and characterize the kernel corresponding to the energy distance used for the Cramer GAN critic. Being an integral probability metric, the MMD benefits from training strategies recently developed for Wasserstein GANs. In experiments, the MMD GAN is able to employ a smaller critic network than the Wasserstein GAN, resulting in a simpler and faster-training algorithm with matching performance. We also propose an improved measure of GAN convergence, the Kernel Inception Distance, and show how to use it to dynamically adapt learning rates during GAN training."
    },
    {
        "paperId": "aab368284210c1bb917ec2d31b84588e3d2d7eb4",
        "url": "https://www.semanticscholar.org/paper/aab368284210c1bb917ec2d31b84588e3d2d7eb4",
        "title": "Unsupervised Representation Learning by Predicting Image Rotations",
        "venue": "International Conference on Learning Representations",
        "year": 2018,
        "citationCount": 3497,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1803.07728, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2475428",
                "name": "Spyros Gidaris"
            },
            {
                "authorId": "2841913",
                "name": "Praveer Singh"
            },
            {
                "authorId": "2505902",
                "name": "N. Komodakis"
            }
        ],
        "abstract": "Over the last years, deep convolutional neural networks (ConvNets) have transformed the field of computer vision thanks to their unparalleled capacity to learn high level semantic image features. However, in order to successfully learn those features, they usually require massive amounts of manually labeled data, which is both expensive and impractical to scale. Therefore, unsupervised semantic feature learning, i.e., learning without requiring manual annotation effort, is of crucial importance in order to successfully harvest the vast amount of visual data that are available today. In our work we propose to learn image features by training ConvNets to recognize the 2d rotation that is applied to the image that it gets as input. We demonstrate both qualitatively and quantitatively that this apparently simple task actually provides a very powerful supervisory signal for semantic feature learning. We exhaustively evaluate our method in various unsupervised feature learning benchmarks and we exhibit in all of them state-of-the-art performance. Specifically, our results on those benchmarks demonstrate dramatic improvements w.r.t. prior state-of-the-art approaches in unsupervised representation learning and thus significantly close the gap with supervised feature learning. For instance, in PASCAL VOC 2007 detection task our unsupervised pre-trained AlexNet model achieves the state-of-the-art (among unsupervised methods) mAP of 54.4% that is only 2.4 points lower from the supervised case. We get similarly striking results when we transfer our unsupervised learned features on various other tasks, such as ImageNet classification, PASCAL classification, PASCAL segmentation, and CIFAR-10 classification. The code and models of our paper will be published on: this https URL ."
    },
    {
        "paperId": "ac225094aab9e7b629bc5b3343e026dea0200c70",
        "url": "https://www.semanticscholar.org/paper/ac225094aab9e7b629bc5b3343e026dea0200c70",
        "title": "Predict then Propagate: Graph Neural Networks meet Personalized PageRank",
        "venue": "International Conference on Learning Representations",
        "year": 2018,
        "citationCount": 1914,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1810.05997, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "51516539",
                "name": "Johannes Klicpera"
            },
            {
                "authorId": "11754930",
                "name": "Aleksandar Bojchevski"
            },
            {
                "authorId": "3075189",
                "name": "Stephan Gnnemann"
            }
        ],
        "abstract": "Neural message passing algorithms for semi-supervised classification on graphs have recently achieved great success. However, for classifying a node these methods only consider nodes that are a few propagation steps away and the size of this utilized neighborhood is hard to extend. In this paper, we use the relationship between graph convolutional networks (GCN) and PageRank to derive an improved propagation scheme based on personalized PageRank. We utilize this propagation procedure to construct a simple model, personalized propagation of neural predictions (PPNP), and its fast approximation, APPNP. Our model's training time is on par or faster and its number of parameters on par or lower than previous models. It leverages a large, adjustable neighborhood for classification and can be easily combined with any neural network. We show that this model outperforms several recently proposed methods for semi-supervised classification in the most thorough study done so far for GCN-like models. Our implementation is available online."
    },
    {
        "paperId": "af3825437b627db1a99f946f7aa773ba8b03befd",
        "url": "https://www.semanticscholar.org/paper/af3825437b627db1a99f946f7aa773ba8b03befd",
        "title": "Learning deep representations by mutual information estimation and maximization",
        "venue": "International Conference on Learning Representations",
        "year": 2018,
        "citationCount": 2888,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1808.06670, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "40482726",
                "name": "R. Devon Hjelm"
            },
            {
                "authorId": "26920432",
                "name": "A. Fedorov"
            },
            {
                "authorId": "1412884705",
                "name": "Samuel Lavoie-Marchildon"
            },
            {
                "authorId": "14007973",
                "name": "Karan Grewal"
            },
            {
                "authorId": "3382568",
                "name": "Adam Trischler"
            },
            {
                "authorId": "1751762",
                "name": "Yoshua Bengio"
            }
        ],
        "abstract": "This work investigates unsupervised learning of representations by maximizing mutual information between an input and the output of a deep neural network encoder. Importantly, we show that structure matters: incorporating knowledge about locality in the input into the objective can significantly improve a representations suitability for downstream tasks. We further control characteristics of the representation by matching to a prior distribution adversarially. Our method, which we call Deep InfoMax (DIM), outperforms a number of popular unsupervised learning methods and compares favorably with fully-supervised learning on several classification tasks in with some standard architectures. DIM opens new avenues for unsupervised learning of representations and is an important step towards flexible formulations of representation learning objectives for specific end-goals."
    },
    {
        "paperId": "c1f457e31b611da727f9aef76c283a18157dfa83",
        "url": "https://www.semanticscholar.org/paper/c1f457e31b611da727f9aef76c283a18157dfa83",
        "title": "DARTS: Differentiable Architecture Search",
        "venue": "International Conference on Learning Representations",
        "year": 2018,
        "citationCount": 4720,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1806.09055, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2391802",
                "name": "Hanxiao Liu"
            },
            {
                "authorId": "34838386",
                "name": "K. Simonyan"
            },
            {
                "authorId": "35729970",
                "name": "Yiming Yang"
            }
        ],
        "abstract": "This paper addresses the scalability challenge of architecture search by formulating the task in a differentiable manner. Unlike conventional approaches of applying evolution or reinforcement learning over a discrete and non-differentiable search space, our method is based on the continuous relaxation of the architecture representation, allowing efficient search of the architecture using gradient descent. Extensive experiments on CIFAR-10, ImageNet, Penn Treebank and WikiText-2 show that our algorithm excels in discovering high-performance convolutional architectures for image classification and recurrent architectures for language modeling, while being orders of magnitude faster than state-of-the-art non-differentiable techniques. Our implementation has been made publicly available to facilitate further research on efficient architecture search algorithms."
    },
    {
        "paperId": "c983653841b6987d9959318f074a595783838576",
        "url": "https://www.semanticscholar.org/paper/c983653841b6987d9959318f074a595783838576",
        "title": "On the Convergence of Adam and Beyond",
        "venue": "International Conference on Learning Representations",
        "year": 2018,
        "citationCount": 2760,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1904.09237, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1981186",
                "name": "Sashank J. Reddi"
            },
            {
                "authorId": "144055676",
                "name": "Satyen Kale"
            },
            {
                "authorId": "49596260",
                "name": "Surinder Kumar"
            }
        ],
        "abstract": "Several recently proposed stochastic optimization methods that have been successfully used in training deep networks such as RMSProp, Adam, Adadelta, Nadam are based on using gradient updates scaled by square roots of exponential moving averages of squared past gradients. In many applications, e.g. learning with large output spaces, it has been empirically observed that these algorithms fail to converge to an optimal solution (or a critical point in nonconvex settings). We show that one cause for such failures is the exponential moving average used in the algorithms. We provide an explicit example of a simple convex optimization setting where Adam does not converge to the optimal solution, and describe the precise problems with the previous analysis of Adam algorithm. Our analysis suggests that the convergence issues can be fixed by endowing such algorithms with `long-term memory' of past gradients, and propose new variants of the Adam algorithm which not only fix the convergence issues but often also lead to improved empirical performance."
    },
    {
        "paperId": "ce4f001c1d8ddb9a95cf54e14240ef02c44bd329",
        "url": "https://www.semanticscholar.org/paper/ce4f001c1d8ddb9a95cf54e14240ef02c44bd329",
        "title": "Attention, Learn to Solve Routing Problems!",
        "venue": "International Conference on Learning Representations",
        "year": 2018,
        "citationCount": 1456,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1803.08475, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "50349716",
                "name": "W. Kool"
            },
            {
                "authorId": "47662867",
                "name": "H. V. Hoof"
            },
            {
                "authorId": "2288504711",
                "name": "Max Welling"
            }
        ],
        "abstract": "The recently presented idea to learn heuristics for combinatorial optimization problems is promising as it can save costly development. However, to push this idea towards practical implementation, we need better models and better ways of training. We contribute in both directions: we propose a model based on attention layers with benefits over the Pointer Network and we show how to train this model using REINFORCE with a simple baseline based on a deterministic greedy rollout, which we find is more efficient than using a value function. We significantly improve over recent learned heuristics for the Travelling Salesman Problem (TSP), getting close to optimal results for problems up to 100 nodes. With the same hyperparameters, we learn strong heuristics for two variants of the Vehicle Routing Problem (VRP), the Orienteering Problem (OP) and (a stochastic variant of) the Prize Collecting TSP (PCTSP), outperforming a wide range of baselines and getting results close to highly optimized and specialized algorithms."
    },
    {
        "paperId": "cf440ccce4a7a8681e238b4f26d5b95109add55d",
        "url": "https://www.semanticscholar.org/paper/cf440ccce4a7a8681e238b4f26d5b95109add55d",
        "title": "SNIP: Single-shot Network Pruning based on Connection Sensitivity",
        "venue": "International Conference on Learning Representations",
        "year": 2018,
        "citationCount": 1365,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1810.02340, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2702448",
                "name": "Namhoon Lee"
            },
            {
                "authorId": "144722114",
                "name": "Thalaiyasingam Ajanthan"
            },
            {
                "authorId": "143635540",
                "name": "Philip H. S. Torr"
            }
        ],
        "abstract": "Pruning large neural networks while maintaining their performance is often desirable due to the reduced space and time complexity. In existing methods, pruning is done within an iterative optimization procedure with either heuristically designed pruning schedules or additional hyperparameters, undermining their utility. In this work, we present a new approach that prunes a given network once at initialization prior to training. To achieve this, we introduce a saliency criterion based on connection sensitivity that identifies structurally important connections in the network for the given task. This eliminates the need for both pretraining and the complex pruning schedule while making it robust to architecture variations. After pruning, the sparse network is trained in the standard way. Our method obtains extremely sparse networks with virtually the same accuracy as the reference network on the MNIST, CIFAR-10, and Tiny-ImageNet classification tasks and is broadly applicable to various architectures including convolutional, residual and recurrent networks. Unlike existing methods, our approach enables us to demonstrate that the retained connections are indeed relevant to the given task."
    },
    {
        "paperId": "d6f6d1504cfedde4efb23e7ec0f42f006062c6a0",
        "url": "https://www.semanticscholar.org/paper/d6f6d1504cfedde4efb23e7ec0f42f006062c6a0",
        "title": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks",
        "venue": "International Conference on Learning Representations",
        "year": 2018,
        "citationCount": 1336,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1810.02054, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "145697585",
                "name": "S. Du"
            },
            {
                "authorId": "22226408",
                "name": "Xiyu Zhai"
            },
            {
                "authorId": "1719347",
                "name": "B. Pczos"
            },
            {
                "authorId": "2109423866",
                "name": "Aarti Singh"
            }
        ],
        "abstract": "One of the mysteries in the success of neural networks is randomly initialized first order methods like gradient descent can achieve zero training loss even though the objective function is non-convex and non-smooth. This paper demystifies this surprising phenomenon for two-layer fully connected ReLU activated neural networks. For an $m$ hidden node shallow neural network with ReLU activation and $n$ training data, we show as long as $m$ is large enough and no two inputs are parallel, randomly initialized gradient descent converges to a globally optimal solution at a linear convergence rate for the quadratic loss function. \nOur analysis relies on the following observation: over-parameterization and random initialization jointly restrict every weight vector to be close to its initialization for all iterations, which allows us to exploit a strong convexity-like property to show that gradient descent converges at a global linear rate to the global optimum. We believe these insights are also useful in analyzing deep models and other first order methods."
    },
    {
        "paperId": "dbc7401e3e75c40d3c720e7db3c906d48bd742d7",
        "url": "https://www.semanticscholar.org/paper/dbc7401e3e75c40d3c720e7db3c906d48bd742d7",
        "title": "Deep Autoencoding Gaussian Mixture Model for Unsupervised Anomaly Detection",
        "venue": "International Conference on Learning Representations",
        "year": 2018,
        "citationCount": 1907,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "2991105",
                "name": "Bo Zong"
            },
            {
                "authorId": "2091568319",
                "name": "Qi Song"
            },
            {
                "authorId": "2984407",
                "name": "Martin Renqiang Min"
            },
            {
                "authorId": "145859270",
                "name": "Wei Cheng"
            },
            {
                "authorId": "2470254",
                "name": "C. Lumezanu"
            },
            {
                "authorId": "1797221",
                "name": "Dae-ki Cho"
            },
            {
                "authorId": "2145225543",
                "name": "Haifeng Chen"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "dd2ebc42a1a4491b4179dec0ca8686d5c66f6bfe",
        "url": "https://www.semanticscholar.org/paper/dd2ebc42a1a4491b4179dec0ca8686d5c66f6bfe",
        "title": "The relativistic discriminator: a key element missing from standard GAN",
        "venue": "International Conference on Learning Representations",
        "year": 2018,
        "citationCount": 1058,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1807.00734, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1401723615",
                "name": "Alexia Jolicoeur-Martineau"
            }
        ],
        "abstract": "In standard generative adversarial network (SGAN), the discriminator estimates the probability that the input data is real. The generator is trained to increase the probability that fake data is real. We argue that it should also simultaneously decrease the probability that real data is real because 1) this would account for a priori knowledge that half of the data in the mini-batch is fake, 2) this would be observed with divergence minimization, and 3) in optimal settings, SGAN would be equivalent to integral probability metric (IPM) GANs. \nWe show that this property can be induced by using a relativistic discriminator which estimate the probability that the given real data is more realistic than a randomly sampled fake data. We also present a variant in which the discriminator estimate the probability that the given real data is more realistic than fake data, on average. We generalize both approaches to non-standard GAN loss functions and we refer to them respectively as Relativistic GANs (RGANs) and Relativistic average GANs (RaGANs). We show that IPM-based GANs are a subset of RGANs which use the identity function. \nEmpirically, we observe that 1) RGANs and RaGANs are significantly more stable and generate higher quality data samples than their non-relativistic counterparts, 2) Standard RaGAN with gradient penalty generate data of better quality than WGAN-GP while only requiring a single discriminator update per generator update (reducing the time taken for reaching the state-of-the-art by 400%), and 3) RaGANs are able to generate plausible high resolutions images (256x256) from a very small sample (N=2011), while GAN and LSGAN cannot; these images are of significantly better quality than the ones generated by WGAN-GP and SGAN with spectral normalization."
    },
    {
        "paperId": "df093d69cd98cf4b26542f53614a79754754eb78",
        "url": "https://www.semanticscholar.org/paper/df093d69cd98cf4b26542f53614a79754754eb78",
        "title": "Meta-Learning for Semi-Supervised Few-Shot Classification",
        "venue": "International Conference on Learning Representations",
        "year": 2018,
        "citationCount": 1383,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1803.00676, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2540599",
                "name": "Mengye Ren"
            },
            {
                "authorId": "2064782825",
                "name": "Eleni Triantafillou"
            },
            {
                "authorId": "49517463",
                "name": "S. Ravi"
            },
            {
                "authorId": "39770136",
                "name": "Jake Snell"
            },
            {
                "authorId": "1754860",
                "name": "Kevin Swersky"
            },
            {
                "authorId": "1763295",
                "name": "J. Tenenbaum"
            },
            {
                "authorId": "1777528",
                "name": "H. Larochelle"
            },
            {
                "authorId": "1804104",
                "name": "R. Zemel"
            }
        ],
        "abstract": "In few-shot classification, we are interested in learning algorithms that train a classifier from only a handful of labeled examples. Recent progress in few-shot classification has featured meta-learning, in which a parameterized model for a learning algorithm is defined and trained on episodes representing different classification problems, each with a small labeled training set and its corresponding test set. In this work, we advance this few-shot classification paradigm towards a scenario where unlabeled examples are also available within each episode. We consider two situations: one where all unlabeled examples are assumed to belong to the same set of classes as the labeled examples of the episode, as well as the more challenging situation where examples from other distractor classes are also provided. To address this paradigm, we propose novel extensions of Prototypical Networks (Snell et al., 2017) that are augmented with the ability to use unlabeled examples when producing prototypes. These models are trained in an end-to-end way on episodes, to learn to leverage the unlabeled examples successfully. We evaluate these methods on versions of the Omniglot and miniImageNet benchmarks, adapted to this new framework augmented with unlabeled examples. We also propose a new split of ImageNet, consisting of a large set of classes, with a hierarchical structure. Our experiments confirm that our Prototypical Networks can learn to improve their predictions due to unlabeled examples, much like a semi-supervised algorithm would."
    },
    {
        "paperId": "e225dd59ef4954db21479cdcbee497624b2d6d0f",
        "url": "https://www.semanticscholar.org/paper/e225dd59ef4954db21479cdcbee497624b2d6d0f",
        "title": "Countering Adversarial Images using Input Transformations",
        "venue": "International Conference on Learning Representations",
        "year": 2018,
        "citationCount": 1523,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1711.00117, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "144993411",
                "name": "Chuan Guo"
            },
            {
                "authorId": "2139712",
                "name": "Mayank Rana"
            },
            {
                "authorId": "5723508",
                "name": "Moustapha Ciss"
            },
            {
                "authorId": "1803520",
                "name": "L. Maaten"
            }
        ],
        "abstract": "This paper investigates strategies that defend against adversarial-example attacks on image-classification systems by transforming the inputs before feeding them to the system. Specifically, we study applying image transformations such as bit-depth reduction, JPEG compression, total variance minimization, and image quilting before feeding the image to a convolutional network classifier. Our experiments on ImageNet show that total variance minimization and image quilting are very effective defenses in practice, in particular, when the network is trained on transformed images. The strength of those defenses lies in their non-differentiable nature and their inherent randomness, which makes it difficult for an adversary to circumvent the defenses. Our best defense eliminates 60% of strong gray-box and 90% of strong black-box attacks by a variety of major attack methods"
    },
    {
        "paperId": "f323407464c4cd492d3fc1afd7170eab08f44d9b",
        "url": "https://www.semanticscholar.org/paper/f323407464c4cd492d3fc1afd7170eab08f44d9b",
        "title": "ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware",
        "venue": "International Conference on Learning Representations",
        "year": 2018,
        "citationCount": 1983,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1812.00332, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "145834074",
                "name": "Han Cai"
            },
            {
                "authorId": "20515689",
                "name": "Ligeng Zhu"
            },
            {
                "authorId": "143840275",
                "name": "Song Han"
            }
        ],
        "abstract": "Neural architecture search (NAS) has a great impact by automatically designing effective neural network architectures. However, the prohibitive computational demand of conventional NAS algorithms (e.g. $10^4$ GPU hours) makes it difficult to \\emph{directly} search the architectures on large-scale tasks (e.g. ImageNet). Differentiable NAS can reduce the cost of GPU hours via a continuous representation of network architecture but suffers from the high GPU memory consumption issue (grow linearly w.r.t. candidate set size). As a result, they need to utilize~\\emph{proxy} tasks, such as training on a smaller dataset, or learning with only a few blocks, or training just for a few epochs. These architectures optimized on proxy tasks are not guaranteed to be optimal on the target task. In this paper, we present \\emph{ProxylessNAS} that can \\emph{directly} learn the architectures for large-scale target tasks and target hardware platforms. We address the high memory consumption issue of differentiable NAS and reduce the computational cost (GPU hours and GPU memory) to the same level of regular training while still allowing a large candidate set. Experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of directness and specialization. On CIFAR-10, our model achieves 2.08\\% test error with only 5.7M parameters, better than the previous state-of-the-art architecture AmoebaNet-B, while using 6$\\times$ fewer parameters. On ImageNet, our model achieves 3.1\\% better top-1 accuracy than MobileNetV2, while being 1.2$\\times$ faster with measured GPU latency. We also apply ProxylessNAS to specialize neural architectures for hardware with direct hardware metrics (e.g. latency) and provide insights for efficient CNN architecture design."
    },
    {
        "paperId": "f7bb1636ced9036b3d0edafc7d82ad43164d41a3",
        "url": "https://www.semanticscholar.org/paper/f7bb1636ced9036b3d0edafc7d82ad43164d41a3",
        "title": "Defense-GAN: Protecting Classifiers Against Adversarial Attacks Using Generative Models",
        "venue": "International Conference on Learning Representations",
        "year": 2018,
        "citationCount": 1231,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1805.06605, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3383048",
                "name": "Pouya Samangouei"
            },
            {
                "authorId": "2747758",
                "name": "Maya Kabkab"
            },
            {
                "authorId": "9215658",
                "name": "R. Chellappa"
            }
        ],
        "abstract": "In recent years, deep neural network approaches have been widely adopted for machine learning tasks, including classification. However, they were shown to be vulnerable to adversarial perturbations: carefully crafted small perturbations can cause misclassification of legitimate images. We propose Defense-GAN, a new framework leveraging the expressive capability of generative models to defend deep neural networks against such attacks. Defense-GAN is trained to model the distribution of unperturbed images. At inference time, it finds a close output to a given image which does not contain the adversarial changes. This output is then fed to the classifier. Our proposed method can be used with any classification model and does not modify the classifier structure or training procedure. It can also be used as a defense against any attack as it does not assume knowledge of the process for generating the adversarial examples. We empirically show that Defense-GAN is consistently effective against different attack methods and improves on existing defense strategies. Our code has been made publicly available at this https URL"
    },
    {
        "paperId": "02b1607af35b48f0bd716367caf6a7428b969369",
        "url": "https://www.semanticscholar.org/paper/02b1607af35b48f0bd716367caf6a7428b969369",
        "title": "AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty",
        "venue": "International Conference on Learning Representations",
        "year": 2019,
        "citationCount": 1481,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1912.02781, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3422872",
                "name": "Dan Hendrycks"
            },
            {
                "authorId": "52227748",
                "name": "Norman Mu"
            },
            {
                "authorId": "8132903",
                "name": "E. D. Cubuk"
            },
            {
                "authorId": "2368067",
                "name": "Barret Zoph"
            },
            {
                "authorId": "2058362",
                "name": "J. Gilmer"
            },
            {
                "authorId": "40627523",
                "name": "Balaji Lakshminarayanan"
            }
        ],
        "abstract": "Modern deep neural networks can achieve high accuracy when the training distribution and test distribution are identically distributed, but this assumption is frequently violated in practice. When the train and test distributions are mismatched, accuracy can plummet. Currently there are few techniques that improve robustness to unforeseen data shifts encountered during deployment. In this work, we propose a technique to improve the robustness and uncertainty estimates of image classifiers. We propose AugMix, a data processing technique that is simple to implement, adds limited computational overhead, and helps models withstand unforeseen corruptions. AugMix significantly improves robustness and uncertainty measures on challenging image classification benchmarks, closing the gap between previous methods and the best possible performance in some cases by more than half."
    },
    {
        "paperId": "0a6a9e6d4e3efd7c69357769305b70097281655f",
        "url": "https://www.semanticscholar.org/paper/0a6a9e6d4e3efd7c69357769305b70097281655f",
        "title": "DropEdge: Towards Deep Graph Convolutional Networks on Node Classification",
        "venue": "International Conference on Learning Representations",
        "year": 2019,
        "citationCount": 1515,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1907.10903, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "48537464",
                "name": "Yu Rong"
            },
            {
                "authorId": "123175679",
                "name": "Wenbing Huang"
            },
            {
                "authorId": "1754673",
                "name": "Tingyang Xu"
            },
            {
                "authorId": "1768190",
                "name": "Junzhou Huang"
            }
        ],
        "abstract": "Over-fitting and over-smoothing are two main obstacles of developing deep Graph Convolutional Networks (GCNs) for node classification. In particular, over-fitting weakens the generalization ability on small dataset, while over-smoothing impedes model training by isolating output representations from the input features with the increase in network depth. This paper proposes DropEdge, a novel and flexible technique to alleviate both issues. At its core, DropEdge randomly removes a certain number of edges from the input graph at each training epoch, acting like a data augmenter and also a message passing reducer. Furthermore, we theoretically demonstrate that DropEdge either retards the convergence speed of over-smoothing or relieves the information loss caused by it. More importantly, our DropEdge is a general skill that can be equipped with many other backbone models (e.g. GCN, ResGCN, GraphSAGE, and JKNet) for enhanced performance. Extensive experiments on several benchmarks verify that DropEdge consistently improves the performance on a variety of both shallow and deep GCNs. The effect of DropEdge on preventing over-smoothing is empirically visualized and validated as well. Codes will be made public upon the publication."
    },
    {
        "paperId": "13c185b8c461034af2634f25dd8a85889e8ee135",
        "url": "https://www.semanticscholar.org/paper/13c185b8c461034af2634f25dd8a85889e8ee135",
        "title": "N-BEATS: Neural basis expansion analysis for interpretable time series forecasting",
        "venue": "International Conference on Learning Representations",
        "year": 2019,
        "citationCount": 1384,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1905.10437, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3313242",
                "name": "Boris N. Oreshkin"
            },
            {
                "authorId": "120982452",
                "name": "Dmitri Carpov"
            },
            {
                "authorId": "2748188",
                "name": "Nicolas Chapados"
            },
            {
                "authorId": "1751762",
                "name": "Yoshua Bengio"
            }
        ],
        "abstract": "We focus on solving the univariate times series point forecasting problem using deep learning. We propose a deep neural architecture based on backward and forward residual links and a very deep stack of fully-connected layers. The architecture has a number of desirable properties, being interpretable, applicable without modification to a wide array of target domains, and fast to train. We test the proposed architecture on several well-known datasets, including M3, M4 and TOURISM competition datasets containing time series from diverse domains. We demonstrate state-of-the-art performance for two configurations of N-BEATS for all the datasets, improving forecast accuracy by 11% over a statistical benchmark and by 3% over last year's winner of the M4 competition, a domain-adjusted hand-crafted hybrid between neural network and statistical time series models. The first configuration of our model does not employ any time-series-specific components and its performance on heterogeneous datasets strongly suggests that, contrarily to received wisdom, deep learning primitives such as residual blocks are by themselves sufficient to solve a wide range of forecasting problems. Finally, we demonstrate how the proposed architecture can be augmented to provide outputs that are interpretable without considerable loss in accuracy."
    },
    {
        "paperId": "197498cb2ad787e4f71c05098cee6b10d9d067bd",
        "url": "https://www.semanticscholar.org/paper/197498cb2ad787e4f71c05098cee6b10d9d067bd",
        "title": "Contrastive Representation Distillation",
        "venue": "International Conference on Learning Representations",
        "year": 2019,
        "citationCount": 1204,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1910.10699, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2476765",
                "name": "Yonglong Tian"
            },
            {
                "authorId": "1707347",
                "name": "Dilip Krishnan"
            },
            {
                "authorId": "2094770",
                "name": "Phillip Isola"
            }
        ],
        "abstract": "Often we wish to transfer representational knowledge from one neural network to another. Examples include distilling a large network into a smaller one, transferring knowledge from one sensory modality to a second, or ensembling a collection of models into a single estimator. Knowledge distillation, the standard approach to these problems, minimizes the KL divergence between the probabilistic outputs of a teacher and student network. We demonstrate that this objective ignores important structural knowledge of the teacher network. This motivates an alternative objective by which we train a student to capture significantly more information in the teacher's representation of the data. We formulate this objective as contrastive learning. Experiments demonstrate that our resulting new objective outperforms knowledge distillation and other cutting-edge distillers on a variety of knowledge transfer tasks, including single model compression, ensemble distillation, and cross-modal transfer. Our method sets a new state-of-the-art in many transfer tasks, and sometimes even outperforms the teacher network when combined with knowledge distillation. Code: this http URL"
    },
    {
        "paperId": "270b95262f2100c279c4c65ef2521473841ca8ad",
        "url": "https://www.semanticscholar.org/paper/270b95262f2100c279c4c65ef2521473841ca8ad",
        "title": "Dream to Control: Learning Behaviors by Latent Imagination",
        "venue": "International Conference on Learning Representations",
        "year": 2019,
        "citationCount": 1629,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1912.01603, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "35006479",
                "name": "Danijar Hafner"
            },
            {
                "authorId": "2542999",
                "name": "T. Lillicrap"
            },
            {
                "authorId": "2503659",
                "name": "Jimmy Ba"
            },
            {
                "authorId": "144739074",
                "name": "Mohammad Norouzi"
            }
        ],
        "abstract": "To select effective actions in complex environments, intelligent agents need to generalize from past experience. World models can represent knowledge about the environment to facilitate such generalization. While learning world models from high-dimensional sensory inputs is becoming feasible through deep learning, there are many potential ways for deriving behaviors from them. We present Dreamer, a reinforcement learning agent that solves long-horizon tasks purely by latent imagination. We efficiently learn behaviors by backpropagating analytic gradients of learned state values through trajectories imagined in the compact state space of a learned world model. On 20 challenging visual control tasks, Dreamer exceeds existing approaches in data-efficiency, computation time, and final performance."
    },
    {
        "paperId": "295065d942abca0711300b2b4c39829551060578",
        "url": "https://www.semanticscholar.org/paper/295065d942abca0711300b2b4c39829551060578",
        "title": "BERTScore: Evaluating Text Generation with BERT",
        "venue": "International Conference on Learning Representations",
        "year": 2019,
        "citationCount": 7370,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1904.09675, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "123437034",
                "name": "Tianyi Zhang"
            },
            {
                "authorId": "145461044",
                "name": "Varsha Kishore"
            },
            {
                "authorId": "24277779",
                "name": "Felix Wu"
            },
            {
                "authorId": "7446832",
                "name": "Kilian Q. Weinberger"
            },
            {
                "authorId": "3167681",
                "name": "Yoav Artzi"
            }
        ],
        "abstract": "We propose BERTScore, an automatic evaluation metric for text generation. Analogously to common metrics, BERTScore computes a similarity score for each token in the candidate sentence with each token in the reference sentence. However, instead of exact matches, we compute token similarity using contextual embeddings. We evaluate using the outputs of 363 machine translation and image captioning systems. BERTScore correlates better with human judgments and provides stronger model selection performance than existing metrics. Finally, we use an adversarial paraphrase detection task to show that BERTScore is more robust to challenging examples when compared to existing metrics."
    },
    {
        "paperId": "2bf7c350a8280e7c593d46a60127f99b21517121",
        "url": "https://www.semanticscholar.org/paper/2bf7c350a8280e7c593d46a60127f99b21517121",
        "title": "On the Variance of the Adaptive Learning Rate and Beyond",
        "venue": "International Conference on Learning Representations",
        "year": 2019,
        "citationCount": 2115,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1908.03265, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "46458310",
                "name": "Liyuan Liu"
            },
            {
                "authorId": "5795999",
                "name": "Haoming Jiang"
            },
            {
                "authorId": "50462546",
                "name": "Pengcheng He"
            },
            {
                "authorId": "2109136147",
                "name": "Weizhu Chen"
            },
            {
                "authorId": "46522098",
                "name": "Xiaodong Liu"
            },
            {
                "authorId": "1800422",
                "name": "Jianfeng Gao"
            },
            {
                "authorId": "153034701",
                "name": "Jiawei Han"
            }
        ],
        "abstract": "The learning rate warmup heuristic achieves remarkable success in stabilizing training, accelerating convergence and improving generalization for adaptive stochastic optimization algorithms like RMSprop and Adam. Here, we study its mechanism in details. Pursuing the theory behind warmup, we identify a problem of the adaptive learning rate (i.e., it has problematically large variance in the early stage), suggest warmup works as a variance reduction technique, and provide both empirical and theoretical evidence to verify our hypothesis. We further propose RAdam, a new variant of Adam, by introducing a term to rectify the variance of the adaptive learning rate. Extensive experimental results on image classification, language modeling, and neural machine translation verify our intuition and demonstrate the effectiveness and robustness of our proposed method. All implementations are available at: this https URL."
    },
    {
        "paperId": "49b64383fe36268410c430352637ed23b16820c5",
        "url": "https://www.semanticscholar.org/paper/49b64383fe36268410c430352637ed23b16820c5",
        "title": "Benchmarking Neural Network Robustness to Common Corruptions and Perturbations",
        "venue": "International Conference on Learning Representations",
        "year": 2019,
        "citationCount": 3926,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1903.12261, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3422872",
                "name": "Dan Hendrycks"
            },
            {
                "authorId": "144299726",
                "name": "Thomas G. Dietterich"
            }
        ],
        "abstract": "In this paper we establish rigorous benchmarks for image classifier robustness. Our first benchmark, ImageNet-C, standardizes and expands the corruption robustness topic, while showing which classifiers are preferable in safety-critical applications. Then we propose a new dataset called ImageNet-P which enables researchers to benchmark a classifier's robustness to common perturbations. Unlike recent robustness research, this benchmark evaluates performance on common corruptions and perturbations not worst-case adversarial perturbations. We find that there are negligible changes in relative corruption robustness from AlexNet classifiers to ResNet classifiers. Afterward we discover ways to enhance corruption and perturbation robustness. We even find that a bypassed adversarial defense provides substantial common perturbation robustness. Together our benchmarks may aid future work toward networks that robustly generalize."
    },
    {
        "paperId": "4aa6298b606941a282d735fa3143da293199d2ca",
        "url": "https://www.semanticscholar.org/paper/4aa6298b606941a282d735fa3143da293199d2ca",
        "title": "VL-BERT: Pre-training of Generic Visual-Linguistic Representations",
        "venue": "International Conference on Learning Representations",
        "year": 2019,
        "citationCount": 1791,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1908.08530, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "145499378",
                "name": "Weijie Su"
            },
            {
                "authorId": "2578924",
                "name": "Xizhou Zhu"
            },
            {
                "authorId": "2112823372",
                "name": "Yue Cao"
            },
            {
                "authorId": "2156072370",
                "name": "Bin Li"
            },
            {
                "authorId": "152309485",
                "name": "Lewei Lu"
            },
            {
                "authorId": "49807919",
                "name": "Furu Wei"
            },
            {
                "authorId": "3304536",
                "name": "Jifeng Dai"
            }
        ],
        "abstract": "We introduce a new pre-trainable generic representation for visual-linguistic tasks, called Visual-Linguistic BERT (VL-BERT for short). VL-BERT adopts the simple yet powerful Transformer model as the backbone, and extends it to take both visual and linguistic embedded features as input. In it, each element of the input is either of a word from the input sentence, or a region-of-interest (RoI) from the input image. It is designed to fit for most of the visual-linguistic downstream tasks. To better exploit the generic representation, we pre-train VL-BERT on the massive-scale Conceptual Captions dataset, together with text-only corpus. Extensive empirical analysis demonstrates that the pre-training procedure can better align the visual-linguistic clues and benefit the downstream tasks, such as visual commonsense reasoning, visual question answering and referring expression comprehension. It is worth noting that VL-BERT achieved the first place of single model on the leaderboard of the VCR benchmark. Code is released at \\url{this https URL}."
    },
    {
        "paperId": "4b244a6778c95b1df8e9e02332ff8d22e675f628",
        "url": "https://www.semanticscholar.org/paper/4b244a6778c95b1df8e9e02332ff8d22e675f628",
        "title": "Composition-based Multi-Relational Graph Convolutional Networks",
        "venue": "International Conference on Learning Representations",
        "year": 2019,
        "citationCount": 1019,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1911.03082, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3404827",
                "name": "Shikhar Vashishth"
            },
            {
                "authorId": "3313909",
                "name": "Soumya Sanyal"
            },
            {
                "authorId": "80503259",
                "name": "Vikram Nitin"
            },
            {
                "authorId": "2408872",
                "name": "P. Talukdar"
            }
        ],
        "abstract": "Graph Convolutional Networks (GCNs) have recently been shown to be quite successful in modeling graph-structured data. However, the primary focus has been on handling simple undirected graphs. Multi-relational graphs are a more general and prevalent form of graphs where each edge has a label and direction associated with it. Most of the existing approaches to handle such graphs suffer from over-parameterization and are restricted to learning representations of nodes only. In this paper, we propose CompGCN, a novel Graph Convolutional framework which jointly embeds both nodes and relations in a relational graph. CompGCN leverages a variety of entity-relation composition operations from Knowledge Graph Embedding techniques and scales with the number of relations. It also generalizes several of the existing multi-relational GCN methods. We evaluate our proposed method on multiple tasks such as node classification, link prediction, and graph classification, and achieve demonstrably superior results. We make the source code of CompGCN available to foster reproducible research."
    },
    {
        "paperId": "7823292e5c4b05c47af91ab6ddf671a0da709e82",
        "url": "https://www.semanticscholar.org/paper/7823292e5c4b05c47af91ab6ddf671a0da709e82",
        "title": "Once for All: Train One Network and Specialize it for Efficient Deployment",
        "venue": "International Conference on Learning Representations",
        "year": 2019,
        "citationCount": 1423,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1908.09791, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "145834074",
                "name": "Han Cai"
            },
            {
                "authorId": "144158271",
                "name": "Chuang Gan"
            },
            {
                "authorId": "143840275",
                "name": "Song Han"
            }
        ],
        "abstract": "We address the challenging problem of efficient inference across many devices and resource constraints, especially on edge devices. Conventional approaches either manually design or use neural architecture search (NAS) to find a specialized neural network and train it from scratch for each case, which is computationally prohibitive (causing $CO_2$ emission as much as 5 cars' lifetime) thus unscalable. In this work, we propose to train a once-for-all (OFA) network that supports diverse architectural settings by decoupling training and search, to reduce the cost. We can quickly get a specialized sub-network by selecting from the OFA network without additional training. To efficiently train OFA networks, we also propose a novel progressive shrinking algorithm, a generalized pruning method that reduces the model size across many more dimensions than pruning (depth, width, kernel size, and resolution). It can obtain a surprisingly large number of sub-networks ($> 10^{19}$) that can fit different hardware platforms and latency constraints while maintaining the same level of accuracy as training independently. On diverse edge devices, OFA consistently outperforms state-of-the-art (SOTA) NAS methods (up to 4.0% ImageNet top1 accuracy improvement over MobileNetV3, or same accuracy but 1.5x faster than MobileNetV3, 2.6x faster than EfficientNet w.r.t measured latency) while reducing many orders of magnitude GPU hours and $CO_2$ emission. In particular, OFA achieves a new SOTA 80.0% ImageNet top-1 accuracy under the mobile setting ($<$600M MACs). OFA is the winning solution for the 3rd Low Power Computer Vision Challenge (LPCVC), DSP classification track and the 4th LPCVC, both classification track and detection track. Code and 50 pre-trained models (for many devices & many latency constraints) are released at this https URL."
    },
    {
        "paperId": "789a7069d1a2d02d784e4821685b216cc63e6ec8",
        "url": "https://www.semanticscholar.org/paper/789a7069d1a2d02d784e4821685b216cc63e6ec8",
        "title": "Strategies for Pre-training Graph Neural Networks",
        "venue": "International Conference on Learning Representations",
        "year": 2019,
        "citationCount": 1630,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1905.12265, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "48594758",
                "name": "Weihua Hu"
            },
            {
                "authorId": "2156641189",
                "name": "Bowen Liu"
            },
            {
                "authorId": "145986494",
                "name": "Joseph Gomes"
            },
            {
                "authorId": "2095762",
                "name": "M. Zitnik"
            },
            {
                "authorId": "145419642",
                "name": "Percy Liang"
            },
            {
                "authorId": "1806271",
                "name": "V. Pande"
            },
            {
                "authorId": "1702139",
                "name": "J. Leskovec"
            }
        ],
        "abstract": "Many applications of machine learning require a model to make accurate pre-dictions on test examples that are distributionally different from training ones, while task-specific labels are scarce during training. An effective approach to this challenge is to pre-train a model on related tasks where data is abundant, and then fine-tune it on a downstream task of interest. While pre-training has been effective in many language and vision domains, it remains an open question how to effectively use pre-training on graph datasets. In this paper, we develop a new strategy and self-supervised methods for pre-training Graph Neural Networks (GNNs). The key to the success of our strategy is to pre-train an expressive GNN at the level of individual nodes as well as entire graphs so that the GNN can learn useful local and global representations simultaneously. We systematically study pre-training on multiple graph classification datasets. We find that naive strategies, which pre-train GNNs at the level of either entire graphs or individual nodes, give limited improvement and can even lead to negative transfer on many downstream tasks. In contrast, our strategy avoids negative transfer and improves generalization significantly across downstream tasks, leading up to 9.4% absolute improvements in ROC-AUC over non-pre-trained models and achieving state-of-the-art performance for molecular property prediction and protein function prediction."
    },
    {
        "paperId": "7a064df1aeada7e69e5173f7d4c8606f4470365b",
        "url": "https://www.semanticscholar.org/paper/7a064df1aeada7e69e5173f7d4c8606f4470365b",
        "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations",
        "venue": "International Conference on Learning Representations",
        "year": 2019,
        "citationCount": 7088,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1909.11942, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2362534",
                "name": "Zhenzhong Lan"
            },
            {
                "authorId": "46221498",
                "name": "Mingda Chen"
            },
            {
                "authorId": "7685850",
                "name": "Sebastian Goodman"
            },
            {
                "authorId": "1700980",
                "name": "Kevin Gimpel"
            },
            {
                "authorId": "48267618",
                "name": "Piyush Sharma"
            },
            {
                "authorId": "1737285",
                "name": "Radu Soricut"
            }
        ],
        "abstract": "Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems, we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and \\squad benchmarks while having fewer parameters compared to BERT-large. The code and the pretrained models are available at this https URL."
    },
    {
        "paperId": "9d5ec23154fb278a765f47ba5ee5150bd441d0de",
        "url": "https://www.semanticscholar.org/paper/9d5ec23154fb278a765f47ba5ee5150bd441d0de",
        "title": "A Closer Look at Few-shot Classification",
        "venue": "International Conference on Learning Representations",
        "year": 2019,
        "citationCount": 1910,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1904.04232, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2109964068",
                "name": "Wei-Yu Chen"
            },
            {
                "authorId": "2108334170",
                "name": "Yen-Cheng Liu"
            },
            {
                "authorId": "145276578",
                "name": "Z. Kira"
            },
            {
                "authorId": "2108898473",
                "name": "Y. Wang"
            },
            {
                "authorId": "3068086",
                "name": "Jia-Bin Huang"
            }
        ],
        "abstract": "Few-shot classification aims to learn a classifier to recognize unseen classes during training with limited labeled examples. While significant progress has been made, the growing complexity of network designs, meta-learning algorithms, and differences in implementation details make a fair comparison difficult. In this paper, we present 1) a consistent comparative analysis of several representative few-shot classification algorithms, with results showing that deeper backbones significantly reduce the performance differences among methods on datasets with limited domain differences, 2) a modified baseline method that surprisingly achieves competitive performance when compared with the state-of-the-art on both the \\miniI and the CUB datasets, and 3) a new experimental setting for evaluating the cross-domain generalization ability for few-shot classification algorithms. Our results reveal that reducing intra-class variation is an important factor when the feature backbone is shallow, but not as critical when using deeper backbones. In a realistic cross-domain evaluation setting, we show that a baseline method with a standard fine-tuning practice compares favorably against other state-of-the-art few-shot learning algorithms."
    },
    {
        "paperId": "bc789aef715498e79a74f857fa090ece9e383bf1",
        "url": "https://www.semanticscholar.org/paper/bc789aef715498e79a74f857fa090ece9e383bf1",
        "title": "Large Batch Optimization for Deep Learning: Training BERT in 76 minutes",
        "venue": "International Conference on Learning Representations",
        "year": 2019,
        "citationCount": 1105,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1904.00962, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "144259229",
                "name": "Yang You"
            },
            {
                "authorId": "2152907411",
                "name": "Jing Li"
            },
            {
                "authorId": "1981186",
                "name": "Sashank J. Reddi"
            },
            {
                "authorId": "50157269",
                "name": "Jonathan Hseu"
            },
            {
                "authorId": "152663162",
                "name": "Sanjiv Kumar"
            },
            {
                "authorId": "1798880",
                "name": "Srinadh Bhojanapalli"
            },
            {
                "authorId": "1718192",
                "name": "Xiaodan Song"
            },
            {
                "authorId": "1700326",
                "name": "J. Demmel"
            },
            {
                "authorId": "1732330",
                "name": "K. Keutzer"
            },
            {
                "authorId": "1793529",
                "name": "Cho-Jui Hsieh"
            }
        ],
        "abstract": "Training large deep neural networks on massive datasets is computationally very challenging. There has been recent surge in interest in using large batch stochastic optimization methods to tackle this issue. The most prominent algorithm in this line of research is LARS, which by employing layerwise adaptive learning rates trains ResNet on ImageNet in a few minutes. However, LARS performs poorly for attention models like BERT, indicating that its performance gains are not consistent across tasks. In this paper, we first study a principled layerwise adaptation strategy to accelerate training of deep neural networks using large mini-batches. Using this strategy, we develop a new layerwise adaptive large batch optimization technique called LAMB; we then provide convergence analysis of LAMB as well as LARS, showing convergence to a stationary point in general nonconvex settings. Our empirical results demonstrate the superior performance of LAMB across various tasks such as BERT and ResNet-50 training with very little hyperparameter tuning. In particular, for BERT training, our optimizer enables use of very large batch sizes of 32868 without any degradation of performance. By increasing the batch size to the memory limit of a TPUv3 Pod, BERT training time can be reduced from 3 days to just 76 minutes (Table 1). The LAMB implementation is available at this https URL"
    },
    {
        "paperId": "c802ceb7a9ff904220c48ee44ae9b671be6d6379",
        "url": "https://www.semanticscholar.org/paper/c802ceb7a9ff904220c48ee44ae9b671be6d6379",
        "title": "On the Convergence of FedAvg on Non-IID Data",
        "venue": "International Conference on Learning Representations",
        "year": 2019,
        "citationCount": 2709,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1907.02189, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2144441175",
                "name": "Xiang Li"
            },
            {
                "authorId": "2112769495",
                "name": "Kaixuan Huang"
            },
            {
                "authorId": "2120825632",
                "name": "Wenhao Yang"
            },
            {
                "authorId": "145940973",
                "name": "Shusen Wang"
            },
            {
                "authorId": "47294286",
                "name": "Zhihua Zhang"
            }
        ],
        "abstract": "Federated learning enables a large amount of edge computing devices to jointly learn a model without data sharing. As a leading algorithm in this setting, Federated Averaging (\\texttt{FedAvg}) runs Stochastic Gradient Descent (SGD) in parallel on a small subset of the total devices and averages the sequences only once in a while. Despite its simplicity, it lacks theoretical guarantees under realistic settings. In this paper, we analyze the convergence of \\texttt{FedAvg} on non-iid data and establish a convergence rate of $\\mathcal{O}(\\frac{1}{T})$ for strongly convex and smooth problems, where $T$ is the number of SGDs. Importantly, our bound demonstrates a trade-off between communication-efficiency and convergence rate. As user devices may be disconnected from the server, we relax the assumption of full device participation to partial device participation and study different averaging schemes; low device participation rate can be achieved without severely slowing down the learning. Our results indicate that heterogeneity of data slows down the convergence, which matches empirical observations. Furthermore, we provide a necessary condition for \\texttt{FedAvg} on non-iid data: the learning rate $\\eta$ must decay, even if full-gradient is used; otherwise, the solution will be $\\Omega (\\eta)$ away from the optimal."
    },
    {
        "paperId": "cf4aa38ae31b43fd07abe13b4ffdb265babb7be1",
        "url": "https://www.semanticscholar.org/paper/cf4aa38ae31b43fd07abe13b4ffdb265babb7be1",
        "title": "The Curious Case of Neural Text Degeneration",
        "venue": "International Conference on Learning Representations",
        "year": 2019,
        "citationCount": 3723,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1904.09751, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "14487640",
                "name": "Ari Holtzman"
            },
            {
                "authorId": "144685020",
                "name": "Jan Buys"
            },
            {
                "authorId": "2152141637",
                "name": "Li Du"
            },
            {
                "authorId": "39191185",
                "name": "Maxwell Forbes"
            },
            {
                "authorId": "1699545",
                "name": "Yejin Choi"
            }
        ],
        "abstract": "Despite considerable advancements with deep neural language models, the enigma of neural text degeneration persists when these models are tested as text generators. The counter-intuitive empirical observation is that even though the use of likelihood as training objective leads to high quality models for a broad range of language understanding tasks, using likelihood as a decoding objective leads to text that is bland and strangely repetitive. \nIn this paper, we reveal surprising distributional differences between human text and machine text. In addition, we find that decoding strategies alone can dramatically effect the quality of machine text, even when generated from exactly the same neural language model. Our findings motivate Nucleus Sampling, a simple but effective method to draw the best out of neural generation. By sampling text from the dynamic nucleus of the probability distribution, which allows for diversity while effectively truncating the less reliable tail of the distribution, the resulting text better demonstrates the quality of human text, yielding enhanced diversity without sacrificing fluency and coherence."
    },
    {
        "paperId": "dcc4c760c3f1cb17f953c487190b735030c33b78",
        "url": "https://www.semanticscholar.org/paper/dcc4c760c3f1cb17f953c487190b735030c33b78",
        "title": "Decoupling Representation and Classifier for Long-Tailed Recognition",
        "venue": "International Conference on Learning Representations",
        "year": 2019,
        "citationCount": 1383,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1910.09217, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2064621368",
                "name": "Bingyi Kang"
            },
            {
                "authorId": "1817030",
                "name": "Saining Xie"
            },
            {
                "authorId": "34849128",
                "name": "Marcus Rohrbach"
            },
            {
                "authorId": "3305169",
                "name": "Zhicheng Yan"
            },
            {
                "authorId": "1821267",
                "name": "Albert Gordo"
            },
            {
                "authorId": "33221685",
                "name": "Jiashi Feng"
            },
            {
                "authorId": "1944225",
                "name": "Yannis Kalantidis"
            }
        ],
        "abstract": "The long-tail distribution of the visual world poses great challenges for deep learning based classification models on how to handle the class imbalance problem. Existing solutions usually involve class-balancing strategies, e.g., by loss re-weighting, data re-sampling, or transfer learning from head- to tail-classes, but most of them adhere to the scheme of jointly learning representations and classifiers. In this work, we decouple the learning procedure into representation learning and classification, and systematically explore how different balancing strategies affect them for long-tailed recognition. The findings are surprising: (1) data imbalance might not be an issue in learning high-quality representations; (2) with representations learned with the simplest instance-balanced (natural) sampling, it is also possible to achieve strong long-tailed recognition ability by adjusting only the classifier. We conduct extensive experiments and set new state-of-the-art performance on common long-tailed benchmarks like ImageNet-LT, Places-LT and iNaturalist, showing that it is possible to outperform carefully designed losses, sampling strategies, even complex modules with memory, by using a straightforward approach that decouples representation and classification. Our code is available at this https URL."
    },
    {
        "paperId": "e04a80263d252a3d8a382ba37a249b9345620570",
        "url": "https://www.semanticscholar.org/paper/e04a80263d252a3d8a382ba37a249b9345620570",
        "title": "Plug and Play Language Models: A Simple Approach to Controlled Text Generation",
        "venue": "International Conference on Learning Representations",
        "year": 2019,
        "citationCount": 1081,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1912.02164, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3491117",
                "name": "Sumanth Dathathri"
            },
            {
                "authorId": "3064807",
                "name": "Andrea Madotto"
            },
            {
                "authorId": "107877389",
                "name": "Janice Lan"
            },
            {
                "authorId": "2052239256",
                "name": "Jane Hung"
            },
            {
                "authorId": "2059429321",
                "name": "Eric Frank"
            },
            {
                "authorId": "34890911",
                "name": "Piero Molino"
            },
            {
                "authorId": "2965424",
                "name": "J. Yosinski"
            },
            {
                "authorId": "48757909",
                "name": "Rosanne Liu"
            }
        ],
        "abstract": "Large transformer-based language models (LMs) trained on huge text corpora have shown unparalleled generation capabilities. However, controlling attributes of the generated language (e.g. switching topic or sentiment) is difficult without modifying the model architecture or fine-tuning on attribute-specific data and entailing the significant cost of retraining. We propose a simple alternative: the Plug and Play Language Model (PPLM) for controllable language generation, which combines a pretrained LM with one or more simple attribute classifiers that guide text generation without any further training of the LM. In the canonical scenario we present, the attribute models are simple classifiers consisting of a user-specified bag of words or a single learned layer with 100,000 times fewer parameters than the LM. Sampling entails a forward and backward pass in which gradients from the attribute model push the LM's hidden activations and thus guide the generation. Model samples demonstrate control over a range of topics and sentiment styles, and extensive automated and human annotated evaluations show attribute alignment and fluency. PPLMs are flexible in that any combination of differentiable attribute models may be used to steer text generation, which will allow for diverse and creative applications beyond the examples given in this paper."
    },
    {
        "paperId": "ea415809bf87ef4b99966c6c50de6cb996a02a97",
        "url": "https://www.semanticscholar.org/paper/ea415809bf87ef4b99966c6c50de6cb996a02a97",
        "title": "Deep double descent: where bigger models and more data hurt",
        "venue": "International Conference on Learning Representations",
        "year": 2019,
        "citationCount": 1049,
        "openAccessPdf": {
            "url": "https://iopscience.iop.org/article/10.1088/1742-5468/ac3a74/pdf",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1912.02292, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2181918",
                "name": "Preetum Nakkiran"
            },
            {
                "authorId": "79952988",
                "name": "Gal Kaplun"
            },
            {
                "authorId": "51041277",
                "name": "Yamini Bansal"
            },
            {
                "authorId": "121270172",
                "name": "Tristan Yang"
            },
            {
                "authorId": "1697211",
                "name": "B. Barak"
            },
            {
                "authorId": "1701686",
                "name": "I. Sutskever"
            }
        ],
        "abstract": "We show that a variety of modern deep learning tasks exhibit a double-descent phenomenon where, as we increase model size, performance first gets worse and then gets better. Moreover, we show that double descent occurs not just as a function of model size, but also as a function of the number of training epochs. We unify the above phenomena by defining a new complexity measure we call the effective model complexity and conjecture a generalized double descent with respect to this measure. Furthermore, our notion of model complexity allows us to identify certain regimes where increasing (even quadrupling) the number of train samples actually hurts test performance."
    },
    {
        "paperId": "fc3e99ebc07b3014f6736a6a7b077edf2f1634c0",
        "url": "https://www.semanticscholar.org/paper/fc3e99ebc07b3014f6736a6a7b077edf2f1634c0",
        "title": "GraphSAINT: Graph Sampling Based Inductive Learning Method",
        "venue": "International Conference on Learning Representations",
        "year": 2019,
        "citationCount": 1079,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1907.04931, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "33352252",
                "name": "Hanqing Zeng"
            },
            {
                "authorId": "1443735039",
                "name": "Hongkuan Zhou"
            },
            {
                "authorId": "2215594",
                "name": "Ajitesh Srivastava"
            },
            {
                "authorId": "2286832947",
                "name": "R. Kannan"
            },
            {
                "authorId": "1728271",
                "name": "V. Prasanna"
            }
        ],
        "abstract": "Graph Convolutional Networks (GCNs) are powerful models for learning representations of attributed this http URL scale GCNs to large graphs, state-of-the-art methods use various layer sampling techniques to alleviate the \"neighbor explosion\" problem during minibatch training. Here we proposeGraphSAINT, a graph sampling based inductive learning method that improves training efficiency in a fundamentally different way. By a change of perspective, GraphSAINT constructs minibatches by sampling the training graph, rather than the nodes or edges across GCN layers. Each iteration, a complete GCN is built from the properly sampled subgraph. Thus, we ensure fixed number of well-connected nodes in all layers. We further propose normalization technique to eliminate bias, and sampling algorithms for variance reduction. Importantly, we can decouple the sampling process from the forward and backward propagation of training, and extend GraphSAINT with other graph samplers and GCN variants. Comparing with strong baselines using layer sampling, GraphSAINT demonstrates superior performance in both accuracy and training time on four large graphs."
    },
    {
        "paperId": "014576b866078524286802b1d0e18628520aa886",
        "url": "https://www.semanticscholar.org/paper/014576b866078524286802b1d0e18628520aa886",
        "title": "Denoising Diffusion Implicit Models",
        "venue": "International Conference on Learning Representations",
        "year": 2020,
        "citationCount": 10094,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2010.02502, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "51453887",
                "name": "Jiaming Song"
            },
            {
                "authorId": "2057110631",
                "name": "Chenlin Meng"
            },
            {
                "authorId": "2066211828",
                "name": "Stefano Ermon"
            }
        ],
        "abstract": "Denoising diffusion probabilistic models (DDPMs) have achieved high quality image generation without adversarial training, yet they require simulating a Markov chain for many steps to produce a sample. To accelerate sampling, we present denoising diffusion implicit models (DDIMs), a more efficient class of iterative implicit probabilistic models with the same training procedure as DDPMs. In DDPMs, the generative process is defined as the reverse of a Markovian diffusion process. We construct a class of non-Markovian diffusion processes that lead to the same training objective, but whose reverse process can be much faster to sample from. We empirically demonstrate that DDIMs can produce high quality samples $10 \\times$ to $50 \\times$ faster in terms of wall-clock time compared to DDPMs, allow us to trade off computation for sample quality, and can perform semantically meaningful image interpolation directly in the latent space."
    },
    {
        "paperId": "04f3203f1214063436d81ce0c2ad7623204da488",
        "url": "https://www.semanticscholar.org/paper/04f3203f1214063436d81ce0c2ad7623204da488",
        "title": "Geom-GCN: Geometric Graph Convolutional Networks",
        "venue": "International Conference on Learning Representations",
        "year": 2020,
        "citationCount": 1337,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2002.05287, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1866075",
                "name": "Hongbin Pei"
            },
            {
                "authorId": "15763974",
                "name": "Bingzhen Wei"
            },
            {
                "authorId": "143922493",
                "name": "K. Chang"
            },
            {
                "authorId": "2113651237",
                "name": "Yu Lei"
            },
            {
                "authorId": "2119658599",
                "name": "Bo Yang"
            }
        ],
        "abstract": "Message-passing neural networks (MPNNs) have been successfully applied in a wide variety of applications in the real world. However, two fundamental weaknesses of MPNNs' aggregators limit their ability to represent graph-structured data: losing the structural information of nodes in neighborhoods and lacking the ability to capture long-range dependencies in disassortative graphs. Few studies have noticed the weaknesses from different perspectives. From the observations on classical neural network and network geometry, we propose a novel geometric aggregation scheme for graph neural networks to overcome the two weaknesses. The behind basic idea is the aggregation on a graph can benefit from a continuous space underlying the graph. The proposed aggregation scheme is permutation-invariant and consists of three modules, node embedding, structural neighborhood, and bi-level aggregation. We also present an implementation of the scheme in graph convolutional networks, termed Geom-GCN, to perform transductive learning on graphs. Experimental results show the proposed Geom-GCN achieved state-of-the-art performance on a wide range of open datasets of graphs."
    },
    {
        "paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500",
        "url": "https://www.semanticscholar.org/paper/055fd6a9f7293269f1b22c1470e63bd02d8d9500",
        "title": "Reformer: The Efficient Transformer",
        "venue": "International Conference on Learning Representations",
        "year": 2020,
        "citationCount": 2676,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2001.04451, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "143808231",
                "name": "Nikita Kitaev"
            },
            {
                "authorId": "40527594",
                "name": "Lukasz Kaiser"
            },
            {
                "authorId": "6639036",
                "name": "Anselm Levskaya"
            }
        ],
        "abstract": "Large Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models can be prohibitively costly, especially on long sequences. We introduce two techniques to improve the efficiency of Transformers. For one, we replace dot-product attention by one that uses locality-sensitive hashing, changing its complexity from O($L^2$) to O($L\\log L$), where $L$ is the length of the sequence. Furthermore, we use reversible residual layers instead of the standard residuals, which allows storing activations only once in the training process instead of $N$ times, where $N$ is the number of layers. The resulting model, the Reformer, performs on par with Transformer models while being much more memory-efficient and much faster on long sequences."
    },
    {
        "paperId": "14b65a86c82e38fce0eb3506e0d4084ad5cdb583",
        "url": "https://www.semanticscholar.org/paper/14b65a86c82e38fce0eb3506e0d4084ad5cdb583",
        "title": "DeBERTa: Decoding-enhanced BERT with Disentangled Attention",
        "venue": "International Conference on Learning Representations",
        "year": 2020,
        "citationCount": 3364,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2006.03654, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "50462546",
                "name": "Pengcheng He"
            },
            {
                "authorId": "2108860856",
                "name": "Xiaodong Liu"
            },
            {
                "authorId": "1800422",
                "name": "Jianfeng Gao"
            },
            {
                "authorId": "2109136147",
                "name": "Weizhu Chen"
            }
        ],
        "abstract": "Recent progress in pre-trained neural language models has significantly improved the performance of many natural language processing (NLP) tasks. In this paper we propose a new model architecture DeBERTa (Decoding-enhanced BERT with disentangled attention) that improves the BERT and RoBERTa models using two novel techniques. The first is the disentangled attention mechanism, where each word is represented using two vectors that encode its content and position, respectively, and the attention weights among words are computed using disentangled matrices on their contents and relative positions. Second, an enhanced mask decoder is used to replace the output softmax layer to predict the masked tokens for model pretraining. We show that these two techniques significantly improve the efficiency of model pre-training and performance of downstream tasks. Compared to RoBERTa-Large, a DeBERTa model trained on half of the training data performs consistently better on a wide range of NLP tasks, achieving improvements on MNLI by +0.9% (90.2% vs. 91.1%), on SQuAD v2.0 by +2.3% (88.4% vs. 90.7%) and RACE by +3.6% (83.2% vs. 86.8%). The DeBERTa code and pre-trained models will be made publicly available at this https URL."
    },
    {
        "paperId": "1623d6ffb6efd94d21537db2b96b91a196842aef",
        "url": "https://www.semanticscholar.org/paper/1623d6ffb6efd94d21537db2b96b91a196842aef",
        "title": "FastSpeech 2: Fast and High-Quality End-to-End Text to Speech",
        "venue": "International Conference on Learning Representations",
        "year": 2020,
        "citationCount": 1627,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2006.04558, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1500435161",
                "name": "Yi Ren"
            },
            {
                "authorId": "2118956725",
                "name": "Chenxu Hu"
            },
            {
                "authorId": "48391466",
                "name": "Xu Tan"
            },
            {
                "authorId": "143826491",
                "name": "Tao Qin"
            },
            {
                "authorId": "47601191",
                "name": "Sheng Zhao"
            },
            {
                "authorId": "47122432",
                "name": "Zhou Zhao"
            },
            {
                "authorId": "2110264337",
                "name": "Tie-Yan Liu"
            }
        ],
        "abstract": "Advanced text to speech (TTS) models such as FastSpeech can synthesize speech significantly faster than previous autoregressive models with comparable quality. The training of FastSpeech model relies on an autoregressive teacher model for duration prediction (to provide more information as input) and knowledge distillation (to simplify the data distribution in output), which can ease the one-to-many mapping problem (i.e., multiple speech variations correspond to the same text) in TTS. However, FastSpeech has several disadvantages: 1) the teacher-student distillation pipeline is complicated, 2) the duration extracted from the teacher model is not accurate enough, and the target mel-spectrograms distilled from teacher model suffer from information loss due to data simplification, both of which limit the voice quality. In this paper, we propose FastSpeech 2, which addresses the issues in FastSpeech and better solves the one-to-many mapping problem in TTS by 1) directly training the model with ground-truth target instead of the simplified output from teacher, and 2) introducing more variation information of speech (e.g., pitch, energy and more accurate duration) as conditional inputs. Specifically, we extract duration, pitch and energy from speech waveform and directly take them as conditional inputs during training and use predicted values during inference. We further design FastSpeech 2s, which is the first attempt to directly generate speech waveform from text in parallel, enjoying the benefit of full end-to-end training and even faster inference than FastSpeech. Experimental results show that 1) FastSpeech 2 and 2s outperform FastSpeech in voice quality with much simplified training pipeline and reduced training time; 2) FastSpeech 2 and 2s can match the voice quality of autoregressive models while enjoying much faster inference speed."
    },
    {
        "paperId": "1882f194cb43828852cc052887671e55a80f945a",
        "url": "https://www.semanticscholar.org/paper/1882f194cb43828852cc052887671e55a80f945a",
        "title": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding",
        "venue": "International Conference on Learning Representations",
        "year": 2020,
        "citationCount": 1600,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2006.16668, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "150077954",
                "name": "Dmitry Lepikhin"
            },
            {
                "authorId": "34946720",
                "name": "HyoukJoong Lee"
            },
            {
                "authorId": "2145139570",
                "name": "Yuanzhong Xu"
            },
            {
                "authorId": "7167328",
                "name": "Dehao Chen"
            },
            {
                "authorId": "2345617",
                "name": "Orhan Firat"
            },
            {
                "authorId": "2145438541",
                "name": "Yanping Huang"
            },
            {
                "authorId": "2048712",
                "name": "M. Krikun"
            },
            {
                "authorId": "1846258",
                "name": "Noam Shazeer"
            },
            {
                "authorId": "2545358",
                "name": "Z. Chen"
            }
        ],
        "abstract": "Neural network scaling has been critical for improving the model quality in many real-world machine learning applications with vast amounts of training data and compute. Although this trend of scaling is affirmed to be a sure-fire approach for better model quality, there are challenges on the path such as the computation cost, ease of programming, and efficient implementation on parallel devices. GShard is a module composed of a set of lightweight annotation APIs and an extension to the XLA compiler. It provides an elegant way to express a wide range of parallel computation patterns with minimal changes to the existing model code. GShard enabled us to scale up multilingual neural machine translation Transformer model with Sparsely-Gated Mixture-of-Experts beyond 600 billion parameters using automatic sharding. We demonstrate that such a giant model can efficiently be trained on 2048 TPU v3 accelerators in 4 days to achieve far superior quality for translation from 100 languages to English compared to the prior art."
    },
    {
        "paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a",
        "url": "https://www.semanticscholar.org/paper/268d347e8a55b5eb82fb5e7d2f800e33c75ab18a",
        "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
        "venue": "International Conference on Learning Representations",
        "year": 2020,
        "citationCount": 54421,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2010.11929, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2841331",
                "name": "Alexey Dosovitskiy"
            },
            {
                "authorId": "39611591",
                "name": "Lucas Beyer"
            },
            {
                "authorId": "144629422",
                "name": "Alexander Kolesnikov"
            },
            {
                "authorId": "3319373",
                "name": "Dirk Weissenborn"
            },
            {
                "authorId": "2743563",
                "name": "Xiaohua Zhai"
            },
            {
                "authorId": "2465270",
                "name": "Thomas Unterthiner"
            },
            {
                "authorId": "2274215058",
                "name": "Mostafa Dehghani"
            },
            {
                "authorId": "46352821",
                "name": "M. Minderer"
            },
            {
                "authorId": "2280399",
                "name": "G. Heigold"
            },
            {
                "authorId": "1802148",
                "name": "S. Gelly"
            },
            {
                "authorId": "39328010",
                "name": "Jakob Uszkoreit"
            },
            {
                "authorId": "2815290",
                "name": "N. Houlsby"
            }
        ],
        "abstract": "While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train."
    },
    {
        "paperId": "2f7dc1ee85e9f6a97810c66016e09ffeed684f03",
        "url": "https://www.semanticscholar.org/paper/2f7dc1ee85e9f6a97810c66016e09ffeed684f03",
        "title": "Fourier Neural Operator for Parametric Partial Differential Equations",
        "venue": "International Conference on Learning Representations",
        "year": 2020,
        "citationCount": 3277,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2010.08895, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1390750243",
                "name": "Zong-Yi Li"
            },
            {
                "authorId": "51219644",
                "name": "Nikola B. Kovachki"
            },
            {
                "authorId": "3371922",
                "name": "K. Azizzadenesheli"
            },
            {
                "authorId": "2048777595",
                "name": "Burigede Liu"
            },
            {
                "authorId": "144304264",
                "name": "K. Bhattacharya"
            },
            {
                "authorId": "2055700747",
                "name": "Andrew M. Stuart"
            },
            {
                "authorId": "2047844",
                "name": "Anima Anandkumar"
            }
        ],
        "abstract": "The classical development of neural networks has primarily focused on learning mappings between finite-dimensional Euclidean spaces. Recently, this has been generalized to neural operators that learn mappings between function spaces. For partial differential equations (PDEs), neural operators directly learn the mapping from any functional parametric dependence to the solution. Thus, they learn an entire family of PDEs, in contrast to classical methods which solve one instance of the equation. In this work, we formulate a new neural operator by parameterizing the integral kernel directly in Fourier space, allowing for an expressive and efficient architecture. We perform experiments on Burgers' equation, Darcy flow, and the Navier-Stokes equation (including the turbulent regime). Our Fourier neural operator shows state-of-the-art performance compared to existing neural network methodologies and it is up to three orders of magnitude faster compared to traditional PDE solvers."
    },
    {
        "paperId": "34bf13e58c7226d615afead0c0f679432502940e",
        "url": "https://www.semanticscholar.org/paper/34bf13e58c7226d615afead0c0f679432502940e",
        "title": "DiffWave: A Versatile Diffusion Model for Audio Synthesis",
        "venue": "International Conference on Learning Representations",
        "year": 2020,
        "citationCount": 1745,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2009.09761, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "33470361",
                "name": "Zhifeng Kong"
            },
            {
                "authorId": "2056440915",
                "name": "Wei Ping"
            },
            {
                "authorId": "34060310",
                "name": "Jiaji Huang"
            },
            {
                "authorId": "2074107557",
                "name": "Kexin Zhao"
            },
            {
                "authorId": "2301680",
                "name": "Bryan Catanzaro"
            }
        ],
        "abstract": "In this work, we propose DiffWave, a versatile Diffusion probabilistic model for conditional and unconditional Waveform generation. The model is non-autoregressive, and converts the white noise signal into structured waveform through a Markov chain with a constant number of steps at synthesis. It is efficiently trained by optimizing a variant of variational bound on the data likelihood. DiffWave produces high-fidelity audios in Different Waveform generation tasks, including neural vocoding conditioned on mel spectrogram, class-conditional generation, and unconditional generation. We demonstrate that DiffWave matches a strong WaveNet vocoder in terms of speech quality~(MOS: 4.44 versus 4.43), while synthesizing orders of magnitude faster. In particular, it significantly outperforms autoregressive and GAN-based waveform models in the challenging unconditional generation task in terms of audio quality and sample diversity from various automatic and human evaluations."
    },
    {
        "paperId": "39ca8f8ff28cc640e3b41a6bd7814ab85c586504",
        "url": "https://www.semanticscholar.org/paper/39ca8f8ff28cc640e3b41a6bd7814ab85c586504",
        "title": "Deformable DETR: Deformable Transformers for End-to-End Object Detection",
        "venue": "International Conference on Learning Representations",
        "year": 2020,
        "citationCount": 6562,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2010.04159, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2578924",
                "name": "Xizhou Zhu"
            },
            {
                "authorId": "145499378",
                "name": "Weijie Su"
            },
            {
                "authorId": "152309485",
                "name": "Lewei Lu"
            },
            {
                "authorId": "2183101614",
                "name": "Bin Li"
            },
            {
                "authorId": "93768810",
                "name": "Xiaogang Wang"
            },
            {
                "authorId": "3304536",
                "name": "Jifeng Dai"
            }
        ],
        "abstract": "DETR has been recently proposed to eliminate the need for many hand-designed components in object detection while demonstrating good performance. However, it suffers from slow convergence and limited feature spatial resolution, due to the limitation of Transformer attention modules in processing image feature maps. To mitigate these issues, we proposed Deformable DETR, whose attention modules only attend to a small set of key sampling points around a reference. Deformable DETR can achieve better performance than DETR (especially on small objects) with 10$\\times$ less training epochs. Extensive experiments on the COCO benchmark demonstrate the effectiveness of our approach. Code shall be released."
    },
    {
        "paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "url": "https://www.semanticscholar.org/paper/3fbf6339273c50b04e886fa9bd4ad18c952a683d",
        "title": "Rethinking Attention with Performers",
        "venue": "International Conference on Learning Representations",
        "year": 2020,
        "citationCount": 1941,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2009.14794, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1805203",
                "name": "K. Choromanski"
            },
            {
                "authorId": "52314889",
                "name": "Valerii Likhosherstov"
            },
            {
                "authorId": "35363891",
                "name": "David Dohan"
            },
            {
                "authorId": "32725720",
                "name": "Xingyou Song"
            },
            {
                "authorId": "3071104",
                "name": "Andreea Gane"
            },
            {
                "authorId": "2227764",
                "name": "Tams Sarls"
            },
            {
                "authorId": "2052793706",
                "name": "Peter Hawkins"
            },
            {
                "authorId": "29827891",
                "name": "Jared Davis"
            },
            {
                "authorId": "1579862074",
                "name": "Afroz Mohiuddin"
            },
            {
                "authorId": "40527594",
                "name": "Lukasz Kaiser"
            },
            {
                "authorId": "2636941",
                "name": "David Belanger"
            },
            {
                "authorId": "2654847",
                "name": "Lucy J. Colwell"
            },
            {
                "authorId": "145689461",
                "name": "Adrian Weller"
            }
        ],
        "abstract": "We introduce Performers, Transformer architectures which can estimate regular (softmax) full-rank-attention Transformers with provable accuracy, but using only linear (as opposed to quadratic) space and time complexity, without relying on any priors such as sparsity or low-rankness. To approximate softmax attention-kernels, Performers use a novel Fast Attention Via positive Orthogonal Random features approach (FAVOR+), which may be of independent interest for scalable kernel methods. FAVOR+ can be also used to efficiently model kernelizable attention mechanisms beyond softmax. This representational power is crucial to accurately compare softmax with other kernels for the first time on large-scale tasks, beyond the reach of regular Transformers, and investigate optimal attention-kernels. Performers are linear architectures fully compatible with regular Transformers and with strong theoretical guarantees: unbiased or nearly-unbiased estimation of the attention matrix, uniform convergence and low estimation variance. We tested Performers on a rich set of tasks stretching from pixel-prediction through text models to protein sequence modeling. We demonstrate competitive results with other examined efficient sparse and dense attention methods, showcasing effectiveness of the novel attention-learning paradigm leveraged by Performers."
    },
    {
        "paperId": "4083958684292f6fa2f5c7fd4f9be975e80145b6",
        "url": "https://www.semanticscholar.org/paper/4083958684292f6fa2f5c7fd4f9be975e80145b6",
        "title": "GraphCodeBERT: Pre-training Code Representations with Data Flow",
        "venue": "International Conference on Learning Representations",
        "year": 2020,
        "citationCount": 1449,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2009.08366, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2278834796",
                "name": "Daya Guo"
            },
            {
                "authorId": "50052368",
                "name": "Shuo Ren"
            },
            {
                "authorId": "2115338656",
                "name": "Shuai Lu"
            },
            {
                "authorId": "51056532",
                "name": "Zhangyin Feng"
            },
            {
                "authorId": "39483833",
                "name": "Duyu Tang"
            },
            {
                "authorId": "1803054",
                "name": "Shujie Liu"
            },
            {
                "authorId": "2135918679",
                "name": "Long Zhou"
            },
            {
                "authorId": "46429989",
                "name": "Nan Duan"
            },
            {
                "authorId": "144926874",
                "name": "Jian Yin"
            },
            {
                "authorId": "71790825",
                "name": "Daxin Jiang"
            },
            {
                "authorId": "143849609",
                "name": "M. Zhou"
            }
        ],
        "abstract": "Pre-trained models for programming language have achieved dramatic empirical improvements on a variety of code-related tasks such as code search, code completion, code summarization, etc. However, existing pre-trained models regard a code snippet as a sequence of tokens, while ignoring the inherent structure of code, which provides crucial code semantics and would enhance the code understanding process. We present GraphCodeBERT, a pre-trained model for programming language that considers the inherent structure of code. Instead of taking syntactic-level structure of code like abstract syntax tree (AST), we use data flow in the pre-training stage, which is a semantic-level structure of code that encodes the relation of \"where-the-value-comes-from\" between variables. Such a semantic-level structure is neat and does not bring an unnecessarily deep hierarchy of AST, the property of which makes the model more efficient. We develop GraphCodeBERT based on Transformer. In addition to using the task of masked language modeling, we introduce two structure-aware pre-training tasks. One is to predict code structure edges, and the other is to align representations between source code and code structure. We implement the model in an efficient way with a graph-guided masked attention function to incorporate the code structure. We evaluate our model on four tasks, including code search, clone detection, code translation, and code refinement. Results show that code structure and newly introduced pre-training tasks can improve GraphCodeBERT and achieves state-of-the-art performance on the four downstream tasks. We further show that the model prefers structure-level attentions over token-level attentions in the task of code search."
    },
    {
        "paperId": "47c528344fedb6cb67a38e43d095b41c34715330",
        "url": "https://www.semanticscholar.org/paper/47c528344fedb6cb67a38e43d095b41c34715330",
        "title": "Adaptive Federated Optimization",
        "venue": "International Conference on Learning Representations",
        "year": 2020,
        "citationCount": 1741,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2003.00295, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1981186",
                "name": "Sashank J. Reddi"
            },
            {
                "authorId": "143676545",
                "name": "Zachary B. Charles"
            },
            {
                "authorId": "1771307",
                "name": "M. Zaheer"
            },
            {
                "authorId": "40449749",
                "name": "Zachary Garrett"
            },
            {
                "authorId": "1387453057",
                "name": "Keith Rush"
            },
            {
                "authorId": "32139366",
                "name": "Jakub Konecn"
            },
            {
                "authorId": "152663162",
                "name": "Sanjiv Kumar"
            },
            {
                "authorId": "145057514",
                "name": "H. B. McMahan"
            }
        ],
        "abstract": "Federated learning is a distributed machine learning paradigm in which a large number of clients coordinate with a central server to learn a model without sharing their own training data. Due to the heterogeneity of the client datasets, standard federated optimization methods such as Federated Averaging (FedAvg) are often difficult to tune and exhibit unfavorable convergence behavior. In non-federated settings, adaptive optimization methods have had notable success in combating such issues. In this work, we propose federated versions of adaptive optimizers, including Adagrad, Adam, and Yogi, and analyze their convergence in the presence of heterogeneous data for general nonconvex settings. Our results highlight the interplay between client heterogeneity and communication efficiency. We also perform extensive experiments on these methods and show that the use of adaptive optimizers can significantly improve the performance of federated learning."
    },
    {
        "paperId": "633e2fbfc0b21e959a244100937c5853afca4853",
        "url": "https://www.semanticscholar.org/paper/633e2fbfc0b21e959a244100937c5853afca4853",
        "title": "Score-Based Generative Modeling through Stochastic Differential Equations",
        "venue": "International Conference on Learning Representations",
        "year": 2020,
        "citationCount": 8744,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2011.13456, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "115504645",
                "name": "Yang Song"
            },
            {
                "authorId": "1407546424",
                "name": "Jascha Narain Sohl-Dickstein"
            },
            {
                "authorId": "1726807",
                "name": "Diederik P. Kingma"
            },
            {
                "authorId": "2109224633",
                "name": "Abhishek Kumar"
            },
            {
                "authorId": "2490652",
                "name": "Stefano Ermon"
            },
            {
                "authorId": "16443937",
                "name": "Ben Poole"
            }
        ],
        "abstract": "Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. Crucially, the reverse-time SDE depends only on the time-dependent gradient field (\\aka, score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in score-based generative modeling and diffusion probabilistic modeling, allowing for new sampling procedures and new modeling capabilities. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, but additionally enables exact likelihood computation, and improved sampling efficiency. In addition, we provide a new way to solve inverse problems with score-based models, as demonstrated with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 2.99 bits/dim, and demonstrate high fidelity generation of 1024 x 1024 images for the first time from a score-based generative model."
    },
    {
        "paperId": "6a5efb990b6558c21d9fdded4884c00ba152cb7c",
        "url": "https://www.semanticscholar.org/paper/6a5efb990b6558c21d9fdded4884c00ba152cb7c",
        "title": "In Search of Lost Domain Generalization",
        "venue": "International Conference on Learning Representations",
        "year": 2020,
        "citationCount": 1324,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2007.01434, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2708454",
                "name": "Ishaan Gulrajani"
            },
            {
                "authorId": "1401804750",
                "name": "David Lopez-Paz"
            }
        ],
        "abstract": "The goal of domain generalization algorithms is to predict well on distributions different from those seen during training. While a myriad of domain generalization algorithms exist, inconsistencies in experimental conditions -- datasets, architectures, and model selection criteria -- render fair and realistic comparisons difficult. In this paper, we are interested in understanding how useful domain generalization algorithms are in realistic settings. As a first step, we realize that model selection is non-trivial for domain generalization tasks. Contrary to prior work, we argue that domain generalization algorithms without a model selection strategy should be regarded as incomplete. Next, we implement DomainBed, a testbed for domain generalization including seven multi-domain datasets, nine baseline algorithms, and three model selection criteria. We conduct extensive experiments using DomainBed and find that, when carefully implemented, empirical risk minimization shows state-of-the-art performance across all datasets. Looking forward, we hope that the release of DomainBed, along with contributions from fellow researchers, will streamline reproducible and rigorous research in domain generalization."
    },
    {
        "paperId": "6d4a87759917132913319960389f17fa1fe8b630",
        "url": "https://www.semanticscholar.org/paper/6d4a87759917132913319960389f17fa1fe8b630",
        "title": "Fast is better than free: Revisiting adversarial training",
        "venue": "International Conference on Learning Representations",
        "year": 2020,
        "citationCount": 1292,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2001.03994, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "51026953",
                "name": "Eric Wong"
            },
            {
                "authorId": "47260842",
                "name": "Leslie Rice"
            },
            {
                "authorId": "145116464",
                "name": "J. Z. Kolter"
            }
        ],
        "abstract": "Adversarial training, a method for learning robust deep networks, is typically assumed to be more expensive than traditional training due to the necessity of constructing adversarial examples via a first-order method like projected gradient decent (PGD). In this paper, we make the surprising discovery that it is possible to train empirically robust models using a much weaker and cheaper adversary, an approach that was previously believed to be ineffective, rendering the method no more costly than standard training in practice. Specifically, we show that adversarial training with the fast gradient sign method (FGSM), when combined with random initialization, is as effective as PGD-based training but has significantly lower cost. Furthermore we show that FGSM adversarial training can be further accelerated by using standard techniques for efficient training of deep networks, allowing us to learn a robust CIFAR10 classifier with 45% robust accuracy to PGD attacks with $\\epsilon=8/255$ in 6 minutes, and a robust ImageNet classifier with 43% robust accuracy at $\\epsilon=2/255$ in 12 hours, in comparison to past work based on \"free\" adversarial training which took 10 and 50 hours to reach the same respective thresholds. Finally, we identify a failure mode referred to as \"catastrophic overfitting\" which may have caused previous attempts to use FGSM adversarial training to fail. All code for reproducing the experiments in this paper as well as pretrained model weights are at this https URL."
    },
    {
        "paperId": "814a4f680b9ba6baba23b93499f4b48af1a27678",
        "url": "https://www.semanticscholar.org/paper/814a4f680b9ba6baba23b93499f4b48af1a27678",
        "title": "Measuring Massive Multitask Language Understanding",
        "venue": "International Conference on Learning Representations",
        "year": 2020,
        "citationCount": 6435,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2009.03300, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3422872",
                "name": "Dan Hendrycks"
            },
            {
                "authorId": "90909974",
                "name": "Collin Burns"
            },
            {
                "authorId": "104444594",
                "name": "Steven Basart"
            },
            {
                "authorId": "1380103052",
                "name": "Andy Zou"
            },
            {
                "authorId": "16787428",
                "name": "Mantas Mazeika"
            },
            {
                "authorId": "143711382",
                "name": "D. Song"
            },
            {
                "authorId": "5164568",
                "name": "J. Steinhardt"
            }
        ],
        "abstract": "We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have near-random accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model's academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings."
    },
    {
        "paperId": "8bf6c69bae0956db13aa9129fedc69fdc1256dce",
        "url": "https://www.semanticscholar.org/paper/8bf6c69bae0956db13aa9129fedc69fdc1256dce",
        "title": "Prototypical Contrastive Learning of Unsupervised Representations",
        "venue": "International Conference on Learning Representations",
        "year": 2020,
        "citationCount": 1093,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2005.04966, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "49299019",
                "name": "Junnan Li"
            },
            {
                "authorId": "2109890021",
                "name": "Pan Zhou"
            },
            {
                "authorId": "2228109",
                "name": "Caiming Xiong"
            },
            {
                "authorId": "2166511",
                "name": "R. Socher"
            },
            {
                "authorId": "1741126",
                "name": "S. Hoi"
            }
        ],
        "abstract": "This paper presents Prototypical Contrastive Learning (PCL), an unsupervised representation learning method that addresses the fundamental limitations of instance-wise contrastive learning. PCL not only learns low-level features for the task of instance discrimination, but more importantly, it implicitly encodes semantic structures of the data into the learned embedding space. Specifically, we introduce prototypes as latent variables to help find the maximum-likelihood estimation of the network parameters in an Expectation-Maximization framework. We iteratively perform E-step as finding the distribution of prototypes via clustering and M-step as optimizing the network via contrastive learning. We propose ProtoNCE loss, a generalized version of the InfoNCE loss for contrastive learning, which encourages representations to be closer to their assigned prototypes. PCL achieves state-of-the-art results on multiple unsupervised representation learning benchmarks, with >10% accuracy improvement in low-resource transfer tasks. Code is available at this https URL."
    },
    {
        "paperId": "998620638cfc7c6b5d7b95fa8645f75723d78372",
        "url": "https://www.semanticscholar.org/paper/998620638cfc7c6b5d7b95fa8645f75723d78372",
        "title": "Federated Learning with Matched Averaging",
        "venue": "International Conference on Learning Representations",
        "year": 2020,
        "citationCount": 1287,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2002.06440, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2109798334",
                "name": "Hongyi Wang"
            },
            {
                "authorId": "8202372",
                "name": "M. Yurochkin"
            },
            {
                "authorId": "3340442",
                "name": "Yuekai Sun"
            },
            {
                "authorId": "1740595",
                "name": "Dimitris Papailiopoulos"
            },
            {
                "authorId": "2216967",
                "name": "Y. Khazaeni"
            }
        ],
        "abstract": "Federated learning allows edge devices to collaboratively learn a shared model while keeping the training data on device, decoupling the ability to do model training from the need to store the data in the cloud. We propose Federated matched averaging (FedMA) algorithm designed for federated learning of modern neural network architectures e.g. convolutional neural networks (CNNs) and LSTMs. FedMA constructs the shared global model in a layer-wise manner by matching and averaging hidden elements (i.e. channels for convolution layers; hidden states for LSTM; neurons for fully connected layers) with similar feature extraction signatures. Our experiments indicate that FedMA outperforms popular state-of-the-art federated learning algorithms on deep CNN and LSTM architectures trained on real world datasets, while improving the communication efficiency."
    },
    {
        "paperId": "9e20f6874feaaf7c9994f9875b1d9cab17a2fd59",
        "url": "https://www.semanticscholar.org/paper/9e20f6874feaaf7c9994f9875b1d9cab17a2fd59",
        "title": "Learning Mesh-Based Simulation with Graph Networks",
        "venue": "International Conference on Learning Representations",
        "year": 2020,
        "citationCount": 1005,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2010.03409, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2054956",
                "name": "T. Pfaff"
            },
            {
                "authorId": "39067762",
                "name": "Meire Fortunato"
            },
            {
                "authorId": "1398105826",
                "name": "Alvaro Sanchez-Gonzalez"
            },
            {
                "authorId": "2019153",
                "name": "P. Battaglia"
            }
        ],
        "abstract": "Mesh-based simulations are central to modeling complex physical systems in many disciplines across science and engineering. Mesh representations support powerful numerical integration methods and their resolution can be adapted to strike favorable trade-offs between accuracy and efficiency. However, high-dimensional scientific simulations are very expensive to run, and solvers and parameters must often be tuned individually to each system studied. Here we introduce MeshGraphNets, a framework for learning mesh-based simulations using graph neural networks. Our model can be trained to pass messages on a mesh graph and to adapt the mesh discretization during forward simulation. Our results show it can accurately predict the dynamics of a wide range of physical systems, including aerodynamics, structural mechanics, and cloth. The model's adaptivity supports learning resolution-independent dynamics and can scale to more complex state spaces at test time. Our method is also highly efficient, running 1-2 orders of magnitude faster than the simulation on which it is trained. Our approach broadens the range of problems on which neural network simulators can operate and promises to improve the efficiency of complex, scientific modeling tasks."
    },
    {
        "paperId": "a2cd073b57be744533152202989228cb4122270a",
        "url": "https://www.semanticscholar.org/paper/a2cd073b57be744533152202989228cb4122270a",
        "title": "Sharpness-Aware Minimization for Efficiently Improving Generalization",
        "venue": "International Conference on Learning Representations",
        "year": 2020,
        "citationCount": 1650,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2010.01412, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2066217414",
                "name": "Pierre Foret"
            },
            {
                "authorId": "145946311",
                "name": "Ariel Kleiner"
            },
            {
                "authorId": "3232655",
                "name": "H. Mobahi"
            },
            {
                "authorId": "3007442",
                "name": "Behnam Neyshabur"
            }
        ],
        "abstract": "In today's heavily overparameterized models, the value of the training loss provides few guarantees on model generalization ability. Indeed, optimizing only the training loss value, as is commonly done, can easily lead to suboptimal model quality. Motivated by the connection between geometry of the loss landscape and generalization---including a generalization bound that we prove here---we introduce a novel, effective procedure for instead simultaneously minimizing loss value and loss sharpness. In particular, our procedure, Sharpness-Aware Minimization (SAM), seeks parameters that lie in neighborhoods having uniformly low loss; this formulation results in a min-max optimization problem on which gradient descent can be performed efficiently. We present empirical results showing that SAM improves model generalization across a variety of benchmark datasets (e.g., CIFAR-{10, 100}, ImageNet, finetuning tasks) and models, yielding novel state-of-the-art performance for several. Additionally, we find that SAM natively provides robustness to label noise on par with that provided by state-of-the-art procedures that specifically target learning with noisy labels."
    },
    {
        "paperId": "b44bb1762640ed72091fd5f5fdc20719a6dc24af",
        "url": "https://www.semanticscholar.org/paper/b44bb1762640ed72091fd5f5fdc20719a6dc24af",
        "title": "Mastering Atari with Discrete World Models",
        "venue": "International Conference on Learning Representations",
        "year": 2020,
        "citationCount": 1040,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2010.02193, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "35006479",
                "name": "Danijar Hafner"
            },
            {
                "authorId": "2542999",
                "name": "T. Lillicrap"
            },
            {
                "authorId": "144739074",
                "name": "Mohammad Norouzi"
            },
            {
                "authorId": "2503659",
                "name": "Jimmy Ba"
            }
        ],
        "abstract": "Intelligent agents need to generalize from past experience to achieve goals in complex environments. World models facilitate such generalization and allow learning behaviors from imagined outcomes to increase sample-efficiency. While learning world models from image inputs has recently become feasible for some tasks, modeling Atari games accurately enough to derive successful behaviors has remained an open challenge for many years. We introduce DreamerV2, a reinforcement learning agent that learns behaviors purely from predictions in the compact latent space of a powerful world model. The world model uses discrete representations and is trained separately from the policy. DreamerV2 constitutes the first agent that achieves human-level performance on the Atari benchmark of 55 tasks by learning behaviors inside a separately trained world model. With the same computational budget and wall-clock time, DreamerV2 reaches 200M frames and exceeds the final performance of the top single-GPU agents IQN and Rainbow."
    },
    {
        "paperId": "c9b8593db099869fe7254aa1fa53f3c9073b0176",
        "url": "https://www.semanticscholar.org/paper/c9b8593db099869fe7254aa1fa53f3c9073b0176",
        "title": "Approximate Nearest Neighbor Negative Contrastive Learning for Dense Text Retrieval",
        "venue": "International Conference on Learning Representations",
        "year": 2020,
        "citationCount": 1424,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2007.00808, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "101655391",
                "name": "Lee Xiong"
            },
            {
                "authorId": "144628574",
                "name": "Chenyan Xiong"
            },
            {
                "authorId": "2110766301",
                "name": "Ye Li"
            },
            {
                "authorId": "1785396874",
                "name": "Kwok-Fung Tang"
            },
            {
                "authorId": "2108415378",
                "name": "Jialin Liu"
            },
            {
                "authorId": "144609235",
                "name": "Paul N. Bennett"
            },
            {
                "authorId": "144643947",
                "name": "Junaid Ahmed"
            },
            {
                "authorId": "2734525",
                "name": "Arnold Overwijk"
            }
        ],
        "abstract": "Conducting text retrieval in a dense learned representation space has many intriguing advantages over sparse retrieval. Yet the effectiveness of dense retrieval (DR) often requires combination with sparse retrieval. In this paper, we identify that the main bottleneck is in the training mechanisms, where the negative instances used in training are not representative of the irrelevant documents in testing. This paper presents Approximate nearest neighbor Negative Contrastive Estimation (ANCE), a training mechanism that constructs negatives from an Approximate Nearest Neighbor (ANN) index of the corpus, which is parallelly updated with the learning process to select more realistic negative training instances. This fundamentally resolves the discrepancy between the data distribution used in the training and testing of DR. In our experiments, ANCE boosts the BERT-Siamese DR model to outperform all competitive dense and sparse retrieval baselines. It nearly matches the accuracy of sparse-retrieval-and-BERT-reranking using dot-product in the ANCE-learned representation space and provides almost 100x speed-up."
    },
    {
        "paperId": "ce435482acc0e195be8d8f002b2655b4c7b08be6",
        "url": "https://www.semanticscholar.org/paper/ce435482acc0e195be8d8f002b2655b4c7b08be6",
        "title": "DivideMix: Learning with Noisy Labels as Semi-supervised Learning",
        "venue": "International Conference on Learning Representations",
        "year": 2020,
        "citationCount": 1218,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2002.07394, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "49299019",
                "name": "Junnan Li"
            },
            {
                "authorId": "2166511",
                "name": "R. Socher"
            },
            {
                "authorId": "1741126",
                "name": "S. Hoi"
            }
        ],
        "abstract": "Deep neural networks are known to be annotation-hungry. Numerous efforts have been devoted to reducing the annotation cost when learning with deep networks. Two prominent directions include learning with noisy labels and semi-supervised learning by exploiting unlabeled data. In this work, we propose DivideMix, a novel framework for learning with noisy labels by leveraging semi-supervised learning techniques. In particular, DivideMix models the per-sample loss distribution with a mixture model to dynamically divide the training data into a labeled set with clean samples and an unlabeled set with noisy samples, and trains the model on both the labeled and unlabeled data in a semi-supervised manner. To avoid confirmation bias, we simultaneously train two diverged networks where each network uses the dataset division from the other network. During the semi-supervised training phase, we improve the MixMatch strategy by performing label co-refinement and label co-guessing on labeled and unlabeled samples, respectively. Experiments on multiple benchmark datasets demonstrate substantial improvements over state-of-the-art methods. Code is available at this https URL ."
    },
    {
        "paperId": "0d0cf5f64c052aa7edc5bb638203616a620557f6",
        "url": "https://www.semanticscholar.org/paper/0d0cf5f64c052aa7edc5bb638203616a620557f6",
        "title": "VICReg: Variance-Invariance-Covariance Regularization for Self-Supervised Learning",
        "venue": "International Conference on Learning Representations",
        "year": 2021,
        "citationCount": 1103,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2105.04906, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1453740540",
                "name": "Adrien Bardes"
            },
            {
                "authorId": "144189388",
                "name": "J. Ponce"
            },
            {
                "authorId": "1688882",
                "name": "Yann LeCun"
            }
        ],
        "abstract": "Recent self-supervised methods for image representation learning are based on maximizing the agreement between embedding vectors from different views of the same image. A trivial solution is obtained when the encoder outputs constant vectors. This collapse problem is often avoided through implicit biases in the learning architecture, that often lack a clear justification or interpretation. In this paper, we introduce VICReg (Variance-Invariance-Covariance Regularization), a method that explicitly avoids the collapse problem with a simple regularization term on the variance of the embeddings along each dimension individually. VICReg combines the variance term with a decorrelation mechanism based on redundancy reduction and covariance regularization, and achieves results on par with the state of the art on several downstream tasks. In addition, we show that incorporating our new variance term into other methods helps stabilize the training and leads to performance improvements."
    },
    {
        "paperId": "180c78b132f6369a384d22a9529551d86c8788d3",
        "url": "https://www.semanticscholar.org/paper/180c78b132f6369a384d22a9529551d86c8788d3",
        "title": "Tent: Fully Test-Time Adaptation by Entropy Minimization",
        "venue": "International Conference on Learning Representations",
        "year": 2021,
        "citationCount": 1465,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "2118962158",
                "name": "Dequan Wang"
            },
            {
                "authorId": "1782282",
                "name": "Evan Shelhamer"
            },
            {
                "authorId": "1821387",
                "name": "Shaoteng Liu"
            },
            {
                "authorId": "1708655",
                "name": "B. Olshausen"
            },
            {
                "authorId": "1753210",
                "name": "Trevor Darrell"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "2c0f4711c9c124a8dc056eaee82a2ca5ef276da8",
        "url": "https://www.semanticscholar.org/paper/2c0f4711c9c124a8dc056eaee82a2ca5ef276da8",
        "title": "FedBN: Federated Learning on Non-IID Features via Local Batch Normalization",
        "venue": "International Conference on Learning Representations",
        "year": 2021,
        "citationCount": 1033,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2102.07623, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2144456293",
                "name": "Xiaoxiao Li"
            },
            {
                "authorId": "2050138741",
                "name": "Meirui Jiang"
            },
            {
                "authorId": "2108168094",
                "name": "Xiaofei Zhang"
            },
            {
                "authorId": "2065698865",
                "name": "Michael Kamp"
            },
            {
                "authorId": "35647880",
                "name": "Q. Dou"
            }
        ],
        "abstract": "The emerging paradigm of federated learning (FL) strives to enable collaborative training of deep models on the network edge without centrally aggregating raw data and hence improving data privacy. In most cases, the assumption of independent and identically distributed samples across local clients does not hold for federated learning setups. Under this setting, neural network training performance may vary significantly according to the data distribution and even hurt training convergence. Most of the previous work has focused on a difference in the distribution of labels or client shifts. Unlike those settings, we address an important problem of FL, e.g., different scanners/sensors in medical imaging, different scenery distribution in autonomous driving (highway vs. city), where local clients store examples with different distributions compared to other clients, which we denote as feature shift non-iid. In this work, we propose an effective method that uses local batch normalization to alleviate the feature shift before averaging models. The resulting scheme, called FedBN, outperforms both classical FedAvg, as well as the state-of-the-art for non-iid data (FedProx) on our extensive experiments. These empirical results are supported by a convergence analysis that shows in a simplified setting that FedBN has a faster convergence rate than FedAvg. Code is available at https://github.com/med-air/FedBN."
    },
    {
        "paperId": "348a855fe01f3f4273bf0ecf851ca688686dbfcc",
        "url": "https://www.semanticscholar.org/paper/348a855fe01f3f4273bf0ecf851ca688686dbfcc",
        "title": "Offline Reinforcement Learning with Implicit Q-Learning",
        "venue": "International Conference on Learning Representations",
        "year": 2021,
        "citationCount": 1178,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2110.06169, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2000906",
                "name": "Ilya Kostrikov"
            },
            {
                "authorId": "3422774",
                "name": "Ashvin Nair"
            },
            {
                "authorId": "1736651",
                "name": "S. Levine"
            }
        ],
        "abstract": "Offline reinforcement learning requires reconciling two conflicting aims: learning a policy that improves over the behavior policy that collected the dataset, while at the same time minimizing the deviation from the behavior policy so as to avoid errors due to distributional shift. This trade-off is critical, because most current offline reinforcement learning methods need to query the value of unseen actions during training to improve the policy, and therefore need to either constrain these actions to be in-distribution, or else regularize their values. We propose an offline RL method that never needs to evaluate actions outside of the dataset, but still enables the learned policy to improve substantially over the best behavior in the data through generalization. The main insight in our work is that, instead of evaluating unseen actions from the latest policy, we can approximate the policy improvement step implicitly by treating the state value function as a random variable, with randomness determined by the action (while still integrating over the dynamics to avoid excessive optimism), and then taking a state conditional upper expectile of this random variable to estimate the value of the best actions in that state. This leverages the generalization capacity of the function approximator to estimate the value of the best available action at a given state without ever directly querying a Q-function with this unseen action. Our algorithm alternates between fitting this upper expectile value function and backing it up into a Q-function. Then, we extract the policy via advantage-weighted behavioral cloning. We dub our method implicit Q-learning (IQL). IQL demonstrates the state-of-the-art performance on D4RL, a standard benchmark for offline reinforcement learning. We also demonstrate that IQL achieves strong performance fine-tuning using online interaction after offline initialization."
    },
    {
        "paperId": "43a87867fe6bf4eb920f97fc753be4b727308923",
        "url": "https://www.semanticscholar.org/paper/43a87867fe6bf4eb920f97fc753be4b727308923",
        "title": "Towards a Unified View of Parameter-Efficient Transfer Learning",
        "venue": "International Conference on Learning Representations",
        "year": 2021,
        "citationCount": 1088,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2110.04366, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "6215698",
                "name": "Junxian He"
            },
            {
                "authorId": "2384711",
                "name": "Chunting Zhou"
            },
            {
                "authorId": "2378954",
                "name": "Xuezhe Ma"
            },
            {
                "authorId": "1400419309",
                "name": "Taylor Berg-Kirkpatrick"
            },
            {
                "authorId": "1700325",
                "name": "Graham Neubig"
            }
        ],
        "abstract": "Fine-tuning large pre-trained language models on downstream tasks has become the de-facto learning paradigm in NLP. However, conventional approaches fine-tune all the parameters of the pre-trained model, which becomes prohibitive as the model size and the number of tasks grow. Recent work has proposed a variety of parameter-efficient transfer learning methods that only fine-tune a small number of (extra) parameters to attain strong performance. While effective, the critical ingredients for success and the connections among the various methods are poorly understood. In this paper, we break down the design of state-of-the-art parameter-efficient transfer learning methods and present a unified framework that establishes connections between them. Specifically, we re-frame them as modifications to specific hidden states in pre-trained models, and define a set of design dimensions along which different methods vary, such as the function to compute the modification and the position to apply the modification. Through comprehensive empirical studies across machine translation, text summarization, language understanding, and text classification benchmarks, we utilize the unified view to identify important design choices in previous methods. Furthermore, our unified framework enables the transfer of design elements across different approaches, and as a result we are able to instantiate new parameter-efficient fine-tuning methods that tune less parameters than previous methods while being more effective, achieving comparable results to fine-tuning all parameters on all four tasks."
    },
    {
        "paperId": "722ad6ac92286507437b31486f47987d6ece05c9",
        "url": "https://www.semanticscholar.org/paper/722ad6ac92286507437b31486f47987d6ece05c9",
        "title": "BEiT: BERT Pre-Training of Image Transformers",
        "venue": "International Conference on Learning Representations",
        "year": 2021,
        "citationCount": 3366,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2106.08254, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "10699417",
                "name": "Hangbo Bao"
            },
            {
                "authorId": "145307652",
                "name": "Li Dong"
            },
            {
                "authorId": "49807919",
                "name": "Furu Wei"
            }
        ],
        "abstract": "We introduce a self-supervised vision representation model BEiT, which stands for Bidirectional Encoder representation from Image Transformers. Following BERT developed in the natural language processing area, we propose a masked image modeling task to pretrain vision Transformers. Specifically, each image has two views in our pre-training, i.e, image patches (such as 16x16 pixels), and visual tokens (i.e., discrete tokens). We first\"tokenize\"the original image into visual tokens. Then we randomly mask some image patches and fed them into the backbone Transformer. The pre-training objective is to recover the original visual tokens based on the corrupted image patches. After pre-training BEiT, we directly fine-tune the model parameters on downstream tasks by appending task layers upon the pretrained encoder. Experimental results on image classification and semantic segmentation show that our model achieves competitive results with previous pre-training methods. For example, base-size BEiT achieves 83.2% top-1 accuracy on ImageNet-1K, significantly outperforming from-scratch DeiT training (81.8%) with the same setup. Moreover, large-size BEiT obtains 86.3% only using ImageNet-1K, even outperforming ViT-L with supervised pre-training on ImageNet-22K (85.2%). The code and pretrained models are available at https://aka.ms/beit."
    },
    {
        "paperId": "972706306f85b1bfb40c7d35c796ad5174eb0c9c",
        "url": "https://www.semanticscholar.org/paper/972706306f85b1bfb40c7d35c796ad5174eb0c9c",
        "title": "DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing",
        "venue": "International Conference on Learning Representations",
        "year": 2021,
        "citationCount": 1561,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2111.09543, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "50462546",
                "name": "Pengcheng He"
            },
            {
                "authorId": "48441311",
                "name": "Jianfeng Gao"
            },
            {
                "authorId": "2109136147",
                "name": "Weizhu Chen"
            }
        ],
        "abstract": "This paper presents a new pre-trained language model, DeBERTaV3, which improves the original DeBERTa model by replacing mask language modeling (MLM) with replaced token detection (RTD), a more sample-efficient pre-training task. Our analysis shows that vanilla embedding sharing in ELECTRA hurts training efficiency and model performance. This is because the training losses of the discriminator and the generator pull token embeddings in different directions, creating the\"tug-of-war\"dynamics. We thus propose a new gradient-disentangled embedding sharing method that avoids the tug-of-war dynamics, improving both training efficiency and the quality of the pre-trained model. We have pre-trained DeBERTaV3 using the same settings as DeBERTa to demonstrate its exceptional performance on a wide range of downstream natural language understanding (NLU) tasks. Taking the GLUE benchmark with eight tasks as an example, the DeBERTaV3 Large model achieves a 91.37% average score, which is 1.37% over DeBERTa and 1.91% over ELECTRA, setting a new state-of-the-art (SOTA) among the models with a similar structure. Furthermore, we have pre-trained a multi-lingual model mDeBERTa and observed a larger improvement over strong baselines compared to English models. For example, the mDeBERTa Base achieves a 79.8% zero-shot cross-lingual accuracy on XNLI and a 3.6% improvement over XLM-R Base, creating a new SOTA on this benchmark. We have made our pre-trained models and inference code publicly available at https://github.com/microsoft/DeBERTa."
    },
    {
        "paperId": "a8ca46b171467ceb2d7652fbfb67fe701ad86092",
        "url": "https://www.semanticscholar.org/paper/a8ca46b171467ceb2d7652fbfb67fe701ad86092",
        "title": "LoRA: Low-Rank Adaptation of Large Language Models",
        "venue": "International Conference on Learning Representations",
        "year": 2021,
        "citationCount": 15140,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2106.09685, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2157840220",
                "name": "J. E. Hu"
            },
            {
                "authorId": "1752875",
                "name": "Yelong Shen"
            },
            {
                "authorId": "104100507",
                "name": "Phillip Wallis"
            },
            {
                "authorId": "1388725932",
                "name": "Zeyuan Allen-Zhu"
            },
            {
                "authorId": "2110486765",
                "name": "Yuanzhi Li"
            },
            {
                "authorId": "2135571585",
                "name": "Shean Wang"
            },
            {
                "authorId": "2109136147",
                "name": "Weizhu Chen"
            }
        ],
        "abstract": "An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA."
    },
    {
        "paperId": "ab30672c8c5e4787f6a5985f26a8f281f0db2fb8",
        "url": "https://www.semanticscholar.org/paper/ab30672c8c5e4787f6a5985f26a8f281f0db2fb8",
        "title": "How Attentive are Graph Attention Networks?",
        "venue": "International Conference on Learning Representations",
        "year": 2021,
        "citationCount": 1447,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2105.14491, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1720739223",
                "name": "Shaked Brody"
            },
            {
                "authorId": "47051926",
                "name": "Uri Alon"
            },
            {
                "authorId": "1743232",
                "name": "Eran Yahav"
            }
        ],
        "abstract": "Graph Attention Networks (GATs) are one of the most popular GNN architectures and are considered as the state-of-the-art architecture for representation learning with graphs. In GAT, every node attends to its neighbors given its own representation as the query. However, in this paper we show that GAT computes a very limited kind of attention: the ranking of the attention scores is unconditioned on the query node. We formally define this restricted kind of attention as static attention and distinguish it from a strictly more expressive dynamic attention. Because GATs use a static attention mechanism, there are simple graph problems that GAT cannot express: in a controlled problem, we show that static attention hinders GAT from even fitting the training data. To remove this limitation, we introduce a simple fix by modifying the order of operations and propose GATv2: a dynamic graph attention variant that is strictly more expressive than GAT. We perform an extensive evaluation and show that GATv2 outperforms GAT across 11 OGB and other benchmarks while we match their parametric costs. Our code is available at https://github.com/tech-srl/how_attentive_are_gats . GATv2 is available as part of the PyTorch Geometric library, the Deep Graph Library, and the TensorFlow GNN library."
    },
    {
        "paperId": "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51",
        "url": "https://www.semanticscholar.org/paper/ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51",
        "title": "Efficiently Modeling Long Sequences with Structured State Spaces",
        "venue": "International Conference on Learning Representations",
        "year": 2021,
        "citationCount": 2799,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2111.00396, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "39499001",
                "name": "Albert Gu"
            },
            {
                "authorId": "1822288",
                "name": "Karan Goel"
            },
            {
                "authorId": "2061444681",
                "name": "Christopher R'e"
            }
        ],
        "abstract": "A central goal of sequence modeling is designing a single principled model that can address sequence data across a range of modalities and tasks, particularly on long-range dependencies. Although conventional models including RNNs, CNNs, and Transformers have specialized variants for capturing long dependencies, they still struggle to scale to very long sequences of $10000$ or more steps. A promising recent approach proposed modeling sequences by simulating the fundamental state space model (SSM) \\( x'(t) = Ax(t) + Bu(t), y(t) = Cx(t) + Du(t) \\), and showed that for appropriate choices of the state matrix \\( A \\), this system could handle long-range dependencies mathematically and empirically. However, this method has prohibitive computation and memory requirements, rendering it infeasible as a general sequence modeling solution. We propose the Structured State Space sequence model (S4) based on a new parameterization for the SSM, and show that it can be computed much more efficiently than prior approaches while preserving their theoretical strengths. Our technique involves conditioning \\( A \\) with a low-rank correction, allowing it to be diagonalized stably and reducing the SSM to the well-studied computation of a Cauchy kernel. S4 achieves strong empirical results across a diverse range of established benchmarks, including (i) 91\\% accuracy on sequential CIFAR-10 with no data augmentation or auxiliary losses, on par with a larger 2-D ResNet, (ii) substantially closing the gap to Transformers on image and language modeling tasks, while performing generation $60\\times$ faster (iii) SoTA on every task from the Long Range Arena benchmark, including solving the challenging Path-X task of length 16k that all prior work fails on, while being as efficient as all competitors."
    },
    {
        "paperId": "cf9b8da26d9b92e75ba49616ed2a1033f59fce14",
        "url": "https://www.semanticscholar.org/paper/cf9b8da26d9b92e75ba49616ed2a1033f59fce14",
        "title": "Open-vocabulary Object Detection via Vision and Language Knowledge Distillation",
        "venue": "International Conference on Learning Representations",
        "year": 2021,
        "citationCount": 1142,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2104.13921, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "10734287",
                "name": "Xiuye Gu"
            },
            {
                "authorId": "2115356502",
                "name": "Tsung-Yi Lin"
            },
            {
                "authorId": "7987770",
                "name": "Weicheng Kuo"
            },
            {
                "authorId": "50355189",
                "name": "Yin Cui"
            }
        ],
        "abstract": "We aim at advancing open-vocabulary object detection, which detects objects described by arbitrary text inputs. The fundamental challenge is the availability of training data. It is costly to further scale up the number of classes contained in existing object detection datasets. To overcome this challenge, we propose ViLD, a training method via Vision and Language knowledge Distillation. Our method distills the knowledge from a pretrained open-vocabulary image classification model (teacher) into a two-stage detector (student). Specifically, we use the teacher model to encode category texts and image regions of object proposals. Then we train a student detector, whose region embeddings of detected boxes are aligned with the text and image embeddings inferred by the teacher. We benchmark on LVIS by holding out all rare categories as novel categories that are not seen during training. ViLD obtains 16.1 mask AP$_r$ with a ResNet-50 backbone, even outperforming the supervised counterpart by 3.8. When trained with a stronger teacher model ALIGN, ViLD achieves 26.3 AP$_r$. The model can directly transfer to other datasets without finetuning, achieving 72.2 AP$_{50}$ on PASCAL VOC, 36.6 AP on COCO and 11.8 AP on Objects365. On COCO, ViLD outperforms the previous state-of-the-art by 4.8 on novel AP and 11.4 on overall AP. Code and demo are open-sourced at https://github.com/tensorflow/tpu/tree/master/models/official/detection/projects/vild."
    },
    {
        "paperId": "da74a10824193be9d3889ce0d6ed4c6f8ee48b9e",
        "url": "https://www.semanticscholar.org/paper/da74a10824193be9d3889ce0d6ed4c6f8ee48b9e",
        "title": "MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer",
        "venue": "International Conference on Learning Representations",
        "year": 2021,
        "citationCount": 1848,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2110.02178, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "144839857",
                "name": "Sachin Mehta"
            },
            {
                "authorId": "143887493",
                "name": "Mohammad Rastegari"
            }
        ],
        "abstract": "Light-weight convolutional neural networks (CNNs) are the de-facto for mobile vision tasks. Their spatial inductive biases allow them to learn representations with fewer parameters across different vision tasks. However, these networks are spatially local. To learn global representations, self-attention-based vision trans-formers (ViTs) have been adopted. Unlike CNNs, ViTs are heavy-weight. In this paper, we ask the following question: is it possible to combine the strengths of CNNs and ViTs to build a light-weight and low latency network for mobile vision tasks? Towards this end, we introduce MobileViT, a light-weight and general-purpose vision transformer for mobile devices. MobileViT presents a different perspective for the global processing of information with transformers, i.e., transformers as convolutions. Our results show that MobileViT significantly outperforms CNN- and ViT-based networks across different tasks and datasets. On the ImageNet-1k dataset, MobileViT achieves top-1 accuracy of 78.4% with about 6 million parameters, which is 3.2% and 6.2% more accurate than MobileNetv3 (CNN-based) and DeIT (ViT-based) for a similar number of parameters. On the MS-COCO object detection task, MobileViT is 5.7% more accurate than MobileNetv3 for a similar number of parameters. Our source code is open-source and available at: https://github.com/apple/ml-cvnets"
    },
    {
        "paperId": "f671a09e3e5922e6d38cb77dda8d76d5ceac2a27",
        "url": "https://www.semanticscholar.org/paper/f671a09e3e5922e6d38cb77dda8d76d5ceac2a27",
        "title": "SDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations",
        "venue": "International Conference on Learning Representations",
        "year": 2021,
        "citationCount": 1882,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2108.01073, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "83262128",
                "name": "Chenlin Meng"
            },
            {
                "authorId": "1490933536",
                "name": "Yutong He"
            },
            {
                "authorId": "2157995251",
                "name": "Yang Song"
            },
            {
                "authorId": "51453887",
                "name": "Jiaming Song"
            },
            {
                "authorId": "3045089",
                "name": "Jiajun Wu"
            },
            {
                "authorId": "1922024303",
                "name": "Jun-Yan Zhu"
            },
            {
                "authorId": "2490652",
                "name": "Stefano Ermon"
            }
        ],
        "abstract": "Guided image synthesis enables everyday users to create and edit photo-realistic images with minimum effort. The key challenge is balancing faithfulness to the user input (e.g., hand-drawn colored strokes) and realism of the synthesized image. Existing GAN-based methods attempt to achieve such balance using either conditional GANs or GAN inversions, which are challenging and often require additional training data or loss functions for individual applications. To address these issues, we introduce a new image synthesis and editing method, Stochastic Differential Editing (SDEdit), based on a diffusion model generative prior, which synthesizes realistic images by iteratively denoising through a stochastic differential equation (SDE). Given an input image with user guide of any type, SDEdit first adds noise to the input, then subsequently denoises the resulting image through the SDE prior to increase its realism. SDEdit does not require task-specific training or inversions and can naturally achieve the balance between realism and faithfulness. SDEdit significantly outperforms state-of-the-art GAN-based methods by up to 98.09% on realism and 91.72% on overall satisfaction scores, according to a human perception study, on multiple tasks, including stroke-based image synthesis and editing as well as image compositing."
    },
    {
        "paperId": "ff0b2681d7b05e16c46dfb71d980cc2f605907cd",
        "url": "https://www.semanticscholar.org/paper/ff0b2681d7b05e16c46dfb71d980cc2f605907cd",
        "title": "Finetuned Language Models Are Zero-Shot Learners",
        "venue": "International Conference on Learning Representations",
        "year": 2021,
        "citationCount": 4573,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2109.01652, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "144026731",
                "name": "Jason Wei"
            },
            {
                "authorId": "40377863",
                "name": "Maarten Bosma"
            },
            {
                "authorId": "2664737",
                "name": "Vincent Zhao"
            },
            {
                "authorId": "2091768",
                "name": "Kelvin Guu"
            },
            {
                "authorId": "40625240",
                "name": "Adams Wei Yu"
            },
            {
                "authorId": "144104130",
                "name": "Brian Lester"
            },
            {
                "authorId": "2140321952",
                "name": "Nan Du"
            },
            {
                "authorId": "2555924",
                "name": "Andrew M. Dai"
            },
            {
                "authorId": "2827616",
                "name": "Quoc V. Le"
            }
        ],
        "abstract": "This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning -- finetuning language models on a collection of tasks described via instructions -- substantially improves zero-shot performance on unseen tasks. We take a 137B parameter pretrained language model and instruction-tune it on over 60 NLP tasks verbalized via natural language instruction templates. We evaluate this instruction-tuned model, which we call FLAN, on unseen task types. FLAN substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 tasks that we evaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number of finetuning datasets, model scale, and natural language instructions are key to the success of instruction tuning."
    },
    {
        "paperId": "004f1d2b1b7d7dcecafdd94daee9c1b0aa3e65cf",
        "url": "https://www.semanticscholar.org/paper/004f1d2b1b7d7dcecafdd94daee9c1b0aa3e65cf",
        "title": "DAB-DETR: Dynamic Anchor Boxes are Better Queries for DETR",
        "venue": "International Conference on Learning Representations",
        "year": 2022,
        "citationCount": 1058,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2201.12329, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "8602739",
                "name": "Shilong Liu"
            },
            {
                "authorId": "2152978390",
                "name": "Feng Li"
            },
            {
                "authorId": "2315254849",
                "name": "Hao Zhang"
            },
            {
                "authorId": "2156963158",
                "name": "X. Yang"
            },
            {
                "authorId": "2689287",
                "name": "Xianbiao Qi"
            },
            {
                "authorId": "2093561216",
                "name": "Hang Su"
            },
            {
                "authorId": "2146280157",
                "name": "Jun Zhu"
            },
            {
                "authorId": "2152834943",
                "name": "Lei Zhang"
            }
        ],
        "abstract": "We present in this paper a novel query formulation using dynamic anchor boxes for DETR (DEtection TRansformer) and offer a deeper understanding of the role of queries in DETR. This new formulation directly uses box coordinates as queries in Transformer decoders and dynamically updates them layer-by-layer. Using box coordinates not only helps using explicit positional priors to improve the query-to-feature similarity and eliminate the slow training convergence issue in DETR, but also allows us to modulate the positional attention map using the box width and height information. Such a design makes it clear that queries in DETR can be implemented as performing soft ROI pooling layer-by-layer in a cascade manner. As a result, it leads to the best performance on MS-COCO benchmark among the DETR-like detection models under the same setting, e.g., AP 45.7\\% using ResNet50-DC5 as backbone trained in 50 epochs. We also conducted extensive experiments to confirm our analysis and verify the effectiveness of our methods. Code is available at \\url{https://github.com/SlongLiu/DAB-DETR}."
    },
    {
        "paperId": "04e541391e8dce14d099d00fb2c21dbbd8afe87f",
        "url": "https://www.semanticscholar.org/paper/04e541391e8dce14d099d00fb2c21dbbd8afe87f",
        "title": "Prompt-to-Prompt Image Editing with Cross Attention Control",
        "venue": "International Conference on Learning Representations",
        "year": 2022,
        "citationCount": 2305,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2208.01626",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2208.01626, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "51437320",
                "name": "Amir Hertz"
            },
            {
                "authorId": "147940380",
                "name": "Ron Mokady"
            },
            {
                "authorId": "144592244",
                "name": "J. Tenenbaum"
            },
            {
                "authorId": "3451442",
                "name": "Kfir Aberman"
            },
            {
                "authorId": "1782328",
                "name": "Y. Pritch"
            },
            {
                "authorId": "1388323541",
                "name": "D. Cohen-Or"
            }
        ],
        "abstract": "Recent large-scale text-driven synthesis models have attracted much attention thanks to their remarkable capabilities of generating highly diverse images that follow given text prompts. Such text-based synthesis methods are particularly appealing to humans who are used to verbally describe their intent. Therefore, it is only natural to extend the text-driven image synthesis to text-driven image editing. Editing is challenging for these generative models, since an innate property of an editing technique is to preserve most of the original image, while in the text-based models, even a small modification of the text prompt often leads to a completely different outcome. State-of-the-art methods mitigate this by requiring the users to provide a spatial mask to localize the edit, hence, ignoring the original structure and content within the masked region. In this paper, we pursue an intuitive prompt-to-prompt editing framework, where the edits are controlled by text only. To this end, we analyze a text-conditioned model in depth and observe that the cross-attention layers are the key to controlling the relation between the spatial layout of the image to each word in the prompt. With this observation, we present several applications which monitor the image synthesis by editing the textual prompt only. This includes localized editing by replacing a word, global editing by adding a specification, and even delicately controlling the extent to which a word is reflected in the image. We present our results over diverse images and prompts, demonstrating high-quality synthesis and fidelity to the edited prompts."
    },
    {
        "paperId": "15736f7c205d961c00378a938daffaacb5a0718d",
        "url": "https://www.semanticscholar.org/paper/15736f7c205d961c00378a938daffaacb5a0718d",
        "title": "Human Motion Diffusion Model",
        "venue": "International Conference on Learning Representations",
        "year": 2022,
        "citationCount": 1027,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2209.14916",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2209.14916, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "81493694",
                "name": "Guy Tevet"
            },
            {
                "authorId": "32317571",
                "name": "Sigal Raab"
            },
            {
                "authorId": "2158660812",
                "name": "Brian Gordon"
            },
            {
                "authorId": "2186404533",
                "name": "Yonatan Shafir"
            },
            {
                "authorId": "2186405166",
                "name": "Daniel Cohen-Or"
            },
            {
                "authorId": "1755628",
                "name": "Amit H. Bermano"
            }
        ],
        "abstract": "Natural and expressive human motion generation is the holy grail of computer animation. It is a challenging task, due to the diversity of possible motion, human perceptual sensitivity to it, and the difficulty of accurately describing it. Therefore, current generative solutions are either low-quality or limited in expressiveness. Diffusion models, which have already shown remarkable generative capabilities in other domains, are promising candidates for human motion due to their many-to-many nature, but they tend to be resource hungry and hard to control. In this paper, we introduce Motion Diffusion Model (MDM), a carefully adapted classifier-free diffusion-based generative model for the human motion domain. MDM is transformer-based, combining insights from motion generation literature. A notable design-choice is the prediction of the sample, rather than the noise, in each diffusion step. This facilitates the use of established geometric losses on the locations and velocities of the motion, such as the foot contact loss. As we demonstrate, MDM is a generic approach, enabling different modes of conditioning, and different generation tasks. We show that our model is trained with lightweight resources and yet achieves state-of-the-art results on leading benchmarks for text-to-motion and action-to-motion. https://guytevet.github.io/mdm-page/ ."
    },
    {
        "paperId": "1d26c947406173145a4665dd7ab255e03494ea28",
        "url": "https://www.semanticscholar.org/paper/1d26c947406173145a4665dd7ab255e03494ea28",
        "title": "GLM-130B: An Open Bilingual Pre-trained Model",
        "venue": "International Conference on Learning Representations",
        "year": 2022,
        "citationCount": 1211,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2210.02414",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2210.02414, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2051712753",
                "name": "Aohan Zeng"
            },
            {
                "authorId": "2111312892",
                "name": "Xiao Liu"
            },
            {
                "authorId": "66395694",
                "name": "Zhengxiao Du"
            },
            {
                "authorId": null,
                "name": "Zihan Wang"
            },
            {
                "authorId": "2051311700",
                "name": "Hanyu Lai"
            },
            {
                "authorId": "145573466",
                "name": "Ming Ding"
            },
            {
                "authorId": "2109506541",
                "name": "Zhuoyi Yang"
            },
            {
                "authorId": "2125063007",
                "name": "Yifan Xu"
            },
            {
                "authorId": "2163967642",
                "name": "Wendi Zheng"
            },
            {
                "authorId": "2186982651",
                "name": "Xiao Xia"
            },
            {
                "authorId": "1403621152",
                "name": "W. Tam"
            },
            {
                "authorId": "2124489983",
                "name": "Zixuan Ma"
            },
            {
                "authorId": "2114921664",
                "name": "Yufei Xue"
            },
            {
                "authorId": "2467444",
                "name": "Jidong Zhai"
            },
            {
                "authorId": "1712168",
                "name": "Wenguang Chen"
            },
            {
                "authorId": "47243067",
                "name": "P. Zhang"
            },
            {
                "authorId": "2047998",
                "name": "Yuxiao Dong"
            },
            {
                "authorId": "2148911956",
                "name": "Jie Tang"
            }
        ],
        "abstract": "We introduce GLM-130B, a bilingual (English and Chinese) pre-trained language model with 130 billion parameters. It is an attempt to open-source a 100B-scale model at least as good as GPT-3 (davinci) and unveil how models of such a scale can be successfully pre-trained. Over the course of this effort, we face numerous unexpected technical and engineering challenges, particularly on loss spikes and divergence. In this paper, we introduce the training process of GLM-130B including its design choices, training strategies for both efficiency and stability, and engineering efforts. The resultant GLM-130B model offers significant outperformance over GPT-3 175B (davinci) on a wide range of popular English benchmarks while the performance advantage is not observed in OPT-175B and BLOOM-176B. It also consistently and significantly outperforms ERNIE TITAN 3.0 260B -- the largest Chinese language model -- across related benchmarks. Finally, we leverage a unique scaling property of GLM-130B to reach INT4 quantization without post training, with almost no performance loss, making it the first among 100B-scale models and more importantly, allowing its effective inference on 4$\\times$RTX 3090 (24G) or 8$\\times$RTX 2080 Ti (11G) GPUs, the most affordable GPUs required for using 100B-scale models. The GLM-130B model weights are publicly accessible and its code, training logs, related toolkit, and lessons learned are open-sourced at \\url{https://github.com/THUDM/GLM-130B/}."
    },
    {
        "paperId": "1e33716e8820b867d5a8aaebab44c2d3135ea4ac",
        "url": "https://www.semanticscholar.org/paper/1e33716e8820b867d5a8aaebab44c2d3135ea4ac",
        "title": "Make-A-Video: Text-to-Video Generation without Text-Video Data",
        "venue": "International Conference on Learning Representations",
        "year": 2022,
        "citationCount": 1768,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2209.14792, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "88622696",
                "name": "Uriel Singer"
            },
            {
                "authorId": "33964593",
                "name": "Adam Polyak"
            },
            {
                "authorId": "2161662704",
                "name": "Thomas Hayes"
            },
            {
                "authorId": "1557290137",
                "name": "Xiaoyue Yin"
            },
            {
                "authorId": "1733982458",
                "name": "Jie An"
            },
            {
                "authorId": "3178508",
                "name": "Songyang Zhang"
            },
            {
                "authorId": "12459700",
                "name": "Qiyuan Hu"
            },
            {
                "authorId": "2110162580",
                "name": "Harry Yang"
            },
            {
                "authorId": "1388005058",
                "name": "Oron Ashual"
            },
            {
                "authorId": "90840812",
                "name": "Oran Gafni"
            },
            {
                "authorId": "153432684",
                "name": "Devi Parikh"
            },
            {
                "authorId": "2118343423",
                "name": "Sonal Gupta"
            },
            {
                "authorId": "2188620",
                "name": "Yaniv Taigman"
            }
        ],
        "abstract": "We propose Make-A-Video -- an approach for directly translating the tremendous recent progress in Text-to-Image (T2I) generation to Text-to-Video (T2V). Our intuition is simple: learn what the world looks like and how it is described from paired text-image data, and learn how the world moves from unsupervised video footage. Make-A-Video has three advantages: (1) it accelerates training of the T2V model (it does not need to learn visual and multimodal representations from scratch), (2) it does not require paired text-video data, and (3) the generated videos inherit the vastness (diversity in aesthetic, fantastical depictions, etc.) of today's image generation models. We design a simple yet effective way to build on T2I models with novel and effective spatial-temporal modules. First, we decompose the full temporal U-Net and attention tensors and approximate them in space and time. Second, we design a spatial temporal pipeline to generate high resolution and frame rate videos with a video decoder, interpolation model and two super resolution models that can enable various applications besides T2V. In all aspects, spatial and temporal resolution, faithfulness to text, and quality, Make-A-Video sets the new state-of-the-art in text-to-video generation, as determined by both qualitative and quantitative measures."
    },
    {
        "paperId": "244054a4254a2147e43a3dad9c124b9b7eb4a04a",
        "url": "https://www.semanticscholar.org/paper/244054a4254a2147e43a3dad9c124b9b7eb4a04a",
        "title": "Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow",
        "venue": "International Conference on Learning Representations",
        "year": 2022,
        "citationCount": 1943,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2209.03003",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2209.03003, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "46521757",
                "name": "Xingchao Liu"
            },
            {
                "authorId": "29777869",
                "name": "Chengyue Gong"
            },
            {
                "authorId": "2165566517",
                "name": "Qiang Liu"
            }
        ],
        "abstract": "We present rectified flow, a surprisingly simple approach to learning (neural) ordinary differential equation (ODE) models to transport between two empirically observed distributions \\pi_0 and \\pi_1, hence providing a unified solution to generative modeling and domain transfer, among various other tasks involving distribution transport. The idea of rectified flow is to learn the ODE to follow the straight paths connecting the points drawn from \\pi_0 and \\pi_1 as much as possible. This is achieved by solving a straightforward nonlinear least squares optimization problem, which can be easily scaled to large models without introducing extra parameters beyond standard supervised learning. The straight paths are special and preferred because they are the shortest paths between two points, and can be simulated exactly without time discretization and hence yield computationally efficient models. We show that the procedure of learning a rectified flow from data, called rectification, turns an arbitrary coupling of \\pi_0 and \\pi_1 to a new deterministic coupling with provably non-increasing convex transport costs. In addition, recursively applying rectification allows us to obtain a sequence of flows with increasingly straight paths, which can be simulated accurately with coarse time discretization in the inference phase. In empirical studies, we show that rectified flow performs superbly on image generation, image-to-image translation, and domain adaptation. In particular, on image generation and translation, our method yields nearly straight flows that give high quality results even with a single Euler discretization step."
    },
    {
        "paperId": "38115e80d805fb0fb8f090dc88ced4b24be07878",
        "url": "https://www.semanticscholar.org/paper/38115e80d805fb0fb8f090dc88ced4b24be07878",
        "title": "CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis",
        "venue": "International Conference on Learning Representations",
        "year": 2022,
        "citationCount": 1341,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2203.13474, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2043490",
                "name": "Erik Nijkamp"
            },
            {
                "authorId": "2063096824",
                "name": "Bo Pang"
            },
            {
                "authorId": "50376014",
                "name": "Hiroaki Hayashi"
            },
            {
                "authorId": "3376969",
                "name": "Lifu Tu"
            },
            {
                "authorId": "46507194",
                "name": "Haiquan Wang"
            },
            {
                "authorId": "2118860628",
                "name": "Yingbo Zhou"
            },
            {
                "authorId": "1702137",
                "name": "S. Savarese"
            },
            {
                "authorId": "2054594326",
                "name": "Caiming Xiong"
            }
        ],
        "abstract": "Program synthesis strives to generate a computer program as a solution to a given problem specification, expressed with input-output examples or natural language descriptions. The prevalence of large language models advances the state-of-the-art for program synthesis, though limited training resources and data impede open access to such models. To democratize this, we train and release a family of large language models up to 16.1B parameters, called CODEGEN, on natural language and programming language data, and open source the training library JAXFORMER. We show the utility of the trained model by demonstrating that it is competitive with the previous state-of-the-art on zero-shot Python code generation on HumanEval. We further investigate the multi-step paradigm for program synthesis, where a single program is factorized into multiple prompts specifying subproblems. To this end, we construct an open benchmark, Multi-Turn Programming Benchmark (MTPB), consisting of 115 diverse problem sets that are factorized into multi-turn prompts. Our analysis on MTPB shows that the same intent provided to CODEGEN in multi-turn fashion significantly improves program synthesis over that provided as a single turn. We make the training library JAXFORMER and model checkpoints available as open source contribution: https://github.com/salesforce/CodeGen."
    },
    {
        "paperId": "4610ffb1b016acaa82a2065ffd1a3adbae1ce722",
        "url": "https://www.semanticscholar.org/paper/4610ffb1b016acaa82a2065ffd1a3adbae1ce722",
        "title": "Large Language Models Are Human-Level Prompt Engineers",
        "venue": "International Conference on Learning Representations",
        "year": 2022,
        "citationCount": 1155,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2211.01910",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2211.01910, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2189664830",
                "name": "Yongchao Zhou"
            },
            {
                "authorId": "2189543935",
                "name": "Andrei Ioan Muresanu"
            },
            {
                "authorId": "2157194659",
                "name": "Ziwen Han"
            },
            {
                "authorId": "73775191",
                "name": "Keiran Paster"
            },
            {
                "authorId": "32305445",
                "name": "Silviu Pitis"
            },
            {
                "authorId": "41228532",
                "name": "Harris Chan"
            },
            {
                "authorId": "2503659",
                "name": "Jimmy Ba"
            }
        ],
        "abstract": "By conditioning on natural language instructions, large language models (LLMs) have displayed impressive capabilities as general-purpose computers. However, task performance depends significantly on the quality of the prompt used to steer the model, and most effective prompts have been handcrafted by humans. Inspired by classical program synthesis and the human approach to prompt engineering, we propose Automatic Prompt Engineer (APE) for automatic instruction generation and selection. In our method, we treat the instruction as the\"program,\"optimized by searching over a pool of instruction candidates proposed by an LLM in order to maximize a chosen score function. To evaluate the quality of the selected instruction, we evaluate the zero-shot performance of another LLM following the selected instruction. Experiments on 24 NLP tasks show that our automatically generated instructions outperform the prior LLM baseline by a large margin and achieve better or comparable performance to the instructions generated by human annotators on 19/24 tasks. We conduct extensive qualitative and quantitative analyses to explore the performance of APE. We show that APE-engineered prompts can be applied to steer models toward truthfulness and/or informativeness, as well as to improve few-shot learning performance by simply prepending them to standard in-context learning prompts. Please check out our webpage at https://sites.google.com/view/automatic-prompt-engineer."
    },
    {
        "paperId": "47696145b3f88c4cc3f3c22035286b5d7ebce09d",
        "url": "https://www.semanticscholar.org/paper/47696145b3f88c4cc3f3c22035286b5d7ebce09d",
        "title": "TimesNet: Temporal 2D-Variation Modeling for General Time Series Analysis",
        "venue": "International Conference on Learning Representations",
        "year": 2022,
        "citationCount": 1478,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2210.02186",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2210.02186, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2051867856",
                "name": "Haixu Wu"
            },
            {
                "authorId": "2112912801",
                "name": "Teng Hu"
            },
            {
                "authorId": "2144386151",
                "name": "Yong Liu"
            },
            {
                "authorId": "2171667749",
                "name": "Hang Zhou"
            },
            {
                "authorId": "2144499343",
                "name": "Jianmin Wang"
            },
            {
                "authorId": "2054275000",
                "name": "Mingsheng Long"
            }
        ],
        "abstract": "Time series analysis is of immense importance in extensive applications, such as weather forecasting, anomaly detection, and action recognition. This paper focuses on temporal variation modeling, which is the common key problem of extensive analysis tasks. Previous methods attempt to accomplish this directly from the 1D time series, which is extremely challenging due to the intricate temporal patterns. Based on the observation of multi-periodicity in time series, we ravel out the complex temporal variations into the multiple intraperiod- and interperiod-variations. To tackle the limitations of 1D time series in representation capability, we extend the analysis of temporal variations into the 2D space by transforming the 1D time series into a set of 2D tensors based on multiple periods. This transformation can embed the intraperiod- and interperiod-variations into the columns and rows of the 2D tensors respectively, making the 2D-variations to be easily modeled by 2D kernels. Technically, we propose the TimesNet with TimesBlock as a task-general backbone for time series analysis. TimesBlock can discover the multi-periodicity adaptively and extract the complex temporal variations from transformed 2D tensors by a parameter-efficient inception block. Our proposed TimesNet achieves consistent state-of-the-art in five mainstream time series analysis tasks, including short- and long-term forecasting, imputation, classification, and anomaly detection. Code is available at this repository: https://github.com/thuml/TimesNet."
    },
    {
        "paperId": "4c94d04afa4309ec2f06bdd0fe3781f91461b362",
        "url": "https://www.semanticscholar.org/paper/4c94d04afa4309ec2f06bdd0fe3781f91461b362",
        "title": "DreamFusion: Text-to-3D using 2D Diffusion",
        "venue": "International Conference on Learning Representations",
        "year": 2022,
        "citationCount": 3116,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2209.14988",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2209.14988, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "16443937",
                "name": "Ben Poole"
            },
            {
                "authorId": "50658699",
                "name": "Ajay Jain"
            },
            {
                "authorId": "50329510",
                "name": "J. Barron"
            },
            {
                "authorId": "2577533",
                "name": "B. Mildenhall"
            }
        ],
        "abstract": "Recent breakthroughs in text-to-image synthesis have been driven by diffusion models trained on billions of image-text pairs. Adapting this approach to 3D synthesis would require large-scale datasets of labeled 3D data and efficient architectures for denoising 3D data, neither of which currently exist. In this work, we circumvent these limitations by using a pretrained 2D text-to-image diffusion model to perform text-to-3D synthesis. We introduce a loss based on probability density distillation that enables the use of a 2D diffusion model as a prior for optimization of a parametric image generator. Using this loss in a DeepDream-like procedure, we optimize a randomly-initialized 3D model (a Neural Radiance Field, or NeRF) via gradient descent such that its 2D renderings from random angles achieve a low loss. The resulting 3D model of the given text can be viewed from any angle, relit by arbitrary illumination, or composited into any 3D environment. Our approach requires no 3D training data and no modifications to the image diffusion model, demonstrating the effectiveness of pretrained image diffusion models as priors."
    },
    {
        "paperId": "5406129d9d7d00dc310671c43597101b0ee93629",
        "url": "https://www.semanticscholar.org/paper/5406129d9d7d00dc310671c43597101b0ee93629",
        "title": "An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion",
        "venue": "International Conference on Learning Representations",
        "year": 2022,
        "citationCount": 2415,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2208.01618",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2208.01618, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "134639223",
                "name": "Rinon Gal"
            },
            {
                "authorId": "1850630812",
                "name": "Yuval Alaluf"
            },
            {
                "authorId": "34815079",
                "name": "Y. Atzmon"
            },
            {
                "authorId": "2819477",
                "name": "Or Patashnik"
            },
            {
                "authorId": "1755628",
                "name": "Amit H. Bermano"
            },
            {
                "authorId": "1732280",
                "name": "Gal Chechik"
            },
            {
                "authorId": "1388323541",
                "name": "D. Cohen-Or"
            }
        ],
        "abstract": "Text-to-image models offer unprecedented freedom to guide creation through natural language. Yet, it is unclear how such freedom can be exercised to generate images of specific unique concepts, modify their appearance, or compose them in new roles and novel scenes. In other words, we ask: how can we use language-guided models to turn our cat into a painting, or imagine a new product based on our favorite toy? Here we present a simple approach that allows such creative freedom. Using only 3-5 images of a user-provided concept, like an object or a style, we learn to represent it through new\"words\"in the embedding space of a frozen text-to-image model. These\"words\"can be composed into natural language sentences, guiding personalized creation in an intuitive way. Notably, we find evidence that a single word embedding is sufficient for capturing unique and varied concepts. We compare our approach to a wide range of baselines, and demonstrate that it can more faithfully portray the concepts across a range of applications and tasks. Our code, data and new words will be available at: https://textual-inversion.github.io"
    },
    {
        "paperId": "5437e8adab596d7294124c0e798708e050e25321",
        "url": "https://www.semanticscholar.org/paper/5437e8adab596d7294124c0e798708e050e25321",
        "title": "Least-to-Most Prompting Enables Complex Reasoning in Large Language Models",
        "venue": "International Conference on Learning Representations",
        "year": 2022,
        "citationCount": 1469,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2205.10625",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2205.10625, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "65855107",
                "name": "Denny Zhou"
            },
            {
                "authorId": "1821614764",
                "name": "Nathanael Scharli"
            },
            {
                "authorId": "2153400663",
                "name": "Le Hou"
            },
            {
                "authorId": "119640649",
                "name": "Jason Wei"
            },
            {
                "authorId": "1471909492",
                "name": "Nathan Scales"
            },
            {
                "authorId": "2275277634",
                "name": "Xuezhi Wang"
            },
            {
                "authorId": "50319359",
                "name": "D. Schuurmans"
            },
            {
                "authorId": "1698617",
                "name": "O. Bousquet"
            },
            {
                "authorId": "1998340269",
                "name": "Quoc Le"
            },
            {
                "authorId": "2226805",
                "name": "Ed H. Chi"
            }
        ],
        "abstract": "Chain-of-thought prompting has demonstrated remarkable performance on various natural language reasoning tasks. However, it tends to perform poorly on tasks which requires solving problems harder than the exemplars shown in the prompts. To overcome this challenge of easy-to-hard generalization, we propose a novel prompting strategy, least-to-most prompting. The key idea in this strategy is to break down a complex problem into a series of simpler subproblems and then solve them in sequence. Solving each subproblem is facilitated by the answers to previously solved subproblems. Our experimental results on tasks related to symbolic manipulation, compositional generalization, and math reasoning reveal that least-to-most prompting is capable of generalizing to more difficult problems than those seen in the prompts. A notable finding is that when the GPT-3 code-davinci-002 model is used with least-to-most prompting, it can solve the compositional generalization benchmark SCAN in any split (including length split) with an accuracy of at least 99% using just 14 exemplars, compared to only 16% accuracy with chain-of-thought prompting. This is particularly noteworthy because neural-symbolic models in the literature that specialize in solving SCAN are trained on the entire training set containing over 15,000 examples. We have included prompts for all the tasks in the Appendix."
    },
    {
        "paperId": "5f19ae1135a9500940978104ec15a5b8751bc7d2",
        "url": "https://www.semanticscholar.org/paper/5f19ae1135a9500940978104ec15a5b8751bc7d2",
        "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models",
        "venue": "International Conference on Learning Representations",
        "year": 2022,
        "citationCount": 5411,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2203.11171, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2275277634",
                "name": "Xuezhi Wang"
            },
            {
                "authorId": "119640649",
                "name": "Jason Wei"
            },
            {
                "authorId": "50319359",
                "name": "D. Schuurmans"
            },
            {
                "authorId": "1998340269",
                "name": "Quoc Le"
            },
            {
                "authorId": "2226805",
                "name": "Ed H. Chi"
            },
            {
                "authorId": "65855107",
                "name": "Denny Zhou"
            }
        ],
        "abstract": "Chain-of-thought prompting combined with pre-trained large language models has achieved encouraging results on complex reasoning tasks. In this paper, we propose a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting. It first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths. Self-consistency leverages the intuition that a complex reasoning problem typically admits multiple different ways of thinking leading to its unique correct answer. Our extensive empirical evaluation shows that self-consistency boosts the performance of chain-of-thought prompting with a striking margin on a range of popular arithmetic and commonsense reasoning benchmarks, including GSM8K (+17.9%), SVAMP (+11.0%), AQuA (+12.2%), StrategyQA (+6.4%) and ARC-challenge (+3.9%)."
    },
    {
        "paperId": "61e46884567be7cad12e999365b16a8d3414b678",
        "url": "https://www.semanticscholar.org/paper/61e46884567be7cad12e999365b16a8d3414b678",
        "title": "Diffusion Posterior Sampling for General Noisy Inverse Problems",
        "venue": "International Conference on Learning Representations",
        "year": 2022,
        "citationCount": 1223,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2209.14687",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2209.14687, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2110872233",
                "name": "Hyungjin Chung"
            },
            {
                "authorId": "2109216792",
                "name": "Jeongsol Kim"
            },
            {
                "authorId": "2179304",
                "name": "Michael T. McCann"
            },
            {
                "authorId": "83695651",
                "name": "M. Klasky"
            },
            {
                "authorId": "2998762",
                "name": "J. C. Ye"
            }
        ],
        "abstract": "Diffusion models have been recently studied as powerful generative inverse problem solvers, owing to their high quality reconstructions and the ease of combining existing iterative solvers. However, most works focus on solving simple linear inverse problems in noiseless settings, which significantly under-represents the complexity of real-world problems. In this work, we extend diffusion solvers to efficiently handle general noisy (non)linear inverse problems via approximation of the posterior sampling. Interestingly, the resulting posterior sampling scheme is a blended version of diffusion sampling with the manifold constrained gradient without a strict measurement consistency projection step, yielding a more desirable generative path in noisy settings compared to the previous studies. Our method demonstrates that diffusion models can incorporate various measurement noise statistics such as Gaussian and Poisson, and also efficiently handle noisy nonlinear inverse problems such as Fourier phase retrieval and non-uniform deblurring. Code available at https://github.com/DPS2022/diffusion-posterior-sampling"
    },
    {
        "paperId": "99832586d55f540f603637e458a292406a0ed75d",
        "url": "https://www.semanticscholar.org/paper/99832586d55f540f603637e458a292406a0ed75d",
        "title": "ReAct: Synergizing Reasoning and Acting in Language Models",
        "venue": "International Conference on Learning Representations",
        "year": 2022,
        "citationCount": 5064,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2210.03629, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2093302161",
                "name": "Shunyu Yao"
            },
            {
                "authorId": "2144551262",
                "name": "Jeffrey Zhao"
            },
            {
                "authorId": "150978762",
                "name": "Dian Yu"
            },
            {
                "authorId": "2140321952",
                "name": "Nan Du"
            },
            {
                "authorId": "1697494",
                "name": "Izhak Shafran"
            },
            {
                "authorId": "144958935",
                "name": "Karthik Narasimhan"
            },
            {
                "authorId": "145144022",
                "name": "Yuan Cao"
            }
        ],
        "abstract": "While large language models (LLMs) have demonstrated impressive capabilities across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well as improved human interpretability and trustworthiness over methods without reasoning or acting components. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generates human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples. Project site with code: https://react-lm.github.io"
    },
    {
        "paperId": "9dc481ec44178e797466bbad968071917842156b",
        "url": "https://www.semanticscholar.org/paper/9dc481ec44178e797466bbad968071917842156b",
        "title": "DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection",
        "venue": "International Conference on Learning Representations",
        "year": 2022,
        "citationCount": 2160,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2203.03605",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2203.03605, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2315254849",
                "name": "Hao Zhang"
            },
            {
                "authorId": "2146312758",
                "name": "Feng Li"
            },
            {
                "authorId": "8602739",
                "name": "Shilong Liu"
            },
            {
                "authorId": "2152834943",
                "name": "Lei Zhang"
            },
            {
                "authorId": "2093561216",
                "name": "Hang Su"
            },
            {
                "authorId": "89006344",
                "name": "Jun-Juan Zhu"
            },
            {
                "authorId": "1726587",
                "name": "L. Ni"
            },
            {
                "authorId": "93596028",
                "name": "H. Shum"
            }
        ],
        "abstract": "We present DINO (\\textbf{D}ETR with \\textbf{I}mproved de\\textbf{N}oising anch\\textbf{O}r boxes), a state-of-the-art end-to-end object detector. % in this paper. DINO improves over previous DETR-like models in performance and efficiency by using a contrastive way for denoising training, a mixed query selection method for anchor initialization, and a look forward twice scheme for box prediction. DINO achieves $49.4$AP in $12$ epochs and $51.3$AP in $24$ epochs on COCO with a ResNet-50 backbone and multi-scale features, yielding a significant improvement of $\\textbf{+6.0}$\\textbf{AP} and $\\textbf{+2.7}$\\textbf{AP}, respectively, compared to DN-DETR, the previous best DETR-like model. DINO scales well in both model size and data size. Without bells and whistles, after pre-training on the Objects365 dataset with a SwinL backbone, DINO obtains the best results on both COCO \\texttt{val2017} ($\\textbf{63.2}$\\textbf{AP}) and \\texttt{test-dev} (\\textbf{$\\textbf{63.3}$AP}). Compared to other models on the leaderboard, DINO significantly reduces its model size and pre-training data size while achieving better results. Our code will be available at \\url{https://github.com/IDEACVR/DINO}."
    },
    {
        "paperId": "af68f10ab5078bfc519caae377c90ee6d9c504e9",
        "url": "https://www.semanticscholar.org/paper/af68f10ab5078bfc519caae377c90ee6d9c504e9",
        "title": "Flow Matching for Generative Modeling",
        "venue": "International Conference on Learning Representations",
        "year": 2022,
        "citationCount": 2802,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2210.02747, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3232072",
                "name": "Y. Lipman"
            },
            {
                "authorId": "51466615",
                "name": "Ricky T. Q. Chen"
            },
            {
                "authorId": "1405681873",
                "name": "Heli Ben-Hamu"
            },
            {
                "authorId": "1729762",
                "name": "Maximilian Nickel"
            },
            {
                "authorId": "123542315",
                "name": "Matt Le"
            }
        ],
        "abstract": "We introduce a new paradigm for generative modeling built on Continuous Normalizing Flows (CNFs), allowing us to train CNFs at unprecedented scale. Specifically, we present the notion of Flow Matching (FM), a simulation-free approach for training CNFs based on regressing vector fields of fixed conditional probability paths. Flow Matching is compatible with a general family of Gaussian probability paths for transforming between noise and data samples -- which subsumes existing diffusion paths as specific instances. Interestingly, we find that employing FM with diffusion paths results in a more robust and stable alternative for training diffusion models. Furthermore, Flow Matching opens the door to training CNFs with other, non-diffusion probability paths. An instance of particular interest is using Optimal Transport (OT) displacement interpolation to define the conditional probability paths. These paths are more efficient than diffusion paths, provide faster training and sampling, and result in better generalization. Training CNFs using Flow Matching on ImageNet leads to consistently better performance than alternative diffusion-based methods in terms of both likelihood and sample quality, and allows fast and reliable sample generation using off-the-shelf numerical ODE solvers."
    },
    {
        "paperId": "dad15404d372a23b4b3bf9a63b3124693df3c85e",
        "url": "https://www.semanticscholar.org/paper/dad15404d372a23b4b3bf9a63b3124693df3c85e",
        "title": "A Time Series is Worth 64 Words: Long-term Forecasting with Transformers",
        "venue": "International Conference on Learning Representations",
        "year": 2022,
        "citationCount": 2481,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2211.14730",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2211.14730, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "152972535",
                "name": "Yuqi Nie"
            },
            {
                "authorId": "144547425",
                "name": "Nam H. Nguyen"
            },
            {
                "authorId": "40913517",
                "name": "Phanwadee Sinthong"
            },
            {
                "authorId": "1682581",
                "name": "J. Kalagnanam"
            }
        ],
        "abstract": "We propose an efficient design of Transformer-based models for multivariate time series forecasting and self-supervised representation learning. It is based on two key components: (i) segmentation of time series into subseries-level patches which are served as input tokens to Transformer; (ii) channel-independence where each channel contains a single univariate time series that shares the same embedding and Transformer weights across all the series. Patching design naturally has three-fold benefit: local semantic information is retained in the embedding; computation and memory usage of the attention maps are quadratically reduced given the same look-back window; and the model can attend longer history. Our channel-independent patch time series Transformer (PatchTST) can improve the long-term forecasting accuracy significantly when compared with that of SOTA Transformer-based models. We also apply our model to self-supervised pre-training tasks and attain excellent fine-tuning performance, which outperforms supervised training on large datasets. Transferring of masked pre-trained representation on one dataset to others also produces SOTA forecasting accuracy. Code is available at: https://github.com/yuqinie98/PatchTST."
    },
    {
        "paperId": "08a80cb34d785258c770acecd302ab41ead46eed",
        "url": "https://www.semanticscholar.org/paper/08a80cb34d785258c770acecd302ab41ead46eed",
        "title": "WizardLM: Empowering Large Pre-Trained Language Models to Follow Complex Instructions",
        "venue": "International Conference on Learning Representations",
        "year": 2023,
        "citationCount": 1142,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2304.12244, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "46747953",
                "name": "Can Xu"
            },
            {
                "authorId": "2112549330",
                "name": "Qingfeng Sun"
            },
            {
                "authorId": "2052687880",
                "name": "Kai Zheng"
            },
            {
                "authorId": "2442662",
                "name": "Xiubo Geng"
            },
            {
                "authorId": "2215218765",
                "name": "Pu Zhao"
            },
            {
                "authorId": "147062881",
                "name": "Jiazhan Feng"
            },
            {
                "authorId": "8801869",
                "name": "Chongyang Tao"
            },
            {
                "authorId": "2086994543",
                "name": "Daxin Jiang"
            }
        ],
        "abstract": "Training large language models (LLMs) with open-domain instruction following data brings colossal success. However, manually creating such instruction data is very time-consuming and labor-intensive. Moreover, humans may struggle to produce high-complexity instructions. In this paper, we show an avenue for creating large amounts of instruction data with varying levels of complexity using LLM instead of humans. Starting with an initial set of instructions, we use our proposed Evol-Instruct to rewrite them step by step into more complex instructions. Then, we mix all generated instruction data to fine-tune LLaMA. We call the resulting model WizardLM. Human evaluations on a complexity-balanced test bed and Vicuna's testset show that instructions from Evol-Instruct are superior to human-created ones. By analyzing the human evaluation results of the high complexity part, we demonstrate that outputs from our WizardLM are preferred to outputs from OpenAI ChatGPT. In GPT-4 automatic evaluation, WizardLM achieves more than 90\\% capacity of ChatGPT on 17 out of 29 skills. Even though WizardLM still lags behind ChatGPT in some aspects, our findings suggest that fine-tuning with AI-evolved instructions is a promising direction for enhancing LLMs. Our code and data are public at https://github.com/nlpxucan/WizardLM"
    },
    {
        "paperId": "0bfc804e31eecfd77f45e4ee7f4d629fffdcd628",
        "url": "https://www.semanticscholar.org/paper/0bfc804e31eecfd77f45e4ee7f4d629fffdcd628",
        "title": "ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs",
        "venue": "International Conference on Learning Representations",
        "year": 2023,
        "citationCount": 1070,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2307.16789",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2307.16789, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "50625437",
                "name": "Yujia Qin"
            },
            {
                "authorId": "2163374235",
                "name": "Shi Liang"
            },
            {
                "authorId": "2114059497",
                "name": "Yining Ye"
            },
            {
                "authorId": "2214586034",
                "name": "Kunlun Zhu"
            },
            {
                "authorId": "2214613855",
                "name": "Lan Yan"
            },
            {
                "authorId": "2191753738",
                "name": "Ya-Ting Lu"
            },
            {
                "authorId": "2149202150",
                "name": "Yankai Lin"
            },
            {
                "authorId": "2214579778",
                "name": "Xin Cong"
            },
            {
                "authorId": "47274259",
                "name": "Xiangru Tang"
            },
            {
                "authorId": "2226120351",
                "name": "Bill Qian"
            },
            {
                "authorId": "2226184989",
                "name": "Sihan Zhao"
            },
            {
                "authorId": "2214603370",
                "name": "Runchu Tian"
            },
            {
                "authorId": "3360722",
                "name": "Ruobing Xie"
            },
            {
                "authorId": "49640256",
                "name": "Jie Zhou"
            },
            {
                "authorId": "152573911",
                "name": "Marc H. Gerstein"
            },
            {
                "authorId": "2144118403",
                "name": "Dahai Li"
            },
            {
                "authorId": "2141313179",
                "name": "Zhiyuan Liu"
            },
            {
                "authorId": "1753344",
                "name": "Maosong Sun"
            }
        ],
        "abstract": "Despite the advancements of open-source large language models (LLMs), e.g., LLaMA, they remain significantly limited in tool-use capabilities, i.e., using external tools (APIs) to fulfill human instructions. The reason is that current instruction tuning largely focuses on basic language tasks but ignores the tool-use domain. This is in contrast to the excellent tool-use capabilities of state-of-the-art (SOTA) closed-source LLMs, e.g., ChatGPT. To bridge this gap, we introduce ToolLLM, a general tool-use framework encompassing data construction, model training, and evaluation. We first present ToolBench, an instruction-tuning dataset for tool use, which is constructed automatically using ChatGPT. Specifically, the construction can be divided into three stages: (i) API collection: we collect 16,464 real-world RESTful APIs spanning 49 categories from RapidAPI Hub; (ii) instruction generation: we prompt ChatGPT to generate diverse instructions involving these APIs, covering both single-tool and multi-tool scenarios; (iii) solution path annotation: we use ChatGPT to search for a valid solution path (chain of API calls) for each instruction. To enhance the reasoning capabilities of LLMs, we develop a novel depth-first search-based decision tree algorithm. It enables LLMs to evaluate multiple reasoning traces and expand the search space. Moreover, to evaluate the tool-use capabilities of LLMs, we develop an automatic evaluator: ToolEval. Based on ToolBench, we fine-tune LLaMA to obtain an LLM ToolLLaMA, and equip it with a neural API retriever to recommend appropriate APIs for each instruction. Experiments show that ToolLLaMA demonstrates a remarkable ability to execute complex instructions and generalize to unseen APIs, and exhibits comparable performance to ChatGPT. Our ToolLLaMA also demonstrates strong zero-shot generalization ability in an out-of-distribution tool-use dataset: APIBench."
    },
    {
        "paperId": "3b6179c293df29e31d31cea46476f104ab6950f2",
        "url": "https://www.semanticscholar.org/paper/3b6179c293df29e31d31cea46476f104ab6950f2",
        "title": "Kosmos-2: Grounding Multimodal Large Language Models to the World",
        "venue": "International Conference on Learning Representations",
        "year": 2023,
        "citationCount": 1013,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2306.14824",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2306.14824, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2087004998",
                "name": "Zhiliang Peng"
            },
            {
                "authorId": "51456429",
                "name": "Wenhui Wang"
            },
            {
                "authorId": "145307652",
                "name": "Li Dong"
            },
            {
                "authorId": "34128716",
                "name": "Y. Hao"
            },
            {
                "authorId": "3110003",
                "name": "Shaohan Huang"
            },
            {
                "authorId": "2118866998",
                "name": "Shuming Ma"
            },
            {
                "authorId": "49807919",
                "name": "Furu Wei"
            }
        ],
        "abstract": "We introduce Kosmos-2, a Multimodal Large Language Model (MLLM), enabling new capabilities of perceiving object descriptions (e.g., bounding boxes) and grounding text to the visual world. Specifically, we represent refer expressions as links in Markdown, i.e., ``[text span](bounding boxes)'', where object descriptions are sequences of location tokens. Together with multimodal corpora, we construct large-scale data of grounded image-text pairs (called GrIT) to train the model. In addition to the existing capabilities of MLLMs (e.g., perceiving general modalities, following instructions, and performing in-context learning), Kosmos-2 integrates the grounding capability into downstream applications. We evaluate Kosmos-2 on a wide range of tasks, including (i) multimodal grounding, such as referring expression comprehension, and phrase grounding, (ii) multimodal referring, such as referring expression generation, (iii) perception-language tasks, and (iv) language understanding and generation. This work lays out the foundation for the development of Embodiment AI and sheds light on the big convergence of language, multimodal perception, action, and world modeling, which is a key step toward artificial general intelligence. Code and pretrained models are available at https://aka.ms/kosmos-2."
    },
    {
        "paperId": "823ca4778e1027f2f0b356df051d762dcecaaba0",
        "url": "https://www.semanticscholar.org/paper/823ca4778e1027f2f0b356df051d762dcecaaba0",
        "title": "FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning",
        "venue": "International Conference on Learning Representations",
        "year": 2023,
        "citationCount": 2039,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2307.08691",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2307.08691, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "24593911",
                "name": "Tri Dao"
            }
        ],
        "abstract": "Scaling Transformers to longer sequence lengths has been a major problem in the last several years, promising to improve performance in language modeling and high-resolution image understanding, as well as to unlock new applications in code, audio, and video generation. The attention layer is the main bottleneck in scaling to longer sequences, as its runtime and memory increase quadratically in the sequence length. FlashAttention exploits the asymmetric GPU memory hierarchy to bring significant memory saving (linear instead of quadratic) and runtime speedup (2-4$\\times$ compared to optimized baselines), with no approximation. However, FlashAttention is still not nearly as fast as optimized matrix-multiply (GEMM) operations, reaching only 25-40\\% of the theoretical maximum FLOPs/s. We observe that the inefficiency is due to suboptimal work partitioning between different thread blocks and warps on the GPU, causing either low-occupancy or unnecessary shared memory reads/writes. We propose FlashAttention-2, with better work partitioning to address these issues. In particular, we (1) tweak the algorithm to reduce the number of non-matmul FLOPs (2) parallelize the attention computation, even for a single head, across different thread blocks to increase occupancy, and (3) within each thread block, distribute the work between warps to reduce communication through shared memory. These yield around 2$\\times$ speedup compared to FlashAttention, reaching 50-73\\% of the theoretical maximum FLOPs/s on A100 and getting close to the efficiency of GEMM operations. We empirically validate that when used end-to-end to train GPT-style models, FlashAttention-2 reaches training speed of up to 225 TFLOPs/s per A100 GPU (72\\% model FLOPs utilization)."
    },
    {
        "paperId": "8946891e94831adc8cddb0d32311cce2445c96d2",
        "url": "https://www.semanticscholar.org/paper/8946891e94831adc8cddb0d32311cce2445c96d2",
        "title": "MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts",
        "venue": "International Conference on Learning Representations",
        "year": 2023,
        "citationCount": 1128,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.02255, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2887562",
                "name": "Pan Lu"
            },
            {
                "authorId": "103404553",
                "name": "Hritik Bansal"
            },
            {
                "authorId": "2143749775",
                "name": "Tony Xia"
            },
            {
                "authorId": "2144174497",
                "name": "Jiacheng Liu"
            },
            {
                "authorId": "2109738542",
                "name": "Chun-yue Li"
            },
            {
                "authorId": "2548384",
                "name": "Hannaneh Hajishirzi"
            },
            {
                "authorId": "47413820",
                "name": "Hao Cheng"
            },
            {
                "authorId": "2256646491",
                "name": "Kai-Wei Chang"
            },
            {
                "authorId": "2253458981",
                "name": "Michel Galley"
            },
            {
                "authorId": "2256227183",
                "name": "Jianfeng Gao"
            }
        ],
        "abstract": "Large Language Models (LLMs) and Large Multimodal Models (LMMs) exhibit impressive problem-solving skills in many tasks and domains, but their ability in mathematical reasoning in visual contexts has not been systematically studied. To bridge this gap, we present MathVista, a benchmark designed to combine challenges from diverse mathematical and visual tasks. It consists of 6,141 examples, derived from 28 existing multimodal datasets involving mathematics and 3 newly created datasets (i.e., IQTest, FunctionQA, and PaperQA). Completing these tasks requires fine-grained, deep visual understanding and compositional reasoning, which all state-of-the-art foundation models find challenging. With MathVista, we have conducted a comprehensive, quantitative evaluation of 12 prominent foundation models. The best-performing GPT-4V model achieves an overall accuracy of 49.9%, substantially outperforming Bard, the second-best performer, by 15.1%. Our in-depth analysis reveals that the superiority of GPT-4V is mainly attributed to its enhanced visual perception and mathematical reasoning. However, GPT-4V still falls short of human performance by 10.4%, as it often struggles to understand complex figures and perform rigorous reasoning. This significant gap underscores the critical role that MathVista will play in the development of general-purpose AI agents capable of tackling mathematically intensive and visually rich real-world tasks. We further explore the new ability of self-verification, the application of self-consistency, and the interactive chatbot capabilities of GPT-4V, highlighting its promising potential for future research. The project is available at https://mathvista.github.io/."
    },
    {
        "paperId": "94a5f96308729e31c1ffbc0f0618db87795092fe",
        "url": "https://www.semanticscholar.org/paper/94a5f96308729e31c1ffbc0f0618db87795092fe",
        "title": "SWE-bench: Can Language Models Resolve Real-World GitHub Issues?",
        "venue": "International Conference on Learning Representations",
        "year": 2023,
        "citationCount": 1255,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2310.06770",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.06770, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2155269683",
                "name": "Carlos E. Jimenez"
            },
            {
                "authorId": "2109727379",
                "name": "John Yang"
            },
            {
                "authorId": "2127066887",
                "name": "Alexander Wettig"
            },
            {
                "authorId": "2256984659",
                "name": "Shunyu Yao"
            },
            {
                "authorId": "2256993083",
                "name": "Kexin Pei"
            },
            {
                "authorId": "40170001",
                "name": "Ofir Press"
            },
            {
                "authorId": "2135381714",
                "name": "Karthik Narasimhan"
            }
        ],
        "abstract": "Language models have outpaced our ability to evaluate them effectively, but for their future development it is essential to study the frontier of their capabilities. We find real-world software engineering to be a rich, sustainable, and challenging testbed for evaluating the next generation of language models. To this end, we introduce SWE-bench, an evaluation framework consisting of $2,294$ software engineering problems drawn from real GitHub issues and corresponding pull requests across $12$ popular Python repositories. Given a codebase along with a description of an issue to be resolved, a language model is tasked with editing the codebase to address the issue. Resolving issues in SWE-bench frequently requires understanding and coordinating changes across multiple functions, classes, and even files simultaneously, calling for models to interact with execution environments, process extremely long contexts and perform complex reasoning that goes far beyond traditional code generation tasks. Our evaluations show that both state-of-the-art proprietary models and our fine-tuned model SWE-Llama can resolve only the simplest issues. The best-performing model, Claude 2, is able to solve a mere $1.96$% of the issues. Advances on SWE-bench represent steps towards LMs that are more practical, intelligent, and autonomous."
    },
    {
        "paperId": "afeeb8f5018eebb1a1d334b94dbbfc48d167efef",
        "url": "https://www.semanticscholar.org/paper/afeeb8f5018eebb1a1d334b94dbbfc48d167efef",
        "title": "iTransformer: Inverted Transformers Are Effective for Time Series Forecasting",
        "venue": "International Conference on Learning Representations",
        "year": 2023,
        "citationCount": 1159,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2310.06625",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.06625, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2257375992",
                "name": "Yong Liu"
            },
            {
                "authorId": "2203368846",
                "name": "Tengge Hu"
            },
            {
                "authorId": "2257340579",
                "name": "Haoran Zhang"
            },
            {
                "authorId": "2051867856",
                "name": "Haixu Wu"
            },
            {
                "authorId": "2255363760",
                "name": "Shiyu Wang"
            },
            {
                "authorId": "2253908414",
                "name": "Lintao Ma"
            },
            {
                "authorId": "2054275000",
                "name": "Mingsheng Long"
            }
        ],
        "abstract": "The recent boom of linear forecasting models questions the ongoing passion for architectural modifications of Transformer-based forecasters. These forecasters leverage Transformers to model the global dependencies over temporal tokens of time series, with each token formed by multiple variates of the same timestamp. However, Transformers are challenged in forecasting series with larger lookback windows due to performance degradation and computation explosion. Besides, the embedding for each temporal token fuses multiple variates that represent potential delayed events and distinct physical measurements, which may fail in learning variate-centric representations and result in meaningless attention maps. In this work, we reflect on the competent duties of Transformer components and repurpose the Transformer architecture without any modification to the basic components. We propose iTransformer that simply applies the attention and feed-forward network on the inverted dimensions. Specifically, the time points of individual series are embedded into variate tokens which are utilized by the attention mechanism to capture multivariate correlations; meanwhile, the feed-forward network is applied for each variate token to learn nonlinear representations. The iTransformer model achieves state-of-the-art on challenging real-world datasets, which further empowers the Transformer family with promoted performance, generalization ability across different variates, and better utilization of arbitrary lookback windows, making it a nice alternative as the fundamental backbone of time series forecasting. Code is available at this repository: https://github.com/thuml/iTransformer."
    },
    {
        "paperId": "be8db99310602d66bba64bcf41a572c45816fbfc",
        "url": "https://www.semanticscholar.org/paper/be8db99310602d66bba64bcf41a572c45816fbfc",
        "title": "Let's Verify Step by Step",
        "venue": "International Conference on Learning Representations",
        "year": 2023,
        "citationCount": 2161,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2305.20050",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.20050, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2219360905",
                "name": "H. Lightman"
            },
            {
                "authorId": "13622184",
                "name": "Vineet Kosaraju"
            },
            {
                "authorId": "51178856",
                "name": "Yura Burda"
            },
            {
                "authorId": "144632352",
                "name": "Harrison Edwards"
            },
            {
                "authorId": "40566201",
                "name": "Bowen Baker"
            },
            {
                "authorId": "2110664120",
                "name": "Teddy Lee"
            },
            {
                "authorId": "2990741",
                "name": "Jan Leike"
            },
            {
                "authorId": "47971768",
                "name": "John Schulman"
            },
            {
                "authorId": "1701686",
                "name": "I. Sutskever"
            },
            {
                "authorId": "6062736",
                "name": "Karl Cobbe"
            }
        ],
        "abstract": "In recent years, large language models have greatly improved in their ability to perform complex multi-step reasoning. However, even state-of-the-art models still regularly produce logical mistakes. To train more reliable models, we can turn either to outcome supervision, which provides feedback for a final result, or process supervision, which provides feedback for each intermediate reasoning step. Given the importance of training reliable models, and given the high cost of human feedback, it is important to carefully compare the both methods. Recent work has already begun this comparison, but many questions still remain. We conduct our own investigation, finding that process supervision significantly outperforms outcome supervision for training models to solve problems from the challenging MATH dataset. Our process-supervised model solves 78% of problems from a representative subset of the MATH test set. Additionally, we show that active learning significantly improves the efficacy of process supervision. To support related research, we also release PRM800K, the complete dataset of 800,000 step-level human feedback labels used to train our best reward model."
    },
    {
        "paperId": "c1caa303549764d220ff17dc1785985dd1ba6047",
        "url": "https://www.semanticscholar.org/paper/c1caa303549764d220ff17dc1785985dd1ba6047",
        "title": "AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning",
        "venue": "International Conference on Learning Representations",
        "year": 2023,
        "citationCount": 1255,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2307.04725",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2307.04725, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2188212675",
                "name": "Yuwei Guo"
            },
            {
                "authorId": "49984891",
                "name": "Ceyuan Yang"
            },
            {
                "authorId": "36290866",
                "name": "Anyi Rao"
            },
            {
                "authorId": "2119049364",
                "name": "Yaohui Wang"
            },
            {
                "authorId": "145858545",
                "name": "Y. Qiao"
            },
            {
                "authorId": "1807606",
                "name": "Dahua Lin"
            },
            {
                "authorId": "144445937",
                "name": "Bo Dai"
            }
        ],
        "abstract": "With the advance of text-to-image (T2I) diffusion models (e.g., Stable Diffusion) and corresponding personalization techniques such as DreamBooth and LoRA, everyone can manifest their imagination into high-quality images at an affordable cost. However, adding motion dynamics to existing high-quality personalized T2Is and enabling them to generate animations remains an open challenge. In this paper, we present AnimateDiff, a practical framework for animating personalized T2I models without requiring model-specific tuning. At the core of our framework is a plug-and-play motion module that can be trained once and seamlessly integrated into any personalized T2Is originating from the same base T2I. Through our proposed training strategy, the motion module effectively learns transferable motion priors from real-world videos. Once trained, the motion module can be inserted into a personalized T2I model to form a personalized animation generator. We further propose MotionLoRA, a lightweight fine-tuning technique for AnimateDiff that enables a pre-trained motion module to adapt to new motion patterns, such as different shot types, at a low training and data collection cost. We evaluate AnimateDiff and MotionLoRA on several public representative personalized T2I models collected from the community. The results demonstrate that our approaches help these models generate temporally smooth animation clips while preserving the visual quality and motion diversity. Codes and pre-trained weights are available at https://github.com/guoyww/AnimateDiff."
    },
    {
        "paperId": "ca6a2bc279be5a3349a22bfd6866ed633d18734b",
        "url": "https://www.semanticscholar.org/paper/ca6a2bc279be5a3349a22bfd6866ed633d18734b",
        "title": "MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models",
        "venue": "International Conference on Learning Representations",
        "year": 2023,
        "citationCount": 2677,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2304.10592",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2304.10592, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1388731230",
                "name": "Deyao Zhu"
            },
            {
                "authorId": "2153417252",
                "name": "Jun Chen"
            },
            {
                "authorId": "2151708219",
                "name": "Xiaoqian Shen"
            },
            {
                "authorId": "2144440192",
                "name": "Xiang Li"
            },
            {
                "authorId": "1712479",
                "name": "Mohamed Elhoseiny"
            }
        ],
        "abstract": "The recent GPT-4 has demonstrated extraordinary multi-modal abilities, such as directly generating websites from handwritten text and identifying humorous elements within images. These features are rarely observed in previous vision-language models. However, the technical details behind GPT-4 continue to remain undisclosed. We believe that the enhanced multi-modal generation capabilities of GPT-4 stem from the utilization of sophisticated large language models (LLM). To examine this phenomenon, we present MiniGPT-4, which aligns a frozen visual encoder with a frozen advanced LLM, Vicuna, using one projection layer. Our work, for the first time, uncovers that properly aligning the visual features with an advanced large language model can possess numerous advanced multi-modal abilities demonstrated by GPT-4, such as detailed image description generation and website creation from hand-drawn drafts. Furthermore, we also observe other emerging capabilities in MiniGPT-4, including writing stories and poems inspired by given images, teaching users how to cook based on food photos, and so on. In our experiment, we found that the model trained on short image caption pairs could produce unnatural language outputs (e.g., repetition and fragmentation). To address this problem, we curate a detailed image description dataset in the second stage to finetune the model, which consequently improves the model's generation reliability and overall usability. Our code, pre-trained model, and collected dataset are available at https://minigpt-4.github.io/."
    },
    {
        "paperId": "d7890d1906d95c4ae4c430b350455156d6d8aed9",
        "url": "https://www.semanticscholar.org/paper/d7890d1906d95c4ae4c430b350455156d6d8aed9",
        "title": "SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis",
        "venue": "International Conference on Learning Representations",
        "year": 2023,
        "citationCount": 3737,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2307.01952, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2221125727",
                "name": "Dustin Podell"
            },
            {
                "authorId": "2221127565",
                "name": "Zion English"
            },
            {
                "authorId": "2221126982",
                "name": "Kyle Lacey"
            },
            {
                "authorId": "119843260",
                "name": "A. Blattmann"
            },
            {
                "authorId": "102541178",
                "name": "Tim Dockhorn"
            },
            {
                "authorId": "2188737195",
                "name": "Jonas Muller"
            },
            {
                "authorId": "2215904682",
                "name": "Joe Penna"
            },
            {
                "authorId": "1660819540",
                "name": "Robin Rombach"
            }
        ],
        "abstract": "We present SDXL, a latent diffusion model for text-to-image synthesis. Compared to previous versions of Stable Diffusion, SDXL leverages a three times larger UNet backbone: The increase of model parameters is mainly due to more attention blocks and a larger cross-attention context as SDXL uses a second text encoder. We design multiple novel conditioning schemes and train SDXL on multiple aspect ratios. We also introduce a refinement model which is used to improve the visual fidelity of samples generated by SDXL using a post-hoc image-to-image technique. We demonstrate that SDXL shows drastically improved performance compared the previous versions of Stable Diffusion and achieves results competitive with those of black-box state-of-the-art image generators. In the spirit of promoting open research and fostering transparency in large model training and evaluation, we provide access to code and model weights at https://github.com/Stability-AI/generative-models"
    },
    {
        "paperId": "ddbd8fe782ac98e9c64dd98710687a962195dd9b",
        "url": "https://www.semanticscholar.org/paper/ddbd8fe782ac98e9c64dd98710687a962195dd9b",
        "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection",
        "venue": "International Conference on Learning Representations",
        "year": 2023,
        "citationCount": 1275,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.11511, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "35584853",
                "name": "Akari Asai"
            },
            {
                "authorId": "7806955",
                "name": "Zeqiu Wu"
            },
            {
                "authorId": "1705260",
                "name": "Yizhong Wang"
            },
            {
                "authorId": "2707234",
                "name": "Avirup Sil"
            },
            {
                "authorId": "2548384",
                "name": "Hannaneh Hajishirzi"
            }
        ],
        "abstract": "Despite their remarkable capabilities, large language models (LLMs) often produce responses containing factual inaccuracies due to their sole reliance on the parametric knowledge they encapsulate. Retrieval-Augmented Generation (RAG), an ad hoc approach that augments LMs with retrieval of relevant knowledge, decreases such issues. However, indiscriminately retrieving and incorporating a fixed number of retrieved passages, regardless of whether retrieval is necessary, or passages are relevant, diminishes LM versatility or can lead to unhelpful response generation. We introduce a new framework called Self-Reflective Retrieval-Augmented Generation (Self-RAG) that enhances an LM's quality and factuality through retrieval and self-reflection. Our framework trains a single arbitrary LM that adaptively retrieves passages on-demand, and generates and reflects on retrieved passages and its own generations using special tokens, called reflection tokens. Generating reflection tokens makes the LM controllable during the inference phase, enabling it to tailor its behavior to diverse task requirements. Experiments show that Self-RAG (7B and 13B parameters) significantly outperforms state-of-the-art LLMs and retrieval-augmented models on a diverse set of tasks. Specifically, Self-RAG outperforms ChatGPT and retrieval-augmented Llama2-chat on Open-domain QA, reasoning and fact verification tasks, and it shows significant gains in improving factuality and citation accuracy for long-form generations relative to these models."
    },
    {
        "paperId": "fdc53c2c10742464087c0525f77e32604827a21d",
        "url": "https://www.semanticscholar.org/paper/fdc53c2c10742464087c0525f77e32604827a21d",
        "title": "Efficient Streaming Language Models with Attention Sinks",
        "venue": "International Conference on Learning Representations",
        "year": 2023,
        "citationCount": 1230,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2309.17453",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2309.17453, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2046958974",
                "name": "Guangxuan Xiao"
            },
            {
                "authorId": "2249538771",
                "name": "Yuandong Tian"
            },
            {
                "authorId": "2249538643",
                "name": "Beidi Chen"
            },
            {
                "authorId": "2249530374",
                "name": "Song Han"
            },
            {
                "authorId": "2249533054",
                "name": "Mike Lewis"
            }
        ],
        "abstract": "Deploying Large Language Models (LLMs) in streaming applications such as multi-round dialogue, where long interactions are expected, is urgently needed but poses two major challenges. Firstly, during the decoding stage, caching previous tokens' Key and Value states (KV) consumes extensive memory. Secondly, popular LLMs cannot generalize to longer texts than the training sequence length. Window attention, where only the most recent KVs are cached, is a natural approach -- but we show that it fails when the text length surpasses the cache size. We observe an interesting phenomenon, namely attention sink, that keeping the KV of initial tokens will largely recover the performance of window attention. In this paper, we first demonstrate that the emergence of attention sink is due to the strong attention scores towards initial tokens as a\"sink\"even if they are not semantically important. Based on the above analysis, we introduce StreamingLLM, an efficient framework that enables LLMs trained with a finite length attention window to generalize to infinite sequence lengths without any fine-tuning. We show that StreamingLLM can enable Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language modeling with up to 4 million tokens and more. In addition, we discover that adding a placeholder token as a dedicated attention sink during pre-training can further improve streaming deployment. In streaming settings, StreamingLLM outperforms the sliding window recomputation baseline by up to 22.2x speedup. Code and datasets are provided at https://github.com/mit-han-lab/streaming-llm."
    },
    {
        "paperId": "14fdab35cc6288083a38a92392af3f1da0b95a90",
        "url": "https://www.semanticscholar.org/paper/14fdab35cc6288083a38a92392af3f1da0b95a90",
        "title": "KAN: Kolmogorov-Arnold Networks",
        "venue": "International Conference on Learning Representations",
        "year": 2024,
        "citationCount": 1203,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.19756, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2298997886",
                "name": "Ziming Liu"
            },
            {
                "authorId": "2299114926",
                "name": "Yixuan Wang"
            },
            {
                "authorId": "2298970079",
                "name": "Sachin Vaidya"
            },
            {
                "authorId": "2298969843",
                "name": "Fabian Ruehle"
            },
            {
                "authorId": "2298969998",
                "name": "James Halverson"
            },
            {
                "authorId": "2303854843",
                "name": "Marin Soljacic"
            },
            {
                "authorId": "2298969388",
                "name": "Thomas Y. Hou"
            },
            {
                "authorId": "2298969894",
                "name": "Max Tegmark"
            }
        ],
        "abstract": "Inspired by the Kolmogorov-Arnold representation theorem, we propose Kolmogorov-Arnold Networks (KANs) as promising alternatives to Multi-Layer Perceptrons (MLPs). While MLPs have fixed activation functions on nodes (\"neurons\"), KANs have learnable activation functions on edges (\"weights\"). KANs have no linear weights at all -- every weight parameter is replaced by a univariate function parametrized as a spline. We show that this seemingly simple change makes KANs outperform MLPs in terms of accuracy and interpretability. For accuracy, much smaller KANs can achieve comparable or better accuracy than much larger MLPs in data fitting and PDE solving. Theoretically and empirically, KANs possess faster neural scaling laws than MLPs. For interpretability, KANs can be intuitively visualized and can easily interact with human users. Through two examples in mathematics and physics, KANs are shown to be useful collaborators helping scientists (re)discover mathematical and physical laws. In summary, KANs are promising alternatives for MLPs, opening opportunities for further improving today's deep learning models which rely heavily on MLPs."
    },
    {
        "paperId": "7b248d78573ccf0dca6aa2cec2743d3eccaa9d1a",
        "url": "https://www.semanticscholar.org/paper/7b248d78573ccf0dca6aa2cec2743d3eccaa9d1a",
        "title": "CogVideoX: Text-to-Video Diffusion Models with An Expert Transformer",
        "venue": "International Conference on Learning Representations",
        "year": 2024,
        "citationCount": 1258,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.06072, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2303231681",
                "name": "Zhuoyi Yang"
            },
            {
                "authorId": "2238205354",
                "name": "Jiayan Teng"
            },
            {
                "authorId": "2163967642",
                "name": "Wendi Zheng"
            },
            {
                "authorId": "2055623340",
                "name": "Ming Ding"
            },
            {
                "authorId": "2305795673",
                "name": "Shiyu Huang"
            },
            {
                "authorId": "2214082934",
                "name": "Jiazheng Xu"
            },
            {
                "authorId": "2315948290",
                "name": "Yuanming Yang"
            },
            {
                "authorId": "2105844599",
                "name": "Wenyi Hong"
            },
            {
                "authorId": "2268628279",
                "name": "Xiaohan Zhang"
            },
            {
                "authorId": "2307077651",
                "name": "Guanyu Feng"
            },
            {
                "authorId": "2307075814",
                "name": "Da Yin"
            },
            {
                "authorId": "2290625851",
                "name": "Xiaotao Gu"
            },
            {
                "authorId": "2316099643",
                "name": "Yuxuan Zhang"
            },
            {
                "authorId": "2265518149",
                "name": "Weihan Wang"
            },
            {
                "authorId": "2306161782",
                "name": "Yean Cheng"
            },
            {
                "authorId": "2315952736",
                "name": "Ting Liu"
            },
            {
                "authorId": "2288066971",
                "name": "Bin Xu"
            },
            {
                "authorId": "2243402027",
                "name": "Yuxiao Dong"
            },
            {
                "authorId": "2238207092",
                "name": "Jie Tang"
            }
        ],
        "abstract": "We present CogVideoX, a large-scale text-to-video generation model based on diffusion transformer, which can generate 10-second continuous videos aligned with text prompt, with a frame rate of 16 fps and resolution of 768 * 1360 pixels. Previous video generation models often had limited movement and short durations, and is difficult to generate videos with coherent narratives based on text. We propose several designs to address these issues. First, we propose a 3D Variational Autoencoder (VAE) to compress videos along both spatial and temporal dimensions, to improve both compression rate and video fidelity. Second, to improve the text-video alignment, we propose an expert transformer with the expert adaptive LayerNorm to facilitate the deep fusion between the two modalities. Third, by employing a progressive training and multi-resolution frame pack technique, CogVideoX is adept at producing coherent, long-duration, different shape videos characterized by significant motions. In addition, we develop an effective text-video data processing pipeline that includes various data preprocessing strategies and a video captioning method, greatly contributing to the generation quality and semantic alignment. Results show that CogVideoX demonstrates state-of-the-art performance across both multiple machine metrics and human evaluations. The model weight of both 3D Causal VAE, Video caption model and CogVideoX are publicly available at https://github.com/THUDM/CogVideo."
    },
    {
        "paperId": "92a09cdfc19f3f582d89c28c1b4f386299cc69e1",
        "url": "https://www.semanticscholar.org/paper/92a09cdfc19f3f582d89c28c1b4f386299cc69e1",
        "title": "SAM 2: Segment Anything in Images and Videos",
        "venue": "International Conference on Learning Representations",
        "year": 2024,
        "citationCount": 2128,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.00714, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2308102494",
                "name": "Nikhila Ravi"
            },
            {
                "authorId": "151352107",
                "name": "Valentin Gabeur"
            },
            {
                "authorId": "2314514593",
                "name": "Yuan-Ting Hu"
            },
            {
                "authorId": "2314374069",
                "name": "Ronghang Hu"
            },
            {
                "authorId": "52190116",
                "name": "Chaitanya K. Ryali"
            },
            {
                "authorId": "2314306272",
                "name": "Tengyu Ma"
            },
            {
                "authorId": "2314116200",
                "name": "Haitham Khedr"
            },
            {
                "authorId": "2320227019",
                "name": "Roman Rdle"
            },
            {
                "authorId": "2213549340",
                "name": "Chlo Rolland"
            },
            {
                "authorId": "2314116847",
                "name": "Laura Gustafson"
            },
            {
                "authorId": "13131689",
                "name": "Eric Mintun"
            },
            {
                "authorId": "2314142914",
                "name": "Junting Pan"
            },
            {
                "authorId": "3085301",
                "name": "Kalyan Vasudev Alwala"
            },
            {
                "authorId": "3422899",
                "name": "Nicolas Carion"
            },
            {
                "authorId": "2314513936",
                "name": "Chao-Yuan Wu"
            },
            {
                "authorId": "2257205047",
                "name": "Ross B. Girshick"
            },
            {
                "authorId": "2065731243",
                "name": "Piotr Doll'ar"
            },
            {
                "authorId": "2322150",
                "name": "Christoph Feichtenhofer"
            }
        ],
        "abstract": "We present Segment Anything Model 2 (SAM 2), a foundation model towards solving promptable visual segmentation in images and videos. We build a data engine, which improves model and data via user interaction, to collect the largest video segmentation dataset to date. Our model is a simple transformer architecture with streaming memory for real-time video processing. SAM 2 trained on our data provides strong performance across a wide range of tasks. In video segmentation, we observe better accuracy, using 3x fewer interactions than prior approaches. In image segmentation, our model is more accurate and 6x faster than the Segment Anything Model (SAM). We believe that our data, model, and insights will serve as a significant milestone for video segmentation and related perception tasks. We are releasing our main model, dataset, as well as code for model training and our demo."
    },
    {
        "paperId": "14fdab35cc6288083a38a92392af3f1da0b95a90",
        "url": "https://www.semanticscholar.org/paper/14fdab35cc6288083a38a92392af3f1da0b95a90",
        "title": "KAN: Kolmogorov-Arnold Networks",
        "venue": "International Conference on Learning Representations",
        "year": 2024,
        "citationCount": 1203,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.19756, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2298997886",
                "name": "Ziming Liu"
            },
            {
                "authorId": "2299114926",
                "name": "Yixuan Wang"
            },
            {
                "authorId": "2298970079",
                "name": "Sachin Vaidya"
            },
            {
                "authorId": "2298969843",
                "name": "Fabian Ruehle"
            },
            {
                "authorId": "2298969998",
                "name": "James Halverson"
            },
            {
                "authorId": "2303854843",
                "name": "Marin Soljacic"
            },
            {
                "authorId": "2298969388",
                "name": "Thomas Y. Hou"
            },
            {
                "authorId": "2298969894",
                "name": "Max Tegmark"
            }
        ],
        "abstract": "Inspired by the Kolmogorov-Arnold representation theorem, we propose Kolmogorov-Arnold Networks (KANs) as promising alternatives to Multi-Layer Perceptrons (MLPs). While MLPs have fixed activation functions on nodes (\"neurons\"), KANs have learnable activation functions on edges (\"weights\"). KANs have no linear weights at all -- every weight parameter is replaced by a univariate function parametrized as a spline. We show that this seemingly simple change makes KANs outperform MLPs in terms of accuracy and interpretability. For accuracy, much smaller KANs can achieve comparable or better accuracy than much larger MLPs in data fitting and PDE solving. Theoretically and empirically, KANs possess faster neural scaling laws than MLPs. For interpretability, KANs can be intuitively visualized and can easily interact with human users. Through two examples in mathematics and physics, KANs are shown to be useful collaborators helping scientists (re)discover mathematical and physical laws. In summary, KANs are promising alternatives for MLPs, opening opportunities for further improving today's deep learning models which rely heavily on MLPs."
    },
    {
        "paperId": "7b248d78573ccf0dca6aa2cec2743d3eccaa9d1a",
        "url": "https://www.semanticscholar.org/paper/7b248d78573ccf0dca6aa2cec2743d3eccaa9d1a",
        "title": "CogVideoX: Text-to-Video Diffusion Models with An Expert Transformer",
        "venue": "International Conference on Learning Representations",
        "year": 2024,
        "citationCount": 1258,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.06072, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2303231681",
                "name": "Zhuoyi Yang"
            },
            {
                "authorId": "2238205354",
                "name": "Jiayan Teng"
            },
            {
                "authorId": "2163967642",
                "name": "Wendi Zheng"
            },
            {
                "authorId": "2055623340",
                "name": "Ming Ding"
            },
            {
                "authorId": "2305795673",
                "name": "Shiyu Huang"
            },
            {
                "authorId": "2214082934",
                "name": "Jiazheng Xu"
            },
            {
                "authorId": "2315948290",
                "name": "Yuanming Yang"
            },
            {
                "authorId": "2105844599",
                "name": "Wenyi Hong"
            },
            {
                "authorId": "2268628279",
                "name": "Xiaohan Zhang"
            },
            {
                "authorId": "2307077651",
                "name": "Guanyu Feng"
            },
            {
                "authorId": "2307075814",
                "name": "Da Yin"
            },
            {
                "authorId": "2290625851",
                "name": "Xiaotao Gu"
            },
            {
                "authorId": "2316099643",
                "name": "Yuxuan Zhang"
            },
            {
                "authorId": "2265518149",
                "name": "Weihan Wang"
            },
            {
                "authorId": "2306161782",
                "name": "Yean Cheng"
            },
            {
                "authorId": "2315952736",
                "name": "Ting Liu"
            },
            {
                "authorId": "2288066971",
                "name": "Bin Xu"
            },
            {
                "authorId": "2243402027",
                "name": "Yuxiao Dong"
            },
            {
                "authorId": "2238207092",
                "name": "Jie Tang"
            }
        ],
        "abstract": "We present CogVideoX, a large-scale text-to-video generation model based on diffusion transformer, which can generate 10-second continuous videos aligned with text prompt, with a frame rate of 16 fps and resolution of 768 * 1360 pixels. Previous video generation models often had limited movement and short durations, and is difficult to generate videos with coherent narratives based on text. We propose several designs to address these issues. First, we propose a 3D Variational Autoencoder (VAE) to compress videos along both spatial and temporal dimensions, to improve both compression rate and video fidelity. Second, to improve the text-video alignment, we propose an expert transformer with the expert adaptive LayerNorm to facilitate the deep fusion between the two modalities. Third, by employing a progressive training and multi-resolution frame pack technique, CogVideoX is adept at producing coherent, long-duration, different shape videos characterized by significant motions. In addition, we develop an effective text-video data processing pipeline that includes various data preprocessing strategies and a video captioning method, greatly contributing to the generation quality and semantic alignment. Results show that CogVideoX demonstrates state-of-the-art performance across both multiple machine metrics and human evaluations. The model weight of both 3D Causal VAE, Video caption model and CogVideoX are publicly available at https://github.com/THUDM/CogVideo."
    },
    {
        "paperId": "92a09cdfc19f3f582d89c28c1b4f386299cc69e1",
        "url": "https://www.semanticscholar.org/paper/92a09cdfc19f3f582d89c28c1b4f386299cc69e1",
        "title": "SAM 2: Segment Anything in Images and Videos",
        "venue": "International Conference on Learning Representations",
        "year": 2024,
        "citationCount": 2128,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.00714, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2308102494",
                "name": "Nikhila Ravi"
            },
            {
                "authorId": "151352107",
                "name": "Valentin Gabeur"
            },
            {
                "authorId": "2314514593",
                "name": "Yuan-Ting Hu"
            },
            {
                "authorId": "2314374069",
                "name": "Ronghang Hu"
            },
            {
                "authorId": "52190116",
                "name": "Chaitanya K. Ryali"
            },
            {
                "authorId": "2314306272",
                "name": "Tengyu Ma"
            },
            {
                "authorId": "2314116200",
                "name": "Haitham Khedr"
            },
            {
                "authorId": "2320227019",
                "name": "Roman Rdle"
            },
            {
                "authorId": "2213549340",
                "name": "Chlo Rolland"
            },
            {
                "authorId": "2314116847",
                "name": "Laura Gustafson"
            },
            {
                "authorId": "13131689",
                "name": "Eric Mintun"
            },
            {
                "authorId": "2314142914",
                "name": "Junting Pan"
            },
            {
                "authorId": "3085301",
                "name": "Kalyan Vasudev Alwala"
            },
            {
                "authorId": "3422899",
                "name": "Nicolas Carion"
            },
            {
                "authorId": "2314513936",
                "name": "Chao-Yuan Wu"
            },
            {
                "authorId": "2257205047",
                "name": "Ross B. Girshick"
            },
            {
                "authorId": "2065731243",
                "name": "Piotr Doll'ar"
            },
            {
                "authorId": "2322150",
                "name": "Christoph Feichtenhofer"
            }
        ],
        "abstract": "We present Segment Anything Model 2 (SAM 2), a foundation model towards solving promptable visual segmentation in images and videos. We build a data engine, which improves model and data via user interaction, to collect the largest video segmentation dataset to date. Our model is a simple transformer architecture with streaming memory for real-time video processing. SAM 2 trained on our data provides strong performance across a wide range of tasks. In video segmentation, we observe better accuracy, using 3x fewer interactions than prior approaches. In image segmentation, our model is more accurate and 6x faster than the Segment Anything Model (SAM). We believe that our data, model, and insights will serve as a significant milestone for video segmentation and related perception tasks. We are releasing our main model, dataset, as well as code for model training and our demo."
    },
    {
        "paperId": "44f3ac3277c2eb6e5599739eb875888c46e21d4c",
        "url": "https://www.semanticscholar.org/paper/44f3ac3277c2eb6e5599739eb875888c46e21d4c",
        "title": "Human Detection Using Oriented Histograms of Flow and Appearance",
        "venue": "European Conference on Computer Vision",
        "year": 2006,
        "citationCount": 1956,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/11744047_33?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/11744047_33, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "48950628",
                "name": "Navneet Dalal"
            },
            {
                "authorId": "1756114",
                "name": "B. Triggs"
            },
            {
                "authorId": "2462253",
                "name": "C. Schmid"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "6ae643b467ce873de1ce7962a7fa24dda1a28e68",
        "url": "https://www.semanticscholar.org/paper/6ae643b467ce873de1ce7962a7fa24dda1a28e68",
        "title": "Sampling Strategies for Bag-of-Features Image Classification",
        "venue": "European Conference on Computer Vision",
        "year": 2006,
        "citationCount": 1151,
        "openAccessPdf": {
            "url": "https://link.springer.com/content/pdf/10.1007%2F11744085_38.pdf",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/11744085_38?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/11744085_38, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2064850418",
                "name": "Eric Nowak"
            },
            {
                "authorId": "82117876",
                "name": "F. Jurie"
            },
            {
                "authorId": "1756114",
                "name": "B. Triggs"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "8772877ceb40d6d8685655145034740f3df7baad",
        "url": "https://www.semanticscholar.org/paper/8772877ceb40d6d8685655145034740f3df7baad",
        "title": "Studying Aesthetics in Photographic Images Using a Computational Approach",
        "venue": "European Conference on Computer Vision",
        "year": 2006,
        "citationCount": 1017,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/11744078_23?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/11744078_23, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "40490812",
                "name": "R. Datta"
            },
            {
                "authorId": "5113463",
                "name": "D. Joshi"
            },
            {
                "authorId": "40116905",
                "name": "Jia Li"
            },
            {
                "authorId": "48094094",
                "name": "James Ze Wang"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "9ad3179098eb9ac7f0c81bf89b6bdcde91752880",
        "url": "https://www.semanticscholar.org/paper/9ad3179098eb9ac7f0c81bf89b6bdcde91752880",
        "title": "SURF: Speeded Up Robust Features",
        "venue": "European Conference on Computer Vision",
        "year": 2006,
        "citationCount": 10464,
        "openAccessPdf": {
            "url": "https://link.springer.com/content/pdf/10.1007/11744023_32.pdf",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/11744023_32?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/11744023_32, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2195306",
                "name": "Herbert Bay"
            },
            {
                "authorId": "1704728",
                "name": "T. Tuytelaars"
            },
            {
                "authorId": "1681236",
                "name": "L. Gool"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "b5b3dc4389e904933c1c8fc36d15398e904a288f",
        "url": "https://www.semanticscholar.org/paper/b5b3dc4389e904933c1c8fc36d15398e904a288f",
        "title": "Region Covariance: A Fast Descriptor for Detection and Classification",
        "venue": "European Conference on Computer Vision",
        "year": 2006,
        "citationCount": 1469,
        "openAccessPdf": {
            "url": "https://link.springer.com/content/pdf/10.1007/11744047_45.pdf",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/11744047_45?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/11744047_45, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2577513",
                "name": "Oncel Tuzel"
            },
            {
                "authorId": "29905643",
                "name": "F. Porikli"
            },
            {
                "authorId": "145776090",
                "name": "P. Meer"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "e0408181bccb7e3754dd5e6785ec47d8beb8b6bd",
        "url": "https://www.semanticscholar.org/paper/e0408181bccb7e3754dd5e6785ec47d8beb8b6bd",
        "title": "Machine Learning for High-Speed Corner Detection",
        "venue": "European Conference on Computer Vision",
        "year": 2006,
        "citationCount": 4964,
        "openAccessPdf": {
            "url": "https://link.springer.com/content/pdf/10.1007/11744023_34.pdf",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/11744023_34?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/11744023_34, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1721991",
                "name": "E. Rosten"
            },
            {
                "authorId": "144418842",
                "name": "T. Drummond"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "f07260deea7dea6dfe82d208731f4790e37cb3b3",
        "url": "https://www.semanticscholar.org/paper/f07260deea7dea6dfe82d208731f4790e37cb3b3",
        "title": "TextonBoost: Joint Appearance, Shape and Context Modeling for Multi-class Object Recognition and Segmentation",
        "venue": "European Conference on Computer Vision",
        "year": 2006,
        "citationCount": 1387,
        "openAccessPdf": {
            "url": "https://link.springer.com/content/pdf/10.1007/11744023_1.pdf",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/11744023_1?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/11744023_1, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "143774737",
                "name": "J. Shotton"
            },
            {
                "authorId": "33652486",
                "name": "J. Winn"
            },
            {
                "authorId": "1756036",
                "name": "C. Rother"
            },
            {
                "authorId": "1716777",
                "name": "A. Criminisi"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "04554de05a3a9ebb1890d25aaa7e34544a0d32a7",
        "url": "https://www.semanticscholar.org/paper/04554de05a3a9ebb1890d25aaa7e34544a0d32a7",
        "title": "Active Matching",
        "venue": "European Conference on Computer Vision",
        "year": 2008,
        "citationCount": 1120,
        "openAccessPdf": {
            "url": "https://link.springer.com/content/pdf/10.1007/978-3-540-88682-2_7.pdf",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/978-3-540-88682-2_7?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/978-3-540-88682-2_7, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1885768",
                "name": "M. Chli"
            },
            {
                "authorId": "2052135690",
                "name": "A. Davison"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "08f624f7ee5c3b05b1b604357fb1532241e208db",
        "url": "https://www.semanticscholar.org/paper/08f624f7ee5c3b05b1b604357fb1532241e208db",
        "title": "Segmentation and Recognition Using Structure from Motion Point Clouds",
        "venue": "European Conference on Computer Vision",
        "year": 2008,
        "citationCount": 1078,
        "openAccessPdf": {
            "url": "https://link.springer.com/content/pdf/10.1007%2F978-3-540-88682-2_5.pdf",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/978-3-540-88682-2_5?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/978-3-540-88682-2_5, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3309893",
                "name": "G. Brostow"
            },
            {
                "authorId": "143774737",
                "name": "J. Shotton"
            },
            {
                "authorId": "1783467",
                "name": "Julien Fauqueur"
            },
            {
                "authorId": "1745672",
                "name": "R. Cipolla"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "117d576d72515e900e6fc5a4a0e7f1d0142a8924",
        "url": "https://www.semanticscholar.org/paper/117d576d72515e900e6fc5a4a0e7f1d0142a8924",
        "title": "An Efficient Dense and Scale-Invariant Spatio-Temporal Interest Point Detector",
        "venue": "European Conference on Computer Vision",
        "year": 2008,
        "citationCount": 1034,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/978-3-540-88688-4_48?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/978-3-540-88688-4_48, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2072996455",
                "name": "Geert Willems"
            },
            {
                "authorId": "1704728",
                "name": "T. Tuytelaars"
            },
            {
                "authorId": "1681236",
                "name": "L. Gool"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "27d69a2d96600efb66fd907d8287ca3b6e734c59",
        "url": "https://www.semanticscholar.org/paper/27d69a2d96600efb66fd907d8287ca3b6e734c59",
        "title": "Semi-supervised On-Line Boosting for Robust Tracking",
        "venue": "European Conference on Computer Vision",
        "year": 2008,
        "citationCount": 1259,
        "openAccessPdf": {
            "url": "https://link.springer.com/content/pdf/10.1007/978-3-540-88682-2_19.pdf",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/978-3-540-88682-2_19?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/978-3-540-88682-2_19, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "145551629",
                "name": "H. Grabner"
            },
            {
                "authorId": "1695579",
                "name": "C. Leistner"
            },
            {
                "authorId": "144746444",
                "name": "H. Bischof"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "5a2bfaa724ba37134eb55c29644f8576c3d64c96",
        "url": "https://www.semanticscholar.org/paper/5a2bfaa724ba37134eb55c29644f8576c3d64c96",
        "title": "Hamming Embedding and Weak Geometric Consistency for Large Scale Image Search",
        "venue": "European Conference on Computer Vision",
        "year": 2008,
        "citationCount": 2079,
        "openAccessPdf": {
            "url": "https://link.springer.com/content/pdf/10.1007/978-3-540-88682-2_24.pdf",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/978-3-540-88682-2_24?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/978-3-540-88682-2_24, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1681054",
                "name": "H. Jgou"
            },
            {
                "authorId": "3271933",
                "name": "Matthijs Douze"
            },
            {
                "authorId": "2462253",
                "name": "C. Schmid"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "d745cf8c51032996b5fee6b19e1b5321c14797eb",
        "url": "https://www.semanticscholar.org/paper/d745cf8c51032996b5fee6b19e1b5321c14797eb",
        "title": "Viewpoint Invariant Pedestrian Recognition with an Ensemble of Localized Features",
        "venue": "European Conference on Computer Vision",
        "year": 2008,
        "citationCount": 1601,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/978-3-540-88682-2_21?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/978-3-540-88682-2_21, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "144944479",
                "name": "Douglas Gray"
            },
            {
                "authorId": "145492205",
                "name": "Hai Tao"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "1a3df5c0a4471af6fe13433951758efc9d81846d",
        "url": "https://www.semanticscholar.org/paper/1a3df5c0a4471af6fe13433951758efc9d81846d",
        "title": "Two-Phase Kernel Estimation for Robust Motion Deblurring",
        "venue": "European Conference on Computer Vision",
        "year": 2010,
        "citationCount": 1120,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/978-3-642-15549-9_12?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/978-3-642-15549-9_12, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "144574904",
                "name": "Li Xu"
            },
            {
                "authorId": "1729056",
                "name": "Jiaya Jia"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "2145f5cbac48df8ed9f7695606c34e98b26cf5a9",
        "url": "https://www.semanticscholar.org/paper/2145f5cbac48df8ed9f7695606c34e98b26cf5a9",
        "title": "BRIEF: Binary Robust Independent Elementary Features",
        "venue": "European Conference on Computer Vision",
        "year": 2010,
        "citationCount": 3898,
        "openAccessPdf": {
            "url": "https://link.springer.com/content/pdf/10.1007%2F978-3-642-15561-1_56.pdf",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/978-3-642-15561-1_56?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/978-3-642-15561-1_56, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3230011",
                "name": "Michael Calonder"
            },
            {
                "authorId": "1689738",
                "name": "V. Lepetit"
            },
            {
                "authorId": "2404667",
                "name": "C. Strecha"
            },
            {
                "authorId": "1717736",
                "name": "P. Fua"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "29eddac19e736100e31aae759ad10028ac1e28eb",
        "url": "https://www.semanticscholar.org/paper/29eddac19e736100e31aae759ad10028ac1e28eb",
        "title": "Unique Signatures of Histograms for Local Surface Description",
        "venue": "European Conference on Computer Vision",
        "year": 2010,
        "citationCount": 1595,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/978-3-642-15558-1_26?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/978-3-642-15558-1_26, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2266326",
                "name": "Federico Tombari"
            },
            {
                "authorId": "2607607",
                "name": "Samuele Salti"
            },
            {
                "authorId": "9395079",
                "name": "L. D. Stefano"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "39f3b1804b8df5be645a1dcb4a876e128385d9be",
        "url": "https://www.semanticscholar.org/paper/39f3b1804b8df5be645a1dcb4a876e128385d9be",
        "title": "Improving the Fisher Kernel for Large-Scale Image Classification",
        "venue": "European Conference on Computer Vision",
        "year": 2010,
        "citationCount": 2856,
        "openAccessPdf": {
            "url": "https://link.springer.com/content/pdf/10.1007/978-3-642-15561-1_11.pdf",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/978-3-642-15561-1_11?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/978-3-642-15561-1_11, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1723883",
                "name": "Florent Perronnin"
            },
            {
                "authorId": "143995438",
                "name": "Jorge Snchez"
            },
            {
                "authorId": "1722052",
                "name": "Thomas Mensink"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "5d9a3036181676e187c9c0ff995d8bed1db3557d",
        "url": "https://www.semanticscholar.org/paper/5d9a3036181676e187c9c0ff995d8bed1db3557d",
        "title": "Adapting Visual Category Models to New Domains",
        "venue": "European Conference on Computer Vision",
        "year": 2010,
        "citationCount": 3103,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/978-3-642-15561-1_16?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/978-3-642-15561-1_16, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2903226",
                "name": "Kate Saenko"
            },
            {
                "authorId": "1692670",
                "name": "Brian Kulis"
            },
            {
                "authorId": "1739548",
                "name": "Mario Fritz"
            },
            {
                "authorId": "1753210",
                "name": "Trevor Darrell"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "d957ad316f7145c054d2dcbd47949869e46776b0",
        "url": "https://www.semanticscholar.org/paper/d957ad316f7145c054d2dcbd47949869e46776b0",
        "title": "ClassCut for Unsupervised Class Segmentation",
        "venue": "European Conference on Computer Vision",
        "year": 2010,
        "citationCount": 1006,
        "openAccessPdf": {
            "url": "https://www.pure.ed.ac.uk/ws/files/17737009/Alexe_et_al_2010_Class_cut_for_Unsupervised.pdf",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/978-3-642-15555-0_28?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/978-3-642-15555-0_28, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2365442",
                "name": "B. Alexe"
            },
            {
                "authorId": "1879646",
                "name": "Thomas Deselaers"
            },
            {
                "authorId": "143865718",
                "name": "V. Ferrari"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "eaaed23a2d94feb2f1c3ff22a25777c7a78f3141",
        "url": "https://www.semanticscholar.org/paper/eaaed23a2d94feb2f1c3ff22a25777c7a78f3141",
        "title": "Every Picture Tells a Story: Generating Sentences from Images",
        "venue": "European Conference on Computer Vision",
        "year": 2010,
        "citationCount": 1262,
        "openAccessPdf": {
            "url": "https://link.springer.com/content/pdf/10.1007%2F978-3-642-15561-1_2.pdf",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/978-3-642-15561-1_2?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/978-3-642-15561-1_2, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "143787583",
                "name": "Ali Farhadi"
            },
            {
                "authorId": "2375823284",
                "name": "Mohsen Hejrati"
            },
            {
                "authorId": "21160985",
                "name": "M. Sadeghi"
            },
            {
                "authorId": "2052690705",
                "name": "Peter Young"
            },
            {
                "authorId": "3125805",
                "name": "Cyrus Rashtchian"
            },
            {
                "authorId": "3118681",
                "name": "J. Hockenmaier"
            },
            {
                "authorId": "144016256",
                "name": "D. Forsyth"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "5b4e50860d61095bb5fb65eaa367b131923917be",
        "url": "https://www.semanticscholar.org/paper/5b4e50860d61095bb5fb65eaa367b131923917be",
        "title": "Exploiting the Circulant Structure of Tracking-by-Detection with Kernels",
        "venue": "European Conference on Computer Vision",
        "year": 2012,
        "citationCount": 2333,
        "openAccessPdf": {
            "url": "https://link.springer.com/content/pdf/10.1007/978-3-642-33765-9_50.pdf",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/978-3-642-33765-9_50?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/978-3-642-33765-9_50, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "143848064",
                "name": "Joo F. Henriques"
            },
            {
                "authorId": "144489408",
                "name": "Rui Caseiro"
            },
            {
                "authorId": "145784436",
                "name": "P. Martins"
            },
            {
                "authorId": "2182210",
                "name": "Jorge P. Batista"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "7d53f0c87c8ab0de6f3e74515e3ffaf3fab40c62",
        "url": "https://www.semanticscholar.org/paper/7d53f0c87c8ab0de6f3e74515e3ffaf3fab40c62",
        "title": "A Naturalistic Open Source Movie for Optical Flow Evaluation",
        "venue": "European Conference on Computer Vision",
        "year": 2012,
        "citationCount": 2203,
        "openAccessPdf": {
            "url": "https://link.springer.com/content/pdf/10.1007/978-3-642-33783-3_44.pdf",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/978-3-642-33783-3_44?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/978-3-642-33783-3_44, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2056985772",
                "name": "Daniel J. Butler"
            },
            {
                "authorId": "49820715",
                "name": "Jonas Wulff"
            },
            {
                "authorId": "2715753",
                "name": "G. Stanley"
            },
            {
                "authorId": "2105795",
                "name": "Michael J. Black"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "9d57723b4908397654fb1846d37db403d8b2b56a",
        "url": "https://www.semanticscholar.org/paper/9d57723b4908397654fb1846d37db403d8b2b56a",
        "title": "Real-Time Compressive Tracking",
        "venue": "European Conference on Computer Vision",
        "year": 2012,
        "citationCount": 1555,
        "openAccessPdf": {
            "url": "https://link.springer.com/content/pdf/10.1007/978-3-642-33712-3_62.pdf",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/978-3-642-33712-3_62?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/978-3-642-33712-3_62, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3198263",
                "name": "Kaihua Zhang"
            },
            {
                "authorId": "36685537",
                "name": "Lei Zhang"
            },
            {
                "authorId": "1715634",
                "name": "Ming-Hsuan Yang"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "c1994ba5946456fc70948c549daf62363f13fa2d",
        "url": "https://www.semanticscholar.org/paper/c1994ba5946456fc70948c549daf62363f13fa2d",
        "title": "Indoor Segmentation and Support Inference from RGBD Images",
        "venue": "European Conference on Computer Vision",
        "year": 2012,
        "citationCount": 6209,
        "openAccessPdf": {
            "url": "https://link.springer.com/content/pdf/10.1007/978-3-642-33715-4_54.pdf",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/978-3-642-33715-4_54?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/978-3-642-33715-4_54, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2286640",
                "name": "N. Silberman"
            },
            {
                "authorId": "2433269",
                "name": "Derek Hoiem"
            },
            {
                "authorId": "143967473",
                "name": "Pushmeet Kohli"
            },
            {
                "authorId": "2276554",
                "name": "R. Fergus"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "dd89b0332ae4f980d8200ec2c1230c50cd66259f",
        "url": "https://www.semanticscholar.org/paper/dd89b0332ae4f980d8200ec2c1230c50cd66259f",
        "title": "KAZE Features",
        "venue": "European Conference on Computer Vision",
        "year": 2012,
        "citationCount": 1269,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/978-3-642-33783-3_16?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/978-3-642-33783-3_16, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1728631",
                "name": "P. Alcantarilla"
            },
            {
                "authorId": "1748980",
                "name": "A. Bartoli"
            },
            {
                "authorId": "2052135690",
                "name": "A. Davison"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "1a2a770d23b4a171fa81de62a78a3deb0588f238",
        "url": "https://www.semanticscholar.org/paper/1a2a770d23b4a171fa81de62a78a3deb0588f238",
        "title": "Visualizing and Understanding Convolutional Networks",
        "venue": "European Conference on Computer Vision",
        "year": 2013,
        "citationCount": 16621,
        "openAccessPdf": {
            "url": "https://link.springer.com/content/pdf/10.1007/978-3-319-10590-1_53.pdf",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1311.2901, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "48799969",
                "name": "Matthew D. Zeiler"
            },
            {
                "authorId": "2276554",
                "name": "R. Fergus"
            }
        ],
        "abstract": "Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark Krizhevsky et al. [18]. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we explore both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. Used in a diagnostic role, these visualizations allow us to find model architectures that outperform Krizhevsky et al on the ImageNet classification benchmark. We also perform an ablation study to discover the performance contribution from different model layers. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets."
    },
    {
        "paperId": "0504945cc2d03550fecb6ff02e637f9421107c25",
        "url": "https://www.semanticscholar.org/paper/0504945cc2d03550fecb6ff02e637f9421107c25",
        "title": "Learning a Deep Convolutional Network for Image Super-Resolution",
        "venue": "European Conference on Computer Vision",
        "year": 2014,
        "citationCount": 5449,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/978-3-319-10593-2_13?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/978-3-319-10593-2_13, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "30459277",
                "name": "Chao Dong"
            },
            {
                "authorId": "1717179",
                "name": "Chen Change Loy"
            },
            {
                "authorId": "39353098",
                "name": "Kaiming He"
            },
            {
                "authorId": "50295995",
                "name": "Xiaoou Tang"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "2c0b510ebf995ef172cb64ed7ce24aa7903dced8",
        "url": "https://www.semanticscholar.org/paper/2c0b510ebf995ef172cb64ed7ce24aa7903dced8",
        "title": "Learning Rich Features from RGB-D Images for Object Detection and Segmentation",
        "venue": "European Conference on Computer Vision",
        "year": 2014,
        "citationCount": 1607,
        "openAccessPdf": {
            "url": "https://link.springer.com/content/pdf/10.1007/978-3-319-10584-0_23.pdf",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1407.5736, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "144157872",
                "name": "Saurabh Gupta"
            },
            {
                "authorId": "2983898",
                "name": "Ross B. Girshick"
            },
            {
                "authorId": "1778133",
                "name": "Pablo Arbelez"
            },
            {
                "authorId": "143751119",
                "name": "Jitendra Malik"
            }
        ],
        "abstract": "In this paper we study the problem of object detection for RGB-D images using semantically rich image and depth features. We propose a new geocentric embedding for depth images that encodes height above ground and angle with gravity for each pixel in addition to the horizontal disparity. We demonstrate that this geocentric embedding works better than using raw depth images for learning feature representations with convolutional neural networks. Our final object detection system achieves an average precision of 37.3%, which is a 56% relative improvement over existing methods. We then focus on the task of instance segmentation where we label pixels belonging to object instances found by our detector. For this task, we propose a decision forest approach that classifies pixels in the detection window as foreground or background using a family of unary and binary tests that query shape and geocentric pose features. Finally, we use the output from our object detectors in an existing superpixel classification framework for semantic scene segmentation and achieve a 24% relative improvement over current state-of-the-art for the object categories that we study. We believe advances such as those represented in this paper will facilitate the use of perception in fields like robotics."
    },
    {
        "paperId": "342786659379879f58bf5c4ff43c84c83a6a7389",
        "url": "https://www.semanticscholar.org/paper/342786659379879f58bf5c4ff43c84c83a6a7389",
        "title": "Simultaneous Detection and Segmentation",
        "venue": "European Conference on Computer Vision",
        "year": 2014,
        "citationCount": 1325,
        "openAccessPdf": {
            "url": "https://link.springer.com/content/pdf/10.1007/978-3-319-10584-0_20.pdf",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1407.1808, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1790580",
                "name": "Bharath Hariharan"
            },
            {
                "authorId": "1778133",
                "name": "Pablo Arbelez"
            },
            {
                "authorId": "2983898",
                "name": "Ross B. Girshick"
            },
            {
                "authorId": "143751119",
                "name": "Jitendra Malik"
            }
        ],
        "abstract": "We aim to detect all instances of a category in an image and, for each instance, mark the pixels that belong to it. We call this task Simultaneous Detection and Segmentation (SDS). Unlike classical bounding box detection, SDS requires a segmentation and not just a box. Unlike classical semantic segmentation, we require individual object instances. We build on recent work that uses convolutional neural networks to classify category-independent region proposals (R-CNN [16]), introducing a novel architecture tailored for SDS. We then use category-specific, top-down figure-ground predictions to refine our bottom-up proposals. We show a 7 point boost (16% relative) over our baselines on SDS, a 5 point boost (10% relative) over state-of-the-art on semantic segmentation, and state-of-the-art performance in object detection. Finally, we provide diagnostic tools that unpack performance and provide directions for future work."
    },
    {
        "paperId": "71b7178df5d2b112d07e45038cb5637208659ff7",
        "url": "https://www.semanticscholar.org/paper/71b7178df5d2b112d07e45038cb5637208659ff7",
        "title": "Microsoft COCO: Common Objects in Context",
        "venue": "European Conference on Computer Vision",
        "year": 2014,
        "citationCount": 49175,
        "openAccessPdf": {
            "url": "https://link.springer.com/content/pdf/10.1007%2F978-3-319-10602-1_48.pdf",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1405.0312, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "33493200",
                "name": "Tsung-Yi Lin"
            },
            {
                "authorId": "145854440",
                "name": "M. Maire"
            },
            {
                "authorId": "50172592",
                "name": "Serge J. Belongie"
            },
            {
                "authorId": "48966748",
                "name": "James Hays"
            },
            {
                "authorId": "1690922",
                "name": "P. Perona"
            },
            {
                "authorId": "1770537",
                "name": "Deva Ramanan"
            },
            {
                "authorId": "3127283",
                "name": "Piotr Dollr"
            },
            {
                "authorId": "1699161",
                "name": "C. L. Zitnick"
            }
        ],
        "abstract": "We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model."
    },
    {
        "paperId": "8e3f12804882b60ad5f59aad92755c5edb34860e",
        "url": "https://www.semanticscholar.org/paper/8e3f12804882b60ad5f59aad92755c5edb34860e",
        "title": "Food-101 - Mining Discriminative Components with Random Forests",
        "venue": "European Conference on Computer Vision",
        "year": 2014,
        "citationCount": 2946,
        "openAccessPdf": {
            "url": "https://link.springer.com/content/pdf/10.1007/978-3-319-10599-4_29.pdf",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/978-3-319-10599-4_29?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/978-3-319-10599-4_29, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1696393",
                "name": "Lukas Bossard"
            },
            {
                "authorId": "2737253",
                "name": "M. Guillaumin"
            },
            {
                "authorId": "1681236",
                "name": "L. Gool"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "98bb60748eb8ef7a671cdd22faa87e377fd13060",
        "url": "https://www.semanticscholar.org/paper/98bb60748eb8ef7a671cdd22faa87e377fd13060",
        "title": "Part-Based R-CNNs for Fine-Grained Category Detection",
        "venue": "European Conference on Computer Vision",
        "year": 2014,
        "citationCount": 1283,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1407.3867, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2152329702",
                "name": "Ning Zhang"
            },
            {
                "authorId": "7408951",
                "name": "Jeff Donahue"
            },
            {
                "authorId": "2983898",
                "name": "Ross B. Girshick"
            },
            {
                "authorId": "1753210",
                "name": "Trevor Darrell"
            }
        ],
        "abstract": "Semantic part localization can facilitate fine-grained categorization by explicitly isolating subtle appearance differences associated with specific object parts. Methods for pose-normalized representations have been proposed, but generally presume bounding box annotations at test time due to the difficulty of object detection. We propose a model for fine-grained categorization that overcomes these limitations by leveraging deep convolutional features computed on bottom-up region proposals. Our method learns whole-object and part detectors, enforces learned geometric constraints between them, and predicts a fine-grained category from a pose-normalized representation. Experiments on the Caltech-UCSD bird dataset confirm that our method outperforms state-of-the-art fine-grained categorization methods in an end-to-end evaluation without requiring a bounding box at test time."
    },
    {
        "paperId": "a99add9d76d849a8d47b93532703e4ca0f683b92",
        "url": "https://www.semanticscholar.org/paper/a99add9d76d849a8d47b93532703e4ca0f683b92",
        "title": "Multi-scale Orderless Pooling of Deep Convolutional Activation Features",
        "venue": "European Conference on Computer Vision",
        "year": 2014,
        "citationCount": 1107,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1403.1840, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "5115386",
                "name": "Yunchao Gong"
            },
            {
                "authorId": "39060743",
                "name": "Liwei Wang"
            },
            {
                "authorId": "37495246",
                "name": "Ruiqi Guo"
            },
            {
                "authorId": "1749609",
                "name": "Svetlana Lazebnik"
            }
        ],
        "abstract": "Deep convolutional neural networks (CNN) have shown their promise as a universal representation for recognition. However, global CNN activations lack geometric invariance, which limits their robustness for classification and matching of highly variable scenes. To improve the invariance of CNN activations without degrading their discriminative power, this paper presents a simple but effective scheme called multi-scale orderless pooling (MOP-CNN). This scheme extracts CNN activations for local patches at multiple scale levels, performs orderless VLAD pooling of these activations at each level separately, and concatenates the result. The resulting MOP-CNN representation can be used as a generic feature for either supervised or unsupervised recognition tasks, from image classification to instance-level retrieval; it consistently outperforms global CNN activations without requiring any joint training of prediction layers for a particular target dataset. In absolute terms, it achieves state-of-the-art results on the challenging SUN397 and MIT Indoor Scenes classification datasets, and competitive results on ILSVRC2012/2013 classification and INRIA Holidays retrieval datasets."
    },
    {
        "paperId": "b183947ee15718b45546eda6b01e179b9a95421f",
        "url": "https://www.semanticscholar.org/paper/b183947ee15718b45546eda6b01e179b9a95421f",
        "title": "Edge Boxes: Locating Object Proposals from Edges",
        "venue": "European Conference on Computer Vision",
        "year": 2014,
        "citationCount": 2846,
        "openAccessPdf": {
            "url": "https://link.springer.com/content/pdf/10.1007/978-3-319-10602-1_26.pdf",
            "status": "HYBRID",
            "license": "other-oa",
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/978-3-319-10602-1_26?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/978-3-319-10602-1_26, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1699161",
                "name": "C. L. Zitnick"
            },
            {
                "authorId": "3127283",
                "name": "Piotr Dollr"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "b1b421b3d803dbaafb981947245875cf546e24ee",
        "url": "https://www.semanticscholar.org/paper/b1b421b3d803dbaafb981947245875cf546e24ee",
        "title": "MEEM: Robust Tracking via Multiple Experts Using Entropy Minimization",
        "venue": "European Conference on Computer Vision",
        "year": 2014,
        "citationCount": 1125,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/978-3-319-10599-4_13?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/978-3-319-10599-4_13, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1519062024",
                "name": "Jianming Zhang"
            },
            {
                "authorId": "2863531",
                "name": "Shugao Ma"
            },
            {
                "authorId": "1749590",
                "name": "S. Sclaroff"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "c13cb6dfd26a1b545d50d05b52c99eb87b1c82b2",
        "url": "https://www.semanticscholar.org/paper/c13cb6dfd26a1b545d50d05b52c99eb87b1c82b2",
        "title": "LSD-SLAM: Large-Scale Direct Monocular SLAM",
        "venue": "European Conference on Computer Vision",
        "year": 2014,
        "citationCount": 3872,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/978-3-319-10605-2_54?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/978-3-319-10605-2_54, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "35152266",
                "name": "Jakob J. Engel"
            },
            {
                "authorId": "2982639",
                "name": "Thomas Schps"
            },
            {
                "authorId": "1695302",
                "name": "D. Cremers"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "f500b1a7df00f67c417673e0538d86abb8a333fa",
        "url": "https://www.semanticscholar.org/paper/f500b1a7df00f67c417673e0538d86abb8a333fa",
        "title": "Facial Landmark Detection by Deep Multi-task Learning",
        "venue": "European Conference on Computer Vision",
        "year": 2014,
        "citationCount": 1505,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/978-3-319-10599-4_7?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/978-3-319-10599-4_7, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3152448",
                "name": "Zhanpeng Zhang"
            },
            {
                "authorId": "47571885",
                "name": "Ping Luo"
            },
            {
                "authorId": "1717179",
                "name": "Chen Change Loy"
            },
            {
                "authorId": "50295995",
                "name": "Xiaoou Tang"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "4d7a9197433acbfb24ef0e9d0f33ed1699e4a5b0",
        "url": "https://www.semanticscholar.org/paper/4d7a9197433acbfb24ef0e9d0f33ed1699e4a5b0",
        "title": "SSD: Single Shot MultiBox Detector",
        "venue": "European Conference on Computer Vision",
        "year": 2015,
        "citationCount": 33193,
        "openAccessPdf": {
            "url": "https://link.springer.com/content/pdf/10.1007/978-3-319-46448-0_2.pdf",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1512.02325, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "46641573",
                "name": "W. Liu"
            },
            {
                "authorId": "1838674",
                "name": "Dragomir Anguelov"
            },
            {
                "authorId": "1761978",
                "name": "D. Erhan"
            },
            {
                "authorId": "2574060",
                "name": "Christian Szegedy"
            },
            {
                "authorId": "144828948",
                "name": "Scott E. Reed"
            },
            {
                "authorId": "2084646762",
                "name": "Cheng-Yang Fu"
            },
            {
                "authorId": "39668247",
                "name": "A. Berg"
            }
        ],
        "abstract": "We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. Our SSD model is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stage and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, MS COCO, and ILSVRC datasets confirm that SSD has comparable accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. Compared to other single stage methods, SSD has much better accuracy, even with a smaller input image size. For $300\\times 300$ input, SSD achieves 72.1% mAP on VOC2007 test at 58 FPS on a Nvidia Titan X and for $500\\times 500$ input, SSD achieves 75.1% mAP, outperforming a comparable state of the art Faster R-CNN model. Code is available at this https URL ."
    },
    {
        "paperId": "d7da9bddc31fd6e851f6b06a894d613c3529d09c",
        "url": "https://www.semanticscholar.org/paper/d7da9bddc31fd6e851f6b06a894d613c3529d09c",
        "title": "What's the Point: Semantic Segmentation with Point Supervision",
        "venue": "European Conference on Computer Vision",
        "year": 2015,
        "citationCount": 1057,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1506.02106, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1927362",
                "name": "Amy Bearman"
            },
            {
                "authorId": "2314850892",
                "name": "Olga Russakovsky"
            },
            {
                "authorId": "143865718",
                "name": "V. Ferrari"
            },
            {
                "authorId": "48004138",
                "name": "Li Fei-Fei"
            }
        ],
        "abstract": "The semantic image segmentation task presents a trade-off between test time accuracy and training time annotation cost. Detailed per-pixel annotations enable training accurate models but are very time-consuming to obtain; image-level class labels are an order of magnitude cheaper but result in less accurate models. We take a natural step from image-level annotation towards stronger supervision: we ask annotators to point to an object if one exists. We incorporate this point supervision along with a novel objectness potential in the training loss function of a CNN model. Experimental results on the PASCAL VOC 2012 benchmark reveal that the combined effect of point-level supervision and objectness potential yields an improvement of \\(12.9\\,\\%\\) mIOU over image-level supervision. Further, we demonstrate that models trained with point-level supervision are more accurate than models trained with image-level, squiggle-level or full supervision given a fixed annotation budget."
    },
    {
        "paperId": "102a2096ba2e2947dc252445f764e7583b557680",
        "url": "https://www.semanticscholar.org/paper/102a2096ba2e2947dc252445f764e7583b557680",
        "title": "Precomputed Real-Time Texture Synthesis with Markovian Generative Adversarial Networks",
        "venue": "European Conference on Computer Vision",
        "year": 2016,
        "citationCount": 1498,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1604.04382, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2118672354",
                "name": "Chuan Li"
            },
            {
                "authorId": "1723149",
                "name": "Michael Wand"
            }
        ],
        "abstract": "This paper proposes Markovian Generative Adversarial Networks (MGANs), a method for training generative networks for efficient texture synthesis. While deep neural network approaches have recently demonstrated remarkable results in terms of synthesis quality, they still come at considerable computational costs (minutes of run-time for low-res images). Our paper addresses this efficiency issue. Instead of a numerical deconvolution in previous work, we precompute a feed-forward, strided convolutional network that captures the feature statistics of Markovian patches and is able to directly generate outputs of arbitrary dimensions. Such network can directly decode brown noise to realistic texture, or photos to artistic paintings. With adversarial training, we obtain quality comparable to recent neural texture synthesis methods. As no optimization is required at generation time, our run-time performance (0.25 M pixel images at 25 Hz) surpasses previous neural texture synthesizers by a significant margin (at least 500 times faster). We apply this idea to texture synthesis, style transfer, and video stylization."
    },
    {
        "paperId": "21334d1aac5422da88780f8e24e181bfa15ef0e1",
        "url": "https://www.semanticscholar.org/paper/21334d1aac5422da88780f8e24e181bfa15ef0e1",
        "title": "Hollywood in Homes: Crowdsourcing Data Collection for Activity Understanding",
        "venue": "European Conference on Computer Vision",
        "year": 2016,
        "citationCount": 1345,
        "openAccessPdf": {
            "url": "https://link.springer.com/content/pdf/10.1007/978-3-319-46448-0_31.pdf",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1604.01753, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "34280810",
                "name": "Gunnar A. Sigurdsson"
            },
            {
                "authorId": "2397485103",
                "name": "Gl Varol"
            },
            {
                "authorId": "39849136",
                "name": "X. Wang"
            },
            {
                "authorId": "143787583",
                "name": "Ali Farhadi"
            },
            {
                "authorId": "143991676",
                "name": "I. Laptev"
            },
            {
                "authorId": "1726095131",
                "name": "A. Gupta"
            }
        ],
        "abstract": "Computer vision has a great potential to help our daily lives by searching for lost keys, watering flowers or reminding us to take a pill. To succeed with such tasks, computer vision methods need to be trained from real and diverse examples of our daily dynamic scenes. While most of such scenes are not particularly exciting, they typically do not appear on YouTube, in movies or TV broadcasts. So how do we collect sufficiently many diverse but boring samples representing our lives? We propose a novel Hollywood in Homes approach to collect such data. Instead of shooting videos in the lab, we ensure diversity by distributing and crowdsourcing the whole process of video creation from script writing to video recording and annotation. Following this procedure we collect a new dataset, Charades, with hundreds of people recording videos in their own homes, acting out casual everyday activities. The dataset is composed of 9,848 annotated videos with an average length of 30 s, showing activities of 267 people from three continents. Each video is annotated by multiple free-text descriptions, action labels, action intervals and classes of interacted objects. In total, Charades provides 27,847 video descriptions, 66,500 temporally localized intervals for 157 action classes and 41,104 labels for 46 object classes. Using this rich data, we evaluate and provide baseline results for several tasks including action recognition and automatic description generation. We believe that the realism, diversity, and casual nature of this dataset will present unique challenges and new opportunities for computer vision community."
    },
    {
        "paperId": "27850781e39df9f750e05409b8072261124068e8",
        "url": "https://www.semanticscholar.org/paper/27850781e39df9f750e05409b8072261124068e8",
        "title": "A Benchmark and Simulator for UAV Tracking",
        "venue": "European Conference on Computer Vision",
        "year": 2016,
        "citationCount": 1734,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/978-3-319-46448-0_27?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/978-3-319-46448-0_27, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "153013284",
                "name": "Matthias Mueller"
            },
            {
                "authorId": "153372499",
                "name": "Neil G. Smith"
            },
            {
                "authorId": "2931652",
                "name": "Bernard Ghanem"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "29efbe391950ae438c63d86ad5c82b2942efb0b4",
        "url": "https://www.semanticscholar.org/paper/29efbe391950ae438c63d86ad5c82b2942efb0b4",
        "title": "Modeling Context in Referring Expressions",
        "venue": "European Conference on Computer Vision",
        "year": 2016,
        "citationCount": 1510,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1608.00272, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1714982",
                "name": "Licheng Yu"
            },
            {
                "authorId": "3451188",
                "name": "Patrick Poirson"
            },
            {
                "authorId": "70599695",
                "name": "Shan Yang"
            },
            {
                "authorId": "39668247",
                "name": "A. Berg"
            },
            {
                "authorId": "1685538",
                "name": "Tamara L. Berg"
            }
        ],
        "abstract": "Humans refer to objects in their environments all the time, especially in dialogue with other people. We explore generating and comprehending natural language referring expressions for objects in images. In particular, we focus on incorporating better measures of visual context into referring expression models and find that visual comparison to other objects within an image helps improve performance significantly. We also develop methods to tie the language generation process together, so that we generate expressions for all objects of a particular category jointly. Evaluation on three recent datasets - RefCOCO, RefCOCO+, and RefCOCOg (Datasets and toolbox can be downloaded from https://github.com/lichengunc/refer), shows the advantages of our methods for both referring expression generation and comprehension."
    },
    {
        "paperId": "2ec8f7e0257a07d3914322b36072d1bbcd58a1e0",
        "url": "https://www.semanticscholar.org/paper/2ec8f7e0257a07d3914322b36072d1bbcd58a1e0",
        "title": "Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles",
        "venue": "European Conference on Computer Vision",
        "year": 2016,
        "citationCount": 3158,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1603.09246, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1811235696",
                "name": "M. Noroozi"
            },
            {
                "authorId": "145646305",
                "name": "P. Favaro"
            }
        ],
        "abstract": "We propose a novel unsupervised learning approach to build features suitable for object detection and classification. The features are pre-trained on a large dataset without human annotation and later transferred via fine-tuning on a different, smaller and labeled dataset. The pre-training consists of solving jigsaw puzzles of natural images. To facilitate the transfer of features to other tasks, we introduce the context-free network (CFN), a siamese-ennead convolutional neural network. The features correspond to the columns of the CFN and they process image tiles independently (i.e., free of context). The later layers of the CFN then use the features to identify their geometric arrangement. Our experimental evaluations show that the learned features capture semantically relevant content. We pre-train the CFN on the training set of the ILSVRC2012 dataset and transfer the features on the combined training and validation set of Pascal VOC 2007 for object detection (via fast RCNN) and classification. These features outperform all current unsupervised features with \\(51.8\\,\\%\\) for detection and \\(68.6\\,\\%\\) for classification, and reduce the gap with supervised learning (\\(56.5\\,\\%\\) and \\(78.2\\,\\%\\) respectively)."
    },
    {
        "paperId": "392e419efe21b6e2cc1125f27482d22bf3c677be",
        "url": "https://www.semanticscholar.org/paper/392e419efe21b6e2cc1125f27482d22bf3c677be",
        "title": "Single Image Dehazing via Multi-scale Convolutional Neural Networks",
        "venue": "European Conference on Computer Vision",
        "year": 2016,
        "citationCount": 1620,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/978-3-319-46475-6_10?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/978-3-319-46475-6_10, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "144850642",
                "name": "Wenqi Ren"
            },
            {
                "authorId": "50152671",
                "name": "Sibo Liu"
            },
            {
                "authorId": "2108906565",
                "name": "Hua Zhang"
            },
            {
                "authorId": "9416881",
                "name": "Jin-shan Pan"
            },
            {
                "authorId": "1719250",
                "name": "Xiaochun Cao"
            },
            {
                "authorId": "1715634",
                "name": "Ming-Hsuan Yang"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "4233b07033a1ef8af188383f30602a5fd0aa2181",
        "url": "https://www.semanticscholar.org/paper/4233b07033a1ef8af188383f30602a5fd0aa2181",
        "title": "Keep It SMPL: Automatic Estimation of 3D Human Pose and Shape from a Single Image",
        "venue": "European Conference on Computer Vision",
        "year": 2016,
        "citationCount": 1714,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1607.08128, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2988774",
                "name": "Federica Bogo"
            },
            {
                "authorId": "20615377",
                "name": "Angjoo Kanazawa"
            },
            {
                "authorId": "3266545",
                "name": "Christoph Lassner"
            },
            {
                "authorId": "2871555",
                "name": "Peter Gehler"
            },
            {
                "authorId": "143881914",
                "name": "J. Romero"
            },
            {
                "authorId": "2105795",
                "name": "Michael J. Black"
            }
        ],
        "abstract": "We describe the first method to automatically estimate the 3D pose of the human body as well as its 3D shape from a single unconstrained image. We estimate a full 3D mesh and show that 2D joints alone carry a surprising amount of information about body shape. The problem is challenging because of the complexity of the human body, articulation, occlusion, clothing, lighting, and the inherent ambiguity in inferring 3D from 2D. To solve this, we first use a recently published CNN-based method, DeepCut, to predict (bottom-up) the 2D body joint locations. We then fit (top-down) a recently published statistical body shape model, called SMPL, to the 2D joints. We do so by minimizing an objective function that penalizes the error between the projected 3D model joints and detected 2D joints. Because SMPL captures correlations in human shape across the population, we are able to robustly fit it to very little data. We further leverage the 3D model to prevent solutions that cause interpenetration. We evaluate our method, SMPLify, on the Leeds Sports, HumanEva, and Human3.6M datasets, showing superior pose accuracy with respect to the state of the art."
    },
    {
        "paperId": "4603cb8e05258bb0572ae912ad20903b8f99f4b1",
        "url": "https://www.semanticscholar.org/paper/4603cb8e05258bb0572ae912ad20903b8f99f4b1",
        "title": "MS-Celeb-1M: A Dataset and Benchmark for Large-Scale Face Recognition",
        "venue": "European Conference on Computer Vision",
        "year": 2016,
        "citationCount": 2110,
        "openAccessPdf": {
            "url": "https://link.springer.com/content/pdf/10.1007/978-3-319-46487-9_6.pdf",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1607.08221, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3133575",
                "name": "Yandong Guo"
            },
            {
                "authorId": "39089563",
                "name": "Lei Zhang"
            },
            {
                "authorId": "2324337266",
                "name": "Yuxiao Hu"
            },
            {
                "authorId": "144137069",
                "name": "Xiaodong He"
            },
            {
                "authorId": "1800422",
                "name": "Jianfeng Gao"
            }
        ],
        "abstract": "In this paper, we design a benchmark task and provide the associated datasets for recognizing face images and link them to corresponding entity keys in a knowledge base. More specifically, we propose a benchmark task to recognize one million celebrities from their face images, by using all the possibly collected face images of this individual on the web as training data. The rich information provided by the knowledge base helps to conduct disambiguation and improve the recognition accuracy, and contributes to various real-world applications, such as image captioning and news video analysis. Associated with this task, we design and provide concrete measurement set, evaluation protocol, as well as training data. We also present in details our experiment setup and report promising baseline results. Our benchmark task could lead to one of the largest classification problems in computer vision. To the best of our knowledge, our training dataset, which contains 10M images in version 1, is the largest publicly available one in the world."
    },
    {
        "paperId": "47cbebf6c7139b79db2ecb80d69b8effdc4cc276",
        "url": "https://www.semanticscholar.org/paper/47cbebf6c7139b79db2ecb80d69b8effdc4cc276",
        "title": "LIFT: Learned Invariant Feature Transform",
        "venue": "European Conference on Computer Vision",
        "year": 2016,
        "citationCount": 1298,
        "openAccessPdf": {
            "url": "https://link.springer.com/content/pdf/10.1007/978-3-319-46466-4_28.pdf",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1603.09114, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2906716",
                "name": "K. M. Yi"
            },
            {
                "authorId": "1995333",
                "name": "Eduard Trulls"
            },
            {
                "authorId": "1689738",
                "name": "V. Lepetit"
            },
            {
                "authorId": "1717736",
                "name": "P. Fua"
            }
        ],
        "abstract": "We introduce a novel Deep Network architecture that implements the full feature point handling pipeline, that is, detection, orientation estimation, and feature description. While previous works have successfully tackled each one of these problems individually, we show how to learn to do all three in a unified manner while preserving end-to-end differentiability. We then demonstrate that our Deep pipeline outperforms state-of-the-art methods on a number of benchmark datasets, without the need of retraining."
    },
    {
        "paperId": "4cfd770ccecae1c0b4248bc800d7fd35c817bbbd",
        "url": "https://www.semanticscholar.org/paper/4cfd770ccecae1c0b4248bc800d7fd35c817bbbd",
        "title": "A Discriminative Feature Learning Approach for Deep Face Recognition",
        "venue": "European Conference on Computer Vision",
        "year": 2016,
        "citationCount": 3872,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/978-3-319-46478-7_31?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/978-3-319-46478-7_31, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "145357606",
                "name": "Yandong Wen"
            },
            {
                "authorId": "3393556",
                "name": "Kaipeng Zhang"
            },
            {
                "authorId": "2111316339",
                "name": "Zhifeng Li"
            },
            {
                "authorId": "143970608",
                "name": "Y. Qiao"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "4d3beb32fa0efdd10280bad003ef37e5f62f6cbd",
        "url": "https://www.semanticscholar.org/paper/4d3beb32fa0efdd10280bad003ef37e5f62f6cbd",
        "title": "Unsupervised CNN for Single View Depth Estimation: Geometry to the Rescue",
        "venue": "European Conference on Computer Vision",
        "year": 2016,
        "citationCount": 1608,
        "openAccessPdf": {
            "url": "https://link.springer.com/content/pdf/10.1007/978-3-319-46484-8_45.pdf",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1603.04992, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "144751013",
                "name": "Ravi Garg"
            },
            {
                "authorId": "144828045",
                "name": "B. V. Kumar"
            },
            {
                "authorId": "145575177",
                "name": "G. Carneiro"
            },
            {
                "authorId": "145950884",
                "name": "I. Reid"
            }
        ],
        "abstract": "A significant weakness of most current deep Convolutional Neural Networks is the need to train them using vast amounts of manually labelled data. In this work we propose a unsupervised framework to learn a deep convolutional neural network for single view depth prediction, without requiring a pre-training stage or annotated ground-truth depths. We achieve this by training the network in a manner analogous to an autoencoder. At training time we consider a pair of images, source and target, with small, known camera motion between the two such as a stereo pair. We train the convolutional encoder for the task of predicting the depth map for the source image. To do so, we explicitly generate an inverse warp of the target image using the predicted depth and known inter-view displacement, to reconstruct the source image; the photometric error in the reconstruction is the reconstruction loss for the encoder. The acquisition of this training data is considerably simpler than for equivalent systems, requiring no manual annotation, nor calibration of depth sensor to camera. We show that our network trained on less than half of the KITTI dataset gives comparable performance to that of the state-of-the-art supervised methods for single view depth estimation."
    },
    {
        "paperId": "4d9506257186023b78cf19ed4f9e77a4ae4fa0f0",
        "url": "https://www.semanticscholar.org/paper/4d9506257186023b78cf19ed4f9e77a4ae4fa0f0",
        "title": "Visual Relationship Detection with Language Priors",
        "venue": "European Conference on Computer Vision",
        "year": 2016,
        "citationCount": 1191,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1608.00187, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2336035422",
                "name": "Cewu Lu"
            },
            {
                "authorId": "145237361",
                "name": "Ranjay Krishna"
            },
            {
                "authorId": "145879842",
                "name": "Michael S. Bernstein"
            },
            {
                "authorId": "48004138",
                "name": "Li Fei-Fei"
            }
        ],
        "abstract": "Visual relationships capture a wide variety of interactions between pairs of objects in images (e.g. man riding bicycle and man pushing bicycle). Consequently, the set of possible relationships is extremely large and it is difficult to obtain sufficient training examples for all possible relationships. Because of this limitation, previous work on visual relationship detection has concentrated on predicting only a handful of relationships. Though most relationships are infrequent, their objects (e.g. man and bicycle) and predicates (e.g. riding and pushing) independently occur more frequently. We propose a model that uses this insight to train visual models for objects and predicates individually and later combines them together to predict multiple relationships per image. We improve on prior work by leveraging language priors from semantic word embeddings to finetune the likelihood of a predicted relationship. Our model can scale to predict thousands of types of relationships from a few examples. Additionally, we localize the objects in the predicted relationships as bounding boxes in the image. We further demonstrate that understanding relationships can improve content based image retrieval."
    },
    {
        "paperId": "4d9d25e67ebabbfc0acd63798f1a260cb2c8a9bd",
        "url": "https://www.semanticscholar.org/paper/4d9d25e67ebabbfc0acd63798f1a260cb2c8a9bd",
        "title": "Playing for Data: Ground Truth from Computer Games",
        "venue": "European Conference on Computer Vision",
        "year": 2016,
        "citationCount": 2164,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1608.02192, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2724721",
                "name": "Stephan R. Richter"
            },
            {
                "authorId": "143729959",
                "name": "Vibhav Vineet"
            },
            {
                "authorId": "145920814",
                "name": "S. Roth"
            },
            {
                "authorId": "145231047",
                "name": "V. Koltun"
            }
        ],
        "abstract": "Recent progress in computer vision has been driven by high-capacity models trained on large datasets. Unfortunately, creating large datasets with pixel-level labels has been extremely costly due to the amount of human effort required. In this paper, we present an approach to rapidly creating pixel-accurate semantic label maps for images extracted from modern computer games. Although the source code and the internal operation of commercial games are inaccessible, we show that associations between image patches can be reconstructed from the communication between the game and the graphics hardware. This enables rapid propagation of semantic labels within and across images synthesized by the game, with no access to the source code or the content. We validate the presented approach by producing dense pixel-level semantic annotations for 25 thousand images synthesized by a photorealistic open-world computer game. Experiments on semantic segmentation datasets show that using the acquired data to supplement real-world images significantly increases accuracy and that the acquired data enables reducing the amount of hand-labeled real-world data: models trained with game data and just \\(\\tfrac{1}{3}\\) of the CamVid training set outperform models trained on the complete CamVid training set."
    },
    {
        "paperId": "51db1f3c8dfc7d4077da39c96bb90a6358128111",
        "url": "https://www.semanticscholar.org/paper/51db1f3c8dfc7d4077da39c96bb90a6358128111",
        "title": "Deep Networks with Stochastic Depth",
        "venue": "European Conference on Computer Vision",
        "year": 2016,
        "citationCount": 2540,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1603.09382, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "143983679",
                "name": "Gao Huang"
            },
            {
                "authorId": "2117103358",
                "name": "Yu Sun"
            },
            {
                "authorId": "2109168016",
                "name": "Zhuang Liu"
            },
            {
                "authorId": "3371029",
                "name": "Daniel Sedra"
            },
            {
                "authorId": "7446832",
                "name": "Kilian Q. Weinberger"
            }
        ],
        "abstract": "Very deep convolutional networks with hundreds of layers have led to significant reductions in error on competitive benchmarks. Although the unmatched expressiveness of the many layers can be highly desirable at test time, training very deep networks comes with its own set of challenges. The gradients can vanish, the forward flow often diminishes, and the training time can be painfully slow. To address these problems, we propose stochastic depth, a training procedure that enables the seemingly contradictory setup to train short networks and use deep networks at test time. We start with very deep networks but during training, for each mini-batch, randomly drop a subset of layers and bypass them with the identity function. This simple approach complements the recent success of residual networks. It reduces training time substantially and improves the test error significantly on almost all data sets that we used for evaluation. With stochastic depth we can increase the depth of residual networks even beyond 1200 layers and still yield meaningful improvements in test error (4.91 % on CIFAR-10)."
    },
    {
        "paperId": "5f0850ec47a17f22ba2611a5cb67a30cb02cf306",
        "url": "https://www.semanticscholar.org/paper/5f0850ec47a17f22ba2611a5cb67a30cb02cf306",
        "title": "Learning to Track at 100 FPS with Deep Regression Networks",
        "venue": "European Conference on Computer Vision",
        "year": 2016,
        "citationCount": 1241,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1604.01802, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "145641013",
                "name": "David Held"
            },
            {
                "authorId": "144867807",
                "name": "S. Thrun"
            },
            {
                "authorId": "1702137",
                "name": "S. Savarese"
            }
        ],
        "abstract": "Machine learning techniques are often used in computer vision due to their ability to leverage large amounts of training data to improve performance. Unfortunately, most generic object trackers are still trained from scratch online and do not benefit from the large number of videos that are readily available for offline training. We propose a method for offline training of neural networks that can track novel objects at test-time at 100 fps. Our tracker is significantly faster than previous methods that use neural networks for tracking, which are typically very slow to run and not practical for real-time applications. Our tracker uses a simple feed-forward network with no online training required. The tracker learns a generic relationship between object motion and appearance and can be used to track novel objects that do not appear in the training set. We test our network on a standard tracking benchmark to demonstrate our tracker's state-of-the-art performance. Further, our performance improves as we add more videos to our offline training set. To the best of our knowledge, our tracker is the first neural-network tracker that learns to track generic objects at 100 fps."
    },
    {
        "paperId": "5fd235751a9a3e79cfd7599f5e8b4ce7d7baf801",
        "url": "https://www.semanticscholar.org/paper/5fd235751a9a3e79cfd7599f5e8b4ce7d7baf801",
        "title": "Beyond Correlation Filters: Learning Continuous Convolution Operators for Visual Tracking",
        "venue": "European Conference on Computer Vision",
        "year": 2016,
        "citationCount": 1745,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1608.03773, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2488938",
                "name": "Martin Danelljan"
            },
            {
                "authorId": "48325722",
                "name": "Andreas Robinson"
            },
            {
                "authorId": "2358803",
                "name": "F. Khan"
            },
            {
                "authorId": "2228323",
                "name": "M. Felsberg"
            }
        ],
        "abstract": "Discriminative Correlation Filters (DCF) have demonstrated excellent performance for visual object tracking. The key to their success is the ability to efficiently exploit available negative data b ..."
    },
    {
        "paperId": "77f0a39b8e02686fd85b01971f8feb7f60971f80",
        "url": "https://www.semanticscholar.org/paper/77f0a39b8e02686fd85b01971f8feb7f60971f80",
        "title": "Identity Mappings in Deep Residual Networks",
        "venue": "European Conference on Computer Vision",
        "year": 2016,
        "citationCount": 10835,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1603.05027, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "39353098",
                "name": "Kaiming He"
            },
            {
                "authorId": "1771551",
                "name": "X. Zhang"
            },
            {
                "authorId": "3080683",
                "name": "Shaoqing Ren"
            },
            {
                "authorId": null,
                "name": "Jian Sun"
            }
        ],
        "abstract": "Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which makes training easier and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR-10 (4.62 % error) and CIFAR-100, and a 200-layer ResNet on ImageNet. Code is available at: https://github.com/KaimingHe/resnet-1k-layers."
    },
    {
        "paperId": "79da740db9006b2aa3e7b571d038ec895e323121",
        "url": "https://www.semanticscholar.org/paper/79da740db9006b2aa3e7b571d038ec895e323121",
        "title": "Accelerating the Super-Resolution Convolutional Neural Network",
        "venue": "European Conference on Computer Vision",
        "year": 2016,
        "citationCount": 3270,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1608.00367, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "30459277",
                "name": "Chao Dong"
            },
            {
                "authorId": "1717179",
                "name": "Chen Change Loy"
            },
            {
                "authorId": "50295995",
                "name": "Xiaoou Tang"
            }
        ],
        "abstract": "As a successful deep model applied in image super-resolution (SR), the Super-Resolution Convolutional Neural Network (SRCNN) [1, 2] has demonstrated superior performance to the previous hand-crafted models either in speed and restoration quality. However, the high computational cost still hinders it from practical usage that demands real-time performance (24 fps). In this paper, we aim at accelerating the current SRCNN, and propose a compact hourglass-shape CNN structure for faster and better SR. We re-design the SRCNN structure mainly in three aspects. First, we introduce a deconvolution layer at the end of the network, then the mapping is learned directly from the original low-resolution image (without interpolation) to the high-resolution one. Second, we reformulate the mapping layer by shrinking the input feature dimension before mapping and expanding back afterwards. Third, we adopt smaller filter sizes but more mapping layers. The proposed model achieves a speed up of more than 40 times with even superior restoration quality. Further, we present the parameter settings that can achieve real-time performance on a generic CPU while still maintaining good performance. A corresponding transfer strategy is also proposed for fast training and testing across different upscaling factors."
    },
    {
        "paperId": "7f7948942a3064c053d7b85f178994eddaabff87",
        "url": "https://www.semanticscholar.org/paper/7f7948942a3064c053d7b85f178994eddaabff87",
        "title": "Fast Global Registration",
        "venue": "European Conference on Computer Vision",
        "year": 2016,
        "citationCount": 1011,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/978-3-319-46475-6_47?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/978-3-319-46475-6_47, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "7451623",
                "name": "Qian-Yi Zhou"
            },
            {
                "authorId": "2870153",
                "name": "Jaesik Park"
            },
            {
                "authorId": "145231047",
                "name": "V. Koltun"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "8201e6e687f2de477258e9be53ba7b73ee30d7de",
        "url": "https://www.semanticscholar.org/paper/8201e6e687f2de477258e9be53ba7b73ee30d7de",
        "title": "Colorful Image Colorization",
        "venue": "European Conference on Computer Vision",
        "year": 2016,
        "citationCount": 3660,
        "openAccessPdf": {
            "url": "https://link.springer.com/content/pdf/10.1007/978-3-319-46487-9_40.pdf",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1603.08511, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2844849",
                "name": "Richard Zhang"
            },
            {
                "authorId": "2094770",
                "name": "Phillip Isola"
            },
            {
                "authorId": "1763086",
                "name": "Alexei A. Efros"
            }
        ],
        "abstract": "Given a grayscale photograph as input, this paper attacks the problem of hallucinating a plausible color version of the photograph. This problem is clearly underconstrained, so previous approaches have either relied on significant user interaction or resulted in desaturated colorizations. We propose a fully automatic approach that produces vibrant and realistic colorizations. We embrace the underlying uncertainty of the problem by posing it as a classification task and use class-rebalancing at training time to increase the diversity of colors in the result. The system is implemented as a feed-forward pass in a CNN at test time and is trained on over a million color images. We evaluate our algorithm using a colorization Turing test, asking human participants to choose between a generated and ground truth color image. Our method successfully fools humans on 32 % of the trials, significantly higher than previous methods. Moreover, we show that colorization can be a powerful pretext task for self-supervised feature learning, acting as a cross-channel encoder. This approach results in state-of-the-art performance on several feature learning benchmarks."
    },
    {
        "paperId": "848938e6199bad08f1db6f3239b260cfa901e95f",
        "url": "https://www.semanticscholar.org/paper/848938e6199bad08f1db6f3239b260cfa901e95f",
        "title": "Stacked Hourglass Networks for Human Pose Estimation",
        "venue": "European Conference on Computer Vision",
        "year": 2016,
        "citationCount": 5322,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1603.06937, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "31688710",
                "name": "Alejandro Newell"
            },
            {
                "authorId": "34284131",
                "name": "Kaiyu Yang"
            },
            {
                "authorId": "153302678",
                "name": "Jia Deng"
            }
        ],
        "abstract": "This work introduces a novel convolutional network architecture for the task of human pose estimation. Features are processed across all scales and consolidated to best capture the various spatial relationships associated with the body. We show how repeated bottom-up, top-down processing used in conjunction with intermediate supervision is critical to improving the performance of the network. We refer to the architecture as a stacked hourglass network based on the successive steps of pooling and upsampling that are done to produce a final set of predictions. State-of-the-art results are achieved on the FLIC and MPII benchmarks outcompeting all recent methods."
    },
    {
        "paperId": "8e63784bd5a24d5e3035e2a11753e65e6e56625d",
        "url": "https://www.semanticscholar.org/paper/8e63784bd5a24d5e3035e2a11753e65e6e56625d",
        "title": "Learning Representations for Automatic Colorization",
        "venue": "European Conference on Computer Vision",
        "year": 2016,
        "citationCount": 1042,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1603.06668, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "34652567",
                "name": "Gustav Larsson"
            },
            {
                "authorId": "145854440",
                "name": "M. Maire"
            },
            {
                "authorId": "2490189",
                "name": "Gregory Shakhnarovich"
            }
        ],
        "abstract": "We develop a fully automatic image colorization system. Our approach leverages recent advances in deep networks, exploiting both low-level and semantic representations. As many scene elements naturally appear according to multimodal color distributions, we train our model to predict per-pixel color histograms. This intermediate output can be used to automatically generate a color image, or further manipulated prior to image formation. On both fully and partially automatic colorization tasks, we outperform existing methods. We also explore colorization as a vehicle for self-supervised visual representation learning."
    },
    {
        "paperId": "915c4bb289b3642489e904c65a47fa56efb60658",
        "url": "https://www.semanticscholar.org/paper/915c4bb289b3642489e904c65a47fa56efb60658",
        "title": "Perceptual Losses for Real-Time Style Transfer and Super-Resolution",
        "venue": "European Conference on Computer Vision",
        "year": 2016,
        "citationCount": 11018,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1603.08155, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2115231104",
                "name": "Justin Johnson"
            },
            {
                "authorId": "3304525",
                "name": "Alexandre Alahi"
            },
            {
                "authorId": "48004138",
                "name": "Li Fei-Fei"
            }
        ],
        "abstract": "We consider image transformation problems, where an input image is transformed into an output image. Recent methods for such problems typically train feed-forward convolutional neural networks using a per-pixel loss between the output and ground-truth images. Parallel work has shown that high-quality images can be generated by defining and optimizing perceptual loss functions based on high-level features extracted from pretrained networks. We combine the benefits of both approaches, and propose the use of perceptual loss functions for training feed-forward networks for image transformation tasks. We show results on image style transfer, where a feed-forward network is trained to solve the optimization problem proposed by Gatys et al. in real-time. Compared to the optimization-based method, our network gives similar qualitative results but is three orders of magnitude faster. We also experiment with single-image super-resolution, where replacing a per-pixel loss with a perceptual loss gives visually pleasing results."
    },
    {
        "paperId": "9afbd70a4727df98a0c38c437b94b14eba6577c4",
        "url": "https://www.semanticscholar.org/paper/9afbd70a4727df98a0c38c437b94b14eba6577c4",
        "title": "Spatio-Temporal LSTM with Trust Gates for 3D Human Action Recognition",
        "venue": "European Conference on Computer Vision",
        "year": 2016,
        "citationCount": 1179,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1607.07043, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2270302579",
                "name": "Jun Liu"
            },
            {
                "authorId": "3000984",
                "name": "Amir Shahroudy"
            },
            {
                "authorId": "2271201592",
                "name": "Dong Xu"
            },
            {
                "authorId": "2270961300",
                "name": "Gang Wang"
            }
        ],
        "abstract": "3D action recognition  analysis of human actions based on 3D skeleton data  becomes popular recently due to its succinctness, robustness, and view-invariant representation. Recent attempts on this problem suggested to develop RNN-based learning methods to model the contextual dependency in the temporal domain. In this paper, we extend this idea to spatio-temporal domains to analyze the hidden sources of action-related information within the input data over both domains concurrently. Inspired by the graphical structure of the human skeleton, we further propose a more powerful tree-structure based traversal method. To handle the noise and occlusion in 3D skeleton data, we introduce new gating mechanism within LSTM to learn the reliability of the sequential input data and accordingly adjust its effect on updating the long-term context information stored in the memory cell. Our method achieves state-of-the-art performance on 4 challenging benchmark datasets for 3D human action analysis."
    },
    {
        "paperId": "a2f4cae1acba37426372718fc30745055c8c2140",
        "url": "https://www.semanticscholar.org/paper/a2f4cae1acba37426372718fc30745055c8c2140",
        "title": "DeeperCut: A Deeper, Stronger, and Faster Multi-person Pose Estimation Model",
        "venue": "European Conference on Computer Vision",
        "year": 2016,
        "citationCount": 1162,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1605.03170, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3205238",
                "name": "Eldar Insafutdinov"
            },
            {
                "authorId": "2299109",
                "name": "L. Pishchulin"
            },
            {
                "authorId": "16576043",
                "name": "Bjoern Andres"
            },
            {
                "authorId": "1906895",
                "name": "Mykhaylo Andriluka"
            },
            {
                "authorId": "48920094",
                "name": "B. Schiele"
            }
        ],
        "abstract": "The goal of this paper is to advance the state-of-the-art of articulated pose estimation in scenes with multiple people. To that end we contribute on three fronts. We propose (1) improved body part detectors that generate effective bottom-up proposals for body parts; (2) novel image-conditioned pairwise terms that allow to assemble the proposals into a variable number of consistent body part configurations; and (3) an incremental optimization strategy that explores the search space more efficiently thus leading both to better performance and significant speed-up factors. Evaluation is done on two single-person and two multi-person pose estimation benchmarks. The proposed approach significantly outperforms best known multi-person pose estimation results while demonstrating competitive performance on the task of single person pose estimation (Models and code available at http://pose.mpi-inf.mpg.de)."
    },
    {
        "paperId": "a9032d632e04399e46efe0668d8b19993aff6dbc",
        "url": "https://www.semanticscholar.org/paper/a9032d632e04399e46efe0668d8b19993aff6dbc",
        "title": "3D-R2N2: A Unified Approach for Single and Multi-view 3D Object Reconstruction",
        "venue": "European Conference on Computer Vision",
        "year": 2016,
        "citationCount": 1823,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1604.00449, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2144103",
                "name": "C. Choy"
            },
            {
                "authorId": "2068265",
                "name": "Danfei Xu"
            },
            {
                "authorId": "39813007",
                "name": "JunYoung Gwak"
            },
            {
                "authorId": "143887468",
                "name": "Kevin Chen"
            },
            {
                "authorId": "1702137",
                "name": "S. Savarese"
            }
        ],
        "abstract": "Inspired by the recent success of methods that employ shape priors to achieve robust 3D reconstructions, we propose a novel recurrent neural network architecture that we call the 3D Recurrent Reconstruction Neural Network (3D-R2N2). The network learns a mapping from images of objects to their underlying 3D shapes from a large collection of synthetic data [13]. Our network takes in one or more images of an object instance from arbitrary viewpoints and outputs a reconstruction of the object in the form of a 3D occupancy grid. Unlike most of the previous works, our network does not require any image annotations or object class labels for training or testing. Our extensive experimental analysis shows that our reconstruction framework (i) outperforms the state-of-the-art methods for single view reconstruction, and (ii) enables the 3D reconstruction of objects in situations when traditional SFM/SLAM methods fail (because of lack of texture and/or wide baseline)."
    },
    {
        "paperId": "b649a98ce77ece8cd7638bb74ab77d22d9be77e7",
        "url": "https://www.semanticscholar.org/paper/b649a98ce77ece8cd7638bb74ab77d22d9be77e7",
        "title": "XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks",
        "venue": "European Conference on Computer Vision",
        "year": 2016,
        "citationCount": 4576,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1603.05279, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "143887493",
                "name": "Mohammad Rastegari"
            },
            {
                "authorId": "2004053",
                "name": "Vicente Ordonez"
            },
            {
                "authorId": "40497777",
                "name": "Joseph Redmon"
            },
            {
                "authorId": "143787583",
                "name": "Ali Farhadi"
            }
        ],
        "abstract": "We propose two efficient approximations to standard convolutional neural networks: Binary-Weight-Networks and XNOR-Networks. In Binary-Weight-Networks, the filters are approximated with binary values resulting in 32\\(\\times \\) memory saving. In XNOR-Networks, both the filters and the input to convolutional layers are binary. XNOR-Networks approximate convolutions using primarily binary operations. This results in 58\\(\\times \\) faster convolutional operations (in terms of number of the high precision operations) and 32\\(\\times \\) memory savings. XNOR-Nets offer the possibility of running state-of-the-art networks on CPUs (rather than GPUs) in real-time. Our binary networks are simple, accurate, efficient, and work on challenging visual tasks. We evaluate our approach on the ImageNet classification task. The classification accuracy with a Binary-Weight-Network version of AlexNet is the same as the full-precision AlexNet. We compare our method with recent network binarization methods, BinaryConnect and BinaryNets, and outperform these methods by large margins on ImageNet, more than \\(16\\,\\%\\) in top-1 accuracy. Our code is available at: http://allenai.org/plato/xnornet."
    },
    {
        "paperId": "c0387e788a52f10bf35d4d50659cfa515d89fbec",
        "url": "https://www.semanticscholar.org/paper/c0387e788a52f10bf35d4d50659cfa515d89fbec",
        "title": "MARS: A Video Benchmark for Large-Scale Person Re-Identification",
        "venue": "European Conference on Computer Vision",
        "year": 2016,
        "citationCount": 1042,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/978-3-319-46466-4_52?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/978-3-319-46466-4_52, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "144802394",
                "name": "Liang Zheng"
            },
            {
                "authorId": "32135615",
                "name": "Zhi Bie"
            },
            {
                "authorId": "2108935429",
                "name": "Yifan Sun"
            },
            {
                "authorId": "1688516",
                "name": "Jingdong Wang"
            },
            {
                "authorId": "145422145",
                "name": "Chi Su"
            },
            {
                "authorId": "1678689",
                "name": "Shengjin Wang"
            },
            {
                "authorId": "144876831",
                "name": "Q. Tian"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "d277218265a177a8f2cdce7f88b4152fc1eda66c",
        "url": "https://www.semanticscholar.org/paper/d277218265a177a8f2cdce7f88b4152fc1eda66c",
        "title": "Pixelwise View Selection for Unstructured Multi-View Stereo",
        "venue": "European Conference on Computer Vision",
        "year": 2016,
        "citationCount": 2544,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/978-3-319-46487-9_31?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/978-3-319-46487-9_31, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3010882",
                "name": "Johannes L. Schnberger"
            },
            {
                "authorId": "145797829",
                "name": "Enliang Zheng"
            },
            {
                "authorId": "40454588",
                "name": "Jan-Michael Frahm"
            },
            {
                "authorId": "1742208",
                "name": "M. Pollefeys"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "d6473533e89e5f946a6ff3ad07c6a74ee9b47672",
        "url": "https://www.semanticscholar.org/paper/d6473533e89e5f946a6ff3ad07c6a74ee9b47672",
        "title": "A Unified Multi-scale Deep Convolutional Neural Network for Fast Object Detection",
        "venue": "European Conference on Computer Vision",
        "year": 2016,
        "citationCount": 1532,
        "openAccessPdf": {
            "url": "https://link.springer.com/content/pdf/10.1007/978-3-319-46493-0_22.pdf",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1607.07155, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1773408",
                "name": "Zhaowei Cai"
            },
            {
                "authorId": "33421444",
                "name": "Quanfu Fan"
            },
            {
                "authorId": "1723233",
                "name": "R. Feris"
            },
            {
                "authorId": "1699559",
                "name": "N. Vasconcelos"
            }
        ],
        "abstract": "A unified deep neural network, denoted the multi-scale CNN (MS-CNN), is proposed for fast multi-scale object detection. The MS-CNN consists of a proposal sub-network and a detection sub-network. In the proposal sub-network, detection is performed at multiple output layers, so that receptive fields match objects of different scales. These complementary scale-specific detectors are combined to produce a strong multi-scale object detector. The unified network is learned end-to-end, by optimizing a multi-task loss. Feature upsampling by deconvolution is also explored, as an alternative to input upsampling, to reduce the memory and computation costs. State-of-the-art object detection performance, at up to 15 fps, is reported on datasets, such as KITTI and Caltech, containing a substantial number of small objects."
    },
    {
        "paperId": "ea3d7de6c0880e14455b9acb28f1bc1234321456",
        "url": "https://www.semanticscholar.org/paper/ea3d7de6c0880e14455b9acb28f1bc1234321456",
        "title": "Temporal Segment Networks: Towards Good Practices for Deep Action Recognition",
        "venue": "European Conference on Computer Vision",
        "year": 2016,
        "citationCount": 4120,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1608.00859, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2109120086",
                "name": "Limin Wang"
            },
            {
                "authorId": "3331521",
                "name": "Yuanjun Xiong"
            },
            {
                "authorId": "1915826",
                "name": "Zhe Wang"
            },
            {
                "authorId": "143970608",
                "name": "Y. Qiao"
            },
            {
                "authorId": "1807606",
                "name": "Dahua Lin"
            },
            {
                "authorId": "50295995",
                "name": "Xiaoou Tang"
            },
            {
                "authorId": "1681236",
                "name": "L. Gool"
            }
        ],
        "abstract": "Deep convolutional networks have achieved great success for visual recognition in still images. However, for action recognition in videos, the advantage over traditional methods is not so evident. This paper aims to discover the principles to design effective ConvNet architectures for action recognition in videos and learn these models given limited training samples. Our first contribution is temporal segment network (TSN), a novel framework for video-based action recognition. which is based on the idea of long-range temporal structure modeling. It combines a sparse temporal sampling strategy and video-level supervision to enable efficient and effective learning using the whole action video. The other contribution is our study on a series of good practices in learning ConvNets on video data with the help of temporal segment network. Our approach obtains the state-the-of-art performance on the datasets of HMDB51 (\\( 69.4\\,\\% \\)) and UCF101 (\\( 94.2\\,\\% \\)). We also visualize the learned ConvNet models, which qualitatively demonstrates the effectiveness of temporal segment network and the proposed good practices (Models and code at https://github.com/yjxiong/temporal-segment-networks)."
    },
    {
        "paperId": "f90d9c5615f4a0e3f9a1ce2a0075269b9bab6b5f",
        "url": "https://www.semanticscholar.org/paper/f90d9c5615f4a0e3f9a1ce2a0075269b9bab6b5f",
        "title": "SPICE: Semantic Propositional Image Caption Evaluation",
        "venue": "European Conference on Computer Vision",
        "year": 2016,
        "citationCount": 2150,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1607.08822, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "6965856",
                "name": "Peter Anderson"
            },
            {
                "authorId": "1688071",
                "name": "Basura Fernando"
            },
            {
                "authorId": "145177220",
                "name": "Mark Johnson"
            },
            {
                "authorId": "145273587",
                "name": "Stephen Gould"
            }
        ],
        "abstract": "There is considerable interest in the task of automatically generating image captions. However, evaluation is challenging. Existing automatic evaluation metrics are primarily sensitive to n-gram overlap, which is neither necessary nor sufficient for the task of simulating human judgment. We hypothesize that semantic propositional content is an important component of human caption evaluation, and propose a new automated caption evaluation metric defined over scene graphs coined SPICE. Extensive evaluations across a range of models and datasets indicate that SPICE captures human judgments over model-generated captions better than other automatic metrics (e.g., system-level correlation of 0.88 with human judgments on the MS COCO dataset, versus 0.43 for CIDEr and 0.53 for METEOR). Furthermore, SPICE can answer questions such as which caption-generator best understands colors? and can caption-generators count?"
    },
    {
        "paperId": "fc7822f56dd255a872326b9536a0821bbf0277dd",
        "url": "https://www.semanticscholar.org/paper/fc7822f56dd255a872326b9536a0821bbf0277dd",
        "title": "Generative Visual Manipulation on the Natural Image Manifold",
        "venue": "European Conference on Computer Vision",
        "year": 2016,
        "citationCount": 1432,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1609.03552, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2436356",
                "name": "Jun-Yan Zhu"
            },
            {
                "authorId": "2562966",
                "name": "Philipp Krhenbhl"
            },
            {
                "authorId": "2177801",
                "name": "Eli Shechtman"
            },
            {
                "authorId": "1763086",
                "name": "Alexei A. Efros"
            }
        ],
        "abstract": "Realistic image manipulation is challenging because it requires modifying the image appearance in a user-controlled way, while preserving the realism of the result. Unless the user has considerable artistic skill, it is easy to fall off the manifold of natural images while editing. In this paper, we propose to learn the natural image manifold directly from data using a generative adversarial neural network. We then define a class of image editing operations, and constrain their output to lie on that learned manifold at all times. The model automatically adjusts the output keeping all edits as realistic as possible. All our manipulations are expressed in terms of constrained optimization and are applied in near-real time. We evaluate our algorithm on the task of realistic photo manipulation of shape and color. The presented method can further be used for changing one image to look like the other, as well as generating novel imagery from scratch based on users scribbles."
    },
    {
        "paperId": "35fe3dd3350c32467030884337dde10d5e20ff99",
        "url": "https://www.semanticscholar.org/paper/35fe3dd3350c32467030884337dde10d5e20ff99",
        "title": "ICNet for Real-Time Semantic Segmentation on High-Resolution Images",
        "venue": "European Conference on Computer Vision",
        "year": 2017,
        "citationCount": 1537,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1704.08545, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3459894",
                "name": "Hengshuang Zhao"
            },
            {
                "authorId": "50844674",
                "name": "Xiaojuan Qi"
            },
            {
                "authorId": "2029246",
                "name": "Xiaoyong Shen"
            },
            {
                "authorId": "1788070",
                "name": "Jianping Shi"
            },
            {
                "authorId": "1729056",
                "name": "Jiaya Jia"
            }
        ],
        "abstract": "We focus on the challenging task of real-time semantic segmentation in this paper. It finds many practical applications and yet is with fundamental difficulty of reducing a large portion of computation for pixel-wise label inference. We propose an image cascade network (ICNet) that incorporates multi-resolution branches under proper label guidance to address this challenge. We provide in-depth analysis of our framework and introduce the cascade feature fusion unit to quickly achieve high-quality segmentation. Our system yields real-time inference on a single GPU card with decent quality results evaluated on challenging datasets like Cityscapes, CamVid and COCO-Stuff."
    },
    {
        "paperId": "5f79398057bf0bbda9ff50067bc1f2950c2a2266",
        "url": "https://www.semanticscholar.org/paper/5f79398057bf0bbda9ff50067bc1f2950c2a2266",
        "title": "Progressive Neural Architecture Search",
        "venue": "European Conference on Computer Vision",
        "year": 2017,
        "citationCount": 2094,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1712.00559, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "50557601",
                "name": "Chenxi Liu"
            },
            {
                "authorId": "2368067",
                "name": "Barret Zoph"
            },
            {
                "authorId": "1789737",
                "name": "Jonathon Shlens"
            },
            {
                "authorId": "2052830834",
                "name": "Wei Hua"
            },
            {
                "authorId": "2040091191",
                "name": "Li-Jia Li"
            },
            {
                "authorId": "48004138",
                "name": "Li Fei-Fei"
            },
            {
                "authorId": "145081362",
                "name": "A. Yuille"
            },
            {
                "authorId": "2136435893",
                "name": "Jonathan Huang"
            },
            {
                "authorId": "1702318",
                "name": "K. Murphy"
            }
        ],
        "abstract": "We propose a new method for learning the structure of convolutional neural networks (CNNs) that is more efficient than recent state-of-the-art methods based on reinforcement learning and evolutionary algorithms. Our approach uses a sequential model-based optimization (SMBO) strategy, in which we search for structures in order of increasing complexity, while simultaneously learning a surrogate model to guide the search through structure space. Direct comparison under the same search space shows that our method is up to 5 times more efficient than the RL method of Zoph et al. (2018) in terms of number of models evaluated, and 8 times faster in terms of total compute. The structures we discover in this way achieve state of the art classification accuracies on CIFAR-10 and ImageNet."
    },
    {
        "paperId": "713b0d9005944f80af00addc81b162ca74ea4b14",
        "url": "https://www.semanticscholar.org/paper/713b0d9005944f80af00addc81b162ca74ea4b14",
        "title": "Memory Aware Synapses: Learning what (not) to forget",
        "venue": "European Conference on Computer Vision",
        "year": 2017,
        "citationCount": 1851,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1711.09601, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2855778",
                "name": "Rahaf Aljundi"
            },
            {
                "authorId": "30588063",
                "name": "F. Babiloni"
            },
            {
                "authorId": "2066380705",
                "name": "Mohamed Elhoseiny"
            },
            {
                "authorId": "34849128",
                "name": "Marcus Rohrbach"
            },
            {
                "authorId": "1704728",
                "name": "T. Tuytelaars"
            }
        ],
        "abstract": "Humans can learn in a continuous manner. Old rarely utilized knowledge can be overwritten by new incoming information while important, frequently used knowledge is prevented from being erased. In artificial learning systems, lifelong learning so far has focused mainly on accumulating knowledge over tasks and overcoming catastrophic forgetting. In this paper, we argue that, given the limited model capacity and the unlimited new information to be learned, knowledge has to be preserved or erased selectively. Inspired by neuroplasticity, we propose a novel approach for lifelong learning, coined Memory Aware Synapses (MAS). It computes the importance of the parameters of a neural network in an unsupervised and online manner. Given a new sample which is fed to the network, MAS accumulates an importance measure for each parameter of the network, based on how sensitive the predicted output function is to a change in this parameter. When learning a new task, changes to important parameters can then be penalized, effectively preventing important knowledge related to previous tasks from being overwritten. Further, we show an interesting connection between a local version of our method and Hebb's rule,which is a model for the learning process in the brain. We test our method on a sequence of object recognition tasks and on the challenging problem of learning an embedding for predicting $ $ triplets. We show state-of-the-art performance and, for the first time, the ability to adapt the importance of the parameters based on unlabeled data towards what the network needs (not) to forget, which may vary depending on test conditions."
    },
    {
        "paperId": "753d2a35c9edf5dfcac4ef3a6adc993b657b01f0",
        "url": "https://www.semanticscholar.org/paper/753d2a35c9edf5dfcac4ef3a6adc993b657b01f0",
        "title": "Beyond Part Models: Person Retrieval with Refined Part Pooling",
        "venue": "European Conference on Computer Vision",
        "year": 2017,
        "citationCount": 2317,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1711.09349, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2108935429",
                "name": "Yifan Sun"
            },
            {
                "authorId": "144802394",
                "name": "Liang Zheng"
            },
            {
                "authorId": "7179962",
                "name": "Yi Yang"
            },
            {
                "authorId": "144876831",
                "name": "Q. Tian"
            },
            {
                "authorId": "1678689",
                "name": "Shengjin Wang"
            }
        ],
        "abstract": "Employing part-level features for pedestrian image description offers fine-grained information and has been verified as beneficial for person retrieval in very recent literature. A prerequisite of part discovery is that each part should be well located. Instead of using external cues, e.g., pose estimation, to directly locate parts, this paper lays emphasis on the content consistency within each part. \nSpecifically, we target at learning discriminative part-informed features for person retrieval and make two contributions. (i) A network named Part-based Convolutional Baseline (PCB). Given an image input, it outputs a convolutional descriptor consisting of several part-level features. With a uniform partition strategy, PCB achieves competitive results with the state-of-the-art methods, proving itself as a strong convolutional baseline for person retrieval. \n(ii) A refined part pooling (RPP) method. Uniform partition inevitably incurs outliers in each part, which are in fact more similar to other parts. RPP re-assigns these outliers to the parts they are closest to, resulting in refined parts with enhanced within-part consistency. Experiment confirms that RPP allows PCB to gain another round of performance boost. For instance, on the Market-1501 dataset, we achieve (77.4+4.2)% mAP and (92.3+1.5)% rank-1 accuracy, surpassing the state of the art by a large margin."
    },
    {
        "paperId": "815aa52cfc02961d82415f080384594639a21984",
        "url": "https://www.semanticscholar.org/paper/815aa52cfc02961d82415f080384594639a21984",
        "title": "Rethinking Spatiotemporal Feature Learning: Speed-Accuracy Trade-offs in Video Classification",
        "venue": "European Conference on Computer Vision",
        "year": 2017,
        "citationCount": 1443,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1712.04851, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1817030",
                "name": "Saining Xie"
            },
            {
                "authorId": "2118831303",
                "name": "Chen Sun"
            },
            {
                "authorId": "2136435893",
                "name": "Jonathan Huang"
            },
            {
                "authorId": "144035504",
                "name": "Z. Tu"
            },
            {
                "authorId": "1702318",
                "name": "K. Murphy"
            }
        ],
        "abstract": "Despite the steady progress in video analysis led by the adoption of convolutional neural networks (CNNs), the relative improvement has been less drastic as that in 2D static image classification. Three main challenges exist including spatial (image) feature representation, temporal information representation, and model/computation complexity. It was recently shown by Carreira and Zisserman that 3D CNNs, inflated from 2D networks and pretrained on ImageNet, could be a promising way for spatial and temporal representation learning. However, as for model/computation complexity, 3D CNNs are much more expensive than 2D CNNs and prone to overfit. We seek a balance between speed and accuracy by building an effective and efficient video classification system through systematic exploration of critical network design choices. In particular, we show that it is possible to replace many of the 3D convolutions by low-cost 2D convolutions. Rather surprisingly, best result (in both speed and accuracy) is achieved when replacing the 3D convolutions at the bottom of the network, suggesting that temporal representation learning on high-level semantic features is more useful. Our conclusion generalizes to datasets with very different properties. When combined with several other cost-effective designs including separable spatial/temporal convolution and feature gating, our system results in an effective video classification system that that produces very competitive results on several action classification benchmarks (Kinetics, Something-something, UCF101 and HMDB), as well as two action detection (localization) benchmarks (JHMDB and UCF101-24)."
    },
    {
        "paperId": "8ad12d3ee186403b856639b58d7797aa4b89a6c7",
        "url": "https://www.semanticscholar.org/paper/8ad12d3ee186403b856639b58d7797aa4b89a6c7",
        "title": "Temporal Relational Reasoning in Videos",
        "venue": "European Conference on Computer Vision",
        "year": 2017,
        "citationCount": 1098,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1711.08496, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "145291669",
                "name": "Bolei Zhou"
            },
            {
                "authorId": "50112310",
                "name": "A. Andonian"
            },
            {
                "authorId": "143805211",
                "name": "A. Torralba"
            }
        ],
        "abstract": "Temporal relational reasoning, the ability to link meaningful transformations of objects or entities over time, is a fundamental property of intelligent species. In this paper, we introduce an effective and interpretable network module, the Temporal Relation Network (TRN), designed to learn and reason about temporal dependencies between video frames at multiple time scales. We evaluate TRN-equipped networks on activity recognition tasks using three recent video datasets - Something-Something, Jester, and Charades - which fundamentally depend on temporal relational reasoning. Our results demonstrate that the proposed TRN gives convolutional neural networks a remarkable capacity to discover temporal relations in videos. Through only sparsely sampled video frames, TRN-equipped networks can accurately predict human-object interactions in the Something-Something dataset and identify various human gestures on the Jester dataset with very competitive performance. TRN-equipped networks also outperform two-stream networks and 3D convolution networks in recognizing daily activities in the Charades dataset. Further analyses show that the models learn intuitive and interpretable visual common sense knowledge in videos (Code and models are available at http://relation.csail.mit.edu/.)."
    },
    {
        "paperId": "ff772950f66ac6a57f4201ce1f02f0013ccdc1bb",
        "url": "https://www.semanticscholar.org/paper/ff772950f66ac6a57f4201ce1f02f0013ccdc1bb",
        "title": "Receptive Field Block Net for Accurate and Fast Object Detection",
        "venue": "European Conference on Computer Vision",
        "year": 2017,
        "citationCount": 1440,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1711.07767, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2144363412",
                "name": "Songtao Liu"
            },
            {
                "authorId": "40119164",
                "name": "Di Huang"
            },
            {
                "authorId": "2108702972",
                "name": "Yunhong Wang"
            }
        ],
        "abstract": "Current top-performing object detectors depend on deep CNN backbones, such as ResNet-101 and Inception, benefiting from their powerful feature representations but suffering from high computational costs. Conversely, some lightweight model based detectors fulfil real time processing, while their accuracies are often criticized. In this paper, we explore an alternative to build a fast and accurate detector by strengthening lightweight features using a hand-crafted mechanism. Inspired by the structure of Receptive Fields (RFs) in human visual systems, we propose a novel RF Block (RFB) module, which takes the relationship between the size and eccentricity of RFs into account, to enhance the feature discriminability and robustness. We further assemble RFB to the top of SSD, constructing the RFB Net detector. To evaluate its effectiveness, experiments are conducted on two major benchmarks and the results show that RFB Net is able to reach the performance of advanced very deep detectors while keeping the real-time speed. Code is available at this https URL."
    },
    {
        "paperId": "058dd355ec60af11048ee49a54db599003356322",
        "url": "https://www.semanticscholar.org/paper/058dd355ec60af11048ee49a54db599003356322",
        "title": "End-to-End Incremental Learning",
        "venue": "European Conference on Computer Vision",
        "year": 2018,
        "citationCount": 1252,
        "openAccessPdf": {
            "url": "https://hal.inria.fr/hal-01849366/file/IncrementalLearning_ECCV2018.pdf",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1807.09536, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "144655318",
                "name": "F. M. Castro"
            },
            {
                "authorId": "1398347979",
                "name": "M. Marn-Jimnez"
            },
            {
                "authorId": "1712683",
                "name": "Nicols Guil Mata"
            },
            {
                "authorId": "2462253",
                "name": "C. Schmid"
            },
            {
                "authorId": "72492981",
                "name": "Alahari Karteek"
            }
        ],
        "abstract": "Although deep learning approaches have stood out in recent years due to their state-of-the-art results, they continue to suffer from catastrophic forgetting, a dramatic decrease in overall performance when training with new classes added incrementally. This is due to current neural network architectures requiring the entire dataset, consisting of all the samples from the old as well as the new classes, to update the modela requirement that becomes easily unsustainable as the number of classes grows. We address this issue with our approach to learn deep neural networks incrementally, using new data and only a small exemplar set corresponding to samples from the old classes. This is based on a loss composed of a distillation measure to retain the knowledge acquired from the old classes, and a cross-entropy loss to learn the new classes. Our incremental training is achieved while keeping the entire framework end-to-end, i.e., learning the data representation and the classifier jointly, unlike recent methods with no such guarantees. We evaluate our method extensively on the CIFAR-100 and ImageNet (ILSVRC 2012) image classification datasets, and show state-of-the-art performance."
    },
    {
        "paperId": "0f885fd46064d271d4404cf9bb3d758e1a6f8d55",
        "url": "https://www.semanticscholar.org/paper/0f885fd46064d271d4404cf9bb3d758e1a6f8d55",
        "title": "Exploring the Limits of Weakly Supervised Pretraining",
        "venue": "European Conference on Computer Vision",
        "year": 2018,
        "citationCount": 1426,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1805.00932, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "144542135",
                "name": "D. Mahajan"
            },
            {
                "authorId": "2983898",
                "name": "Ross B. Girshick"
            },
            {
                "authorId": "34066479",
                "name": "Vignesh Ramanathan"
            },
            {
                "authorId": "39353098",
                "name": "Kaiming He"
            },
            {
                "authorId": "2210374",
                "name": "Manohar Paluri"
            },
            {
                "authorId": "1527103472",
                "name": "Yixuan Li"
            },
            {
                "authorId": "1842920",
                "name": "Ashwin R. Bharambe"
            },
            {
                "authorId": "1803520",
                "name": "L. Maaten"
            }
        ],
        "abstract": "State-of-the-art visual perception models for a wide range of tasks rely on supervised pretraining. ImageNet classification is the de facto pretraining task for these models. Yet, ImageNet is now nearly ten years old and is by modern standards \"small\". Even so, relatively little is known about the behavior of pretraining with datasets that are multiple orders of magnitude larger. The reasons are obvious: such datasets are difficult to collect and annotate. In this paper, we present a unique study of transfer learning with large convolutional networks trained to predict hashtags on billions of social media images. Our experiments demonstrate that training for large-scale hashtag prediction leads to excellent results. We show improvements on several image classification and object detection tasks, and report the highest ImageNet-1k single-crop, top-1 accuracy to date: 85.4% (97.6% top-5). We also perform extensive experiments that provide novel empirical data on the relationship between large-scale pretraining and transfer learning performance."
    },
    {
        "paperId": "1717255b6aea01fe956cef998abbc3c399b5d7cf",
        "url": "https://www.semanticscholar.org/paper/1717255b6aea01fe956cef998abbc3c399b5d7cf",
        "title": "AMC: AutoML for Model Compression and Acceleration on Mobile Devices",
        "venue": "European Conference on Computer Vision",
        "year": 2018,
        "citationCount": 1429,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1802.03494, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "39838894",
                "name": "Yihui He"
            },
            {
                "authorId": "46698300",
                "name": "Ji Lin"
            },
            {
                "authorId": "47781592",
                "name": "Zhijian Liu"
            },
            {
                "authorId": "35446689",
                "name": "Hanrui Wang"
            },
            {
                "authorId": "2040091191",
                "name": "Li-Jia Li"
            },
            {
                "authorId": "143840275",
                "name": "Song Han"
            }
        ],
        "abstract": "Model compression is an effective technique to efficiently deploy neural network models on mobile devices which have limited computation resources and tight power budgets. Conventional model compression techniques rely on hand-crafted features and require domain experts to explore the large design space trading off among model size, speed, and accuracy, which is usually sub-optimal and time-consuming. In this paper, we propose AutoML for Model Compression (AMC) which leverages reinforcement learning to efficiently sample the design space and can improve the model compression quality. We achieved state-of-the-art model compression results in a fully automated way without any human efforts. Under 4\\(\\times \\) FLOPs reduction, we achieved 2.7% better accuracy than the hand-crafted model compression method for VGG-16 on ImageNet. We applied this automated, push-the-button compression pipeline to MobileNet-V1 and achieved a speedup of 1.53\\(\\times \\) on the GPU (Titan Xp) and 1.95\\(\\times \\) on an Android phone (Google Pixel 1), with negligible loss of accuracy."
    },
    {
        "paperId": "1d033b30f38642e4b6dd146bb8b464bfb58aad96",
        "url": "https://www.semanticscholar.org/paper/1d033b30f38642e4b6dd146bb8b464bfb58aad96",
        "title": "Deep Clustering for Unsupervised Learning of Visual Features",
        "venue": "European Conference on Computer Vision",
        "year": 2018,
        "citationCount": 2148,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1807.05520, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2062862676",
                "name": "Mathilde Caron"
            },
            {
                "authorId": "2329288",
                "name": "Piotr Bojanowski"
            },
            {
                "authorId": "2319608",
                "name": "Armand Joulin"
            },
            {
                "authorId": "3271933",
                "name": "Matthijs Douze"
            }
        ],
        "abstract": "Clustering is a class of unsupervised learning methods that has been extensively applied and studied in computer vision. Little work has been done to adapt it to the end-to-end training of visual features on large-scale datasets. In this work, we present DeepCluster, a clustering method that jointly learns the parameters of a neural network and the cluster assignments of the resulting features. DeepCluster iteratively groups the features with a standard clustering algorithm, k-means, and uses the subsequent assignments as supervision to update the weights of the network. We apply DeepCluster to the unsupervised training of convolutional neural networks on large datasets like ImageNet and YFCC100M. The resulting model outperforms the current state of the art by a significant margin on all the standard benchmarks."
    },
    {
        "paperId": "2a417a16473e2bcb1c98cd7814bc106760925e60",
        "url": "https://www.semanticscholar.org/paper/2a417a16473e2bcb1c98cd7814bc106760925e60",
        "title": "Image Inpainting for Irregular Holes Using Partial Convolutions",
        "venue": "European Conference on Computer Vision",
        "year": 2018,
        "citationCount": 2127,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1804.07723, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2457939",
                "name": "Guilin Liu"
            },
            {
                "authorId": "3291967",
                "name": "F. Reda"
            },
            {
                "authorId": "143953573",
                "name": "Kevin J. Shih"
            },
            {
                "authorId": "2195314",
                "name": "Ting-Chun Wang"
            },
            {
                "authorId": "29955511",
                "name": "Andrew Tao"
            },
            {
                "authorId": "2301680",
                "name": "Bryan Catanzaro"
            }
        ],
        "abstract": "Existing deep learning based image inpainting methods use a standard convolutional network over the corrupted image, using convolutional filter responses conditioned on both valid pixels as well as the substitute values in the masked holes (typically the mean value). This often leads to artifacts such as color discrepancy and blurriness. Post-processing is usually used to reduce such artifacts, but are expensive and may fail. We propose the use of partial convolutions, where the convolution is masked and renormalized to be conditioned on only valid pixels. We further include a mechanism to automatically generate an updated mask for the next layer as part of the forward pass. Our model outperforms other methods for irregular masks. We show qualitative and quantitative comparisons with other methods to validate our approach."
    },
    {
        "paperId": "3aa21de1a7c97e0458e10ed5730ce160bb436caa",
        "url": "https://www.semanticscholar.org/paper/3aa21de1a7c97e0458e10ed5730ce160bb436caa",
        "title": "Pixel2Mesh: Generating 3D Mesh Models from Single RGB Images",
        "venue": "European Conference on Computer Vision",
        "year": 2018,
        "citationCount": 1420,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1804.01654, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "41017759",
                "name": "Nanyang Wang"
            },
            {
                "authorId": "1591143181",
                "name": "Yinda Zhang"
            },
            {
                "authorId": "3119455",
                "name": "Zhuwen Li"
            },
            {
                "authorId": "35782003",
                "name": "Yanwei Fu"
            },
            {
                "authorId": "46641573",
                "name": "W. Liu"
            },
            {
                "authorId": "1717861",
                "name": "Yu-Gang Jiang"
            }
        ],
        "abstract": "We propose an end-to-end deep learning architecture that produces a 3D shape in triangular mesh from a single color image. Limited by the nature of deep neural network, previous methods usually represent a 3D shape in volume or point cloud, and it is non-trivial to convert them to the more ready-to-use mesh model. Unlike the existing methods, our network represents 3D mesh in a graph-based convolutional neural network and produces correct geometry by progressively deforming an ellipsoid, leveraging perceptual features extracted from the input image. We adopt a coarse-to-fine strategy to make the whole deformation procedure stable, and define various of mesh related losses to capture properties of different levels to guarantee visually appealing and physically accurate 3D geometry. Extensive experiments show that our method not only qualitatively produces mesh model with better details, but also achieves higher 3D shape estimation accuracy compared to the state-of-the-art."
    },
    {
        "paperId": "45dd2a3cd7c27f2e9509b023d702408f5ac11c9d",
        "url": "https://www.semanticscholar.org/paper/45dd2a3cd7c27f2e9509b023d702408f5ac11c9d",
        "title": "Stacked Cross Attention for Image-Text Matching",
        "venue": "European Conference on Computer Vision",
        "year": 2018,
        "citationCount": 1288,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1803.08024, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1863953",
                "name": "Kuang-Huei Lee"
            },
            {
                "authorId": "2145307428",
                "name": "Xi Chen"
            },
            {
                "authorId": "144988571",
                "name": "G. Hua"
            },
            {
                "authorId": "35431603",
                "name": "Houdong Hu"
            },
            {
                "authorId": "144137069",
                "name": "Xiaodong He"
            }
        ],
        "abstract": "In this paper, we study the problem of image-text matching. Inferring the latent semantic alignment between objects or other salient stuffs (e.g. snow, sky, lawn) and the corresponding words in sentences allows to capture fine-grained interplay between vision and language, and makes image-text matching more interpretable. Prior works either simply aggregate the similarity of all possible pairs of regions and words without attending differentially to more and less important words or regions, or use a multi-step attentional process to capture limited number of semantic alignments which is less interpretable. In this paper, we present Stacked Cross Attention to discover the full latent alignments using both image regions and words in sentence as context and infer the image-text similarity. Our approach achieves the state-of-the-art results on the MS-COCO and Flickr30K datasets. On Flickr30K, our approach outperforms the current best methods by 22.1% in text retrieval from image query, and 18.2% in image retrieval with text query (based on Recall@1). On MS-COCO, our approach improves sentence retrieval by 17.8% and image retrieval by 16.6% (based on Recall@1 using the 5K test set)."
    },
    {
        "paperId": "4eee87d960754f755fdec073a160af3e2e31672f",
        "url": "https://www.semanticscholar.org/paper/4eee87d960754f755fdec073a160af3e2e31672f",
        "title": "PSANet: Point-wise Spatial Attention Network for Scene Parsing",
        "venue": "European Conference on Computer Vision",
        "year": 2018,
        "citationCount": 1081,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/978-3-030-01240-3_17?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/978-3-030-01240-3_17, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3459894",
                "name": "Hengshuang Zhao"
            },
            {
                "authorId": "2153911759",
                "name": "Yi Zhang"
            },
            {
                "authorId": "1779129",
                "name": "Shu Liu"
            },
            {
                "authorId": "1788070",
                "name": "Jianping Shi"
            },
            {
                "authorId": "1717179",
                "name": "Chen Change Loy"
            },
            {
                "authorId": "1807606",
                "name": "Dahua Lin"
            },
            {
                "authorId": "1729056",
                "name": "Jiaya Jia"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "60104351ac65115503c9e92e856bcab6a13b0ce8",
        "url": "https://www.semanticscholar.org/paper/60104351ac65115503c9e92e856bcab6a13b0ce8",
        "title": "Multimodal Unsupervised Image-to-Image Translation",
        "venue": "European Conference on Computer Vision",
        "year": 2018,
        "citationCount": 2632,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1804.04732, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "144247007",
                "name": "Xun Huang"
            },
            {
                "authorId": "39793900",
                "name": "Ming-Yu Liu"
            },
            {
                "authorId": "50172592",
                "name": "Serge J. Belongie"
            },
            {
                "authorId": "2376331450",
                "name": "Jan Kautz"
            }
        ],
        "abstract": "Unsupervised image-to-image translation is an important and challenging problem in computer vision. Given an image in the source domain, the goal is to learn the conditional distribution of corresponding images in the target domain, without seeing any pairs of corresponding images. While this conditional distribution is inherently multimodal, existing approaches make an overly simplified assumption, modeling it as a deterministic one-to-one mapping. As a result, they fail to generate diverse outputs from a given source domain image. To address this limitation, we propose a Multimodal Unsupervised Image-to-image Translation (MUNIT) framework. We assume that the image representation can be decomposed into a content code that is domain-invariant, and a style code that captures domain-specific properties. To translate an image to another domain, we recombine its content code with a random style code sampled from the style space of the target domain. We analyze the proposed framework and establish several theoretical results. Extensive experiments with comparisons to the state-of-the-art approaches further demonstrates the advantage of the proposed framework. Moreover, our framework allows users to control the style of translation outputs by providing an example style image. Code and pretrained models are available at this https URL"
    },
    {
        "paperId": "776bc8955e801f6965e85b35d8e2dd6f2f1498ad",
        "url": "https://www.semanticscholar.org/paper/776bc8955e801f6965e85b35d8e2dd6f2f1498ad",
        "title": "Distractor-aware Siamese Networks for Visual Object Tracking",
        "venue": "European Conference on Computer Vision",
        "year": 2018,
        "citationCount": 1272,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1808.06048, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2118932732",
                "name": "Zheng Zhu"
            },
            {
                "authorId": "145805403",
                "name": "Qiang Wang"
            },
            {
                "authorId": "71788673",
                "name": "Bo Li"
            },
            {
                "authorId": "39533001",
                "name": "Wei Wu"
            },
            {
                "authorId": "1721677",
                "name": "Junjie Yan"
            },
            {
                "authorId": "40506509",
                "name": "Weiming Hu"
            }
        ],
        "abstract": "Recently, Siamese networks have drawn great attention in visual tracking community because of their balanced accuracy and speed. However, features used in most Siamese tracking approaches can only discriminate foreground from the non-semantic backgrounds. The semantic backgrounds are always considered as distractors, which hinders the robustness of Siamese trackers. In this paper, we focus on learning distractor-aware Siamese networks for accurate and long-term tracking. To this end, features used in traditional Siamese trackers are analyzed at first. We observe that the imbalanced distribution of training data makes the learned features less discriminative. During the off-line training phase, an effective sampling strategy is introduced to control this distribution and make the model focus on the semantic distractors. During inference, a novel distractor-aware module is designed to perform incremental learning, which can effectively transfer the general embedding to the current video domain. In addition, we extend the proposed approach for long-term tracking by introducing a simple yet effective local-to-global search region strategy. Extensive experiments on benchmarks show that our approach significantly outperforms the state-of-the-arts, yielding 9.6% relative gain in VOT2016 dataset and 35.9% relative gain in UAV20L dataset. The proposed tracker can perform at 160 FPS on short-term benchmarks and 110 FPS on long-term benchmarks."
    },
    {
        "paperId": "7a39a3ca168dfebb2e2d55b3fca0f750b32896da",
        "url": "https://www.semanticscholar.org/paper/7a39a3ca168dfebb2e2d55b3fca0f750b32896da",
        "title": "BiSeNet: Bilateral Segmentation Network for Real-time Semantic Segmentation",
        "venue": "European Conference on Computer Vision",
        "year": 2018,
        "citationCount": 2504,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1808.00897",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1808.00897, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3712271",
                "name": "Changqian Yu"
            },
            {
                "authorId": "2115722333",
                "name": "Jingbo Wang"
            },
            {
                "authorId": "2113567716",
                "name": "Chao Peng"
            },
            {
                "authorId": "40115662",
                "name": "Changxin Gao"
            },
            {
                "authorId": "2116565951",
                "name": "Gang Yu"
            },
            {
                "authorId": "1707161",
                "name": "N. Sang"
            }
        ],
        "abstract": "Semantic segmentation requires both rich spatial information and sizeable receptive field. However, modern approaches usually compromise spatial resolution to achieve real-time inference speed, which leads to poor performance. In this paper, we address this dilemma with a novel Bilateral Segmentation Network (BiSeNet). We first design a Spatial Path with a small stride to preserve the spatial information and generate high-resolution features. Meanwhile, a Context Path with a fast downsampling strategy is employed to obtain sufficient receptive field. On top of the two paths, we introduce a new Feature Fusion Module to combine features efficiently. The proposed architecture makes a right balance between the speed and segmentation performance on Cityscapes, CamVid, and COCO-Stuff datasets. Specifically, for a 2048 \\(\\times \\) 1024 input, we achieve 68.4% Mean IOU on the Cityscapes test dataset with speed of 105 FPS on one NVIDIA Titan XP card, which is significantly faster than the existing methods with comparable performance."
    },
    {
        "paperId": "87ca28235555f7e70cf1edc2a63cda4aef7fee42",
        "url": "https://www.semanticscholar.org/paper/87ca28235555f7e70cf1edc2a63cda4aef7fee42",
        "title": "MVSNet: Depth Inference for Unstructured Multi-view Stereo",
        "venue": "European Conference on Computer Vision",
        "year": 2018,
        "citationCount": 1460,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1804.02505, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "145923104",
                "name": "Yao Yao"
            },
            {
                "authorId": "9484005",
                "name": "Zixin Luo"
            },
            {
                "authorId": "144013684",
                "name": "Shiwei Li"
            },
            {
                "authorId": "3406486",
                "name": "Tian Fang"
            },
            {
                "authorId": "144645904",
                "name": "Long Quan"
            }
        ],
        "abstract": "We present an end-to-end deep learning architecture for depth map inference from multi-view images. In the network, we first extract deep visual image features, and then build the 3D cost volume upon the reference camera frustum via the differentiable homography warping. Next, we apply 3D convolutions to regularize and regress the initial depth map, which is then refined with the reference image to generate the final output. Our framework flexibly adapts arbitrary N-view inputs using a variance-based cost metric that maps multiple features into one cost feature. The proposed MVSNet is demonstrated on the large-scale indoor DTU dataset. With simple post-processing, our method not only significantly outperforms previous state-of-the-arts, but also is several times faster in runtime. We also evaluate MVSNet on the complex outdoor Tanks and Temples dataset, where our method ranks first before April 18, 2018 without any fine-tuning, showing the strong generalization ability of MVSNet."
    },
    {
        "paperId": "9217e28b2273eb3b26e4e9b7b498b4661e6e09f5",
        "url": "https://www.semanticscholar.org/paper/9217e28b2273eb3b26e4e9b7b498b4661e6e09f5",
        "title": "Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation",
        "venue": "European Conference on Computer Vision",
        "year": 2018,
        "citationCount": 15272,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1802.02611, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "34192119",
                "name": "Liang-Chieh Chen"
            },
            {
                "authorId": "1844940337",
                "name": "Yukun Zhu"
            },
            {
                "authorId": "2776496",
                "name": "G. Papandreou"
            },
            {
                "authorId": "3302320",
                "name": "Florian Schroff"
            },
            {
                "authorId": "2595180",
                "name": "Hartwig Adam"
            }
        ],
        "abstract": "Spatial pyramid pooling module or encode-decoder structure are used in deep neural networks for semantic segmentation task. The former networks are able to encode multi-scale contextual information by probing the incoming features with filters or pooling operations at multiple rates and multiple effective fields-of-view, while the latter networks can capture sharper object boundaries by gradually recovering the spatial information. In this work, we propose to combine the advantages from both methods. Specifically, our proposed model, DeepLabv3+, extends DeepLabv3 by adding a simple yet effective decoder module to refine the segmentation results especially along object boundaries. We further explore the Xception model and apply the depthwise separable convolution to both Atrous Spatial Pyramid Pooling and decoder modules, resulting in a faster and stronger encoder-decoder network. We demonstrate the effectiveness of the proposed model on PASCAL VOC 2012 and Cityscapes datasets, achieving the test set performance of 89.0\\% and 82.1\\% without any post-processing. Our paper is accompanied with a publicly available reference implementation of the proposed models in Tensorflow at \\url{this https URL}."
    },
    {
        "paperId": "9775f8964a2eea1c9e35a02b1b906487396ea1f5",
        "url": "https://www.semanticscholar.org/paper/9775f8964a2eea1c9e35a02b1b906487396ea1f5",
        "title": "Image Super-Resolution Using Very Deep Residual Channel Attention Networks",
        "venue": "European Conference on Computer Vision",
        "year": 2018,
        "citationCount": 4967,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1807.02758, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2410227",
                "name": "Yulun Zhang"
            },
            {
                "authorId": "49243413",
                "name": "Kunpeng Li"
            },
            {
                "authorId": "2158257300",
                "name": "Kai Li"
            },
            {
                "authorId": "1491247995",
                "name": "Lichen Wang"
            },
            {
                "authorId": "40296597",
                "name": "Bineng Zhong"
            },
            {
                "authorId": "46956675",
                "name": "Y. Fu"
            }
        ],
        "abstract": "Convolutional neural network (CNN) depth is of crucial importance for image super-resolution (SR). However, we observe that deeper networks for image SR are more difficult to train. The low-resolution inputs and features contain abundant low-frequency information, which is treated equally across channels, hence hindering the representational ability of CNNs. To solve these problems, we propose the very deep residual channel attention networks (RCAN). Specifically, we propose a residual in residual (RIR) structure to form very deep network, which consists of several residual groups with long skip connections. Each residual group contains some residual blocks with short skip connections. Meanwhile, RIR allows abundant low-frequency information to be bypassed through multiple skip connections, making the main network focus on learning high-frequency information. Furthermore, we propose a channel attention mechanism to adaptively rescale channel-wise features by considering interdependencies among channels. Extensive experiments show that our RCAN achieves better accuracy and visual improvements against state-of-the-art methods."
    },
    {
        "paperId": "9958590c281e7a7b524dd594635037143c632c21",
        "url": "https://www.semanticscholar.org/paper/9958590c281e7a7b524dd594635037143c632c21",
        "title": "Riemannian Walk for Incremental Learning: Understanding Forgetting and Intransigence",
        "venue": "European Conference on Computer Vision",
        "year": 2018,
        "citationCount": 1291,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1801.10112",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1801.10112, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "22235380",
                "name": "Arslan Chaudhry"
            },
            {
                "authorId": "144679302",
                "name": "P. Dokania"
            },
            {
                "authorId": "144722114",
                "name": "Thalaiyasingam Ajanthan"
            },
            {
                "authorId": "143635540",
                "name": "Philip H. S. Torr"
            }
        ],
        "abstract": "Incremental learning (IL) has received a lot of attention recently, however, the literature lacks a precise problem definition, proper evaluation settings, and metrics tailored specifically for the IL problem. One of the main objectives of this work is to fill these gaps so as to provide a common ground for better understanding of IL. The main challenge for an IL algorithm is to update the classifier whilst preserving existing knowledge. We observe that, in addition to forgetting, a known issue while preserving knowledge, IL also suffers from a problem we call intransigence, inability of a model to update its knowledge. We introduce two metrics to quantify forgetting and intransigence that allow us to understand, analyse, and gain better insights into the behaviour of IL algorithms. We present RWalk, a generalization of EWC++ (our efficient version of EWC [Kirkpatrick2016EWC]) and Path Integral [Zenke2017Continual] with a theoretically grounded KL-divergence based perspective. We provide a thorough analysis of various IL algorithms on MNIST and CIFAR-100 datasets. In these experiments, RWalk obtains superior results in terms of accuracy, and also provides a better trade-off between forgetting and intransigence."
    },
    {
        "paperId": "9b15362b9a025071aa170f7ed81a761bc057c859",
        "url": "https://www.semanticscholar.org/paper/9b15362b9a025071aa170f7ed81a761bc057c859",
        "title": "Unsupervised Domain Adaptation for Semantic Segmentation via Class-Balanced Self-training",
        "venue": "European Conference on Computer Vision",
        "year": 2018,
        "citationCount": 1430,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/978-3-030-01219-9_18?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/978-3-030-01219-9_18, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2087067777",
                "name": "Yang Zou"
            },
            {
                "authorId": "1751019",
                "name": "Zhiding Yu"
            },
            {
                "authorId": "133728653",
                "name": "B. V. Vijaya Kumar"
            },
            {
                "authorId": "2110201006",
                "name": "Jinsong Wang"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "aaab0bd4d79d4f19109bab0fbcdb05070fb0edd1",
        "url": "https://www.semanticscholar.org/paper/aaab0bd4d79d4f19109bab0fbcdb05070fb0edd1",
        "title": "Unified Perceptual Parsing for Scene Understanding",
        "venue": "European Conference on Computer Vision",
        "year": 2018,
        "citationCount": 2243,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1807.10221, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "15727192",
                "name": "Tete Xiao"
            },
            {
                "authorId": "51149370",
                "name": "Yingcheng Liu"
            },
            {
                "authorId": "145291669",
                "name": "Bolei Zhou"
            },
            {
                "authorId": "2117772986",
                "name": "Yuning Jiang"
            },
            {
                "authorId": null,
                "name": "Jian Sun"
            }
        ],
        "abstract": "Humans recognize the visual world at multiple levels: we effortlessly categorize scenes and detect objects inside, while also identifying the textures and surfaces of the objects along with their different compositional parts. In this paper, we study a new task called Unified Perceptual Parsing, which requires the machine vision systems to recognize as many visual concepts as possible from a given image. A multi-task framework called UPerNet and a training strategy are developed to learn from heterogeneous image annotations. We benchmark our framework on Unified Perceptual Parsing and show that it is able to effectively segment a wide range of concepts from images. The trained networks are further applied to discover visual knowledge in natural scenes (Models are available at https://github.com/CSAILVision/unifiedparsing)."
    },
    {
        "paperId": "bef8694328016889a4b87761984046e1d9cf79b9",
        "url": "https://www.semanticscholar.org/paper/bef8694328016889a4b87761984046e1d9cf79b9",
        "title": "Fast, Accurate, and, Lightweight Super-Resolution with Cascading Residual Network",
        "venue": "European Conference on Computer Vision",
        "year": 2018,
        "citationCount": 1322,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1803.08664, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "40894534",
                "name": "Namhyuk Ahn"
            },
            {
                "authorId": "37768245",
                "name": "Byungkon Kang"
            },
            {
                "authorId": "2892216",
                "name": "Kyung-ah Sohn"
            }
        ],
        "abstract": "In recent years, deep learning methods have been successfully applied to single-image super-resolution tasks. Despite their great performances, deep learning methods cannot be easily applied to real-world applications due to the requirement of heavy computation. In this paper, we address this issue by proposing an accurate and lightweight deep network for image super-resolution. In detail, we design an architecture that implements a cascading mechanism upon a residual network. We also present variant models of the proposed cascading residual network to further improve efficiency. Our extensive experiments show that even with much fewer parameters and operations, our models achieve performance comparable to that of state-of-the-art methods."
    },
    {
        "paperId": "c02b909a514af6b9255315e2d50112845ca5ed0e",
        "url": "https://www.semanticscholar.org/paper/c02b909a514af6b9255315e2d50112845ca5ed0e",
        "title": "ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design",
        "venue": "European Conference on Computer Vision",
        "year": 2018,
        "citationCount": 5846,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1807.11164, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2068605434",
                "name": "Ningning Ma"
            },
            {
                "authorId": "50875121",
                "name": "Xiangyu Zhang"
            },
            {
                "authorId": "16215052",
                "name": "Haitao Zheng"
            },
            {
                "authorId": null,
                "name": "Jian Sun"
            }
        ],
        "abstract": "Currently, the neural network architecture design is mostly guided by the indirect metric of computation complexity, i.e., FLOPs. However, the direct metric, e.g., speed, also depends on the other factors such as memory access cost and platform characterics. Thus, this work proposes to evaluate the direct metric on the target platform, beyond only considering FLOPs. Based on a series of controlled experiments, this work derives several practical guidelines for efficient network design. Accordingly, a new architecture is presented, called ShuffleNet V2. Comprehensive ablation experiments verify that our model is the state-of-the-art in terms of speed and accuracy tradeoff."
    },
    {
        "paperId": "dc9187434a1c27306c61a3317aa942d3402d97c3",
        "url": "https://www.semanticscholar.org/paper/dc9187434a1c27306c61a3317aa942d3402d97c3",
        "title": "Simple Baselines for Human Pose Estimation and Tracking",
        "venue": "European Conference on Computer Vision",
        "year": 2018,
        "citationCount": 1972,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1804.06208, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "144025674",
                "name": "Bin Xiao"
            },
            {
                "authorId": "2119019500",
                "name": "Haiping Wu"
            },
            {
                "authorId": "1732264",
                "name": "Yichen Wei"
            }
        ],
        "abstract": "There has been significant progress on pose estimation and increasing interests on pose tracking in recent years. At the same time, the overall algorithm and system complexity increases as well, making the algorithm analysis and comparison more difficult. This work provides simple and effective baseline methods. They are helpful for inspiring and evaluating new ideas for the field. State-of-the-art results are achieved on challenging benchmarks. The code will be available at this https URL."
    },
    {
        "paperId": "de95601d9e3b20ec51aa33e1f27b1880d2c44ef2",
        "url": "https://www.semanticscholar.org/paper/de95601d9e3b20ec51aa33e1f27b1880d2c44ef2",
        "title": "CBAM: Convolutional Block Attention Module",
        "venue": "European Conference on Computer Vision",
        "year": 2018,
        "citationCount": 20913,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1807.06521, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2262209",
                "name": "Sanghyun Woo"
            },
            {
                "authorId": "2109216573",
                "name": "Jongchan Park"
            },
            {
                "authorId": "1926578",
                "name": "Joon-Young Lee"
            },
            {
                "authorId": "2398271",
                "name": "In-So Kweon"
            }
        ],
        "abstract": "We propose Convolutional Block Attention Module (CBAM), a simple yet effective attention module for feed-forward convolutional neural networks. Given an intermediate feature map, our module sequentially infers attention maps along two separate dimensions, channel and spatial, then the attention maps are multiplied to the input feature map for adaptive feature refinement. Because CBAM is a lightweight and general module, it can be integrated into any CNN architectures seamlessly with negligible overheads and is end-to-end trainable along with base CNNs. We validate our CBAM through extensive experiments on ImageNet-1K, MS COCO detection, and VOC 2007 detection datasets. Our experiments show consistent improvements in classification and detection performances with various models, demonstrating the wide applicability of CBAM. The code and models will be publicly available."
    },
    {
        "paperId": "8733fe2371b615609b04e2e910b1ecfa8e77cbc2",
        "url": "https://www.semanticscholar.org/paper/8733fe2371b615609b04e2e910b1ecfa8e77cbc2",
        "title": "Square Attack: a query-efficient black-box adversarial attack via random search",
        "venue": "European Conference on Computer Vision",
        "year": 2019,
        "citationCount": 1155,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1912.00049, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "47669224",
                "name": "Maksym Andriushchenko"
            },
            {
                "authorId": "39171784",
                "name": "Francesco Croce"
            },
            {
                "authorId": "2288272044",
                "name": "Nicolas Flammarion"
            },
            {
                "authorId": "143610806",
                "name": "Matthias Hein"
            }
        ],
        "abstract": "We propose the Square Attack, a score-based black-box $l_2$- and $l_\\infty$-adversarial attack that does not rely on local gradient information and thus is not affected by gradient masking. Square Attack is based on a randomized search scheme which selects localized square-shaped updates at random positions so that at each iteration the perturbation is situated approximately at the boundary of the feasible set. Our method is significantly more query efficient and achieves a higher success rate compared to the state-of-the-art methods, especially in the untargeted setting. In particular, on ImageNet we improve the average query efficiency in the untargeted setting for various deep networks by a factor of at least $1.8$ and up to $3$ compared to the recent state-of-the-art $l_\\infty$-attack of Al-Dujaili & O'Reilly. Moreover, although our attack is black-box, it can also outperform gradient-based white-box attacks on the standard benchmarks achieving a new state-of-the-art in terms of the success rate. The code of our attack is available at this https URL."
    },
    {
        "paperId": "97f4d09175705be4677d675fa27e55defac44800",
        "url": "https://www.semanticscholar.org/paper/97f4d09175705be4677d675fa27e55defac44800",
        "title": "Contrastive Multiview Coding",
        "venue": "European Conference on Computer Vision",
        "year": 2019,
        "citationCount": 2604,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1906.05849, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2476765",
                "name": "Yonglong Tian"
            },
            {
                "authorId": "1707347",
                "name": "Dilip Krishnan"
            },
            {
                "authorId": "2094770",
                "name": "Phillip Isola"
            }
        ],
        "abstract": "Humans view the world through many sensory channels, e.g., the long-wavelength light channel, viewed by the left eye, or the high-frequency vibrations channel, heard by the right ear. Each view is noisy and incomplete, but important factors, such as physics, geometry, and semantics, tend to be shared between all views (e.g., a \"dog\" can be seen, heard, and felt). We investigate the classic hypothesis that a powerful representation is one that models view-invariant factors. We study this hypothesis under the framework of multiview contrastive learning, where we learn a representation that aims to maximize mutual information between different views of the same scene but is otherwise compact. Our approach scales to any number of views, and is view-agnostic. We analyze key properties of the approach that make it work, finding that the contrastive loss outperforms a popular alternative based on cross-view prediction, and that the more views we learn from, the better the resulting representation captures underlying scene semantics. Our approach achieves state-of-the-art results on image and video unsupervised learning benchmarks. Code is released at: this http URL."
    },
    {
        "paperId": "a62e40be6c2078ace369ce58801b3d5d1dd1a351",
        "url": "https://www.semanticscholar.org/paper/a62e40be6c2078ace369ce58801b3d5d1dd1a351",
        "title": "Towards Real-Time Multi-Object Tracking",
        "venue": "European Conference on Computer Vision",
        "year": 2019,
        "citationCount": 1023,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1909.12605, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "72682780",
                "name": "Zhongdao Wang"
            },
            {
                "authorId": "144802394",
                "name": "Liang Zheng"
            },
            {
                "authorId": "2143076540",
                "name": "Yixuan Liu"
            },
            {
                "authorId": "1678689",
                "name": "Shengjin Wang"
            }
        ],
        "abstract": "Modern multiple object tracking (MOT) systems usually follow the tracking-by-detection paradigm. It has 1) a detection model for target localization and 2) an appearance embedding model for data association. Having the two models separately executed might lead to efficiency problems, as the running time is simply a sum of the two steps without investigating potential structures that can be shared between them. Existing research efforts on real-time MOT usually focus on the association step, so they are essentially real-time association methods but not real-time MOT system. In this paper, we propose an MOT system that allows target detection and appearance embedding to be learned in a shared model. Specifically, we incorporate the appearance embedding model into a single-shot detector, such that the model can simultaneously output detections and the corresponding embeddings. As such, the system is formulated as a multi-task learning problem: there are multiple objectives, i.e., anchor classification, bounding box regression, and embedding learning; and the individual losses are automatically weighted. To our knowledge, this work reports the first (near) real-time MOT system, with a running speed of 18.8 to 24.1 FPS depending on the input resolution. Meanwhile, its tracking accuracy is comparable to the state-of-the-art trackers embodying separate detection and embedding (SDE) learning (64.4% MOTA v.s. 66.1% MOTA on MOT-16 challenge). The code and models are available at this https URL."
    },
    {
        "paperId": "a88c914f5a738d38f02790bb5de41453bf17bde1",
        "url": "https://www.semanticscholar.org/paper/a88c914f5a738d38f02790bb5de41453bf17bde1",
        "title": "Object-Contextual Representations for Semantic Segmentation",
        "venue": "European Conference on Computer Vision",
        "year": 2019,
        "citationCount": 1596,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1909.11065, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "49521390",
                "name": "Yuhui Yuan"
            },
            {
                "authorId": "46772547",
                "name": "Xilin Chen"
            },
            {
                "authorId": "2109534192",
                "name": "Jingdong Wang"
            }
        ],
        "abstract": "In this paper, we address the semantic segmentation problem with a focus on the context aggregation strategy. Our motivation is that the label of a pixel is the category of the object that the pixel belongs to. We present a simple yet effective approach, object-contextual representations, characterizing a pixel by exploiting the representation of the corresponding object class. First, we learn object regions under the supervision of the ground-truth segmentation. Second, we compute the object region representation by aggregating the representations of the pixels lying in the object region. Last, % the representation similarity we compute the relation between each pixel and each object region, and augment the representation of each pixel with the object-contextual representation which is a weighted aggregation of all the object region representations according to their relations with the pixel. We empirically demonstrate that the proposed approach achieves competitive performance on various challenging semantic segmentation benchmarks: Cityscapes, ADE20K, LIP, PASCAL-Context, and COCO-Stuff. Cityscapes, ADE20K, LIP, PASCAL-Context and COCO-Stuff. Our submission \"HRNet + OCR + SegFix\" achieves the 1-st place on the Cityscapes leaderboard by the time of submission. Code is available at: this https URL and this https URL."
    },
    {
        "paperId": "bc51622358d8eea83248ef29402fe10640d07ba6",
        "url": "https://www.semanticscholar.org/paper/bc51622358d8eea83248ef29402fe10640d07ba6",
        "title": "Big Transfer (BiT): General Visual Representation Learning",
        "venue": "European Conference on Computer Vision",
        "year": 2019,
        "citationCount": 1303,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1912.11370, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "144629422",
                "name": "Alexander Kolesnikov"
            },
            {
                "authorId": "39611591",
                "name": "Lucas Beyer"
            },
            {
                "authorId": "2743563",
                "name": "Xiaohua Zhai"
            },
            {
                "authorId": "1794202",
                "name": "J. Puigcerver"
            },
            {
                "authorId": "1470947219",
                "name": "Jessica Yung"
            },
            {
                "authorId": "1802148",
                "name": "S. Gelly"
            },
            {
                "authorId": "2815290",
                "name": "N. Houlsby"
            }
        ],
        "abstract": "Transfer of pre-trained representations improves sample efficiency and simplifies hyperparameter tuning when training deep neural networks for vision. We revisit the paradigm of pre-training on large supervised datasets and fine-tuning the model on a target task. We scale up pre-training, and propose a simple recipe that we call Big Transfer (BiT). By combining a few carefully selected components, and transferring using a simple heuristic, we achieve strong performance on over 20 datasets. BiT performs well across a surprisingly wide range of data regimes -- from 1 example per class to 1M total examples. BiT achieves 87.5% top-1 accuracy on ILSVRC-2012, 99.4% on CIFAR-10, and 76.3% on the 19 task Visual Task Adaptation Benchmark (VTAB). On small datasets, BiT attains 76.8% on ILSVRC-2012 with 10 examples per class, and 97.0% on CIFAR-10 with 10 examples per class. We conduct detailed analysis of the main components that lead to high transfer performance."
    },
    {
        "paperId": "dfc7b58b67c31932b48586b3e23a43cc94695290",
        "url": "https://www.semanticscholar.org/paper/dfc7b58b67c31932b48586b3e23a43cc94695290",
        "title": "UNITER: UNiversal Image-TExt Representation Learning",
        "venue": "European Conference on Computer Vision",
        "year": 2019,
        "citationCount": 2464,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/978-3-030-58577-8_7?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/978-3-030-58577-8_7, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2378902",
                "name": "Yen-Chun Chen"
            },
            {
                "authorId": "50703697",
                "name": "Linjie Li"
            },
            {
                "authorId": "1714982",
                "name": "Licheng Yu"
            },
            {
                "authorId": "1877430",
                "name": "Ahmed El Kholy"
            },
            {
                "authorId": "2054472958",
                "name": "Faisal Ahmed"
            },
            {
                "authorId": "144702900",
                "name": "Zhe Gan"
            },
            {
                "authorId": "145215470",
                "name": "Yu Cheng"
            },
            {
                "authorId": "46700348",
                "name": "Jingjing Liu"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "2ad0f82f256dc8eb7703242f95d901c4ab3c2a06",
        "url": "https://www.semanticscholar.org/paper/2ad0f82f256dc8eb7703242f95d901c4ab3c2a06",
        "title": "Trajectron++: Dynamically-Feasible Trajectory Forecasting with Heterogeneous Data",
        "venue": "European Conference on Computer Vision",
        "year": 2020,
        "citationCount": 1132,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/978-3-030-58523-5_40?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/978-3-030-58523-5_40, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "52186541",
                "name": "Tim Salzmann"
            },
            {
                "authorId": "145156173",
                "name": "B. Ivanovic"
            },
            {
                "authorId": "1846419",
                "name": "Punarjay Chakravarty"
            },
            {
                "authorId": "1696085",
                "name": "M. Pavone"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "3230e2d6b4671cc03974af2219c6d3270e6fac70",
        "url": "https://www.semanticscholar.org/paper/3230e2d6b4671cc03974af2219c6d3270e6fac70",
        "title": "RAFT: Recurrent All-Pairs Field Transforms for Optical Flow",
        "venue": "European Conference on Computer Vision",
        "year": 2020,
        "citationCount": 3324,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2003.12039, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "8048414",
                "name": "Zachary Teed"
            },
            {
                "authorId": "153302678",
                "name": "Jia Deng"
            }
        ],
        "abstract": "We introduce Recurrent All-Pairs Field Transforms (RAFT), a new deep network architecture for optical flow. RAFT extracts per-pixel features, builds multi-scale 4D correlation volumes for all pairs of pixels, and iteratively updates a flow field through a recurrent unit that performs lookups on the correlation volumes. RAFT achieves state-of-the-art performance on the KITTI and Sintel datasets. In addition, RAFT has strong cross-dataset generalization as well as high efficiency in inference time, training speed, and parameter count."
    },
    {
        "paperId": "4def603277e077d11eb6ce030df931330d758207",
        "url": "https://www.semanticscholar.org/paper/4def603277e077d11eb6ce030df931330d758207",
        "title": "Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D",
        "venue": "European Conference on Computer Vision",
        "year": 2020,
        "citationCount": 1359,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2008.05711, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "103688686",
                "name": "Jonah Philion"
            },
            {
                "authorId": "37895334",
                "name": "S. Fidler"
            }
        ],
        "abstract": "The goal of perception for autonomous vehicles is to extract semantic representations from multiple sensors and fuse these representations into a single \"bird's-eye-view\" coordinate frame for consumption by motion planning. We propose a new end-to-end architecture that directly extracts a bird's-eye-view representation of a scene given image data from an arbitrary number of cameras. The core idea behind our approach is to \"lift\" each image individually into a frustum of features for each camera, then \"splat\" all frustums into a rasterized bird's-eye-view grid. By training on the entire camera rig, we provide evidence that our model is able to learn not only how to represent images but how to fuse predictions from all cameras into a single cohesive representation of the scene while being robust to calibration error. On standard bird's-eye-view tasks such as object segmentation and map segmentation, our model outperforms all baselines and prior work. In pursuit of the goal of learning dense representations for motion planning, we show that the representations inferred by our model enable interpretable end-to-end motion planning by \"shooting\" template trajectories into a bird's-eye-view cost map output by our network. We benchmark our approach against models that use oracle depth from lidar. Project page with code: this https URL ."
    },
    {
        "paperId": "5a6732513a1dc0bea059543f208a7556e3e31067",
        "url": "https://www.semanticscholar.org/paper/5a6732513a1dc0bea059543f208a7556e3e31067",
        "title": "Convolutional Occupancy Networks",
        "venue": "European Conference on Computer Vision",
        "year": 2020,
        "citationCount": 1082,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2003.04618, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "49009409",
                "name": "Songyou Peng"
            },
            {
                "authorId": "145048708",
                "name": "Michael Niemeyer"
            },
            {
                "authorId": "8226549",
                "name": "L. Mescheder"
            },
            {
                "authorId": "1742208",
                "name": "M. Pollefeys"
            },
            {
                "authorId": "47237027",
                "name": "Andreas Geiger"
            }
        ],
        "abstract": "Recently, implicit neural representations have gained popularity for learning-based 3D reconstruction. While demonstrating promising results, most implicit approaches are limited to comparably simple geometry of single objects and do not scale to more complicated or large-scale scenes. The key limiting factor of implicit methods is their simple fully-connected network architecture which does not allow for integrating local information in the observations or incorporating inductive biases such as translational equivariance. In this paper, we propose Convolutional Occupancy Networks, a more flexible implicit representation for detailed reconstruction of objects and 3D scenes. By combining convolutional encoders with implicit occupancy decoders, our model incorporates inductive biases, enabling structured reasoning in 3D space. We investigate the effectiveness of the proposed representation by reconstructing complex geometry from noisy point clouds and low-resolution voxel representations. We empirically find that our method enables the fine-grained implicit 3D reconstruction of single objects, scales to large indoor scenes, and generalizes well from synthetic to real data."
    },
    {
        "paperId": "962dc29fdc3fbdc5930a10aba114050b82fe5a3e",
        "url": "https://www.semanticscholar.org/paper/962dc29fdc3fbdc5930a10aba114050b82fe5a3e",
        "title": "End-to-End Object Detection with Transformers",
        "venue": "European Conference on Computer Vision",
        "year": 2020,
        "citationCount": 16281,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2005.12872, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3422899",
                "name": "Nicolas Carion"
            },
            {
                "authorId": "1403239967",
                "name": "Francisco Massa"
            },
            {
                "authorId": "2282478",
                "name": "Gabriel Synnaeve"
            },
            {
                "authorId": "1746841",
                "name": "Nicolas Usunier"
            },
            {
                "authorId": "144843400",
                "name": "Alexander Kirillov"
            },
            {
                "authorId": "2134433",
                "name": "Sergey Zagoruyko"
            }
        ],
        "abstract": "We present a new method that views object detection as a direct set prediction problem. Our approach streamlines the detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the final set of predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster RCNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily generalized to produce panoptic segmentation in a unified manner. We show that it significantly outperforms competitive baselines. Training code and pretrained models are available at this https URL."
    },
    {
        "paperId": "998bd8862ab4193e672bb16fe1aae4d446f7536e",
        "url": "https://www.semanticscholar.org/paper/998bd8862ab4193e672bb16fe1aae4d446f7536e",
        "title": "Contrastive Learning for Unpaired Image-to-Image Translation",
        "venue": "European Conference on Computer Vision",
        "year": 2020,
        "citationCount": 1462,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2007.15651, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2071929129",
                "name": "Taesung Park"
            },
            {
                "authorId": "1763086",
                "name": "Alexei A. Efros"
            },
            {
                "authorId": "2844849",
                "name": "Richard Zhang"
            },
            {
                "authorId": "2436356",
                "name": "Jun-Yan Zhu"
            }
        ],
        "abstract": "In image-to-image translation, each patch in the output should reflect the content of the corresponding patch in the input, independent of domain. We propose a straightforward method for doing so -- maximizing mutual information between the two, using a framework based on contrastive learning. The method encourages two elements (corresponding patches) to map to a similar point in a learned feature space, relative to other elements (other patches) in the dataset, referred to as negatives. We explore several critical design choices for making contrastive learning effective in the image synthesis setting. Notably, we use a multilayer, patch-based approach, rather than operate on entire images. Furthermore, we draw negatives from within the input image itself, rather than from the rest of the dataset. We demonstrate that our framework enables one-sided translation in the unpaired image-to-image translation setting, while improving quality and reducing training time. In addition, our method can even be extended to the training setting where each \"domain\" is only a single image."
    },
    {
        "paperId": "b5ef0f91663f0cbd6910dec9a890c138f7ec10e0",
        "url": "https://www.semanticscholar.org/paper/b5ef0f91663f0cbd6910dec9a890c138f7ec10e0",
        "title": "Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks",
        "venue": "European Conference on Computer Vision",
        "year": 2020,
        "citationCount": 2127,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2004.06165, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "47058148",
                "name": "Xiujun Li"
            },
            {
                "authorId": "1629039205",
                "name": "Xi Yin"
            },
            {
                "authorId": "2109737569",
                "name": "Chunyuan Li"
            },
            {
                "authorId": "2148941781",
                "name": "Xiaowei Hu"
            },
            {
                "authorId": "9325940",
                "name": "Pengchuan Zhang"
            },
            {
                "authorId": "39089563",
                "name": "Lei Zhang"
            },
            {
                "authorId": "29957038",
                "name": "Lijuan Wang"
            },
            {
                "authorId": "35431603",
                "name": "Houdong Hu"
            },
            {
                "authorId": "145307652",
                "name": "Li Dong"
            },
            {
                "authorId": "49807919",
                "name": "Furu Wei"
            },
            {
                "authorId": "1699545",
                "name": "Yejin Choi"
            },
            {
                "authorId": "1800422",
                "name": "Jianfeng Gao"
            }
        ],
        "abstract": "Large-scale pre-training methods of learning cross-modal representations on image-text pairs are becoming popular for vision-language tasks. While existing methods simply concatenate image region features and text features as input to the model to be pre-trained and use self-attention to learn image-text semantic alignments in a brute force manner, in this paper, we propose a new learning method Oscar (Object-Semantics Aligned Pre-training), which uses object tags detected in images as anchor points to significantly ease the learning of alignments. Our method is motivated by the observation that the salient objects in an image can be accurately detected, and are often mentioned in the paired text. We pre-train an Oscar model on the public corpus of 6.5 million text-image pairs, and fine-tune it on downstream tasks, creating new state-of-the-arts on six well-established vision-language understanding and generation tasks."
    },
    {
        "paperId": "d58f07bab3ff01a002d139062ad83d5818c23864",
        "url": "https://www.semanticscholar.org/paper/d58f07bab3ff01a002d139062ad83d5818c23864",
        "title": "Tracking Objects as Points",
        "venue": "European Conference on Computer Vision",
        "year": 2020,
        "citationCount": 1239,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2004.01177, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1390555183",
                "name": "Xingyi Zhou"
            },
            {
                "authorId": "145231047",
                "name": "V. Koltun"
            },
            {
                "authorId": "2562966",
                "name": "Philipp Krhenbhl"
            }
        ],
        "abstract": "Tracking has traditionally been the art of following interest points through space and time. This changed with the rise of powerful deep networks. Nowadays, tracking is dominated by pipelines that perform object detection followed by temporal association, also known as tracking-by-detection. In this paper, we present a simultaneous detection and tracking algorithm that is simpler, faster, and more accurate than the state of the art. Our tracker, CenterTrack, applies a detection model to a pair of images and detections from the prior frame. Given this minimal input, CenterTrack localizes objects and predicts their associations with the previous frame. That's it. CenterTrack is simple, online (no peeking into the future), and real-time. It achieves 67.3% MOTA on the MOT17 challenge at 22 FPS and 89.4% MOTA on the KITTI tracking benchmark at 15 FPS, setting a new state of the art on both datasets. CenterTrack is easily extended to monocular 3D tracking by regressing additional 3D attributes. Using monocular video input, it achieves 28.3% AMOTA@0.2 on the newly released nuScenes 3D tracking benchmark, substantially outperforming the monocular baseline on this benchmark while running at 28 FPS."
    },
    {
        "paperId": "feb1d269982f851f0b6c23dd495f77bc0e458baf",
        "url": "https://www.semanticscholar.org/paper/feb1d269982f851f0b6c23dd495f77bc0e458baf",
        "title": "Rethinking Few-Shot Image Classification: a Good Embedding Is All You Need?",
        "venue": "European Conference on Computer Vision",
        "year": 2020,
        "citationCount": 1005,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2003.11539, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2476765",
                "name": "Yonglong Tian"
            },
            {
                "authorId": "2118462083",
                "name": "Yue Wang"
            },
            {
                "authorId": "1707347",
                "name": "Dilip Krishnan"
            },
            {
                "authorId": "1763295",
                "name": "J. Tenenbaum"
            },
            {
                "authorId": "2094770",
                "name": "Phillip Isola"
            }
        ],
        "abstract": "The focus of recent meta-learning research has been on the development of learning algorithms that can quickly adapt to test time tasks with limited data and low computational cost. Few-shot learning is widely used as one of the standard benchmarks in meta-learning. In this work, we show that a simple baseline: learning a supervised or self-supervised representation on the meta-training set, followed by training a linear classifier on top of this representation, outperforms state-of-the-art few-shot learning methods. An additional boost can be achieved through the use of self-distillation. This demonstrates that using a good learned embedding model can be more effective than sophisticated meta-learning algorithms. We believe that our findings motivate a rethinking of few-shot image classification benchmarks and the associated role of meta-learning algorithms. Code is available at: this http URL."
    },
    {
        "paperId": "7d1ff4ac2390759cbe60dd46b2b9bcabd4a90db4",
        "url": "https://www.semanticscholar.org/paper/7d1ff4ac2390759cbe60dd46b2b9bcabd4a90db4",
        "title": "ByteTrack: Multi-Object Tracking by Associating Every Detection Box",
        "venue": "European Conference on Computer Vision",
        "year": 2021,
        "citationCount": 1916,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2110.06864, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2108464812",
                "name": "Yifu Zhang"
            },
            {
                "authorId": "2075416446",
                "name": "Pei Sun"
            },
            {
                "authorId": "2146417774",
                "name": "Yi Jiang"
            },
            {
                "authorId": "2223692",
                "name": "Dongdong Yu"
            },
            {
                "authorId": "51305314",
                "name": "Zehuan Yuan"
            },
            {
                "authorId": "47571885",
                "name": "Ping Luo"
            },
            {
                "authorId": "2109194747",
                "name": "Wenyu Liu"
            },
            {
                "authorId": "2443233",
                "name": "Xinggang Wang"
            }
        ],
        "abstract": "Multi-object tracking (MOT) aims at estimating bounding boxes and identities of objects in videos. Most methods obtain identities by associating detection boxes whose scores are higher than a threshold. The objects with low detection scores, e.g. occluded objects, are simply thrown away, which brings non-negligible true object missing and fragmented trajectories. To solve this problem, we present a simple, effective and generic association method, tracking by associating almost every detection box instead of only the high score ones. For the low score detection boxes, we utilize their similarities with tracklets to recover true objects and filter out the background detections. When applied to 9 different state-of-the-art trackers, our method achieves consistent improvement on IDF1 score ranging from 1 to 10 points. To put forwards the state-of-the-art performance of MOT, we design a simple and strong tracker, named ByteTrack. For the first time, we achieve 80.3 MOTA, 77.3 IDF1 and 63.1 HOTA on the test set of MOT17 with 30 FPS running speed on a single V100 GPU. ByteTrack also achieves state-of-the-art performance on MOT20, HiEve and BDD100K tracking benchmarks. The source code, pre-trained models with deploy versions and tutorials of applying to other trackers are released at https://github.com/ifzhang/ByteTrack."
    },
    {
        "paperId": "7d4c2c8407e0caf2f907df9954b056a42a92fd13",
        "url": "https://www.semanticscholar.org/paper/7d4c2c8407e0caf2f907df9954b056a42a92fd13",
        "title": "Simple Baselines for Image Restoration",
        "venue": "European Conference on Computer Vision",
        "year": 2022,
        "citationCount": 1224,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2204.04676",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2204.04676, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "49330296",
                "name": "Liangyu Chen"
            },
            {
                "authorId": "2056599721",
                "name": "Xiaojie Chu"
            },
            {
                "authorId": "1771551",
                "name": "X. Zhang"
            },
            {
                "authorId": "2032184078",
                "name": "Jian Sun"
            }
        ],
        "abstract": "Although there have been significant advances in the field of image restoration recently, the system complexity of the state-of-the-art (SOTA) methods is increasing as well, which may hinder the convenient analysis and comparison of methods. In this paper, we propose a simple baseline that exceeds the SOTA methods and is computationally efficient. To further simplify the baseline, we reveal that the nonlinear activation functions, e.g. Sigmoid, ReLU, GELU, Softmax, etc. are not necessary: they could be replaced by multiplication or removed. Thus, we derive a Nonlinear Activation Free Network, namely NAFNet, from the baseline. SOTA results are achieved on various challenging benchmarks, e.g. 33.69 dB PSNR on GoPro (for image deblurring), exceeding the previous SOTA 0.38 dB with only 8.4% of its computational costs; 40.30 dB PSNR on SIDD (for image denoising), exceeding the previous SOTA 0.28 dB with less than half of its computational costs. The code and the pre-trained models are released at https://github.com/megvii-research/NAFNet."
    },
    {
        "paperId": "a09cbcaac305884f043810afc4fa4053099b5970",
        "url": "https://www.semanticscholar.org/paper/a09cbcaac305884f043810afc4fa4053099b5970",
        "title": "Exploring Plain Vision Transformer Backbones for Object Detection",
        "venue": "European Conference on Computer Vision",
        "year": 2022,
        "citationCount": 1027,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2203.16527",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2203.16527, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2366569281",
                "name": "Yanghao Li"
            },
            {
                "authorId": "2053590350",
                "name": "Hanzi Mao"
            },
            {
                "authorId": "2983898",
                "name": "Ross B. Girshick"
            },
            {
                "authorId": "2058350112",
                "name": "Kaiming He"
            }
        ],
        "abstract": "We explore the plain, non-hierarchical Vision Transformer (ViT) as a backbone network for object detection. This design enables the original ViT architecture to be fine-tuned for object detection without needing to redesign a hierarchical backbone for pre-training. With minimal adaptations for fine-tuning, our plain-backbone detector can achieve competitive results. Surprisingly, we observe: (i) it is sufficient to build a simple feature pyramid from a single-scale feature map (without the common FPN design) and (ii) it is sufficient to use window attention (without shifting) aided with very few cross-window propagation blocks. With plain ViT backbones pre-trained as Masked Autoencoders (MAE), our detector, named ViTDet, can compete with the previous leading methods that were all based on hierarchical backbones, reaching up to 61.3 AP_box on the COCO dataset using only ImageNet-1K pre-training. We hope our study will draw attention to research on plain-backbone detectors. Code for ViTDet is available in Detectron2."
    },
    {
        "paperId": "a824c6e214dd0118f70af8bb05d67d94a858d076",
        "url": "https://www.semanticscholar.org/paper/a824c6e214dd0118f70af8bb05d67d94a858d076",
        "title": "BEVFormer: Learning Bird's-Eye-View Representation from Multi-Camera Images via Spatiotemporal Transformers",
        "venue": "European Conference on Computer Vision",
        "year": 2022,
        "citationCount": 1663,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2203.17270",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2203.17270, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2109766685",
                "name": "Zhiqi Li"
            },
            {
                "authorId": "71074736",
                "name": "Wenhai Wang"
            },
            {
                "authorId": "46382329",
                "name": "Hongyang Li"
            },
            {
                "authorId": "41020000",
                "name": "Enze Xie"
            },
            {
                "authorId": "2144553163",
                "name": "Chonghao Sima"
            },
            {
                "authorId": "2115137018",
                "name": "Tong Lu"
            },
            {
                "authorId": "2116045542",
                "name": "Qiao Yu"
            },
            {
                "authorId": "3304536",
                "name": "Jifeng Dai"
            }
        ],
        "abstract": "3D visual perception tasks, including 3D detection and map segmentation based on multi-camera images, are essential for autonomous driving systems. In this work, we present a new framework termed BEVFormer, which learns unified BEV representations with spatiotemporal transformers to support multiple autonomous driving perception tasks. In a nutshell, BEVFormer exploits both spatial and temporal information by interacting with spatial and temporal space through predefined grid-shaped BEV queries. To aggregate spatial information, we design spatial cross-attention that each BEV query extracts the spatial features from the regions of interest across camera views. For temporal information, we propose temporal self-attention to recurrently fuse the history BEV information. Our approach achieves the new state-of-the-art 56.9\\% in terms of NDS metric on the nuScenes \\texttt{test} set, which is 9.0 points higher than previous best arts and on par with the performance of LiDAR-based baselines. We further show that BEVFormer remarkably improves the accuracy of velocity estimation and recall of objects under low visibility conditions. The code is available at \\url{https://github.com/zhiqi-li/BEVFormer}."
    },
    {
        "paperId": "adb272fbdea3631059cf88ab764bb6c2ce29f965",
        "url": "https://www.semanticscholar.org/paper/adb272fbdea3631059cf88ab764bb6c2ce29f965",
        "title": "Visual Prompt Tuning",
        "venue": "European Conference on Computer Vision",
        "year": 2022,
        "citationCount": 2211,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2203.12119",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2203.12119, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "51502783",
                "name": "Menglin Jia"
            },
            {
                "authorId": "34689393",
                "name": "Luming Tang"
            },
            {
                "authorId": "33970300",
                "name": "Bor-Chun Chen"
            },
            {
                "authorId": "2064285348",
                "name": "Claire Cardie"
            },
            {
                "authorId": "2067789287",
                "name": "Serge J. Belongie"
            },
            {
                "authorId": "1790580",
                "name": "Bharath Hariharan"
            },
            {
                "authorId": "153317808",
                "name": "S. Lim"
            }
        ],
        "abstract": "The current modus operandi in adapting pre-trained models involves updating all the backbone parameters, ie, full fine-tuning. This paper introduces Visual Prompt Tuning (VPT) as an efficient and effective alternative to full fine-tuning for large-scale Transformer models in vision. Taking inspiration from recent advances in efficiently tuning large language models, VPT introduces only a small amount (less than 1% of model parameters) of trainable parameters in the input space while keeping the model backbone frozen. Via extensive experiments on a wide variety of downstream recognition tasks, we show that VPT achieves significant performance gains compared to other parameter efficient tuning protocols. Most importantly, VPT even outperforms full fine-tuning in many cases across model capacities and training data scales, while reducing per-task storage cost."
    },
    {
        "paperId": "d267731870c41d977c9d51195c1e2fd018846949",
        "url": "https://www.semanticscholar.org/paper/d267731870c41d977c9d51195c1e2fd018846949",
        "title": "TensoRF: Tensorial Radiance Fields",
        "venue": "European Conference on Computer Vision",
        "year": 2022,
        "citationCount": 1552,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2203.09517",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2203.09517, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "31866333",
                "name": "Anpei Chen"
            },
            {
                "authorId": "2615346",
                "name": "Zexiang Xu"
            },
            {
                "authorId": "47237027",
                "name": "Andreas Geiger"
            },
            {
                "authorId": "2155403153",
                "name": "Jingyi Yu"
            },
            {
                "authorId": "1423723631",
                "name": "Hao Su"
            }
        ],
        "abstract": "We present TensoRF, a novel approach to model and reconstruct radiance fields. Unlike NeRF that purely uses MLPs, we model the radiance field of a scene as a 4D tensor, which represents a 3D voxel grid with per-voxel multi-channel features. Our central idea is to factorize the 4D scene tensor into multiple compact low-rank tensor components. We demonstrate that applying traditional CP decomposition -- that factorizes tensors into rank-one components with compact vectors -- in our framework leads to improvements over vanilla NeRF. To further boost performance, we introduce a novel vector-matrix (VM) decomposition that relaxes the low-rank constraints for two modes of a tensor and factorizes tensors into compact vector and matrix factors. Beyond superior rendering quality, our models with CP and VM decompositions lead to a significantly lower memory footprint in comparison to previous and concurrent works that directly optimize per-voxel features. Experimentally, we demonstrate that TensoRF with CP decomposition achieves fast reconstruction (<30 min) with better rendering quality and even a smaller model size (<4 MB) compared to NeRF. Moreover, TensoRF with VM decomposition further boosts rendering quality and outperforms previous state-of-the-art methods, while reducing the reconstruction time (<10 min) and retaining a compact model size (<75 MB)."
    },
    {
        "paperId": "b37b1dc72b1882858f5120f2cd6883134089a6ed",
        "url": "https://www.semanticscholar.org/paper/b37b1dc72b1882858f5120f2cd6883134089a6ed",
        "title": "MMBench: Is Your Multi-modal Model an All-around Player?",
        "venue": "European Conference on Computer Vision",
        "year": 2023,
        "citationCount": 1637,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2307.06281",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2307.06281, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1604959737",
                "name": "Yuanzhan Liu"
            },
            {
                "authorId": "31463937",
                "name": "Haodong Duan"
            },
            {
                "authorId": "2145784327",
                "name": "Yuanhan Zhang"
            },
            {
                "authorId": "2165247100",
                "name": "Bo Li"
            },
            {
                "authorId": "1734973476",
                "name": "Songyang Zhang"
            },
            {
                "authorId": "2109435100",
                "name": "Wangbo Zhao"
            },
            {
                "authorId": "2112499811",
                "name": "Yike Yuan"
            },
            {
                "authorId": null,
                "name": "Jiaqi Wang"
            },
            {
                "authorId": "3486481",
                "name": "Conghui He"
            },
            {
                "authorId": "2145252993",
                "name": "Ziwei Liu"
            },
            {
                "authorId": "152568027",
                "name": "Kai Chen"
            },
            {
                "authorId": "1807606",
                "name": "Dahua Lin"
            }
        ],
        "abstract": "Large vision-language models (VLMs) have recently achieved remarkable progress, exhibiting impressive multimodal perception and reasoning abilities. However, effectively evaluating these large VLMs remains a major challenge, hindering future development in this domain. Traditional benchmarks like VQAv2 or COCO Caption provide quantitative performance measurements but lack fine-grained ability assessment and robust evaluation metrics. Meanwhile, subjective benchmarks, such as OwlEval, offer comprehensive evaluations of a model's abilities by incorporating human labor, which is not scalable and may display significant bias. In response to these challenges, we propose MMBench, a bilingual benchmark for assessing the multi-modal capabilities of VLMs. MMBench methodically develops a comprehensive evaluation pipeline, primarily comprised of the following key features: 1. MMBench is meticulously curated with well-designed quality control schemes, surpassing existing similar benchmarks in terms of the number and variety of evaluation questions and abilities; 2. MMBench introduces a rigorous CircularEval strategy and incorporates large language models to convert free-form predictions into pre-defined choices, which helps to yield accurate evaluation results for models with limited instruction-following capabilities. 3. MMBench incorporates multiple-choice questions in both English and Chinese versions, enabling an apples-to-apples comparison of VLMs' performance under a bilingual context. To summarize, MMBench is a systematically designed objective benchmark for a robust and holistic evaluation of vision-language models. We hope MMBench will assist the research community in better evaluating their models and facilitate future progress in this area. The evalutation code of MMBench has been integrated into VLMEvalKit: https://github.com/open-compass/VLMEvalKit."
    },
    {
        "paperId": "c3e5a20b844c042d2174263d2fd5b30d8cc8f0b0",
        "url": "https://www.semanticscholar.org/paper/c3e5a20b844c042d2174263d2fd5b30d8cc8f0b0",
        "title": "Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection",
        "venue": "European Conference on Computer Vision",
        "year": 2023,
        "citationCount": 3220,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2303.05499",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2303.05499, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "8602739",
                "name": "Shilong Liu"
            },
            {
                "authorId": "2075413603",
                "name": "Zhaoyang Zeng"
            },
            {
                "authorId": "2143150727",
                "name": "Tianhe Ren"
            },
            {
                "authorId": "2152978390",
                "name": "Feng Li"
            },
            {
                "authorId": "2315254849",
                "name": "Hao Zhang"
            },
            {
                "authorId": "2146105297",
                "name": "Jie Yang"
            },
            {
                "authorId": "2109738542",
                "name": "Chun-yue Li"
            },
            {
                "authorId": "120157163",
                "name": "Jianwei Yang"
            },
            {
                "authorId": "2093561216",
                "name": "Hang Su"
            },
            {
                "authorId": "89006344",
                "name": "Jun-Juan Zhu"
            },
            {
                "authorId": "2152832911",
                "name": "Lei Zhang"
            }
        ],
        "abstract": "In this paper, we present an open-set object detector, called Grounding DINO, by marrying Transformer-based detector DINO with grounded pre-training, which can detect arbitrary objects with human inputs such as category names or referring expressions. The key solution of open-set object detection is introducing language to a closed-set detector for open-set concept generalization. To effectively fuse language and vision modalities, we conceptually divide a closed-set detector into three phases and propose a tight fusion solution, which includes a feature enhancer, a language-guided query selection, and a cross-modality decoder for cross-modality fusion. While previous works mainly evaluate open-set object detection on novel categories, we propose to also perform evaluations on referring expression comprehension for objects specified with attributes. Grounding DINO performs remarkably well on all three settings, including benchmarks on COCO, LVIS, ODinW, and RefCOCO/+/g. Grounding DINO achieves a $52.5$ AP on the COCO detection zero-shot transfer benchmark, i.e., without any training data from COCO. It sets a new record on the ODinW zero-shot benchmark with a mean $26.1$ AP. Code will be available at \\url{https://github.com/IDEA-Research/GroundingDINO}."
    },
    {
        "paperId": "cf70392a3b1ae92fdb1b70448aaddcbd03726d3d",
        "url": "https://www.semanticscholar.org/paper/cf70392a3b1ae92fdb1b70448aaddcbd03726d3d",
        "title": "YOLOv9: Learning What You Want to Learn Using Programmable Gradient Information",
        "venue": "European Conference on Computer Vision",
        "year": 2024,
        "citationCount": 2754,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.13616, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2250543210",
                "name": "Chien-Yao Wang"
            },
            {
                "authorId": "1432235646",
                "name": "I-Hau Yeh"
            },
            {
                "authorId": "2043281157",
                "name": "Hongpeng Liao"
            }
        ],
        "abstract": "Today's deep learning methods focus on how to design the most appropriate objective functions so that the prediction results of the model can be closest to the ground truth. Meanwhile, an appropriate architecture that can facilitate acquisition of enough information for prediction has to be designed. Existing methods ignore a fact that when input data undergoes layer-by-layer feature extraction and spatial transformation, large amount of information will be lost. This paper will delve into the important issues of data loss when data is transmitted through deep networks, namely information bottleneck and reversible functions. We proposed the concept of programmable gradient information (PGI) to cope with the various changes required by deep networks to achieve multiple objectives. PGI can provide complete input information for the target task to calculate objective function, so that reliable gradient information can be obtained to update network weights. In addition, a new lightweight network architecture -- Generalized Efficient Layer Aggregation Network (GELAN), based on gradient path planning is designed. GELAN's architecture confirms that PGI has gained superior results on lightweight models. We verified the proposed GELAN and PGI on MS COCO dataset based object detection. The results show that GELAN only uses conventional convolution operators to achieve better parameter utilization than the state-of-the-art methods developed based on depth-wise convolution. PGI can be used for variety of models from lightweight to large. It can be used to obtain complete information, so that train-from-scratch models can achieve better results than state-of-the-art models pre-trained using large datasets, the comparison results are shown in Figure 1. The source codes are at: https://github.com/WongKinYiu/yolov9."
    },
    {
        "paperId": "cf70392a3b1ae92fdb1b70448aaddcbd03726d3d",
        "url": "https://www.semanticscholar.org/paper/cf70392a3b1ae92fdb1b70448aaddcbd03726d3d",
        "title": "YOLOv9: Learning What You Want to Learn Using Programmable Gradient Information",
        "venue": "European Conference on Computer Vision",
        "year": 2024,
        "citationCount": 2754,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.13616, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2250543210",
                "name": "Chien-Yao Wang"
            },
            {
                "authorId": "1432235646",
                "name": "I-Hau Yeh"
            },
            {
                "authorId": "2043281157",
                "name": "Hongpeng Liao"
            }
        ],
        "abstract": "Today's deep learning methods focus on how to design the most appropriate objective functions so that the prediction results of the model can be closest to the ground truth. Meanwhile, an appropriate architecture that can facilitate acquisition of enough information for prediction has to be designed. Existing methods ignore a fact that when input data undergoes layer-by-layer feature extraction and spatial transformation, large amount of information will be lost. This paper will delve into the important issues of data loss when data is transmitted through deep networks, namely information bottleneck and reversible functions. We proposed the concept of programmable gradient information (PGI) to cope with the various changes required by deep networks to achieve multiple objectives. PGI can provide complete input information for the target task to calculate objective function, so that reliable gradient information can be obtained to update network weights. In addition, a new lightweight network architecture -- Generalized Efficient Layer Aggregation Network (GELAN), based on gradient path planning is designed. GELAN's architecture confirms that PGI has gained superior results on lightweight models. We verified the proposed GELAN and PGI on MS COCO dataset based object detection. The results show that GELAN only uses conventional convolution operators to achieve better parameter utilization than the state-of-the-art methods developed based on depth-wise convolution. PGI can be used for variety of models from lightweight to large. It can be used to obtain complete information, so that train-from-scratch models can achieve better results than state-of-the-art models pre-trained using large datasets, the comparison results are shown in Figure 1. The source codes are at: https://github.com/WongKinYiu/yolov9."
    },
    {
        "paperId": "9fa8d73e572c3ca824a04a5f551b602a17831bc5",
        "url": "https://www.semanticscholar.org/paper/9fa8d73e572c3ca824a04a5f551b602a17831bc5",
        "title": "Domain Adaptation with Structural Correspondence Learning",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2006,
        "citationCount": 1727,
        "openAccessPdf": {
            "url": "https://dl.acm.org/doi/pdf/10.5555/1610075.1610094",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/W06-1615, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2116927",
                "name": "John Blitzer"
            },
            {
                "authorId": "143957226",
                "name": "Ryan T. McDonald"
            },
            {
                "authorId": "145366908",
                "name": "Fernando C Pereira"
            }
        ],
        "abstract": "Discriminative learning methods are widely used in natural language processing. These methods work best when their training and test data are drawn from the same distribution. For many NLP tasks, however, we are confronted with new domains in which labeled data is scarce or non-existent. In such cases, we seek to adapt existing models from a resource-rich source domain to a resource-poor target domain. We introduce structural correspondence learning to automatically induce correspondences among features from different domains. We test our technique on part of speech tagging and show performance gains for varying amounts of source and target training data, as well as improvements in target domain parsing accuracy using our improved tagger."
    },
    {
        "paperId": "1c909ac1c331c0c246a88da047cbdcca9ec9b7e7",
        "url": "https://www.semanticscholar.org/paper/1c909ac1c331c0c246a88da047cbdcca9ec9b7e7",
        "title": "Large-Scale Named Entity Disambiguation Based on Wikipedia Data",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2007,
        "citationCount": 1275,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://aclanthology.org/D07-1074, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "73040249",
                "name": "Silviu Cucerzan"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "5421dbcb7e14766eb3d951910ae8d7892d735a01",
        "url": "https://www.semanticscholar.org/paper/5421dbcb7e14766eb3d951910ae8d7892d735a01",
        "title": "V-Measure: A Conditional Entropy-Based External Cluster Evaluation Measure",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2007,
        "citationCount": 1801,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://aclanthology.org/D07-1043, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3998980",
                "name": "A. Rosenberg"
            },
            {
                "authorId": "144049352",
                "name": "Julia Hirschberg"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "0165568bcc1a819c18564567f2ec15d859be2519",
        "url": "https://www.semanticscholar.org/paper/0165568bcc1a819c18564567f2ec15d859be2519",
        "title": "Cheap and Fast  But is it Good? Evaluating Non-Expert Annotations for Natural Language Tasks",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2008,
        "citationCount": 2447,
        "openAccessPdf": {
            "url": "http://dl.acm.org/ft_gateway.cfm?id=1613751&type=pdf",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/D08-1027, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "144621026",
                "name": "R. Snow"
            },
            {
                "authorId": "1401020033",
                "name": "Brendan T. O'Connor"
            },
            {
                "authorId": "1746807",
                "name": "Dan Jurafsky"
            },
            {
                "authorId": "34699434",
                "name": "A. Ng"
            }
        ],
        "abstract": "Human linguistic annotation is crucial for many natural language processing tasks but can be expensive and time-consuming. We explore the use of Amazon's Mechanical Turk system, a significantly cheaper and faster method for collecting annotations from a broad base of paid non-expert contributors over the Web. We investigate five tasks: affect recognition, word similarity, recognizing textual entailment, event temporal ordering, and word sense disambiguation. For all five, we show high agreement between Mechanical Turk non-expert annotations and existing gold standard labels provided by expert labelers. For the task of affect recognition, we also show that using non-expert labels for training machine learning algorithms can be as effective as using gold standard annotations from experts. We propose a technique for bias correction that significantly improves annotation quality on two tasks. We conclude that many large labeling tasks can be effectively designed and carried out in this method at a fraction of the usual expense."
    },
    {
        "paperId": "487ed99e00bf6803a53a6059ceccd1510a63e72d",
        "url": "https://www.semanticscholar.org/paper/487ed99e00bf6803a53a6059ceccd1510a63e72d",
        "title": "An Analysis of Active Learning Strategies for Sequence Labeling Tasks",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2008,
        "citationCount": 1134,
        "openAccessPdf": {
            "url": "https://dl.acm.org/doi/pdf/10.5555/1613715.1613855",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/D08-1112, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1717452",
                "name": "Burr Settles"
            },
            {
                "authorId": "144557047",
                "name": "M. Craven"
            }
        ],
        "abstract": "Active learning is well-suited to many problems in natural language processing, where unlabeled data may be abundant but annotation is slow and expensive. This paper aims to shed light on the best active learning approaches for sequence labeling tasks such as information extraction and document segmentation. We survey previously used query selection strategies for sequence models, and propose several novel algorithms to address their shortcomings. We also conduct a large-scale empirical comparison using multiple corpora, which demonstrates that our proposed methods advance the state of the art."
    },
    {
        "paperId": "e9a822d6fe66b0cfd0b4c5a10411172b80346bf1",
        "url": "https://www.semanticscholar.org/paper/e9a822d6fe66b0cfd0b4c5a10411172b80346bf1",
        "title": "Labeled LDA: A supervised topic model for credit attribution in multi-labeled corpora",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2009,
        "citationCount": 1471,
        "openAccessPdf": {
            "url": "https://dl.acm.org/doi/pdf/10.5555/1699510.1699543",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/D09-1026, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1878835",
                "name": "Daniel Ramage"
            },
            {
                "authorId": "145385471",
                "name": "David Leo Wright Hall"
            },
            {
                "authorId": "1701451",
                "name": "Ramesh Nallapati"
            },
            {
                "authorId": "144783904",
                "name": "Christopher D. Manning"
            }
        ],
        "abstract": "A significant portion of the world's text is tagged by readers on social bookmarking websites. Credit attribution is an inherent problem in these corpora because most pages have multiple tags, but the tags do not always apply with equal specificity across the whole document. Solving the credit attribution problem requires associating each word in a document with the most appropriate tags and vice versa. This paper introduces Labeled LDA, a topic model that constrains Latent Dirichlet Allocation by defining a one-to-one correspondence between LDA's latent topics and user tags. This allows Labeled LDA to directly learn word-tag correspondences. We demonstrate Labeled LDA's improved expressiveness over traditional LDA with visualizations of a corpus of tagged web pages from del.icio.us. Labeled LDA outperforms SVMs by more than 3 to 1 when extracting tag-specific document snippets. As a multi-label text classifier, our model is competitive with a discriminative baseline on a variety of datasets."
    },
    {
        "paperId": "14935c3ffb1cafd53a23d84bec66388a77422435",
        "url": "https://www.semanticscholar.org/paper/14935c3ffb1cafd53a23d84bec66388a77422435",
        "title": "Named Entity Recognition in Tweets: An Experimental Study",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2011,
        "citationCount": 1421,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://aclanthology.org/D11-1141, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1863425",
                "name": "Alan Ritter"
            },
            {
                "authorId": "15173231",
                "name": "Sam Clark"
            },
            {
                "authorId": "2674444",
                "name": "Mausam"
            },
            {
                "authorId": "1741101",
                "name": "Oren Etzioni"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "cfa2646776405d50533055ceb1b7f050e9014dcb",
        "url": "https://www.semanticscholar.org/paper/cfa2646776405d50533055ceb1b7f050e9014dcb",
        "title": "Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2011,
        "citationCount": 1354,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://aclanthology.org/D11-1014, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2166511",
                "name": "R. Socher"
            },
            {
                "authorId": "143845796",
                "name": "Jeffrey Pennington"
            },
            {
                "authorId": "40150953",
                "name": "E. Huang"
            },
            {
                "authorId": "34699434",
                "name": "A. Ng"
            },
            {
                "authorId": "144783904",
                "name": "Christopher D. Manning"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "d4b651d6a904f69f8fa1dcad4ebe972296af3a9a",
        "url": "https://www.semanticscholar.org/paper/d4b651d6a904f69f8fa1dcad4ebe972296af3a9a",
        "title": "Identifying Relations for Open Information Extraction",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2011,
        "citationCount": 1421,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://aclanthology.org/D11-1142, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "38087946",
                "name": "Anthony Fader"
            },
            {
                "authorId": "144295318",
                "name": "S. Soderland"
            },
            {
                "authorId": "2282542651",
                "name": "O. Etzioni"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "d95738f38d97a030d98508357e4d5c78a4a208ba",
        "url": "https://www.semanticscholar.org/paper/d95738f38d97a030d98508357e4d5c78a4a208ba",
        "title": "Robust Disambiguation of Named Entities in Text",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2011,
        "citationCount": 1148,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://aclanthology.org/D11-1072, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1727527",
                "name": "Johannes Hoffart"
            },
            {
                "authorId": "34796954",
                "name": "Mohamed Amir Yosef"
            },
            {
                "authorId": "3033855",
                "name": "Ilaria Bordino"
            },
            {
                "authorId": "1683588",
                "name": "Hagen Frstenau"
            },
            {
                "authorId": "1717560",
                "name": "Manfred Pinkal"
            },
            {
                "authorId": "3115338",
                "name": "M. Spaniol"
            },
            {
                "authorId": "3326792",
                "name": "Bilyana Taneva"
            },
            {
                "authorId": "1727272",
                "name": "Stefan Thater"
            },
            {
                "authorId": "1751591",
                "name": "G. Weikum"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "ef2d64e448ee5ed2dc26179c8570803ded123a5e",
        "url": "https://www.semanticscholar.org/paper/ef2d64e448ee5ed2dc26179c8570803ded123a5e",
        "title": "Optimizing Semantic Coherence in Topic Models",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2011,
        "citationCount": 1865,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://aclanthology.org/D11-1024, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1705700",
                "name": "David Mimno"
            },
            {
                "authorId": "1831395",
                "name": "Hanna M. Wallach"
            },
            {
                "authorId": "14894697",
                "name": "E. Talley"
            },
            {
                "authorId": "47663240",
                "name": "Miriam Leenders"
            },
            {
                "authorId": "143753639",
                "name": "A. McCallum"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "27e38351e48fe4b7da2775bf94341738bc4da07e",
        "url": "https://www.semanticscholar.org/paper/27e38351e48fe4b7da2775bf94341738bc4da07e",
        "title": "Semantic Compositionality through Recursive Matrix-Vector Spaces",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2012,
        "citationCount": 1410,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://aclanthology.org/D12-1110, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2166511",
                "name": "R. Socher"
            },
            {
                "authorId": "2570381",
                "name": "Brody Huval"
            },
            {
                "authorId": "144783904",
                "name": "Christopher D. Manning"
            },
            {
                "authorId": "34699434",
                "name": "A. Ng"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "687bac2d3320083eb4530bf18bb8f8f721477600",
        "url": "https://www.semanticscholar.org/paper/687bac2d3320083eb4530bf18bb8f8f721477600",
        "title": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2013,
        "citationCount": 9060,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/D13-1170, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2166511",
                "name": "R. Socher"
            },
            {
                "authorId": "24590005",
                "name": "Alex Perelygin"
            },
            {
                "authorId": "2110402830",
                "name": "Jean Wu"
            },
            {
                "authorId": "1964541",
                "name": "Jason Chuang"
            },
            {
                "authorId": "144783904",
                "name": "Christopher D. Manning"
            },
            {
                "authorId": "34699434",
                "name": "A. Ng"
            },
            {
                "authorId": "144922861",
                "name": "Christopher Potts"
            }
        ],
        "abstract": "Semantic word spaces have been very useful but cannot express the meaning of longer phrases in a principled way. Further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition. To remedy this, we introduce a Sentiment Treebank. It includes fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences and presents new challenges for sentiment compositionality. To address them, we introduce the Recursive Neural Tensor Network. When trained on the new treebank, this model outperforms all previous methods on several metrics. It pushes the state of the art in single sentence positive/negative classification from 80% up to 85.4%. The accuracy of predicting fine-grained sentiment labels for all phrases reaches 80.7%, an improvement of 9.7% over bag of features baselines. Lastly, it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases."
    },
    {
        "paperId": "944a1cfd79dbfb6fef460360a0765ba790f4027a",
        "url": "https://www.semanticscholar.org/paper/944a1cfd79dbfb6fef460360a0765ba790f4027a",
        "title": "Recurrent Continuous Translation Models",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2013,
        "citationCount": 1458,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/D13-1176, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2583391",
                "name": "Nal Kalchbrenner"
            },
            {
                "authorId": "1685771",
                "name": "Phil Blunsom"
            }
        ],
        "abstract": "We introduce a class of probabilistic continuous translation models called Recurrent Continuous Translation Models that are purely based on continuous representations for words, phrases and sentences and do not rely on alignments or phrasal translation units. The models have a generation and a conditioning aspect. The generation of the translation is modelled with a target Recurrent Language Model, whereas the conditioning on the source sentence is modelled with a Convolutional Sentence Model. Through various experiments, we show first that our models obtain a perplexity with respect to gold translations that is > 43% lower than that of stateof-the-art alignment-based translation models. Secondly, we show that they are remarkably sensitive to the word order, syntax, and meaning of the source sentence despite lacking alignments. Finally we show that they match a state-of-the-art system when rescoring n-best lists of translations."
    },
    {
        "paperId": "b29447ba499507a259ae9d8f685d60cc1597d7d3",
        "url": "https://www.semanticscholar.org/paper/b29447ba499507a259ae9d8f685d60cc1597d7d3",
        "title": "Semantic Parsing on Freebase from Question-Answer Pairs",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2013,
        "citationCount": 2117,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/D13-1160, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1750652",
                "name": "Jonathan Berant"
            },
            {
                "authorId": "2059149862",
                "name": "A. Chou"
            },
            {
                "authorId": "34765463",
                "name": "Roy Frostig"
            },
            {
                "authorId": "145419642",
                "name": "Percy Liang"
            }
        ],
        "abstract": "In this paper, we train a semantic parser that scales up to Freebase. Instead of relying on annotated logical forms, which is especially expensive to obtain at large scale, we learn from question-answer pairs. The main challenge in this setting is narrowing down the huge number of possible logical predicates for a given question. We tackle this problem in two ways: First, we build a coarse mapping from phrases to predicates using a knowledge base and a large text corpus. Second, we use a bridging operation to generate additional predicates based on neighboring predicates. On the dataset of Cai and Yates (2013), despite not having annotated logical forms, our system outperforms their state-of-the-art parser. Additionally, we collected a more realistic and challenging dataset of question-answer pairs and improves over a natural baseline."
    },
    {
        "paperId": "0b544dfe355a5070b60986319a3f51fb45d1348e",
        "url": "https://www.semanticscholar.org/paper/0b544dfe355a5070b60986319a3f51fb45d1348e",
        "title": "Learning Phrase Representations using RNN EncoderDecoder for Statistical Machine Translation",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2014,
        "citationCount": 25199,
        "openAccessPdf": {
            "url": "https://aclanthology.org/D14-1179.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1406.1078, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1979489",
                "name": "Kyunghyun Cho"
            },
            {
                "authorId": "3158246",
                "name": "B. V. Merrienboer"
            },
            {
                "authorId": "1854385",
                "name": "aglar Glehre"
            },
            {
                "authorId": "3335364",
                "name": "Dzmitry Bahdanau"
            },
            {
                "authorId": "2076086",
                "name": "Fethi Bougares"
            },
            {
                "authorId": "144518416",
                "name": "Holger Schwenk"
            },
            {
                "authorId": "1751762",
                "name": "Yoshua Bengio"
            }
        ],
        "abstract": "In this paper, we propose a novel neural network model called RNN Encoder Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixedlength vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN EncoderDecoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases."
    },
    {
        "paperId": "1f6ba0782862ec12a5ec6d7fb608523d55b0c6ba",
        "url": "https://www.semanticscholar.org/paper/1f6ba0782862ec12a5ec6d7fb608523d55b0c6ba",
        "title": "Convolutional Neural Networks for Sentence Classification",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2014,
        "citationCount": 13889,
        "openAccessPdf": {
            "url": "https://aclanthology.org/D14-1181.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1408.5882, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "38367242",
                "name": "Yoon Kim"
            }
        ],
        "abstract": "We report on a series of experiments with convolutional neural networks (CNN) trained on top of pre-trained word vectors for sentence-level classification tasks. We show that a simple CNN with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks. Learning task-specific vectors through fine-tuning offers further gains in performance. We additionally propose a simple modification to the architecture to allow for the use of both task-specific and static vectors. The CNN models discussed herein improve upon the state of the art on 4 out of 7 tasks, which include sentiment analysis and question classification."
    },
    {
        "paperId": "92c141447f51b6732242376164ff961e464731c8",
        "url": "https://www.semanticscholar.org/paper/92c141447f51b6732242376164ff961e464731c8",
        "title": "ReferItGame: Referring to Objects in Photographs of Natural Scenes",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2014,
        "citationCount": 1579,
        "openAccessPdf": {
            "url": "https://aclanthology.org/D14-1086.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/D14-1086, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3006072",
                "name": "Sahar Kazemzadeh"
            },
            {
                "authorId": "2004053",
                "name": "Vicente Ordonez"
            },
            {
                "authorId": "65828281",
                "name": "M. Matten"
            },
            {
                "authorId": "1685538",
                "name": "Tamara L. Berg"
            }
        ],
        "abstract": "In this paper we introduce a new game to crowd-source natural language referring expressions. By designing a two player game, we can both collect and verify referring expressions directly within the game. To date, the game has produced a dataset containing 130,525 expressions, referring to 96,654 distinct objects, in 19,894 photographs of natural scenes. This dataset is larger and more varied than previous REG datasets and allows us to study referring expressions in real-world scenes. We provide an in depth analysis of the resulting dataset. Based on our findings, we design a new optimization based model for generating referring expressions and perform experimental evaluations on 3 test sets."
    },
    {
        "paperId": "a14045a751f5d8ed387c8630a86a3a2861b90643",
        "url": "https://www.semanticscholar.org/paper/a14045a751f5d8ed387c8630a86a3a2861b90643",
        "title": "A Fast and Accurate Dependency Parser using Neural Networks",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2014,
        "citationCount": 1897,
        "openAccessPdf": {
            "url": "https://doi.org/10.3115/v1/d14-1082",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/D14-1082, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "50536468",
                "name": "Danqi Chen"
            },
            {
                "authorId": "144783904",
                "name": "Christopher D. Manning"
            }
        ],
        "abstract": "Almost all current dependency parsers classify based on millions of sparse indicator features. Not only do these features generalize poorly, but the cost of feature computation restricts parsing speed significantly. In this work, we propose a novel way of learning a neural network classifier for use in a greedy, transition-based dependency parser. Because this classifier learns and uses just a small number of dense features, it can work very fast, while achieving an about 2% improvement in unlabeled and labeled attachment scores on both English and Chinese datasets. Concretely, our parser is able to parse more than 1000 sentences per second at 92.2% unlabeled attachment score on the English Penn Treebank."
    },
    {
        "paperId": "f37e1b62a767a307c046404ca96bc140b3e68cb5",
        "url": "https://www.semanticscholar.org/paper/f37e1b62a767a307c046404ca96bc140b3e68cb5",
        "title": "GloVe: Global Vectors for Word Representation",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2014,
        "citationCount": 33719,
        "openAccessPdf": {
            "url": "https://doi.org/10.3115/v1/d14-1162",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/D14-1162, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "143845796",
                "name": "Jeffrey Pennington"
            },
            {
                "authorId": "2166511",
                "name": "R. Socher"
            },
            {
                "authorId": "144783904",
                "name": "Christopher D. Manning"
            }
        ],
        "abstract": "Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition."
    },
    {
        "paperId": "1ac30af5522c7a50ec4d1ee43fd2bd8652a9bd52",
        "url": "https://www.semanticscholar.org/paper/1ac30af5522c7a50ec4d1ee43fd2bd8652a9bd52",
        "title": "A Neural Attention Model for Abstractive Sentence Summarization",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2015,
        "citationCount": 2794,
        "openAccessPdf": {
            "url": "https://aclanthology.org/D15-1044.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1509.00685, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2531268",
                "name": "Alexander M. Rush"
            },
            {
                "authorId": "3295092",
                "name": "S. Chopra"
            },
            {
                "authorId": "145183709",
                "name": "J. Weston"
            }
        ],
        "abstract": "Summarization based on text extraction is inherently limited, but generation-style abstractive methods have proven challenging to build. In this work, we propose a fully data-driven approach to abstractive sentence summarization. Our method utilizes a local attention-based model that generates each word of the summary conditioned on the input sentence. While the model is structurally simple, it can easily be trained end-to-end and scales to a large amount of training data. The model shows significant performance gains on the DUC-2004 shared task compared with several strong baselines."
    },
    {
        "paperId": "93499a7c7f699b6630a86fad964536f9423bb6d0",
        "url": "https://www.semanticscholar.org/paper/93499a7c7f699b6630a86fad964536f9423bb6d0",
        "title": "Effective Approaches to Attention-based Neural Machine Translation",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2015,
        "citationCount": 8249,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/D15-1166.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1508.04025, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1821711",
                "name": "Thang Luong"
            },
            {
                "authorId": "143950636",
                "name": "Hieu Pham"
            },
            {
                "authorId": "144783904",
                "name": "Christopher D. Manning"
            }
        ],
        "abstract": "An attentional mechanism has lately been used to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation. However, there has been little work exploring useful architectures for attention-based NMT. This paper examines two simple and effective classes of attentional mechanism: a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time. We demonstrate the effectiveness of both approaches on the WMT translation tasks between English and German in both directions. With local attention, we achieve a significant gain of 5.0 BLEU points over non-attentional systems that already incorporate known techniques such as dropout. Our ensemble model using different attention architectures yields a new state-of-the-art result in the WMT15 English to German translation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over the existing best system backed by NMT and an n-gram reranker. 1"
    },
    {
        "paperId": "ecb5336bf7b54a62109f325e7152bb74c4c7f527",
        "url": "https://www.semanticscholar.org/paper/ecb5336bf7b54a62109f325e7152bb74c4c7f527",
        "title": "Document Modeling with Gated Recurrent Neural Network for Sentiment Classification",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2015,
        "citationCount": 1461,
        "openAccessPdf": {
            "url": "https://aclanthology.org/D15-1167.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/D15-1167, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "39483833",
                "name": "Duyu Tang"
            },
            {
                "authorId": "152277111",
                "name": "Bing Qin"
            },
            {
                "authorId": "40282288",
                "name": "Ting Liu"
            }
        ],
        "abstract": "Document level sentiment classification remains a challenge: encoding the intrinsic relations between sentences in the semantic meaning of a document. To address this, we introduce a neural network model to learn vector-based document representation in a unified, bottom-up fashion. The model first learns sentence representation with convolutional neural network or long short-term memory. Afterwards, semantics of sentences and their relations are adaptively encoded in document representation with gated recurrent neural network. We conduct document level sentiment classification on four large-scale review datasets from IMDB and Yelp Dataset Challenge. Experimental results show that: (1) our neural model shows superior performances over several state-of-the-art algorithms; (2) gated recurrent neural network dramatically outperforms standard recurrent neural network in document modeling for sentiment classification. 1"
    },
    {
        "paperId": "f04df4e20a18358ea2f689b4c129781628ef7fc1",
        "url": "https://www.semanticscholar.org/paper/f04df4e20a18358ea2f689b4c129781628ef7fc1",
        "title": "A large annotated corpus for learning natural language inference",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2015,
        "citationCount": 4512,
        "openAccessPdf": {
            "url": "https://doi.org/10.18653/v1/d15-1075",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1508.05326, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3644767",
                "name": "Samuel R. Bowman"
            },
            {
                "authorId": "32301760",
                "name": "Gabor Angeli"
            },
            {
                "authorId": "144922861",
                "name": "Christopher Potts"
            },
            {
                "authorId": "144783904",
                "name": "Christopher D. Manning"
            }
        ],
        "abstract": "Understanding entailment and contradiction is fundamental to understanding natural language, and inference about entailment and contradiction is a valuable testing ground for the development of semantic representations. However, machine learning research in this area has been dramatically limited by the lack of large-scale resources. To address this, we introduce the Stanford Natural Language Inference corpus, a new, freely available collection of labeled sentence pairs, written by humans doing a novel grounded task based on image captioning. At 570K pairs, it is two orders of magnitude larger than all other resources of its type. This increase in scale allows lexicalized classifiers to outperform some sophisticated existing entailment models, and it allows a neural network-based model to perform competitively on natural language inference benchmarks for the first time."
    },
    {
        "paperId": "faa5d6d1285eb5f39f28f275cdd2bd8dfbd53a8d",
        "url": "https://www.semanticscholar.org/paper/faa5d6d1285eb5f39f28f275cdd2bd8dfbd53a8d",
        "title": "Distant Supervision for Relation Extraction via Piecewise Convolutional Neural Networks",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2015,
        "citationCount": 1087,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/D15-1203.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/D15-1203, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1796706",
                "name": "Daojian Zeng"
            },
            {
                "authorId": "2200096",
                "name": "Kang Liu"
            },
            {
                "authorId": "1763402",
                "name": "Yubo Chen"
            },
            {
                "authorId": "1390572170",
                "name": "Jun Zhao"
            }
        ],
        "abstract": "Two problems arise when using distant supervision for relation extraction. First, in this method, an already existing knowledge base is heuristically aligned to texts, and the alignment results are treated as labeled data. However, the heuristic alignment can fail, resulting in wrong label problem. In addition, in previous approaches, statistical models have typically been applied to ad hoc features. The noise that originates from the feature extraction process can cause poor performance. In this paper, we propose a novel model dubbed the Piecewise Convolutional Neural Networks (PCNNs) with multi-instance learning to address these two problems. To solve the first problem, distant supervised relation extraction is treated as a multi-instance problem in which the uncertainty of instance labels is taken into account. To address the latter problem, we avoid feature engineering and instead adopt convolutional architecture with piecewise max pooling to automatically learn relevant features. Experiments show that our method is effective and outperforms several competitive baseline methods."
    },
    {
        "paperId": "05dd7254b632376973f3a1b4d39485da17814df5",
        "url": "https://www.semanticscholar.org/paper/05dd7254b632376973f3a1b4d39485da17814df5",
        "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2016,
        "citationCount": 8882,
        "openAccessPdf": {
            "url": "https://aclanthology.org/D16-1264.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1606.05250, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2706258",
                "name": "Pranav Rajpurkar"
            },
            {
                "authorId": "2151810148",
                "name": "Jian Zhang"
            },
            {
                "authorId": "2787620",
                "name": "Konstantin Lopyrev"
            },
            {
                "authorId": "145419642",
                "name": "Percy Liang"
            }
        ],
        "abstract": "We present the Stanford Question Answering Dataset (SQuAD), a new reading comprehension dataset consisting of 100,000+ questions posed by crowdworkers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage. We analyze the dataset to understand the types of reasoning required to answer the questions, leaning heavily on dependency and constituency trees. We build a strong logistic regression model, which achieves an F1 score of 51.0%, a significant improvement over a simple baseline (20%). However, human performance (86.8%) is much higher, indicating that the dataset presents a good challenge problem for future research. \nThe dataset is freely available at this https URL"
    },
    {
        "paperId": "1298dae5751fb06184f6b067d1503bde8037bdb7",
        "url": "https://www.semanticscholar.org/paper/1298dae5751fb06184f6b067d1503bde8037bdb7",
        "title": "Deep Reinforcement Learning for Dialogue Generation",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2016,
        "citationCount": 1384,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/D16-1127.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1606.01541, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2386020817",
                "name": "Jiwei Li"
            },
            {
                "authorId": "145768639",
                "name": "Will Monroe"
            },
            {
                "authorId": "1863425",
                "name": "Alan Ritter"
            },
            {
                "authorId": "1746807",
                "name": "Dan Jurafsky"
            },
            {
                "authorId": "1947267",
                "name": "Michel Galley"
            },
            {
                "authorId": "1800422",
                "name": "Jianfeng Gao"
            }
        ],
        "abstract": "Recent neural models of dialogue generation offer great promise for generating responses for conversational agents, but tend to be shortsighted, predicting utterances one at a time while ignoring their influence on future outcomes. Modeling the future direction of a dialogue is crucial to generating coherent, interesting dialogues, a need which led traditional NLP models of dialogue to draw on reinforcement learning. In this paper, we show how to integrate these goals, applying deep reinforcement learning to model future reward in chatbot dialogue. The model simulates dialogues between two virtual agents, using policy gradient methods to reward sequences that display three useful conversational properties: informativity (non-repetitive turns), coherence, and ease of answering (related to forward-looking function). We evaluate our model on diversity, length as well as with human judges, showing that the proposed algorithm generates more interactive responses and manages to foster a more sustained conversation in dialogue simulation. This work marks a first step towards learning a neural conversational model based on the long-term success of dialogues."
    },
    {
        "paperId": "129cbad01be98ee88a930e31898cb76be79c41c1",
        "url": "https://www.semanticscholar.org/paper/129cbad01be98ee88a930e31898cb76be79c41c1",
        "title": "How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2016,
        "citationCount": 1354,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/D16-1230.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1603.08023, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2144634184",
                "name": "Chia-Wei Liu"
            },
            {
                "authorId": "2054294",
                "name": "Ryan Lowe"
            },
            {
                "authorId": "35224828",
                "name": "Iulian Serban"
            },
            {
                "authorId": "38107789",
                "name": "Michael Noseworthy"
            },
            {
                "authorId": "1778839",
                "name": "Laurent Charlin"
            },
            {
                "authorId": "145134886",
                "name": "Joelle Pineau"
            }
        ],
        "abstract": "We investigate evaluation metrics for dialogue response generation systems where supervised labels, such as task completion, are not available. Recent works in response generation have adopted metrics from machine translation to compare a model's generated response to a single target response. We show that these metrics correlate very weakly with human judgements in the non-technical Twitter domain, and not at all in the technical Ubuntu domain. We provide quantitative and qualitative results highlighting specific weaknesses in existing metrics, and provide recommendations for future development of better automatic evaluation metrics for dialogue systems."
    },
    {
        "paperId": "12f7de07f9b00315418e381b2bd797d21f12b419",
        "url": "https://www.semanticscholar.org/paper/12f7de07f9b00315418e381b2bd797d21f12b419",
        "title": "Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2016,
        "citationCount": 1540,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/D16-1044.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1606.01847, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2135128997",
                "name": "Akira Fukui"
            },
            {
                "authorId": "3422202",
                "name": "Dong Huk Park"
            },
            {
                "authorId": "3422876",
                "name": "Daylen Yang"
            },
            {
                "authorId": "34721166",
                "name": "Anna Rohrbach"
            },
            {
                "authorId": "1753210",
                "name": "Trevor Darrell"
            },
            {
                "authorId": "34849128",
                "name": "Marcus Rohrbach"
            }
        ],
        "abstract": "Modeling textual or visual information with vector representations trained from large language or visual datasets has been successfully explored in recent years. However, tasks such as visual question answering require combining these vector representations with each other. Approaches to multimodal pooling include element-wise product or sum, as well as concatenation of the visual and textual representations. We hypothesize that these methods are not as expressive as an outer product of the visual and textual vectors. As the outer product is typically infeasible due to its high dimensionality, we instead propose utilizing Multimodal Compact Bilinear pooling (MCB) to efficiently and expressively combine multimodal features. We extensively evaluate MCB on the visual question answering and grounding tasks. We consistently show the benefit of MCB over ablations without MCB. For visual question answering, we present an architecture which uses MCB twice, once for predicting attention over spatial features and again to combine the attended representation with the question representation. This model outperforms the state-of-the-art on the Visual7W dataset and the VQA challenge."
    },
    {
        "paperId": "13fe71da009484f240c46f14d9330e932f8de210",
        "url": "https://www.semanticscholar.org/paper/13fe71da009484f240c46f14d9330e932f8de210",
        "title": "Long Short-Term Memory-Networks for Machine Reading",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2016,
        "citationCount": 1158,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/D16-1053.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1601.06733, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1941442",
                "name": "Jianpeng Cheng"
            },
            {
                "authorId": "145307652",
                "name": "Li Dong"
            },
            {
                "authorId": "1747893",
                "name": "Mirella Lapata"
            }
        ],
        "abstract": "In this paper we address the question of how to render sequence-level networks better at handling structured input. We propose a machine reading simulator which processes text incrementally from left to right and performs shallow reasoning with memory and attention. The reader extends the Long Short-Term Memory architecture with a memory network in place of a single memory cell. This enables adaptive memory usage during recurrence with neural attention, offering a way to weakly induce relations among tokens. The system is initially designed to process a single sequence but we also demonstrate how to integrate it with an encoder-decoder architecture. Experiments on language modeling, sentiment analysis, and natural language inference show that our model matches or outperforms the state of the art."
    },
    {
        "paperId": "2cd8e8f510c89c7c18268e8ad51c061e459ad321",
        "url": "https://www.semanticscholar.org/paper/2cd8e8f510c89c7c18268e8ad51c061e459ad321",
        "title": "A Decomposable Attention Model for Natural Language Inference",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2016,
        "citationCount": 1409,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/D16-1244.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1606.01933, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "144729897",
                "name": "Ankur P. Parikh"
            },
            {
                "authorId": "2556289",
                "name": "Oscar Tckstrm"
            },
            {
                "authorId": "143790066",
                "name": "Dipanjan Das"
            },
            {
                "authorId": "39328010",
                "name": "Jakob Uszkoreit"
            }
        ],
        "abstract": "We propose a simple neural architecture for natural language inference. Our approach uses attention to decompose the problem into subproblems that can be solved separately, thus making it trivially parallelizable. On the Stanford Natural Language Inference (SNLI) dataset, we obtain state-of-the-art results with almost an order of magnitude fewer parameters than previous work and without relying on any word-order information. Adding intra-sentence attention that takes a minimum amount of order into account yields further improvements."
    },
    {
        "paperId": "57a10537978600fd33dcdd48922c791609a4851a",
        "url": "https://www.semanticscholar.org/paper/57a10537978600fd33dcdd48922c791609a4851a",
        "title": "Sequence-Level Knowledge Distillation",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2016,
        "citationCount": 1199,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/D16-1139.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1606.07947, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "38367242",
                "name": "Yoon Kim"
            },
            {
                "authorId": "2531268",
                "name": "Alexander M. Rush"
            }
        ],
        "abstract": "Neural machine translation (NMT) offers a novel alternative formulation of translation that is potentially simpler than statistical approaches. However to reach competitive performance, NMT models need to be exceedingly large. In this paper we consider applying knowledge distillation approaches (Bucila et al., 2006; Hinton et al., 2015) that have proven successful for reducing the size of neural models in other domains to the problem of NMT. We demonstrate that standard knowledge distillation applied to word-level prediction can be effective for NMT, and also introduce two novel sequence-level versions of knowledge distillation that further improve performance, and somewhat surprisingly, seem to eliminate the need for beam search (even when applied on the original teacher model). Our best student model runs 10 times faster than its state-of-the-art teacher with little loss in performance. It is also significantly better than a baseline model trained without knowledge distillation: by 4.2/1.7 BLEU with greedy decoding/beam search. Applying weight pruning on top of knowledge distillation results in a student model that has 13 times fewer parameters than the original teacher model, with a decrease of 0.4 BLEU."
    },
    {
        "paperId": "82bb306038446302cedd20fa986d20640ed88a2e",
        "url": "https://www.semanticscholar.org/paper/82bb306038446302cedd20fa986d20640ed88a2e",
        "title": "Attention-based LSTM for Aspect-level Sentiment Classification",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2016,
        "citationCount": 2104,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/D16-1058.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/D16-1058, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "6285226",
                "name": "Yequan Wang"
            },
            {
                "authorId": "1730108",
                "name": "Minlie Huang"
            },
            {
                "authorId": "145213540",
                "name": "Xiaoyan Zhu"
            },
            {
                "authorId": "2116547240",
                "name": "Li Zhao"
            }
        ],
        "abstract": "Aspect-level sentiment classification is a fine-grained task in sentiment analysis. Since it provides more complete and in-depth results, aspect-level sentiment analysis has received much attention these years. In this paper, we reveal that the sentiment polarity of a sentence is not only determined by the content but is also highly related to the concerned aspect. For instance, The appetizers are ok, but the service is slow., for aspect taste, the polarity is positive while for service, the polarity is negative. Therefore, it is worthwhile to explore the connection between an aspect and the content of a sentence. To this end, we propose an Attention-based Long Short-Term Memory Network for aspect-level sentiment classification. The attention mechanism can concentrate on different parts of a sentence when different aspects are taken as input. We experiment on the SemEval 2014 dataset and results show that our model achieves state-ofthe-art performance on aspect-level sentiment classification."
    },
    {
        "paperId": "bba5f2852b1db8a18004eb7328efa5e1d57cc62a",
        "url": "https://www.semanticscholar.org/paper/bba5f2852b1db8a18004eb7328efa5e1d57cc62a",
        "title": "Key-Value Memory Networks for Directly Reading Documents",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2016,
        "citationCount": 1032,
        "openAccessPdf": {
            "url": "https://aclanthology.org/D16-1147.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1606.03126, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "143622869",
                "name": "Alexander H. Miller"
            },
            {
                "authorId": "2064150446",
                "name": "Adam Fisch"
            },
            {
                "authorId": "34176020",
                "name": "Jesse Dodge"
            },
            {
                "authorId": "145926563",
                "name": "Amir-Hossein Karimi"
            },
            {
                "authorId": "1713934",
                "name": "Antoine Bordes"
            },
            {
                "authorId": "145183709",
                "name": "J. Weston"
            }
        ],
        "abstract": "Directly reading documents and being able to answer questions from them is an unsolved challenge. To avoid its inherent difficulty, question answering (QA) has been directed towards using Knowledge Bases (KBs) instead, which has proven effective. Unfortunately KBs often suffer from being too restrictive, as the schema cannot support certain types of answers, and too sparse, e.g. Wikipedia contains much more information than Freebase. In this work we introduce a new method, Key-Value Memory Networks, that makes reading documents more viable by utilizing different encodings in the addressing and output stages of the memory read operation. To compare using KBs, information extraction or Wikipedia documents directly in a single framework we construct an analysis tool, WikiMovies, a QA dataset that contains raw text alongside a preprocessed KB, in the domain of movies. Our method reduces the gap between all three settings. It also achieves state-of-the-art results on the existing WikiQA benchmark."
    },
    {
        "paperId": "135bafc83e9a73c88e759f98a28edfdb5c02f81d",
        "url": "https://www.semanticscholar.org/paper/135bafc83e9a73c88e759f98a28edfdb5c02f81d",
        "title": "Men Also Like Shopping: Reducing Gender Bias Amplification using Corpus-level Constraints",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2017,
        "citationCount": 1032,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/D17-1323.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1707.09457, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "33524946",
                "name": "Jieyu Zhao"
            },
            {
                "authorId": "1785372925",
                "name": "Tianlu Wang"
            },
            {
                "authorId": "2064210",
                "name": "Mark Yatskar"
            },
            {
                "authorId": "2004053",
                "name": "Vicente Ordonez"
            },
            {
                "authorId": "2782886",
                "name": "Kai-Wei Chang"
            }
        ],
        "abstract": "Language is increasingly being used to de-fine rich visual recognition problems with supporting image collections sourced from the web. Structured prediction models are used in these tasks to take advantage of correlations between co-occurring labels and visual input but risk inadvertently encoding social biases found in web corpora. In this work, we study data and models associated with multilabel object classification and visual semantic role labeling. We find that (a) datasets for these tasks contain significant gender bias and (b) models trained on these datasets further amplify existing bias. For example, the activity cooking is over 33% more likely to involve females than males in a training set, and a trained model further amplifies the disparity to 68% at test time. We propose to inject corpus-level constraints for calibrating existing structured prediction models and design an algorithm based on Lagrangian relaxation for collective inference. Our method results in almost no performance loss for the underlying recognition task but decreases the magnitude of bias amplification by 47.5% and 40.5% for multilabel classification and visual semantic role labeling, respectively"
    },
    {
        "paperId": "5a96f2bfa2deae2bc35b250251d5fbe82ef4932b",
        "url": "https://www.semanticscholar.org/paper/5a96f2bfa2deae2bc35b250251d5fbe82ef4932b",
        "title": "Tensor Fusion Network for Multimodal Sentiment Analysis",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2017,
        "citationCount": 1535,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/D17-1115.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1707.07250, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "144802290",
                "name": "Amir Zadeh"
            },
            {
                "authorId": "2108827470",
                "name": "Minghai Chen"
            },
            {
                "authorId": "1746416",
                "name": "Soujanya Poria"
            },
            {
                "authorId": "49943757",
                "name": "E. Cambria"
            },
            {
                "authorId": "49933077",
                "name": "Louis-philippe Morency"
            }
        ],
        "abstract": "Multimodal sentiment analysis is an increasingly popular research area, which extends the conventional language-based definition of sentiment analysis to a multimodal setup where other relevant modalities accompany language. In this paper, we pose the problem of multimodal sentiment analysis as modeling intra-modality and inter-modality dynamics. We introduce a novel model, termed Tensor Fusion Networks, which learns both such dynamics end-to-end. The proposed approach is tailored for the volatile nature of spoken language in online videos as well as accompanying gestures and voice. In the experiments, our model outperforms state-of-the-art approaches for both multimodal and unimodal sentiment analysis."
    },
    {
        "paperId": "636a79420d838eabe4af7fb25d6437de45ab64e8",
        "url": "https://www.semanticscholar.org/paper/636a79420d838eabe4af7fb25d6437de45ab64e8",
        "title": "RACE: Large-scale ReAding Comprehension Dataset From Examinations",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2017,
        "citationCount": 1508,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/D17-1082.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1704.04683, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1857734",
                "name": "Guokun Lai"
            },
            {
                "authorId": "1912046",
                "name": "Qizhe Xie"
            },
            {
                "authorId": "2391802",
                "name": "Hanxiao Liu"
            },
            {
                "authorId": "35729970",
                "name": "Yiming Yang"
            },
            {
                "authorId": "144547315",
                "name": "E. Hovy"
            }
        ],
        "abstract": "We present RACE, a new dataset for benchmark evaluation of methods in the reading comprehension task. Collected from the English exams for middle and high school Chinese students in the age range between 12 to 18, RACE consists of near 28,000 passages and near 100,000 questions generated by human experts (English instructors), and covers a variety of topics which are carefully designed for evaluating the students ability in understanding and reasoning. In particular, the proportion of questions that requires reasoning is much larger in RACE than that in other benchmark datasets for reading comprehension, and there is a significant gap between the performance of the state-of-the-art models (43%) and the ceiling human performance (95%). We hope this new dataset can serve as a valuable resource for research and evaluation in machine comprehension. The dataset is freely available at http://www.cs.cmu.edu/~glai1/data/race/ and the code is available at https://github.com/qizhex/RACE_AR_baselines."
    },
    {
        "paperId": "ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c",
        "url": "https://www.semanticscholar.org/paper/ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c",
        "title": "Supervised Learning of Universal Sentence Representations from Natural Language Inference Data",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2017,
        "citationCount": 2173,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/D17-1070.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1705.02364, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2480903",
                "name": "Alexis Conneau"
            },
            {
                "authorId": "1743722",
                "name": "Douwe Kiela"
            },
            {
                "authorId": "144518416",
                "name": "Holger Schwenk"
            },
            {
                "authorId": "2934336",
                "name": "Loc Barrault"
            },
            {
                "authorId": "1713934",
                "name": "Antoine Bordes"
            }
        ],
        "abstract": "Many modern NLP systems rely on word embeddings, previously trained in an unsupervised manner on large corpora, as base features. Efforts to obtain embeddings for larger chunks of text, such as sentences, have however not been so successful. Several attempts at learning unsupervised representations of sentences have not reached satisfactory enough performance to be widely adopted. In this paper, we show how universal sentence representations trained using the supervised data of the Stanford Natural Language Inference datasets can consistently outperform unsupervised methods like SkipThought vectors on a wide range of transfer tasks. Much like how computer vision uses ImageNet to obtain features, which can then be transferred to other tasks, our work tends to indicate the suitability of natural language inference for transfer learning to other NLP tasks. Our encoder is publicly available."
    },
    {
        "paperId": "ffb949d3493c3b2f3c9acf9c75cb03938933ddf0",
        "url": "https://www.semanticscholar.org/paper/ffb949d3493c3b2f3c9acf9c75cb03938933ddf0",
        "title": "Adversarial Examples for Evaluating Reading Comprehension Systems",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2017,
        "citationCount": 1669,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/D17-1215.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1707.07328, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3422908",
                "name": "Robin Jia"
            },
            {
                "authorId": "145419642",
                "name": "Percy Liang"
            }
        ],
        "abstract": "Standard accuracy metrics indicate that reading comprehension systems are making rapid progress, but the extent to which these systems truly understand language remains unclear. To reward systems with real language understanding abilities, we propose an adversarial evaluation scheme for the Stanford Question Answering Dataset (SQuAD). Our method tests whether systems can answer questions about paragraphs that contain adversarially inserted sentences, which are automatically generated to distract computer systems without changing the correct answer or misleading humans. In this adversarial setting, the accuracy of sixteen published models drops from an average of 75% F1 score to 36%; when the adversary is allowed to add ungrammatical sequences of words, average accuracy on four models decreases further to 7%. We hope our insights will motivate the development of new models that understand language more precisely."
    },
    {
        "paperId": "1536e8958697c5364f68b2e2448905dbbeb3a0ca",
        "url": "https://www.semanticscholar.org/paper/1536e8958697c5364f68b2e2448905dbbeb3a0ca",
        "title": "Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2018,
        "citationCount": 2019,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/D18-1260.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1809.02789, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "39980906",
                "name": "Todor Mihaylov"
            },
            {
                "authorId": "48323507",
                "name": "Peter Clark"
            },
            {
                "authorId": "2236429",
                "name": "Tushar Khot"
            },
            {
                "authorId": "48229640",
                "name": "Ashish Sabharwal"
            }
        ],
        "abstract": "We present a new kind of question answering dataset, OpenBookQA, modeled after open book exams for assessing human understanding of a subject. The open book that comes with our questions is a set of 1326 elementary level science facts. Roughly 6000 questions probe an understanding of these facts and their application to novel situations. This requires combining an open book fact (e.g., metals conduct electricity) with broad common knowledge (e.g., a suit of armor is made of metal) obtained from other sources. While existing QA datasets over documents or knowledge bases, being generally self-contained, focus on linguistic understanding, OpenBookQA probes a deeper understanding of both the topicin the context of common knowledgeand the language it is expressed in. Human performance on OpenBookQA is close to 92%, but many state-of-the-art pre-trained QA methods perform surprisingly poorly, worse than several simple neural baselines we develop. Our oracle experiments designed to circumvent the knowledge retrieval bottleneck demonstrate the value of both the open book and additional facts. We leave it as a challenge to solve the retrieval problem in this multi-hop setting and to close the large gap to human performance."
    },
    {
        "paperId": "1c3112ef8a346b9817382ed34a8c146c53d5bcf5",
        "url": "https://www.semanticscholar.org/paper/1c3112ef8a346b9817382ed34a8c146c53d5bcf5",
        "title": "XNLI: Evaluating Cross-lingual Sentence Representations",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2018,
        "citationCount": 1520,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/D18-1269.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1809.05053, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2480903",
                "name": "Alexis Conneau"
            },
            {
                "authorId": "1830914",
                "name": "Guillaume Lample"
            },
            {
                "authorId": "1905713",
                "name": "Ruty Rinott"
            },
            {
                "authorId": "81840293",
                "name": "Adina Williams"
            },
            {
                "authorId": "3644767",
                "name": "Samuel R. Bowman"
            },
            {
                "authorId": "144518416",
                "name": "Holger Schwenk"
            },
            {
                "authorId": "1759422",
                "name": "Veselin Stoyanov"
            }
        ],
        "abstract": "State-of-the-art natural language processing systems rely on supervision in the form of annotated data to learn competent models. These models are generally trained on data in a single language (usually English), and cannot be directly used beyond that language. Since collecting data in every language is not realistic, there has been a growing interest in cross-lingual language understanding (XLU) and low-resource cross-language transfer. In this work, we construct an evaluation set for XLU by extending the development and test sets of the Multi-Genre Natural Language Inference Corpus (MultiNLI) to 14 languages, including low-resource languages such as Swahili and Urdu. We hope that our dataset, dubbed XNLI, will catalyze research in cross-lingual sentence understanding by providing an informative standard evaluation task. In addition, we provide several baselines for multilingual sentence understanding, including two based on machine translation systems, and two that use parallel data to train aligned multilingual bag-of-words and LSTM encoders. We find that XNLI represents a practical and challenging evaluation suite, and that directly translating the test data yields the best performance among available baselines."
    },
    {
        "paperId": "22655979df781d222eaf812b0d325fa9adf11594",
        "url": "https://www.semanticscholar.org/paper/22655979df781d222eaf812b0d325fa9adf11594",
        "title": "HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2018,
        "citationCount": 3549,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/D18-1259.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1809.09600, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2109512754",
                "name": "Zhilin Yang"
            },
            {
                "authorId": "50531624",
                "name": "Peng Qi"
            },
            {
                "authorId": "35097114",
                "name": "Saizheng Zhang"
            },
            {
                "authorId": "1751762",
                "name": "Yoshua Bengio"
            },
            {
                "authorId": "50056360",
                "name": "William W. Cohen"
            },
            {
                "authorId": "145124475",
                "name": "R. Salakhutdinov"
            },
            {
                "authorId": "144783904",
                "name": "Christopher D. Manning"
            }
        ],
        "abstract": "Existing question answering (QA) datasets fail to train QA systems to perform complex reasoning and provide explanations for answers. We introduce HotpotQA, a new dataset with 113k Wikipedia-based question-answer pairs with four key features: (1) the questions require finding and reasoning over multiple supporting documents to answer; (2) the questions are diverse and not constrained to any pre-existing knowledge bases or knowledge schemas; (3) we provide sentence-level supporting facts required for reasoning, allowing QA systems to reason with strong supervision and explain the predictions; (4) we offer a new type of factoid comparison questions to test QA systems ability to extract relevant facts and perform necessary comparison. We show that HotpotQA is challenging for the latest QA systems, and the supporting facts enable models to improve performance and make explainable predictions."
    },
    {
        "paperId": "305b2cf37e5dece81e95c92883d5a6e28ac93b22",
        "url": "https://www.semanticscholar.org/paper/305b2cf37e5dece81e95c92883d5a6e28ac93b22",
        "title": "Dont Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2018,
        "citationCount": 1892,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/D18-1206.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1808.08745, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "143790499",
                "name": "Shashi Narayan"
            },
            {
                "authorId": "40146204",
                "name": "Shay B. Cohen"
            },
            {
                "authorId": "1747893",
                "name": "Mirella Lapata"
            }
        ],
        "abstract": "We introduce extreme summarization, a new single-document summarization task which does not favor extractive strategies and calls for an abstractive modeling approach. The idea is to create a short, one-sentence news summary answering the question What is the article about?. We collect a real-world, large-scale dataset for this task by harvesting online articles from the British Broadcasting Corporation (BBC). We propose a novel abstractive model which is conditioned on the articles topics and based entirely on convolutional neural networks. We demonstrate experimentally that this architecture captures long-range dependencies in a document and recognizes pertinent content, outperforming an oracle extractive system and state-of-the-art abstractive approaches when evaluated automatically and by humans."
    },
    {
        "paperId": "8e773b1840b894603c06b677a0f15ebcf0f26378",
        "url": "https://www.semanticscholar.org/paper/8e773b1840b894603c06b677a0f15ebcf0f26378",
        "title": "Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2018,
        "citationCount": 1554,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/D18-1425.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1809.08887, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "48881008",
                "name": "Tao Yu"
            },
            {
                "authorId": "144142360",
                "name": "Rui Zhang"
            },
            {
                "authorId": "151083432",
                "name": "Kai-Chou Yang"
            },
            {
                "authorId": "19168196",
                "name": "Michihiro Yasunaga"
            },
            {
                "authorId": "2111236786",
                "name": "Dongxu Wang"
            },
            {
                "authorId": "2052868823",
                "name": "Zifan Li"
            },
            {
                "authorId": "2109970984",
                "name": "James Ma"
            },
            {
                "authorId": "47841931",
                "name": "Irene Z Li"
            },
            {
                "authorId": "2053125043",
                "name": "Qingning Yao"
            },
            {
                "authorId": "51448842",
                "name": "Shanelle Roman"
            },
            {
                "authorId": "2144370906",
                "name": "Zilin Zhang"
            },
            {
                "authorId": "9215251",
                "name": "Dragomir R. Radev"
            }
        ],
        "abstract": "We present Spider, a large-scale complex and cross-domain semantic parsing and text-to-SQL dataset annotated by 11 college students. It consists of 10,181 questions and 5,693 unique complex SQL queries on 200 databases with multiple tables covering 138 different domains. We define a new complex and cross-domain semantic parsing and text-to-SQL task so that different complicated SQL queries and databases appear in train and test sets. In this way, the task requires the model to generalize well to both new SQL queries and new database schemas. Therefore, Spider is distinct from most of the previous semantic parsing tasks because they all use a single database and have the exact same program in the train set and the test set. We experiment with various state-of-the-art models and the best model achieves only 9.7% exact matching accuracy on a database split setting. This shows that Spider presents a strong challenge for future research. Our dataset and task with the most recent updates are publicly available at https://yale-lily.github.io/seq2sql/spider."
    },
    {
        "paperId": "b5246fa284f86b544a7c31f050b3bd0defd053fd",
        "url": "https://www.semanticscholar.org/paper/b5246fa284f86b544a7c31f050b3bd0defd053fd",
        "title": "SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2018,
        "citationCount": 3863,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/D18-2012.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1808.06226, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1765329",
                "name": "Taku Kudo"
            },
            {
                "authorId": "2113584859",
                "name": "John Richardson"
            }
        ],
        "abstract": "This paper describes SentencePiece, a language-independent subword tokenizer and detokenizer designed for Neural-based text processing, including Neural Machine Translation. It provides open-source C++ and Python implementations for subword units. While existing subword segmentation tools assume that the input is pre-tokenized into word sequences, SentencePiece can train subword models directly from raw sentences, which allows us to make a purely end-to-end and language independent system. We perform a validation experiment of NMT on English-Japanese machine translation, and find that it is possible to achieve comparable accuracy to direct subword training from raw sentences. We also compare the performance of subword training and segmentation with various configurations. SentencePiece is available under the Apache 2 license at https://github.com/google/sentencepiece."
    },
    {
        "paperId": "f4a5503783487eba5c5e34b1d02c09016b244b1d",
        "url": "https://www.semanticscholar.org/paper/f4a5503783487eba5c5e34b1d02c09016b244b1d",
        "title": "MultiWOZ - A Large-Scale Multi-Domain Wizard-of-Oz Dataset for Task-Oriented Dialogue Modelling",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2018,
        "citationCount": 1414,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/D18-1547.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1810.00278, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "17895970",
                "name": "Pawe Budzianowski"
            },
            {
                "authorId": "144256365",
                "name": "Tsung-Hsien Wen"
            },
            {
                "authorId": "33870107",
                "name": "Bo-Hsiang Tseng"
            },
            {
                "authorId": "3450866",
                "name": "I. Casanueva"
            },
            {
                "authorId": "2295429",
                "name": "Stefan Ultes"
            },
            {
                "authorId": "2065760904",
                "name": "Osman Ramadan"
            },
            {
                "authorId": "1768624",
                "name": "Milica Gasic"
            }
        ],
        "abstract": "Even though machine learning has become the major scene in dialogue research community, the real breakthrough has been blocked by the scale of data available.To address this fundamental obstacle, we introduce the Multi-Domain Wizard-of-Oz dataset (MultiWOZ), a fully-labeled collection of human-human written conversations spanning over multiple domains and topics.At a size of 10k dialogues, it is at least one order of magnitude larger than all previous annotated task-oriented corpora.The contribution of this work apart from the open-sourced dataset is two-fold:firstly, a detailed description of the data collection procedure along with a summary of data structure and analysis is provided. The proposed data-collection pipeline is entirely based on crowd-sourcing without the need of hiring professional annotators;secondly, a set of benchmark results of belief tracking, dialogue act and response generation is reported, which shows the usability of the data and sets a baseline for future studies."
    },
    {
        "paperId": "0c3c4c88c7b07596221ac640c7b7102686e3eae3",
        "url": "https://www.semanticscholar.org/paper/0c3c4c88c7b07596221ac640c7b7102686e3eae3",
        "title": "PubMedQA: A Dataset for Biomedical Research Question Answering",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2019,
        "citationCount": 1265,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/D19-1259.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1909.06146, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "144255060",
                "name": "Qiao Jin"
            },
            {
                "authorId": "34994191",
                "name": "Bhuwan Dhingra"
            },
            {
                "authorId": "2049408469",
                "name": "Zhengping Liu"
            },
            {
                "authorId": "50056360",
                "name": "William W. Cohen"
            },
            {
                "authorId": "2244761",
                "name": "Xinghua Lu"
            }
        ],
        "abstract": "We introduce PubMedQA, a novel biomedical question answering (QA) dataset collected from PubMed abstracts. The task of PubMedQA is to answer research questions with yes/no/maybe (e.g.: Do preoperative statins reduce atrial fibrillation after coronary artery bypass grafting?) using the corresponding abstracts. PubMedQA has 1k expert-annotated, 61.2k unlabeled and 211.3k artificially generated QA instances. Each PubMedQA instance is composed of (1) a question which is either an existing research article title or derived from one, (2) a context which is the corresponding abstract without its conclusion, (3) a long answer, which is the conclusion of the abstract and, presumably, answers the research question, and (4) a yes/no/maybe answer which summarizes the conclusion. PubMedQA is the first QA dataset where reasoning over biomedical research texts, especially their quantitative contents, is required to answer the questions. Our best performing model, multi-phase fine-tuning of BioBERT with long answer bag-of-word statistics as additional supervision, achieves 68.1% accuracy, compared to single human performance of 78.0% accuracy and majority-baseline of 55.2% accuracy, leaving much room for improvement. PubMedQA is publicly available at https://pubmedqa.github.io."
    },
    {
        "paperId": "156d217b0a911af97fa1b5a71dc909ccef7a8028",
        "url": "https://www.semanticscholar.org/paper/156d217b0a911af97fa1b5a71dc909ccef7a8028",
        "title": "SciBERT: A Pretrained Language Model for Scientific Text",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2019,
        "citationCount": 3444,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/D19-1371.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1903.10676, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "46181066",
                "name": "Iz Beltagy"
            },
            {
                "authorId": "46258841",
                "name": "Kyle Lo"
            },
            {
                "authorId": "2527954",
                "name": "Arman Cohan"
            }
        ],
        "abstract": "Obtaining large-scale annotated data for NLP tasks in the scientific domain is challenging and expensive. We release SciBERT, a pretrained language model based on BERT (Devlin et. al., 2018) to address the lack of high-quality, large-scale labeled scientific data. SciBERT leverages unsupervised pretraining on a large multi-domain corpus of scientific publications to improve performance on downstream scientific NLP tasks. We evaluate on a suite of tasks including sequence tagging, sentence classification and dependency parsing, with datasets from a variety of scientific domains. We demonstrate statistically significant improvements over BERT and achieve new state-of-the-art results on several of these tasks. The code and pretrained models are available at https://github.com/allenai/scibert/."
    },
    {
        "paperId": "162cad5df347bdac469331df540440b320b5aa21",
        "url": "https://www.semanticscholar.org/paper/162cad5df347bdac469331df540440b320b5aa21",
        "title": "EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2019,
        "citationCount": 2185,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/D19-1670.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1901.11196, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "144026731",
                "name": "Jason Wei"
            },
            {
                "authorId": "2132971481",
                "name": "Kai Zou"
            }
        ],
        "abstract": "We present EDA: easy data augmentation techniques for boosting performance on text classification tasks. EDA consists of four simple but powerful operations: synonym replacement, random insertion, random swap, and random deletion. On five text classification tasks, we show that EDA improves performance for both convolutional and recurrent neural networks. EDA demonstrates particularly strong results for smaller datasets; on average, across five datasets, training with EDA while using only 50% of the available training set achieved the same accuracy as normal training with all available data. We also performed extensive ablation studies and suggest parameters for practical use."
    },
    {
        "paperId": "41d49ec6f73ab5621ab8e8cb5ddb677a886ccc76",
        "url": "https://www.semanticscholar.org/paper/41d49ec6f73ab5621ab8e8cb5ddb677a886ccc76",
        "title": "Justifying Recommendations using Distantly-Labeled Reviews and Fine-Grained Aspects",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2019,
        "citationCount": 1569,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/D19-1018.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/D19-1018, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2148023",
                "name": "Jianmo Ni"
            },
            {
                "authorId": "97483167",
                "name": "Jiacheng Li"
            },
            {
                "authorId": "35660011",
                "name": "Julian McAuley"
            }
        ],
        "abstract": "Several recent works have considered the problem of generating reviews (or tips) as a form of explanation as to why a recommendation might match a customers interests. While promising, we demonstrate that existing approaches struggle (in terms of both quality and content) to generate justifications that are relevant to users decision-making process. We seek to introduce new datasets and methods to address the recommendation justification task. In terms of data, we first propose an extractive approach to identify review segments which justify users intentions; this approach is then used to distantly label massive review corpora and construct large-scale personalized recommendation justification datasets. In terms of generation, we are able to design two personalized generation models with this data: (1) a reference-based Seq2Seq model with aspect-planning which can generate justifications covering different aspects, and (2) an aspect-conditional masked language model which can generate diverse justifications based on templates extracted from justification histories. We conduct experiments on two real-world datasets which show that our model is capable of generating convincing and diverse justifications."
    },
    {
        "paperId": "63748e59f4e106cbda6b65939b77589f40e48fcb",
        "url": "https://www.semanticscholar.org/paper/63748e59f4e106cbda6b65939b77589f40e48fcb",
        "title": "Text Summarization with Pretrained Encoders",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2019,
        "citationCount": 1563,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/D19-1387.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1908.08345, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "39798499",
                "name": "Yang Liu"
            },
            {
                "authorId": "1747893",
                "name": "Mirella Lapata"
            }
        ],
        "abstract": "Bidirectional Encoder Representations from Transformers (BERT) represents the latest incarnation of pretrained language models which have recently advanced a wide range of natural language processing tasks. In this paper, we showcase how BERT can be usefully applied in text summarization and propose a general framework for both extractive and abstractive models. We introduce a novel document-level encoder based on BERT which is able to express the semantics of a document and obtain representations for its sentences. Our extractive model is built on top of this encoder by stacking several inter-sentence Transformer layers. For abstractive summarization, we propose a new fine-tuning schedule which adopts different optimizers for the encoder and the decoder as a means of alleviating the mismatch between the two (the former is pretrained while the latter is not). We also demonstrate that a two-staged fine-tuning approach can further boost the quality of the generated summaries. Experiments on three datasets show that our model achieves state-of-the-art results across the board in both extractive and abstractive settings."
    },
    {
        "paperId": "79c93274429d6355959f1e4374c2147bb81ea649",
        "url": "https://www.semanticscholar.org/paper/79c93274429d6355959f1e4374c2147bb81ea649",
        "title": "LXMERT: Learning Cross-Modality Encoder Representations from Transformers",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2019,
        "citationCount": 2755,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/D19-1514.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1908.07490, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3218666",
                "name": "Hao Hao Tan"
            },
            {
                "authorId": "143977268",
                "name": "Mohit Bansal"
            }
        ],
        "abstract": "Vision-and-language reasoning requires an understanding of visual concepts, language semantics, and, most importantly, the alignment and relationships between these two modalities. We thus propose the LXMERT (Learning Cross-Modality Encoder Representations from Transformers) framework to learn these vision-and-language connections. In LXMERT, we build a large-scale Transformer model that consists of three encoders: an object relationship encoder, a language encoder, and a cross-modality encoder. Next, to endow our model with the capability of connecting vision and language semantics, we pre-train the model with large amounts of image-and-sentence pairs, via five diverse representative pre-training tasks: masked language modeling, masked object prediction (feature regression and label classification), cross-modality matching, and image question answering. These tasks help in learning both intra-modality and cross-modality relationships. After fine-tuning from our pre-trained parameters, our model achieves the state-of-the-art results on two visual question answering datasets (i.e., VQA and GQA). We also show the generalizability of our pre-trained cross-modality model by adapting it to a challenging visual-reasoning task, NLVR2, and improve the previous best result by 22% absolute (54% to 76%). Lastly, we demonstrate detailed ablation studies to prove that both our novel model components and pre-training strategies significantly contribute to our strong results. Code and pre-trained models publicly available at: https://github.com/airsplay/lxmert"
    },
    {
        "paperId": "93d63ec754f29fa22572615320afe0521f7ec66d",
        "url": "https://www.semanticscholar.org/paper/93d63ec754f29fa22572615320afe0521f7ec66d",
        "title": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2019,
        "citationCount": 15348,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/D19-1410.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1908.10084, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2959414",
                "name": "Nils Reimers"
            },
            {
                "authorId": "1730400",
                "name": "Iryna Gurevych"
            }
        ],
        "abstract": "BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations (~65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods."
    },
    {
        "paperId": "9d7902e834d5d1d35179962c7a5b9d16623b0d39",
        "url": "https://www.semanticscholar.org/paper/9d7902e834d5d1d35179962c7a5b9d16623b0d39",
        "title": "How Contextual are Contextualized Word Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2019,
        "citationCount": 1000,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/D19-1006.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1909.00512, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "10324691",
                "name": "Kawin Ethayarajh"
            }
        ],
        "abstract": "Replacing static word embeddings with contextualized word representations has yielded significant improvements on many NLP tasks. However, just how contextual are the contextualized representations produced by models such as ELMo and BERT? Are there infinitely many context-specific representations for each word, or are words essentially assigned one of a finite number of word-sense representations? For one, we find that the contextualized representations of all words are not isotropic in any layer of the contextualizing model. While representations of the same word in different contexts still have a greater cosine similarity than those of two different words, this self-similarity is much lower in upper layers. This suggests that upper layers of contextualizing models produce more context-specific representations, much like how upper layers of LSTMs produce more task-specific representations. In all layers of ELMo, BERT, and GPT-2, on average, less than 5% of the variance in a words contextualized representations can be explained by a static embedding for that word, providing some justification for the success of contextualized representations."
    },
    {
        "paperId": "ce177672b00ddf46e4906157a7e997ca9338b8b9",
        "url": "https://www.semanticscholar.org/paper/ce177672b00ddf46e4906157a7e997ca9338b8b9",
        "title": "Attention is not not Explanation",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2019,
        "citationCount": 1020,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/D19-1002.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1908.04626, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "35823986",
                "name": "Sarah Wiegreffe"
            },
            {
                "authorId": "1826312",
                "name": "Yuval Pinter"
            }
        ],
        "abstract": "Attention mechanisms play a central role in NLP systems, especially within recurrent neural network (RNN) models. Recently, there has been increasing interest in whether or not the intermediate representations offered by these modules may be used to explain the reasoning for a models prediction, and consequently reach insights regarding the models decision-making process. A recent paper claims that Attention is not Explanation (Jain and Wallace, 2019). We challenge many of the assumptions underlying this work, arguing that such a claim depends on ones definition of explanation, and that testing it needs to take into account all elements of the model. We propose four alternative tests to determine when/whether attention can be used as explanation: a simple uniform-weights baseline; a variance calibration based on multiple random seed runs; a diagnostic framework using frozen weights from pretrained models; and an end-to-end adversarial attention training protocol. Each allows for meaningful interpretation of attention mechanisms in RNN models. We show that even when reliable adversarial distributions can be found, they dont perform well on the simple diagnostic, indicating that prior work does not disprove the usefulness of attention mechanisms for explainability."
    },
    {
        "paperId": "d0086b86103a620a86bc918746df0aa642e2a8a3",
        "url": "https://www.semanticscholar.org/paper/d0086b86103a620a86bc918746df0aa642e2a8a3",
        "title": "Language Models as Knowledge Bases?",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2019,
        "citationCount": 2964,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/D19-1250.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1909.01066, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "40052301",
                "name": "F. Petroni"
            },
            {
                "authorId": "2620211",
                "name": "Tim Rocktschel"
            },
            {
                "authorId": "145222654",
                "name": "Patrick Lewis"
            },
            {
                "authorId": "152918929",
                "name": "A. Bakhtin"
            },
            {
                "authorId": "39417610",
                "name": "Yuxiang Wu"
            },
            {
                "authorId": "143622869",
                "name": "Alexander H. Miller"
            },
            {
                "authorId": "48662861",
                "name": "Sebastian Riedel"
            }
        ],
        "abstract": "Recent progress in pretraining language models on large textual corpora led to a surge of improvements for downstream NLP tasks. Whilst learning linguistic knowledge, these models may also be storing relational knowledge present in the training data, and may be able to answer queries structured as fill-in-the-blank cloze statements. Language models have many advantages over structured knowledge bases: they require no schema engineering, allow practitioners to query about an open class of relations, are easy to extend to more data, and require no human supervision to train. We present an in-depth analysis of the relational knowledge already present (without fine-tuning) in a wide range of state-of-the-art pretrained language models. We find that (i) without fine-tuning, BERT contains relational knowledge competitive with traditional NLP methods that have some access to oracle knowledge, (ii) BERT also does remarkably well on open-domain question answering against a supervised baseline, and (iii) certain types of factual knowledge are learned much more readily than others by standard language model pretraining approaches. The surprisingly strong ability of these models to recall factual knowledge without any fine-tuning demonstrates their potential as unsupervised open-domain QA systems. The code to reproduce our analysis is available at https://github.com/facebookresearch/LAMA."
    },
    {
        "paperId": "4a54d58a4b20e4f3af25cea3c188a12082a95e02",
        "url": "https://www.semanticscholar.org/paper/4a54d58a4b20e4f3af25cea3c188a12082a95e02",
        "title": "Transformer Feed-Forward Layers Are Key-Value Memories",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2020,
        "citationCount": 1121,
        "openAccessPdf": {
            "url": "https://aclanthology.org/2021.emnlp-main.446.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2012.14913, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "22245981",
                "name": "Mor Geva"
            },
            {
                "authorId": "39347554",
                "name": "R. Schuster"
            },
            {
                "authorId": "1750652",
                "name": "Jonathan Berant"
            },
            {
                "authorId": "39455775",
                "name": "Omer Levy"
            }
        ],
        "abstract": "Feed-forward layers constitute two-thirds of a transformer models parameters, yet their role in the network remains under-explored. We show that feed-forward layers in transformer-based language models operate as key-value memories, where each key correlates with textual patterns in the training examples, and each value induces a distribution over the output vocabulary. Our experiments show that the learned patterns are human-interpretable, and that lower layers tend to capture shallow patterns, while upper layers learn more semantic ones. The values complement the keys input patterns by inducing output distributions that concentrate probability mass on tokens likely to appear immediately after each pattern, particularly in the upper layers. Finally, we demonstrate that the output of a feed-forward layer is a composition of its memories, which is subsequently refined throughout the models layers via residual connections to produce the final output distribution."
    },
    {
        "paperId": "72cdd6ebe0221fb568ef20534f44ba5b35190a56",
        "url": "https://www.semanticscholar.org/paper/72cdd6ebe0221fb568ef20534f44ba5b35190a56",
        "title": "BERTweet: A pre-trained language model for English Tweets",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2020,
        "citationCount": 1008,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/2020.emnlp-demos.2.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2005.10200, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "34691913",
                "name": "Dat Quoc Nguyen"
            },
            {
                "authorId": "143768607",
                "name": "Thanh Vu"
            },
            {
                "authorId": "1398541475",
                "name": "A. Nguyen"
            }
        ],
        "abstract": "We present BERTweet, the first public large-scale pre-trained language model for English Tweets. Our BERTweet, having the same architecture as BERT-base (Devlin et al., 2019), is trained using the RoBERTa pre-training procedure (Liu et al., 2019). Experiments show that BERTweet outperforms strong baselines RoBERTa-base and XLM-R-base (Conneau et al., 2020), producing better performance results than the previous state-of-the-art models on three Tweet NLP tasks: Part-of-speech tagging, Named-entity recognition and text classification. We release BERTweet under the MIT License to facilitate future research and applications on Tweet data. Our BERTweet is available at https://github.com/VinAIResearch/BERTweet"
    },
    {
        "paperId": "8f34ee2ec88e8b19b2736de55eb170539d26e527",
        "url": "https://www.semanticscholar.org/paper/8f34ee2ec88e8b19b2736de55eb170539d26e527",
        "title": "Making Monolingual Sentence Embeddings Multilingual Using Knowledge Distillation",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2020,
        "citationCount": 1194,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/2020.emnlp-main.365.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2004.09813, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2959414",
                "name": "Nils Reimers"
            },
            {
                "authorId": "1730400",
                "name": "Iryna Gurevych"
            }
        ],
        "abstract": "We present an easy and efficient method to extend existing sentence embedding models to new languages. This allows to create multilingual versions from previously monolingual models. The training is based on the idea that a translated sentence should be mapped to the same location in the vector space as the original sentence. We use the original (monolingual) model to generate sentence embeddings for the source language and then train a new system on translated sentences to mimic the original model. Compared to other methods for training multilingual sentence embeddings, this approach has several advantages: It is easy to extend existing models with relatively few samples to new languages, it is easier to ensure desired properties for the vector space, and the hardware requirements for training is lower. We demonstrate the effectiveness of our approach for 10 languages from various language families. Code to extend sentence embeddings models to more than 400 languages is publicly available."
    },
    {
        "paperId": "9e67b9758520e49016ab66bafb974d2e1ed762d1",
        "url": "https://www.semanticscholar.org/paper/9e67b9758520e49016ab66bafb974d2e1ed762d1",
        "title": "COMET: A Neural Framework for MT Evaluation",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2020,
        "citationCount": 1349,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/2020.emnlp-main.213.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2009.09025, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "15631652",
                "name": "Ricardo Rei"
            },
            {
                "authorId": "40163298",
                "name": "Craig Alan Stewart"
            },
            {
                "authorId": "50588992",
                "name": "Ana C. Farinha"
            },
            {
                "authorId": "1784914",
                "name": "A. Lavie"
            }
        ],
        "abstract": "We present COMET, a neural framework for training multilingual machine translation evaluation models which obtains new state-of-the-art levels of correlation with human judgements. Our framework leverages recent breakthroughs in cross-lingual pretrained language modeling resulting in highly multilingual and adaptable MT evaluation models that exploit information from both the source input and a target-language reference translation in order to more accurately predict MT quality. To showcase our framework, we train three models with different types of human judgements: Direct Assessments, Human-mediated Translation Edit Rate and Multidimensional Quality Metrics. Our models achieve new state-of-the-art performance on the WMT 2019 Metrics shared task and demonstrate robustness to high-performing systems."
    },
    {
        "paperId": "b26f2037f769d5ffc5f7bdcec2de8da28ec14bee",
        "url": "https://www.semanticscholar.org/paper/b26f2037f769d5ffc5f7bdcec2de8da28ec14bee",
        "title": "Dense Passage Retrieval for Open-Domain Question Answering",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2020,
        "citationCount": 4745,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/2020.emnlp-main.550.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2004.04906, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2067091563",
                "name": "Vladimir Karpukhin"
            },
            {
                "authorId": "9185192",
                "name": "Barlas Ouz"
            },
            {
                "authorId": "48872685",
                "name": "Sewon Min"
            },
            {
                "authorId": "145222654",
                "name": "Patrick Lewis"
            },
            {
                "authorId": "51183248",
                "name": "Ledell Yu Wu"
            },
            {
                "authorId": "2068070",
                "name": "Sergey Edunov"
            },
            {
                "authorId": "50536468",
                "name": "Danqi Chen"
            },
            {
                "authorId": "144105277",
                "name": "Wen-tau Yih"
            }
        ],
        "abstract": "Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system largely by 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks."
    },
    {
        "paperId": "38b0567e83386ddc294d6c81b541deacbd8e3c2a",
        "url": "https://www.semanticscholar.org/paper/38b0567e83386ddc294d6c81b541deacbd8e3c2a",
        "title": "CLIPScore: A Reference-free Evaluation Metric for Image Captioning",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2021,
        "citationCount": 2241,
        "openAccessPdf": {
            "url": "https://aclanthology.org/2021.emnlp-main.595.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2104.08718, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2689239",
                "name": "Jack Hessel"
            },
            {
                "authorId": "14487640",
                "name": "Ari Holtzman"
            },
            {
                "authorId": "39191185",
                "name": "Maxwell Forbes"
            },
            {
                "authorId": "39227408",
                "name": "Ronan Le Bras"
            },
            {
                "authorId": "1699545",
                "name": "Yejin Choi"
            }
        ],
        "abstract": "Image captioning has conventionally relied on reference-based automatic evaluations, where machine captions are compared against captions written by humans. This is in contrast to the reference-free manner in which humans assess caption quality. In this paper, we report the surprising empirical finding that CLIP (Radford et al., 2021), a cross-modal model pretrained on 400M image+caption pairs from the web, can be used for robust automatic evaluation of image captioning without the need for references. Experiments spanning several corpora demonstrate that our new reference-free metric, CLIPScore, achieves the highest correlation with human judgements, outperforming existing reference-based metrics like CIDEr and SPICE. Information gain experiments demonstrate that CLIPScore, with its tight focus on image-text compatibility, is complementary to existing reference-based metrics that emphasize text-text similarities. Thus, we also present a reference-augmented version, RefCLIPScore, which achieves even higher correlation. Beyond literal description tasks, several case studies reveal domains where CLIPScore performs well (clip-art images, alt-text rating), but also where it is relatively weaker in comparison to reference-based metrics, e.g., news captions that require richer contextual knowledge."
    },
    {
        "paperId": "a30f912f8c5e2a2bfb06351d4578e1ba3fa37896",
        "url": "https://www.semanticscholar.org/paper/a30f912f8c5e2a2bfb06351d4578e1ba3fa37896",
        "title": "CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2021,
        "citationCount": 1967,
        "openAccessPdf": {
            "url": "https://aclanthology.org/2021.emnlp-main.685.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2109.00859, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "49416727",
                "name": "Yue Wang"
            },
            {
                "authorId": "2108528154",
                "name": "Weishi Wang"
            },
            {
                "authorId": "2708940",
                "name": "Shafiq R. Joty"
            },
            {
                "authorId": "1741126",
                "name": "S. Hoi"
            }
        ],
        "abstract": "Pre-trained models for Natural Languages (NL) like BERT and GPT have been recently shown to transfer well to Programming Languages (PL) and largely benefit a broad set of code-related tasks. Despite their success, most current methods either rely on an encoder-only (or decoder-only) pre-training that is suboptimal for generation (resp. understanding) tasks or process the code snippet in the same way as NL, neglecting the special characteristics of PL such as token types. We present CodeT5, a unified pre-trained encoder-decoder Transformer model that better leverages the code semantics conveyed from the developer-assigned identifiers. Our model employs a unified framework to seamlessly support both code understanding and generation tasks and allows for multi-task learning. Besides, we propose a novel identifier-aware pre-training task that enables the model to distinguish which code tokens are identifiers and to recover them when they are masked. Furthermore, we propose to exploit the user-written code comments with a bimodal dual generation task for better NL-PL alignment. Comprehensive experiments show that CodeT5 significantly outperforms prior methods on understanding tasks such as code defect detection and clone detection, and generation tasks across various directions including PL-NL, NL-PL, and PL-PL. Further analysis reveals that our model can better capture semantic information from code. Our code and pre-trained models are released at https://github.com/salesforce/CodeT5."
    },
    {
        "paperId": "c26759e6c701201af2f62f7ee4eb68742b5bf085",
        "url": "https://www.semanticscholar.org/paper/c26759e6c701201af2f62f7ee4eb68742b5bf085",
        "title": "SimCSE: Simple Contrastive Learning of Sentence Embeddings",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2021,
        "citationCount": 3990,
        "openAccessPdf": {
            "url": "https://aclanthology.org/2021.emnlp-main.552.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2104.08821, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "4800645",
                "name": "Tianyu Gao"
            },
            {
                "authorId": "2087141625",
                "name": "Xingcheng Yao"
            },
            {
                "authorId": "50536468",
                "name": "Danqi Chen"
            }
        ],
        "abstract": "This paper presents SimCSE, a simple contrastive learning framework that greatly advances the state-of-the-art sentence embeddings. We first describe an unsupervised approach, which takes an input sentence and predicts itself in a contrastive objective, with only standard dropout used as noise. This simple method works surprisingly well, performing on par with previous supervised counterparts. We find that dropout acts as minimal data augmentation and removing it leads to a representation collapse. Then, we propose a supervised approach, which incorporates annotated pairs from natural language inference datasets into our contrastive learning framework, by using entailment pairs as positives and contradiction pairs as hard negatives. We evaluate SimCSE on standard semantic textual similarity (STS) tasks, and our unsupervised and supervised models using BERT base achieve an average of 76.3% and 81.6% Spearmans correlation respectively, a 4.2% and 2.2% improvement compared to previous best results. We also showboth theoretically and empiricallythat contrastive learning objective regularizes pre-trained embeddings anisotropic space to be more uniform, and it better aligns positive pairs when supervised signals are available."
    },
    {
        "paperId": "ffdbd7f0b03b85747b001b4734d5ee31b5229aa4",
        "url": "https://www.semanticscholar.org/paper/ffdbd7f0b03b85747b001b4734d5ee31b5229aa4",
        "title": "The Power of Scale for Parameter-Efficient Prompt Tuning",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2021,
        "citationCount": 4918,
        "openAccessPdf": {
            "url": "https://aclanthology.org/2021.emnlp-main.243.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2104.08691, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "144104130",
                "name": "Brian Lester"
            },
            {
                "authorId": "1388360943",
                "name": "Rami Al-Rfou"
            },
            {
                "authorId": "40832517",
                "name": "Noah Constant"
            }
        ],
        "abstract": "In this work, we explore prompt tuning, a simple yet effective mechanism for learning soft prompts to condition frozen language models to perform specific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts are learned through backpropagation and can be tuned to incorporate signals from any number of labeled examples. Our end-to-end learned approach outperforms GPT-3s few-shot learning by a large margin. More remarkably, through ablations on model size using T5, we show that prompt tuning becomes more competitive with scale: as models exceed billions of parameters, our method closes the gap and matches the strong performance of model tuning (where all model weights are tuned). This finding is especially relevant because large models are costly to share and serve and the ability to reuse one frozen model for multiple downstream tasks can ease this burden. Our method can be seen as a simplification of the recently proposed prefix tuning of Li and Liang (2021) and we provide a comparison to this and other similar approaches. Finally, we show that conditioning a frozen model with soft prompts confers benefits in robustness to domain transfer and enables efficient prompt ensembling. We release code and model checkpoints to reproduce our experiments."
    },
    {
        "paperId": "06d7cb8c8816360feb33c3367073e0ef66d7d0b0",
        "url": "https://www.semanticscholar.org/paper/06d7cb8c8816360feb33c3367073e0ef66d7d0b0",
        "title": "Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2022,
        "citationCount": 1003,
        "openAccessPdf": {
            "url": "https://aclanthology.org/2022.emnlp-main.340.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2204.07705, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1705260",
                "name": "Yizhong Wang"
            },
            {
                "authorId": "1817207",
                "name": "Swaroop Mishra"
            },
            {
                "authorId": "1805993128",
                "name": "Pegah Alipoormolabashi"
            },
            {
                "authorId": "2156538832",
                "name": "Yeganeh Kordi"
            },
            {
                "authorId": "2162779320",
                "name": "Amirreza Mirzaei"
            },
            {
                "authorId": "1667817604",
                "name": "Anjana Arunkumar"
            },
            {
                "authorId": "2063938464",
                "name": "Arjun Ashok"
            },
            {
                "authorId": "2162779517",
                "name": "Arut Selvan Dhanasekaran"
            },
            {
                "authorId": "2064353087",
                "name": "Atharva Naik"
            },
            {
                "authorId": "1689466093",
                "name": "David Stap"
            },
            {
                "authorId": "2074097046",
                "name": "Eshaan Pathak"
            },
            {
                "authorId": "8458211",
                "name": "Giannis Karamanolakis"
            },
            {
                "authorId": "2162779839",
                "name": "H. Lai"
            },
            {
                "authorId": "93841942",
                "name": "I. Purohit"
            },
            {
                "authorId": "9377739",
                "name": "Ishani Mondal"
            },
            {
                "authorId": "2110871234",
                "name": "Jacob Anderson"
            },
            {
                "authorId": "2158994272",
                "name": "Kirby Kuznia"
            },
            {
                "authorId": "2162779709",
                "name": "Krima Doshi"
            },
            {
                "authorId": "81331041",
                "name": "Maitreya Patel"
            },
            {
                "authorId": "50494955",
                "name": "Kuntal Kumar Pal"
            },
            {
                "authorId": "40879549",
                "name": "M. Moradshahi"
            },
            {
                "authorId": "1423660254",
                "name": "Mihir Parmar"
            },
            {
                "authorId": "1576655836",
                "name": "Mirali Purohit"
            },
            {
                "authorId": "2067056655",
                "name": "Neeraj Varshney"
            },
            {
                "authorId": "2162781785",
                "name": "Phani Rohitha Kaza"
            },
            {
                "authorId": "39765564",
                "name": "Pulkit Verma"
            },
            {
                "authorId": "2159207824",
                "name": "Ravsehaj Singh Puri"
            },
            {
                "authorId": "72254820",
                "name": "Rushang Karia"
            },
            {
                "authorId": "51479145",
                "name": "Shailaja Keyur Sampat"
            },
            {
                "authorId": "1380075136",
                "name": "Savan Doshi"
            },
            {
                "authorId": "2112127876",
                "name": "Siddhartha Mishra"
            },
            {
                "authorId": "2150098899",
                "name": "Sujan Reddy"
            },
            {
                "authorId": "19255781",
                "name": "Sumanta Patro"
            },
            {
                "authorId": "2126503480",
                "name": "Tanay Dixit"
            },
            {
                "authorId": "2144058688",
                "name": "Xudong Shen"
            },
            {
                "authorId": "2064619864",
                "name": "Chitta Baral"
            },
            {
                "authorId": "1699545",
                "name": "Yejin Choi"
            },
            {
                "authorId": "144365875",
                "name": "Noah A. Smith"
            },
            {
                "authorId": "2548384",
                "name": "Hannaneh Hajishirzi"
            },
            {
                "authorId": "1783281",
                "name": "Daniel Khashabi"
            }
        ],
        "abstract": "How well can NLP models generalize to a variety of unseen tasks when provided with task instructions? To address this question, we first introduce Super-NaturalInstructions, a benchmark of 1,616 diverse NLP tasks and their expert-written instructions. Our collection covers 76 distinct task types, including but not limited to classification, extraction, infilling, sequence tagging, text rewriting, and text composition. This large and diverse collection of tasks enables rigorous benchmarking of cross-task generalization under instructionstraining models to follow instructions on a subset of tasks and evaluating them on the remaining unseen ones.Furthermore, we build Tk-Instruct, a transformer model trained to follow a variety of in-context instructions (plain language task definitions or k-shot examples). Our experiments show that Tk-Instruct outperforms existing instruction-following models such as InstructGPT by over 9% on our benchmark despite being an order of magnitude smaller. We further analyze generalization as a function of various scaling parameters, such as the number of observed tasks, the number of instances per task, and model sizes. We hope our dataset and model facilitate future progress towards more general-purpose NLP models."
    },
    {
        "paperId": "f4df78183261538e718066331898ee5cad7cad05",
        "url": "https://www.semanticscholar.org/paper/f4df78183261538e718066331898ee5cad7cad05",
        "title": "Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2022,
        "citationCount": 1799,
        "openAccessPdf": {
            "url": "https://aclanthology.org/2022.emnlp-main.759.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2202.12837, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "48872685",
                "name": "Sewon Min"
            },
            {
                "authorId": "2156533327",
                "name": "Xinxi Lyu"
            },
            {
                "authorId": "14487640",
                "name": "Ari Holtzman"
            },
            {
                "authorId": "2347956",
                "name": "Mikel Artetxe"
            },
            {
                "authorId": "35084211",
                "name": "M. Lewis"
            },
            {
                "authorId": "2548384",
                "name": "Hannaneh Hajishirzi"
            },
            {
                "authorId": "1982950",
                "name": "Luke Zettlemoyer"
            }
        ],
        "abstract": "Large language models (LMs) are able to in-context learnperform a new task via inference alone by conditioning on a few input-label pairs (demonstrations) and making predictions for new inputs. However, there has been little understanding of how the model learns and which aspects of the demonstrations contribute to end task performance. In this paper, we show that ground truth demonstrations are in fact not requiredrandomly replacing labels in the demonstrations barely hurts performance on a range of classification and multi-choce tasks, consistently over 12 different models including GPT-3. Instead, we find that other aspects of the demonstrations are the key drivers of endtask performance, including the fact that they provide a few examples of (1) the label space, (2) the distribution of the input text, and (3) the overall format of the sequence. Together, our analysis provides a new way of understanding how and why in-context learning works, while opening up new questions about how much can be learned from large language models through inference alone."
    },
    {
        "paperId": "107fb6eec2febbae12db29bf3e311aaf5680027c",
        "url": "https://www.semanticscholar.org/paper/107fb6eec2febbae12db29bf3e311aaf5680027c",
        "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2023,
        "citationCount": 1159,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.10122, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2254329478",
                "name": "Bin Lin"
            },
            {
                "authorId": "2256419313",
                "name": "Bin Zhu"
            },
            {
                "authorId": "2267264117",
                "name": "Yang Ye"
            },
            {
                "authorId": "2253434320",
                "name": "Munan Ning"
            },
            {
                "authorId": "2185571736",
                "name": "Peng Jin"
            },
            {
                "authorId": "2266507381",
                "name": "Li Yuan"
            }
        ],
        "abstract": "Large Vision-Language Model (LVLM) has enhanced the performance of various downstream tasks in visual-language understanding. Most existing approaches encode images and videos into separate feature spaces, which are then fed as inputs to large language models. However, due to the lack of unified tokenization for images and videos, namely misalignment before projection, it becomes challenging for a Large Language Model (LLM) to learn multi-modal interactions from several poor projection layers.In this work, we unify visual representation into the language feature space to advance the foundational LLM towards a unified LVLM. As a result, we establish a simple but robust LVLM baseline, Video-LLaVA, which learns from a mixed dataset of images and videos, mutually enhancing each other.As a result, Video-LLaVA outperforms Video-ChatGPT by 5.8%, 9.9%, 18.6%, and 10.1% on MSRVTT, MSVD, TGIF, and ActivityNet, respectively. Additionally, our Video-LLaVA also achieves superior performances on a broad range of 9 image benchmarks.Notably, extensive experiments demonstrate that Video-LLaVA mutually benefits images and videos within a unified visual representation, outperforming models designed specifically for images or videos. We aim for this work to provide modest insights into the multi-modal inputs for the LLM."
    },
    {
        "paperId": "206400aba5f12f734cdd2e4ab48ef6014ea60773",
        "url": "https://www.semanticscholar.org/paper/206400aba5f12f734cdd2e4ab48ef6014ea60773",
        "title": "Evaluating Object Hallucination in Large Vision-Language Models",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2023,
        "citationCount": 1234,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2305.10355",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.10355, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2209136299",
                "name": "Yifan Li"
            },
            {
                "authorId": "2111895473",
                "name": "Yifan Du"
            },
            {
                "authorId": "1423651904",
                "name": "Kun Zhou"
            },
            {
                "authorId": "1823719",
                "name": "Jinpeng Wang"
            },
            {
                "authorId": "2542603",
                "name": "Wayne Xin Zhao"
            },
            {
                "authorId": "153693432",
                "name": "Ji-rong Wen"
            }
        ],
        "abstract": "Inspired by the superior language abilities of large language models (LLM), large vision-language models (LVLM) have been recently explored by integrating powerful LLMs for improving the performance on complex multimodal tasks. Despite the promising progress on LVLMs, we find that LVLMs suffer from the hallucination problem, i.e. they tend to generate objects that are inconsistent with the target images in the descriptions. To investigate it, this work presents the first systematic study on object hallucination of LVLMs. We conduct the evaluation experiments on several representative LVLMs, and show that they mostly suffer from severe object hallucination issue. We further discuss that the visual instructions may influence the hallucination, and find that: objects that frequently occur in the visual instructions or co-occur with the image objects, are obviously prone to be hallucinated by LVLMs. Besides, we find that existing evaluation methods might be affected by the input instructions and generation styles of LVLMs. Thus, we further design an improved evaluation method for object hallucination by proposing a polling-based query method called POPE. Experiment results demonstrate that our POPE can evaluate the object hallucination in a more stable and flexible way. Our codes and data are publicly available at https://github.com/RUCAIBox/POPE."
    },
    {
        "paperId": "381ab7a640f5b46b62f7e08d1af4a8e0d3eadd55",
        "url": "https://www.semanticscholar.org/paper/381ab7a640f5b46b62f7e08d1af4a8e0d3eadd55",
        "title": "G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2023,
        "citationCount": 1750,
        "openAccessPdf": {
            "url": "https://aclanthology.org/2023.emnlp-main.153.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2303.16634, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2152797401",
                "name": "Yang Liu"
            },
            {
                "authorId": "3310951",
                "name": "Dan Iter"
            },
            {
                "authorId": "2110197273",
                "name": "Yichong Xu"
            },
            {
                "authorId": "2146294891",
                "name": "Shuo Wang"
            },
            {
                "authorId": "8233965",
                "name": "Ruochen Xu"
            },
            {
                "authorId": "8652308",
                "name": "Chenguang Zhu"
            }
        ],
        "abstract": "The quality of texts generated by natural language generation (NLG) systems is hard to measure automatically. Conventional reference-based metrics, such as BLEU and ROUGE, have been shown to have relatively low correlation with human judgments, especially for tasks that require creativity and diversity. Recent studies suggest using large language models (LLMs) as reference-free metrics for NLG evaluation, which have the benefit of being applicable to new tasks that lack human references. However, these LLM-based evaluators still have lower human correspondence than medium-size neural evaluators. In this work, we present G-Eval, a framework of using large language models with chain-of-thoughts (CoT) and a form-filling paradigm, to assess the quality of NLG outputs. We experiment with two generation tasks, text summarization and dialogue generation. We show that G-Eval with GPT-4 as the backbone model achieves a Spearman correlation of 0.514 with human on summarization task, outperforming all previous methods by a large margin. We also propose preliminary analysis on the behavior of LLM-based evaluators, and highlight the potential issue of LLM-based evaluators having a bias towards the LLM-generated texts. The code is at https://github.com/nlpyang/geval"
    },
    {
        "paperId": "5ae6fb6b5a3c7df515ff4a82ac9673bae6a8e200",
        "url": "https://www.semanticscholar.org/paper/5ae6fb6b5a3c7df515ff4a82ac9673bae6a8e200",
        "title": "GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2023,
        "citationCount": 1085,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2305.13245",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.13245, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1643737606",
                "name": "J. Ainslie"
            },
            {
                "authorId": "1405626394",
                "name": "J. Lee-Thorp"
            },
            {
                "authorId": "21379393",
                "name": "Michiel de Jong"
            },
            {
                "authorId": "51199981",
                "name": "Yury Zemlyanskiy"
            },
            {
                "authorId": "2218311992",
                "name": "Federico Lebr'on"
            },
            {
                "authorId": "144074891",
                "name": "Sumit K. Sanghai"
            }
        ],
        "abstract": "Multi-query attention (MQA), which only uses a single key-value head, drastically speeds up decoder inference. However, MQA can lead to quality degradation, and moreover it may not be desirable to train a separate model just for faster inference. We (1) propose a recipe for uptraining existing multi-head language model checkpoints into models with MQA using 5% of original pre-training compute, and (2) introduce grouped-query attention (GQA), a generalization of multi-query attention which uses an intermediate (more than one, less than number of query heads) number of key-value heads. We show that uptrained GQA achieves quality close to multi-head attention with comparable speed to MQA."
    },
    {
        "paperId": "5d321194696f1f75cf9da045e6022b2f20ba5b9c",
        "url": "https://www.semanticscholar.org/paper/5d321194696f1f75cf9da045e6022b2f20ba5b9c",
        "title": "Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "year": 2023,
        "citationCount": 1470,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2306.02858",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2306.02858, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2119077859",
                "name": "Hang Zhang"
            },
            {
                "authorId": "40613621",
                "name": "Xin Li"
            },
            {
                "authorId": "1996394",
                "name": "Lidong Bing"
            }
        ],
        "abstract": "We present Video-LLaMA a multi-modal framework that empowers Large Language Models (LLMs) with the capability of understanding both visual and auditory content in the video. Video-LLaMA bootstraps cross-modal training from the frozen pre-trained visual and audio encoders and the frozen LLMs. Unlike previous works that complement LLMs to process the visual or audio signals only, Video-LLaMA enables video comprehension by tackling two challenges: (1) capturing the temporal changes in visual scenes, (2) integrating audio-visual signals. To counter the first challenge, we propose a Video Q-former to assemble a pre-trained image encoder into our video encoder and introduce a video-to-text generation task to learn video-language correspondence. For the second challenge, we leverage ImageBind, a universal embedding model aligning multiple modalities, as the pre-trained audio encoder and introduce an Audio Q-former on top of ImageBind to learn reasonable auditory query embeddings for the LLM module. To align the output of both visual and audio encoders with LLM's embedding space, we first train Video-LLaMA on massive video/image-caption pairs and then tune our model with visual-instruction datasets of moderate amount but higher quality. We found Video-LLaMA shows the ability to perceive and comprehend video content and generate meaningful responses grounded in the visual and auditory information presented in the videos."
    },
    {
        "paperId": "12c7fc38debaf3589e712973642246bd54fe63b3",
        "url": "https://www.semanticscholar.org/paper/12c7fc38debaf3589e712973642246bd54fe63b3",
        "title": "Shape matching and object recognition using low distortion correspondences",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2005,
        "citationCount": 1023,
        "openAccessPdf": {
            "url": "http://luthuli.cs.uiuc.edu/~daf/courses/Optimization/Combinatorialpapers/01467245.pdf",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR.2005.320?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR.2005.320, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "39668247",
                "name": "A. Berg"
            },
            {
                "authorId": "1685538",
                "name": "Tamara L. Berg"
            },
            {
                "authorId": "143751119",
                "name": "Jitendra Malik"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "1854005a7178b2df6afaacdcf91bc35d90616075",
        "url": "https://www.semanticscholar.org/paper/1854005a7178b2df6afaacdcf91bc35d90616075",
        "title": "Pedestrian detection in crowded scenes",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2005,
        "citationCount": 1011,
        "openAccessPdf": {
            "url": "http://www.cis.pku.edu.cn/faculty/vision/zhangchao/Summer2007/10-42_01.PDF",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR.2005.272?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR.2005.272, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1789756",
                "name": "B. Leibe"
            },
            {
                "authorId": "2083209039",
                "name": "Edgar Seemann"
            },
            {
                "authorId": "48920094",
                "name": "B. Schiele"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "2077d0f30507d51a0d3bbec4957d55e817d66a59",
        "url": "https://www.semanticscholar.org/paper/2077d0f30507d51a0d3bbec4957d55e817d66a59",
        "title": "Fields of Experts: a framework for learning image priors",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2005,
        "citationCount": 1218,
        "openAccessPdf": {
            "url": "http://www.wisdom.weizmann.ac.il/%7Emica/CVspring06/papers/fields_of_experts/Black_foe.pdf",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR.2005.160?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR.2005.160, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "145920814",
                "name": "S. Roth"
            },
            {
                "authorId": "2105795",
                "name": "Michael J. Black"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "2c497e0774f0b776ed0dc4a8747386d2fbafa6b6",
        "url": "https://www.semanticscholar.org/paper/2c497e0774f0b776ed0dc4a8747386d2fbafa6b6",
        "title": "ARTag, a fiducial marker system using digital techniques",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2005,
        "citationCount": 1018,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR.2005.74?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR.2005.74, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "46800279",
                "name": "M. Fiala"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "4f140022ae2a37b8186b34e690012a58fee0c820",
        "url": "https://www.semanticscholar.org/paper/4f140022ae2a37b8186b34e690012a58fee0c820",
        "title": "Overview of the face recognition grand challenge",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2005,
        "citationCount": 2757,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR.2005.268?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR.2005.268, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "145691986",
                "name": "P. Phillips"
            },
            {
                "authorId": "1704876",
                "name": "P. Flynn"
            },
            {
                "authorId": "2067993",
                "name": "W. T. Scruggs"
            },
            {
                "authorId": "143759604",
                "name": "K. Bowyer"
            },
            {
                "authorId": "2153056438",
                "name": "Jin Chang"
            },
            {
                "authorId": "2054926934",
                "name": "Kevin Hoffman"
            },
            {
                "authorId": "39544740",
                "name": "Joe Marques"
            },
            {
                "authorId": "3221917",
                "name": "Jaesik Min"
            },
            {
                "authorId": "2061326",
                "name": "W. Worek"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "7a2252ccce2b65abc3759149b5c06587cc318e2f",
        "url": "https://www.semanticscholar.org/paper/7a2252ccce2b65abc3759149b5c06587cc318e2f",
        "title": "A Bayesian hierarchical model for learning natural scene categories",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2005,
        "citationCount": 4142,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR.2005.16?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR.2005.16, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "48004138",
                "name": "Li Fei-Fei"
            },
            {
                "authorId": "1690922",
                "name": "P. Perona"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "9c842b2926fd60b9e6ff80fee28c65e7c1ae5f1d",
        "url": "https://www.semanticscholar.org/paper/9c842b2926fd60b9e6ff80fee28c65e7c1ae5f1d",
        "title": "A non-local algorithm for image denoising",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2005,
        "citationCount": 7540,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR.2005.38?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR.2005.38, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "8070492",
                "name": "A. Buades"
            },
            {
                "authorId": "38689796",
                "name": "B. Coll"
            },
            {
                "authorId": "27053481",
                "name": "J. Morel"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "cfaae9b6857b834043606df3342d8dc97524aa9d",
        "url": "https://www.semanticscholar.org/paper/cfaae9b6857b834043606df3342d8dc97524aa9d",
        "title": "Learning a similarity metric discriminatively, with application to face verification",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2005,
        "citationCount": 4450,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR.2005.202?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR.2005.202, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3295092",
                "name": "S. Chopra"
            },
            {
                "authorId": "2315504",
                "name": "R. Hadsell"
            },
            {
                "authorId": "1688882",
                "name": "Yann LeCun"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "d243b5eb81a8501cc0477c47a4ce7d4feb524aee",
        "url": "https://www.semanticscholar.org/paper/d243b5eb81a8501cc0477c47a4ce7d4feb524aee",
        "title": "Level set evolution without re-initialization: a new variational formulation",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2005,
        "citationCount": 2180,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR.2005.213?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR.2005.213, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2109304804",
                "name": "Chunming Li"
            },
            {
                "authorId": null,
                "name": "Chenyang Xu"
            },
            {
                "authorId": "2076144",
                "name": "C. Gui"
            },
            {
                "authorId": "11370804",
                "name": "M. Fox"
            }
        ],
        "abstract": "In this paper, we present a new variational formulation for geometric active contours that forces the level set function to be close to a signed distance function, and therefore completely eliminates the need of the costly re-initialization procedure. Our variational formulation consists of an internal energy term that penalizes the deviation of the level set function from a signed distance function, and an external energy term that drives the motion of the zero level set toward the desired image features, such as object boundaries. The resulting evolution of the level set function is the gradient flow that minimizes the overall energy functional. The proposed variational level set formulation has three main advantages over the traditional level set formulations. First, a significantly larger time step can be used for numerically solving the evolution partial differential equation, and therefore speeds up the curve evolution. Second, the level set function can be initialized with general functions that are more efficient to construct and easier to use in practice than the widely used signed distance function. Third, the level set evolution in our formulation can be easily implemented by simple finite difference scheme and is computationally more efficient. The proposed algorithm has been applied to both simulated and real images with promising results."
    },
    {
        "paperId": "e4f4c2bad99161cd10b063dec855902d2da0ee72",
        "url": "https://www.semanticscholar.org/paper/e4f4c2bad99161cd10b063dec855902d2da0ee72",
        "title": "Matching with PROSAC - progressive sample consensus",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2005,
        "citationCount": 1306,
        "openAccessPdf": {
            "url": "https://dspace.cvut.cz/bitstream/10467/9496/1/2005-Matching-with-PROSAC-progressive-sample-consensus.pdf",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR.2005.221?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR.2005.221, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1700928",
                "name": "Ondej Chum"
            },
            {
                "authorId": "145564537",
                "name": "Jiri Matas"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "e8b12467bdc20bde976750b8a28decdb33246d1d",
        "url": "https://www.semanticscholar.org/paper/e8b12467bdc20bde976750b8a28decdb33246d1d",
        "title": "Histograms of oriented gradients for human detection",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2005,
        "citationCount": 35105,
        "openAccessPdf": {
            "url": "https://hal.inria.fr/inria-00548512/file/hog_cvpr2005.pdf",
            "status": "GREEN",
            "license": "other-oa",
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR.2005.177?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR.2005.177, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "48950628",
                "name": "Navneet Dalal"
            },
            {
                "authorId": "1756114",
                "name": "B. Triggs"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "075bfb99ce2dbaa2005500dff90f893b7caa68c2",
        "url": "https://www.semanticscholar.org/paper/075bfb99ce2dbaa2005500dff90f893b7caa68c2",
        "title": "On-line Boosting and Vision",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2006,
        "citationCount": 1132,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR.2006.215?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR.2006.215, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "145551629",
                "name": "H. Grabner"
            },
            {
                "authorId": "144746444",
                "name": "H. Bischof"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "15cd7d675e499d6e53014916d7cf4a1714341f6a",
        "url": "https://www.semanticscholar.org/paper/15cd7d675e499d6e53014916d7cf4a1714341f6a",
        "title": "Robust Fragments-based Tracking using the Integral Histogram",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2006,
        "citationCount": 1550,
        "openAccessPdf": {
            "url": "http://www.cs.technion.ac.il/~amita/fragtrack/fragtrack_cvpr06.pdf",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR.2006.256?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR.2006.256, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "31982493",
                "name": "Amit Adam"
            },
            {
                "authorId": "1747801",
                "name": "E. Rivlin"
            },
            {
                "authorId": "1782918",
                "name": "I. Shimshoni"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "39265f8247f2dc6c394de0378a7027b435246fe3",
        "url": "https://www.semanticscholar.org/paper/39265f8247f2dc6c394de0378a7027b435246fe3",
        "title": "A Comparison and Evaluation of Multi-View Stereo Reconstruction Algorithms",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2006,
        "citationCount": 2802,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR.2006.19?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR.2006.19, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1679223",
                "name": "S. Seitz"
            },
            {
                "authorId": "143800609",
                "name": "B. Curless"
            },
            {
                "authorId": "144065185",
                "name": "J. Diebel"
            },
            {
                "authorId": "1709053",
                "name": "D. Scharstein"
            },
            {
                "authorId": "1717841",
                "name": "R. Szeliski"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "4081e007d7eced95cc618164e976a80d44ff5f4e",
        "url": "https://www.semanticscholar.org/paper/4081e007d7eced95cc618164e976a80d44ff5f4e",
        "title": "Putting Objects in Perspective",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2006,
        "citationCount": 1002,
        "openAccessPdf": {
            "url": "https://figshare.com/articles/journal_contribution/Putting_Objects_in_Perspective/6559433/1/files/12041726.pdf",
            "status": "CLOSED",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s11263-008-0137-5?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s11263-008-0137-5, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2433269",
                "name": "Derek Hoiem"
            },
            {
                "authorId": "1763086",
                "name": "Alexei A. Efros"
            },
            {
                "authorId": "145670946",
                "name": "M. Hebert"
            }
        ],
        "abstract": "Image understanding requires not only individually estimating elements of the visual world but also capturing the interplay among them. In this paper, we provide a framework for placing local object detection in the context of the overall 3D scene by modeling the interdependence of objects, surface orientations, and camera viewpoint. Most object detection methods consider all scales and locations in the image as equally likely. We show that with probabilistic estimates of 3D geometry, both in terms of surfaces and world coordinates, we can put objects into perspective and model the scale and location variance in the image. Our approach reflects the cyclical nature of the problem by allowing probabilistic object hypotheses to refine geometry and vice-versa. Our framework allows painless substitution of almost any object detector and is easily extended to include other aspects of image understanding. Our results confirm the benefits of our integrated approach."
    },
    {
        "paperId": "46f30e94dd3d5902141c5fbe58d0bc9189545c76",
        "url": "https://www.semanticscholar.org/paper/46f30e94dd3d5902141c5fbe58d0bc9189545c76",
        "title": "Dimensionality Reduction by Learning an Invariant Mapping",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2006,
        "citationCount": 5567,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR.2006.100?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR.2006.100, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2315504",
                "name": "R. Hadsell"
            },
            {
                "authorId": "3295092",
                "name": "S. Chopra"
            },
            {
                "authorId": "1688882",
                "name": "Yann LeCun"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "6dbaff29d3898cf60f63f5a34cb9610ebb75220c",
        "url": "https://www.semanticscholar.org/paper/6dbaff29d3898cf60f63f5a34cb9610ebb75220c",
        "title": "Beyond Bags of Features: Spatial Pyramid Matching for Recognizing Natural Scene Categories",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2006,
        "citationCount": 8839,
        "openAccessPdf": {
            "url": "http://hal.inria.fr/docs/00/54/85/85/PDF/cvpr06_lana.pdf",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR.2006.68?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR.2006.68, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1749609",
                "name": "Svetlana Lazebnik"
            },
            {
                "authorId": "2462253",
                "name": "C. Schmid"
            },
            {
                "authorId": "144189388",
                "name": "J. Ponce"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "b3e7d3e37e67af7f4546b46051063bea1b62dbae",
        "url": "https://www.semanticscholar.org/paper/b3e7d3e37e67af7f4546b46051063bea1b62dbae",
        "title": "Scalable Recognition with a Vocabulary Tree",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2006,
        "citationCount": 4128,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR.2006.264?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR.2006.264, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3083483",
                "name": "D. Nistr"
            },
            {
                "authorId": "3086037",
                "name": "Henrik Stewnius"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "ceb0e1a86dc35e21ce5f0524c8476f15e1b08988",
        "url": "https://www.semanticscholar.org/paper/ceb0e1a86dc35e21ce5f0524c8476f15e1b08988",
        "title": "SVM-KNN: Discriminative Nearest Neighbor Classification for Visual Category Recognition",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2006,
        "citationCount": 1371,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR.2006.301?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR.2006.301, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "145140331",
                "name": "Haotong Zhang"
            },
            {
                "authorId": "39668247",
                "name": "A. Berg"
            },
            {
                "authorId": "145854440",
                "name": "M. Maire"
            },
            {
                "authorId": "143751119",
                "name": "Jitendra Malik"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "d6cff906315e29b61afcf14bf7e6fe40f4d74ea5",
        "url": "https://www.semanticscholar.org/paper/d6cff906315e29b61afcf14bf7e6fe40f4d74ea5",
        "title": "A large-scale hierarchical image database",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2009,
        "citationCount": 1072,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "153302678",
                "name": "Jia Deng"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "0302bb2d5476540cfb21467473f5eca843caf90b",
        "url": "https://www.semanticscholar.org/paper/0302bb2d5476540cfb21467473f5eca843caf90b",
        "title": "Unbiased look at dataset bias",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2011,
        "citationCount": 2669,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR.2011.5995347?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR.2011.5995347, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "143805211",
                "name": "A. Torralba"
            },
            {
                "authorId": "1763086",
                "name": "Alexei A. Efros"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "0baa66007d2cfe8e98720310ad0ed7bdee7a873d",
        "url": "https://www.semanticscholar.org/paper/0baa66007d2cfe8e98720310ad0ed7bdee7a873d",
        "title": "Entropy rate superpixel segmentation",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2011,
        "citationCount": 1015,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR.2011.5995323?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR.2011.5995323, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "39793900",
                "name": "Ming-Yu Liu"
            },
            {
                "authorId": "2577513",
                "name": "Oncel Tuzel"
            },
            {
                "authorId": "145686644",
                "name": "Srikumar Ramalingam"
            },
            {
                "authorId": "9215658",
                "name": "R. Chellappa"
            }
        ],
        "abstract": "We propose a new objective function for superpixel segmentation. This objective function consists of two components: entropy rate of a random walk on a graph and a balancing term. The entropy rate favors formation of compact and homogeneous clusters, while the balancing function encourages clusters with similar sizes. We present a novel graph construction for images and show that this construction induces a matroid  a combinatorial structure that generalizes the concept of linear independence in vector spaces. The segmentation is then given by the graph topology that maximizes the objective function under the matroid constraint. By exploiting submodular and mono-tonic properties of the objective function, we develop an efficient greedy algorithm. Furthermore, we prove an approximation bound of  for the optimality of the solution. Extensive experiments on the Berkeley segmentation benchmark show that the proposed algorithm outperforms the state of the art in all the standard evaluation metrics."
    },
    {
        "paperId": "2915510a39448503ee873f9693cd3808ca74bd81",
        "url": "https://www.semanticscholar.org/paper/2915510a39448503ee873f9693cd3808ca74bd81",
        "title": "Real-time human pose recognition in parts from single depth images",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2011,
        "citationCount": 5372,
        "openAccessPdf": {
            "url": "http://www.cs.cmu.edu/afs/cs.cmu.edu/academic/class/15869-f11/www/readings/shotton11_skeleton.pdf",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1145/2398356.2398381?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/2398356.2398381, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "143774737",
                "name": "J. Shotton"
            },
            {
                "authorId": "34824003",
                "name": "T. Sharp"
            },
            {
                "authorId": "2078931040",
                "name": "A. Kipman"
            },
            {
                "authorId": "47139824",
                "name": "A. Fitzgibbon"
            },
            {
                "authorId": "2848295",
                "name": "M. Finocchio"
            },
            {
                "authorId": "145162067",
                "name": "A. Blake"
            },
            {
                "authorId": "40636177",
                "name": "Mat Cook"
            },
            {
                "authorId": "144564063",
                "name": "Richard Moore"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "33bb4ce0aec03af26286f2ed50afc7e06e1ef9eb",
        "url": "https://www.semanticscholar.org/paper/33bb4ce0aec03af26286f2ed50afc7e06e1ef9eb",
        "title": "Blind deconvolution using a normalized sparsity measure",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2011,
        "citationCount": 1164,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR.2011.5995521?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR.2011.5995521, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1707347",
                "name": "Dilip Krishnan"
            },
            {
                "authorId": "12426289",
                "name": "T. Tay"
            },
            {
                "authorId": "2276554",
                "name": "R. Fergus"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "42269d0438c0ae4ca892334946ed779999691074",
        "url": "https://www.semanticscholar.org/paper/42269d0438c0ae4ca892334946ed779999691074",
        "title": "Learning hierarchical invariant spatio-temporal features for action recognition with independent subspace analysis",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2011,
        "citationCount": 1121,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR.2011.5995496?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR.2011.5995496, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2827616",
                "name": "Quoc V. Le"
            },
            {
                "authorId": "2860351",
                "name": "Will Y. Zou"
            },
            {
                "authorId": "34149749",
                "name": "Serena Yeung"
            },
            {
                "authorId": "34699434",
                "name": "A. Ng"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "5310c57262146852280fb6ee4006c4abc91a7569",
        "url": "https://www.semanticscholar.org/paper/5310c57262146852280fb6ee4006c4abc91a7569",
        "title": "Articulated pose estimation with flexible mixtures-of-parts",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2011,
        "citationCount": 1207,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR.2011.5995741?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR.2011.5995741, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2143685864",
                "name": "Yi Yang"
            },
            {
                "authorId": "1770537",
                "name": "Deva Ramanan"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "7670e3c493126c8b2f211ddfc45f89de905f1d38",
        "url": "https://www.semanticscholar.org/paper/7670e3c493126c8b2f211ddfc45f89de905f1d38",
        "title": "Global contrast based salient region detection",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2011,
        "citationCount": 3799,
        "openAccessPdf": {
            "url": "https://repository.kaust.edu.sa/bitstream/10754/622089/1/05995344.pdf",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR.2011.5995344?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR.2011.5995344, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "37535930",
                "name": "Ming-Ming Cheng"
            },
            {
                "authorId": "2116646265",
                "name": "Guo-Xin Zhang"
            },
            {
                "authorId": "1710455",
                "name": "N. Mitra"
            },
            {
                "authorId": "143713756",
                "name": "Xiaolei Huang"
            },
            {
                "authorId": "145140922",
                "name": "Shimin Hu"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "aba7b76c300db4159592ee2933d8796176d1e737",
        "url": "https://www.semanticscholar.org/paper/aba7b76c300db4159592ee2933d8796176d1e737",
        "title": "Action recognition by dense trajectories",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2011,
        "citationCount": 2376,
        "openAccessPdf": {
            "url": "http://hal.inria.fr/docs/00/58/38/18/PDF/wang_cvpr11.pdf",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR.2011.5995407?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR.2011.5995407, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "46506697",
                "name": "Heng Wang"
            },
            {
                "authorId": "2909350",
                "name": "Alexander Klser"
            },
            {
                "authorId": "2462253",
                "name": "C. Schmid"
            },
            {
                "authorId": "1689269",
                "name": "Cheng-Lin Liu"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "b3f09ea2a8cc1d82c2b27d71dd8f7451d178beaf",
        "url": "https://www.semanticscholar.org/paper/b3f09ea2a8cc1d82c2b27d71dd8f7451d178beaf",
        "title": "Iterative quantization: A procrustean approach to learning binary codes",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2011,
        "citationCount": 1203,
        "openAccessPdf": {
            "url": "http://www.cs.unc.edu/~lazebnik/publications/cvpr11_small_code.pdf",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR.2011.5995432?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR.2011.5995432, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "5115386",
                "name": "Yunchao Gong"
            },
            {
                "authorId": "1749609",
                "name": "Svetlana Lazebnik"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "f61a7a7cd13e2702f0fbacc05e13b355c1e297e2",
        "url": "https://www.semanticscholar.org/paper/f61a7a7cd13e2702f0fbacc05e13b355c1e297e2",
        "title": "Face recognition in unconstrained videos with matched background similarity",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2011,
        "citationCount": 1444,
        "openAccessPdf": {
            "url": "http://www.cs.tau.ac.il/%7Ewolf/papers/lvfw.pdf",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR.2011.5995566?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR.2011.5995566, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "145128145",
                "name": "Lior Wolf"
            },
            {
                "authorId": "1756099",
                "name": "Tal Hassner"
            },
            {
                "authorId": "3352629",
                "name": "I. Maoz"
            }
        ],
        "abstract": "Recognizing faces in unconstrained videos is a task of mounting importance. While obviously related to face recognition in still images, it has its own unique characteristics and algorithmic requirements. Over the years several methods have been suggested for this problem, and a few benchmark data sets have been assembled to facilitate its study. However, there is a sizable gap between the actual application needs and the current state of the art. In this paper we make the following contributions. (a) We present a comprehensive database of labeled videos of faces in challenging, uncontrolled conditions (i.e., in the wild), the YouTube Faces database, along with benchmark, pair-matching tests1. (b) We employ our benchmark to survey and compare the performance of a large variety of existing video face recognition techniques. Finally, (c) we describe a novel set-to-set similarity measure, the Matched Background Similarity (MBGS). This similarity is shown to considerably improve performance on the benchmark tests."
    },
    {
        "paperId": "f908d2fb9cfcaff17030a912e4811fb02aaeec03",
        "url": "https://www.semanticscholar.org/paper/f908d2fb9cfcaff17030a912e4811fb02aaeec03",
        "title": "Fast cost-volume filtering for visual correspondence and beyond",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2011,
        "citationCount": 1171,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR.2011.5995372?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR.2011.5995372, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2086328",
                "name": "Christoph Rhemann"
            },
            {
                "authorId": "3115485",
                "name": "A. Hosni"
            },
            {
                "authorId": "2873656",
                "name": "M. Bleyer"
            },
            {
                "authorId": "1756036",
                "name": "C. Rother"
            },
            {
                "authorId": "1990797",
                "name": "M. Gelautz"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "15f102c3c9f4d4fe6ba105e221df48c6e8902b3b",
        "url": "https://www.semanticscholar.org/paper/15f102c3c9f4d4fe6ba105e221df48c6e8902b3b",
        "title": "From captions to visual concepts and back",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2014,
        "citationCount": 1344,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1411.4952",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1411.4952, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2113484216",
                "name": "Hao Fang"
            },
            {
                "authorId": "144157872",
                "name": "Saurabh Gupta"
            },
            {
                "authorId": "3346186",
                "name": "F. Iandola"
            },
            {
                "authorId": "2100612",
                "name": "R. Srivastava"
            },
            {
                "authorId": "144718788",
                "name": "L. Deng"
            },
            {
                "authorId": "3127283",
                "name": "Piotr Dollr"
            },
            {
                "authorId": "1800422",
                "name": "Jianfeng Gao"
            },
            {
                "authorId": "144137069",
                "name": "Xiaodong He"
            },
            {
                "authorId": "49501003",
                "name": "Margaret Mitchell"
            },
            {
                "authorId": "144189092",
                "name": "John C. Platt"
            },
            {
                "authorId": "1699161",
                "name": "C. L. Zitnick"
            },
            {
                "authorId": "1681543",
                "name": "G. Zweig"
            }
        ],
        "abstract": "This paper presents a novel approach for automatically generating image descriptions: visual detectors, language models, and multimodal similarity models learnt directly from a dataset of image captions. We use multiple instance learning to train visual detectors for words that commonly occur in captions, including many different parts of speech such as nouns, verbs, and adjectives. The word detector outputs serve as conditional inputs to a maximum-entropy language model. The language model learns from a set of over 400,000 image descriptions to capture the statistics of word usage. We capture global semantics by re-ranking caption candidates using sentence-level features and a deep multimodal similarity model. Our system is state-of-the-art on the official Microsoft COCO benchmark, producing a BLEU-4 score of 29.1%. When human judges compare the system captions to ones written by other people on our held-out test set, the system captions have equal or better quality 34% of the time."
    },
    {
        "paperId": "1c734a14c2325cb76783ca0431862c7f04a69268",
        "url": "https://www.semanticscholar.org/paper/1c734a14c2325cb76783ca0431862c7f04a69268",
        "title": "Deep Domain Confusion: Maximizing for Domain Invariance",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2014,
        "citationCount": 2765,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1412.3474, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2368132",
                "name": "Eric Tzeng"
            },
            {
                "authorId": "50196944",
                "name": "Judy Hoffman"
            },
            {
                "authorId": "46388894",
                "name": "N. Zhang"
            },
            {
                "authorId": "2903226",
                "name": "Kate Saenko"
            },
            {
                "authorId": "1753210",
                "name": "Trevor Darrell"
            }
        ],
        "abstract": "Recent reports suggest that a generic supervised deep CNN model trained on a large-scale dataset reduces, but does not remove, dataset bias on a standard benchmark. Fine-tuning deep models in a new domain can require a significant amount of data, which for many applications is simply not available. We propose a new CNN architecture which introduces an adaptation layer and an additional domain confusion loss, to learn a representation that is both semantically meaningful and domain invariant. We additionally show that a domain confusion metric can be used for model selection to determine the dimension of an adaptation layer and the best position for the layer in the CNN architecture. Our proposed adaptation method offers empirical performance which exceeds previously published results on a standard benchmark visual domain adaptation task."
    },
    {
        "paperId": "258986132bf17755fe8263e42429fe73218c1534",
        "url": "https://www.semanticscholar.org/paper/258986132bf17755fe8263e42429fe73218c1534",
        "title": "CIDEr: Consensus-based image description evaluation",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2014,
        "citationCount": 5102,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/1411.5726.pdf",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1411.5726, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "8137017",
                "name": "Ramakrishna Vedantam"
            },
            {
                "authorId": "1699161",
                "name": "C. L. Zitnick"
            },
            {
                "authorId": "153432684",
                "name": "Devi Parikh"
            }
        ],
        "abstract": "Automatically describing an image with a sentence is a long-standing challenge in computer vision and natural language processing. Due to recent progress in object detection, attribute classification, action recognition, etc., there is renewed interest in this area. However, evaluating the quality of descriptions has proven to be challenging. We propose a novel paradigm for evaluating image descriptions that uses human consensus. This paradigm consists of three main parts: a new triplet-based method of collecting human annotations to measure consensus, a new automated metric that captures consensus, and two new datasets: PASCAL-50S and ABSTRACT-50S that contain 50 sentences describing each image. Our simple metric captures human judgment of consensus better than existing metrics across sentences generated by various sources. We also evaluate five state-of-the-art image description approaches using this new protocol and provide a benchmark for future comparisons. A version of CIDEr named CIDEr-D is available as a part of MS COCO evaluation server to enable systematic evaluation and benchmarking."
    },
    {
        "paperId": "428db42e86f6d51292e23fa57797e35cecd0e2ee",
        "url": "https://www.semanticscholar.org/paper/428db42e86f6d51292e23fa57797e35cecd0e2ee",
        "title": "Hypercolumns for object segmentation and fine-grained localization",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2014,
        "citationCount": 1626,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/1411.5752",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1411.5752, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1790580",
                "name": "Bharath Hariharan"
            },
            {
                "authorId": "1778133",
                "name": "Pablo Arbelez"
            },
            {
                "authorId": "2983898",
                "name": "Ross B. Girshick"
            },
            {
                "authorId": "143751119",
                "name": "Jitendra Malik"
            }
        ],
        "abstract": "Recognition algorithms based on convolutional networks (CNNs) typically use the output of the last layer as a feature representation. However, the information in this layer may be too coarse spatially to allow precise localization. On the contrary, earlier layers may be precise in localization but will not capture semantics. To get the best of both worlds, we define the hypercolumn at a pixel as the vector of activations of all CNN units above that pixel. Using hypercolumns as pixel descriptors, we show results on three fine-grained localization tasks: simultaneous detection and segmentation [22], where we improve state-of-the-art from 49.7 mean APr [22] to 60.0, keypoint localization, where we get a 3.3 point boost over [20], and part labeling, where we show a 6.6 point gain over a strong baseline."
    },
    {
        "paperId": "4543670c4b2d88a9b67525e0084044adef94ae76",
        "url": "https://www.semanticscholar.org/paper/4543670c4b2d88a9b67525e0084044adef94ae76",
        "title": "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2014,
        "citationCount": 3418,
        "openAccessPdf": {
            "url": "http://yosinski.com/media/papers/Nguyen__2014__arXiv__Deep_Neural_Networks_are_Easily_Fooled.pdf",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1412.1897, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "151414531",
                "name": "Anh Totti Nguyen"
            },
            {
                "authorId": "2965424",
                "name": "J. Yosinski"
            },
            {
                "authorId": "2552141",
                "name": "J. Clune"
            }
        ],
        "abstract": "Deep neural networks (DNNs) have recently been achieving state-of-the-art performance on a variety of pattern-recognition tasks, most notably visual classification problems. Given that DNNs are now able to classify objects in images with near-human-level performance, questions naturally arise as to what differences remain between computer and human vision. A recent study [30] revealed that changing an image (e.g. of a lion) in a way imperceptible to humans can cause a DNN to label the image as something else entirely (e.g. mislabeling a lion a library). Here we show a related result: it is easy to produce images that are completely unrecognizable to humans, but that state-of-the-art DNNs believe to be recognizable objects with 99.99% confidence (e.g. labeling with certainty that white noise static is a lion). Specifically, we take convolutional neural networks trained to perform well on either the ImageNet or MNIST datasets and then find images with evolutionary algorithms or gradient ascent that DNNs label with high confidence as belonging to each dataset class. It is possible to produce images totally unrecognizable to human eyes that DNNs believe with near certainty are familiar objects, which we call fooling images (more generally, fooling examples). Our results shed light on interesting differences between human vision and current DNNs, and raise questions about the generality of DNN computer vision."
    },
    {
        "paperId": "4d790c8fae40357d24813d085fa74a436847fb49",
        "url": "https://www.semanticscholar.org/paper/4d790c8fae40357d24813d085fa74a436847fb49",
        "title": "Understanding deep image representations by inverting them",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2014,
        "citationCount": 2043,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1412.0035",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1412.0035, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "32694028",
                "name": "Aravindh Mahendran"
            },
            {
                "authorId": "1687524",
                "name": "A. Vedaldi"
            }
        ],
        "abstract": "Image representations, from SIFT and Bag of Visual Words to Convolutional Neural Networks (CNNs), are a crucial component of almost any image understanding system. Nevertheless, our understanding of them remains limited. In this paper we conduct a direct analysis of the visual information contained in representations by asking the following question: given an encoding of an image, to which extent is it possible to reconstruct the image itself? To answer this question we contribute a general framework to invert representations. We show that this method can invert representations such as HOG more accurately than recent alternatives while being applicable to CNNs too. We then use this technique to study the inverse of recent state-of-the-art CNN image representations for the first time. Among our findings, we show that several layers in CNNs retain photographically accurate information about the image, with different degrees of geometric and photometric invariance."
    },
    {
        "paperId": "55e022fb7581bb9e1fce678d21fb25ffbb3fbb88",
        "url": "https://www.semanticscholar.org/paper/55e022fb7581bb9e1fce678d21fb25ffbb3fbb88",
        "title": "Deep visual-semantic alignments for generating image descriptions",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2014,
        "citationCount": 5855,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1412.2306",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1412.2306, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2354728",
                "name": "A. Karpathy"
            },
            {
                "authorId": "48004138",
                "name": "Li Fei-Fei"
            }
        ],
        "abstract": "We present a model that generates natural language descriptions of images and their regions. Our approach leverages datasets of images and their sentence descriptions to learn about the inter-modal correspondences between language and visual data. Our alignment model is based on a novel combination of Convolutional Neural Networks over image regions, bidirectional Recurrent Neural Networks over sentences, and a structured objective that aligns the two modalities through a multimodal embedding. We then describe a Multimodal Recurrent Neural Network architecture that uses the inferred alignments to learn to generate novel descriptions of image regions. We demonstrate that our alignment model produces state of the art results in retrieval experiments on Flickr8K, Flickr30K and MSCOCO datasets. We then show that the generated descriptions significantly outperform retrieval baselines on both full images and on a new dataset of region-level annotations."
    },
    {
        "paperId": "6fc6803df5f9ae505cae5b2f178ade4062c768d0",
        "url": "https://www.semanticscholar.org/paper/6fc6803df5f9ae505cae5b2f178ade4062c768d0",
        "title": "Fully convolutional networks for semantic segmentation",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2014,
        "citationCount": 40584,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/1411.4038",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1411.4038, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1782282",
                "name": "Evan Shelhamer"
            },
            {
                "authorId": "2117314646",
                "name": "Jonathan Long"
            },
            {
                "authorId": "1753210",
                "name": "Trevor Darrell"
            }
        ],
        "abstract": "Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build fully convolutional networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet [20], the VGG net [31], and GoogLeNet [32]) into fully convolutional networks and transfer their learned representations by fine-tuning [3] to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20% relative improvement to 62.2% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes less than one fifth of a second for a typical image."
    },
    {
        "paperId": "7c8a51d04522496c43db68f2582efd45eaf59fea",
        "url": "https://www.semanticscholar.org/paper/7c8a51d04522496c43db68f2582efd45eaf59fea",
        "title": "3D ShapeNets: A deep representation for volumetric shapes",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2014,
        "citationCount": 6118,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1406.5670",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR.2015.7298801?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR.2015.7298801, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "152247501",
                "name": "Zhirong Wu"
            },
            {
                "authorId": "3340170",
                "name": "Shuran Song"
            },
            {
                "authorId": "2556428",
                "name": "A. Khosla"
            },
            {
                "authorId": "1807197",
                "name": "F. Yu"
            },
            {
                "authorId": "3064662",
                "name": "Linguang Zhang"
            },
            {
                "authorId": "50295995",
                "name": "Xiaoou Tang"
            },
            {
                "authorId": "40599257",
                "name": "Jianxiong Xiao"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "8ad35df17ae4064dd174690efb04d347428f1117",
        "url": "https://www.semanticscholar.org/paper/8ad35df17ae4064dd174690efb04d347428f1117",
        "title": "Convolutional neural networks at constrained time cost",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2014,
        "citationCount": 1348,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1412.1710",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1412.1710, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "39353098",
                "name": "Kaiming He"
            },
            {
                "authorId": null,
                "name": "Jian Sun"
            }
        ],
        "abstract": "Though recent advanced convolutional neural networks (CNNs) have been improving the image recognition accuracy, the models are getting more complex and time-consuming. For real-world applications in industrial and commercial scenarios, engineers and developers are often faced with the requirement of constrained time budget. In this paper, we investigate the accuracy of CNNs under constrained time cost. Under this constraint, the designs of the network architectures should exhibit as trade-offs among the factors like depth, numbers of filters, filter sizes, etc. With a series of controlled comparisons, we progressively modify a baseline model while preserving its time complexity. This is also helpful for understanding the importance of the factors in network designs. We present an architecture that achieves very competitive accuracy in the ImageNet dataset (11.8% top-5 error, 10-view test), yet is 20% faster than AlexNet [14] (16.0% top-5 error, 10-view test)."
    },
    {
        "paperId": "caccc069e658ea397c9faf673e74c959c734ff53",
        "url": "https://www.semanticscholar.org/paper/caccc069e658ea397c9faf673e74c959c734ff53",
        "title": "Evaluation of output embeddings for fine-grained image classification",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2014,
        "citationCount": 1065,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/1409.8403",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1409.8403, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2893664",
                "name": "Zeynep Akata"
            },
            {
                "authorId": "144828948",
                "name": "Scott E. Reed"
            },
            {
                "authorId": "47215599",
                "name": "D. Walter"
            },
            {
                "authorId": "1697141",
                "name": "Honglak Lee"
            },
            {
                "authorId": "48920094",
                "name": "B. Schiele"
            }
        ],
        "abstract": "Image classification has advanced significantly in recent years with the availability of large-scale image sets. However, fine-grained classification remains a major challenge due to the annotation cost of large numbers of fine-grained categories. This project shows that compelling classification performance can be achieved on such categories even without labeled training data. Given image and class embeddings, we learn a compatibility function such that matching embeddings are assigned a higher score than mismatching ones; zero-shot classification of an image proceeds by finding the label yielding the highest joint compatibility score. We use state-of-the-art image features and focus on different supervised attributes and unsupervised output embeddings either derived from hierarchies or learned from unlabeled text corpora. We establish a substantially improved state-of-the-art on the Animals with Attributes and Caltech-UCSD Birds datasets. Most encouragingly, we demonstrate that purely unsupervised output embeddings (learned from Wikipedia and improved with finegrained text) achieve compelling results, even outperforming the previous supervised state-of-the-art. By combining different output embeddings, we further improve results."
    },
    {
        "paperId": "d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0",
        "url": "https://www.semanticscholar.org/paper/d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0",
        "title": "Show and tell: A neural image caption generator",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2014,
        "citationCount": 6354,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1411.4555",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1411.4555, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1689108",
                "name": "O. Vinyals"
            },
            {
                "authorId": "1726415",
                "name": "Alexander Toshev"
            },
            {
                "authorId": "1751569",
                "name": "Samy Bengio"
            },
            {
                "authorId": "1761978",
                "name": "D. Erhan"
            }
        ],
        "abstract": "Automatically describing the content of an image is a fundamental problem in artificial intelligence that connects computer vision and natural language processing. In this paper, we present a generative model based on a deep recurrent architecture that combines recent advances in computer vision and machine translation and that can be used to generate natural sentences describing an image. The model is trained to maximize the likelihood of the target description sentence given the training image. Experiments on several datasets show the accuracy of the model and the fluency of the language it learns solely from image descriptions. Our model is often quite accurate, which we verify both qualitatively and quantitatively. For instance, while the current state-of-the-art BLEU-1 score (the higher the better) on the Pascal dataset is 25, our approach yields 59, to be compared to human performance around 69. We also show BLEU-1 score improvements on Flickr30k, from 56 to 66, and on SBU, from 19 to 28. Lastly, on the newly released COCO dataset, we achieve a BLEU-4 of 27.7, which is the current state-of-the-art."
    },
    {
        "paperId": "e15cf50aa89fee8535703b9f9512fca5bfc43327",
        "url": "https://www.semanticscholar.org/paper/e15cf50aa89fee8535703b9f9512fca5bfc43327",
        "title": "Going deeper with convolutions",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2014,
        "citationCount": 46105,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/1409.4842",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1409.4842, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2574060",
                "name": "Christian Szegedy"
            },
            {
                "authorId": "2157222093",
                "name": "Wei Liu"
            },
            {
                "authorId": "39978391",
                "name": "Yangqing Jia"
            },
            {
                "authorId": "3142556",
                "name": "P. Sermanet"
            },
            {
                "authorId": "144828948",
                "name": "Scott E. Reed"
            },
            {
                "authorId": "1838674",
                "name": "Dragomir Anguelov"
            },
            {
                "authorId": "1761978",
                "name": "D. Erhan"
            },
            {
                "authorId": "2657155",
                "name": "Vincent Vanhoucke"
            },
            {
                "authorId": "39863668",
                "name": "Andrew Rabinovich"
            }
        ],
        "abstract": "We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection."
    },
    {
        "paperId": "ebcea2d842d3d4e320500086aff0deb4cb4412ff",
        "url": "https://www.semanticscholar.org/paper/ebcea2d842d3d4e320500086aff0deb4cb4412ff",
        "title": "Efficient object localization using Convolutional Networks",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2014,
        "citationCount": 1420,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1411.4280",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1411.4280, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2704494",
                "name": "Jonathan Tompson"
            },
            {
                "authorId": "2558463",
                "name": "Ross Goroshin"
            },
            {
                "authorId": "49147969",
                "name": "Arjun Jain"
            },
            {
                "authorId": "1688882",
                "name": "Yann LeCun"
            },
            {
                "authorId": "2428034",
                "name": "C. Bregler"
            }
        ],
        "abstract": "Recent state-of-the-art performance on human-body pose estimation has been achieved with Deep Convolutional Networks (ConvNets). Traditional ConvNet architectures include pooling and sub-sampling layers which reduce computational requirements, introduce invariance and prevent over-training. These benefits of pooling come at the cost of reduced localization accuracy. We introduce a novel architecture which includes an efficient `position refinement' model that is trained to estimate the joint offset location within a small region of the image. This refinement model is jointly trained in cascade with a state-of-the-art ConvNet model [21] to achieve improved accuracy in human joint location estimation. We show that the variance of our detector approaches the variance of human annotations on the FLIC [20] dataset and outperforms all existing approaches on the MPII-human-pose dataset [1]."
    },
    {
        "paperId": "f01fc808592ea7c473a69a6e7484040a435f36d9",
        "url": "https://www.semanticscholar.org/paper/f01fc808592ea7c473a69a6e7484040a435f36d9",
        "title": "Long-term recurrent convolutional networks for visual recognition and description",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2014,
        "citationCount": 6267,
        "openAccessPdf": {
            "url": "https://doi.org/10.1109/tpami.2016.2599174",
            "status": "GREEN",
            "license": "publisher-specific-oa",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1411.4389, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "7408951",
                "name": "Jeff Donahue"
            },
            {
                "authorId": "2234342",
                "name": "Lisa Anne Hendricks"
            },
            {
                "authorId": "34849128",
                "name": "Marcus Rohrbach"
            },
            {
                "authorId": "1811430",
                "name": "Subhashini Venugopalan"
            },
            {
                "authorId": "1687120",
                "name": "S. Guadarrama"
            },
            {
                "authorId": "2903226",
                "name": "Kate Saenko"
            },
            {
                "authorId": "1753210",
                "name": "Trevor Darrell"
            }
        ],
        "abstract": "Models based on deep convolutional networks have dominated recent image interpretation tasks; we investigate whether models which are also recurrent, or temporally deep, are effective for tasks involving sequences, visual and otherwise. We develop a novel recurrent convolutional architecture suitable for large-scale visual learning which is end-to-end trainable, and demonstrate the value of these models on benchmark video recognition tasks, image description and retrieval problems, and video narration challenges. In contrast to current models which assume a fixed spatio-temporal receptive field or simple temporal averaging for sequential processing, recurrent convolutional models are doubly deep in that they can be compositional in spatial and temporal layers. Such models may have advantages when target concepts are complex and/or training data are limited. Learning long-term dependencies is possible when nonlinearities are incorporated into the network state updates. Long-term RNN models are appealing in that they directly can map variable-length inputs (e.g., video frames) to variable length outputs (e.g., natural language text) and can model complex temporal dynamics; yet they can be optimized with backpropagation. Our recurrent long-term models are directly connected to modern visual convnet models and can be jointly trained to simultaneously learn temporal dynamics and convolutional perceptual representations. Our results show such models have distinct advantages over state-of-the-art models for recognition or generation which are separately defined and/or optimized."
    },
    {
        "paperId": "f0822cc18e502e15a8c153773492934201e4c8f9",
        "url": "https://www.semanticscholar.org/paper/f0822cc18e502e15a8c153773492934201e4c8f9",
        "title": "Person re-identification by Local Maximal Occurrence representation and metric learning",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2014,
        "citationCount": 2117,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/1406.4216",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR.2015.7298832?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR.2015.7298832, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "40397682",
                "name": "Shengcai Liao"
            },
            {
                "authorId": "143955754",
                "name": "Yang Hu"
            },
            {
                "authorId": "101002663",
                "name": "Xiangyu Zhu"
            },
            {
                "authorId": "34679741",
                "name": "S. Li"
            }
        ],
        "abstract": "Person re-identification is an important technique towards automatic search of a person's presence in a surveillance video. Two fundamental problems are critical for person re-identification, feature representation and metric learning. An effective feature representation should be robust to illumination and viewpoint changes, and a discriminant metric should be learned to match various person images. In this paper, we propose an effective feature representation called Local Maximal Occurrence (LOMO), and a subspace and metric learning method called Cross-view Quadratic Discriminant Analysis (XQDA). The LOMO feature analyzes the horizontal occurrence of local features, and maximizes the occurrence to make a stable representation against viewpoint changes. Besides, to handle illumination variations, we apply the Retinex transform and a scale invariant texture operator. To learn a discriminant metric, we propose to learn a discriminant low dimensional subspace by cross-view quadratic discriminant analysis, and simultaneously, a QDA metric is learned on the derived subspace. We also present a practical computation method for XQDA, as well as its regularization. Experiments on four challenging person re-identification databases, VIPeR, QMUL GRID, CUHK Campus, and CUHK03, show that the proposed method improves the state-of-the-art rank-1 identification rates by 2.2%, 4.88%, 28.91%, and 31.55% on the four databases, respectively."
    },
    {
        "paperId": "06c06885fd53b2cbd407704cf14f658842ed48e5",
        "url": "https://www.semanticscholar.org/paper/06c06885fd53b2cbd407704cf14f658842ed48e5",
        "title": "Deeply-Recursive Convolutional Network for Image Super-Resolution",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2015,
        "citationCount": 2687,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1511.04491",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1511.04491, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2109183942",
                "name": "Jiwon Kim"
            },
            {
                "authorId": "2119170990",
                "name": "Jung Kwon Lee"
            },
            {
                "authorId": "2135837",
                "name": "Kyoung Mu Lee"
            }
        ],
        "abstract": "We propose an image super-resolution method (SR) using a deeply-recursive convolutional network (DRCN). Our network has a very deep recursive layer (up to 16 recursions). Increasing recursion depth can improve performance without introducing new parameters for additional convolutions. Albeit advantages, learning a DRCN is very hard with a standard gradient descent method due to exploding/ vanishing gradients. To ease the difficulty of training, we propose two extensions: recursive-supervision and skip-connection. Our method outperforms previous methods by a large margin."
    },
    {
        "paperId": "0a28efacb92d16e6e0dd4d87b5aca91b28be8853",
        "url": "https://www.semanticscholar.org/paper/0a28efacb92d16e6e0dd4d87b5aca91b28be8853",
        "title": "ActivityNet: A large-scale video benchmark for human activity understanding",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2015,
        "citationCount": 2793,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR.2015.7298698?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR.2015.7298698, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3175258",
                "name": "Fabian Caba Heilbron"
            },
            {
                "authorId": "144201025",
                "name": "Victor Escorcia"
            },
            {
                "authorId": "2931652",
                "name": "Bernard Ghanem"
            },
            {
                "authorId": "9200530",
                "name": "Juan Carlos Niebles"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "0f12a3aaf3851078d93a9bba4e3ebece6d4bcfe5",
        "url": "https://www.semanticscholar.org/paper/0f12a3aaf3851078d93a9bba4e3ebece6d4bcfe5",
        "title": "Staple: Complementary Learners for Real-Time Tracking",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2015,
        "citationCount": 1629,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/1512.01355",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1512.01355, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2271057",
                "name": "Luca Bertinetto"
            },
            {
                "authorId": "1881617",
                "name": "Jack Valmadre"
            },
            {
                "authorId": "143777501",
                "name": "S. Golodetz"
            },
            {
                "authorId": "3336488",
                "name": "O. Mikk"
            },
            {
                "authorId": "143635540",
                "name": "Philip H. S. Torr"
            }
        ],
        "abstract": "Correlation Filter-based trackers have recently achieved excellent performance, showing great robustness to challenging situations exhibiting motion blur and illumination changes. However, since the model that they learn depends strongly on the spatial layout of the tracked object, they are notoriously sensitive to deformation. Models based on colour statistics have complementary traits: they cope well with variation in shape, but suffer when illumination is not consistent throughout a sequence. Moreover, colour distributions alone can be insufficiently discriminative. In this paper, we show that a simple tracker combining complementary cues in a ridge regression framework can operate faster than 80 FPS and outperform not only all entries in the popular VOT14 competition, but also recent and far more sophisticated trackers according to multiple benchmarks."
    },
    {
        "paperId": "1839e17555160bd897b978c48b8ebd13dd21445f",
        "url": "https://www.semanticscholar.org/paper/1839e17555160bd897b978c48b8ebd13dd21445f",
        "title": "Hierarchical recurrent neural network for skeleton based action recognition",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2015,
        "citationCount": 1984,
        "openAccessPdf": {
            "url": "http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Du_Hierarchical_Recurrent_Neural_2015_CVPR_paper.pdf",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR.2015.7298714?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR.2015.7298714, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2111867908",
                "name": "Yong Du"
            },
            {
                "authorId": null,
                "name": "Wei Wang"
            },
            {
                "authorId": "123865558",
                "name": "Liang Wang"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "1ced31e02234bc3d1092ffb2c7442ffbd51cb309",
        "url": "https://www.semanticscholar.org/paper/1ced31e02234bc3d1092ffb2c7442ffbd51cb309",
        "title": "A Large Dataset to Train Convolutional Networks for Disparity, Optical Flow, and Scene Flow Estimation",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2015,
        "citationCount": 2865,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1512.02134",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1512.02134, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "153200643",
                "name": "N. Mayer"
            },
            {
                "authorId": "48105320",
                "name": "Eddy Ilg"
            },
            {
                "authorId": "2880264",
                "name": "Philip Husser"
            },
            {
                "authorId": "152702479",
                "name": "P. Fischer"
            },
            {
                "authorId": "1695302",
                "name": "D. Cremers"
            },
            {
                "authorId": "2841331",
                "name": "Alexey Dosovitskiy"
            },
            {
                "authorId": "1710872",
                "name": "T. Brox"
            }
        ],
        "abstract": "Recent work has shown that optical flow estimation can be formulated as a supervised learning task and can be successfully solved with convolutional networks. Training of the so-called FlowNet was enabled by a large synthetically generated dataset. The present paper extends the concept of optical flow estimation via convolutional networks to disparity and scene flow estimation. To this end, we propose three synthetic stereo video datasets with sufficient realism, variation, and size to successfully train large networks. Our datasets are the first large-scale datasets to enable training and evaluation of scene flow methods. Besides the datasets, we present a convolutional network for real-time disparity estimation that provides state-of-the-art results. By combining a flow and disparity estimation network and training it jointly, we demonstrate the first scene flow estimation with a convolutional network."
    },
    {
        "paperId": "1e9b1f6061ef779e3ad0819c2832a29168eaeb9d",
        "url": "https://www.semanticscholar.org/paper/1e9b1f6061ef779e3ad0819c2832a29168eaeb9d",
        "title": "Instance-Aware Semantic Segmentation via Multi-task Network Cascades",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2015,
        "citationCount": 1256,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/1512.04412",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1512.04412, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3304536",
                "name": "Jifeng Dai"
            },
            {
                "authorId": "39353098",
                "name": "Kaiming He"
            },
            {
                "authorId": null,
                "name": "Jian Sun"
            }
        ],
        "abstract": "Semantic segmentation research has recently witnessed rapid progress, but many leading methods are unable to identify object instances. In this paper, we present Multitask Network Cascades for instance-aware semantic segmentation. Our model consists of three networks, respectively differentiating instances, estimating masks, and categorizing objects. These networks form a cascaded structure, and are designed to share their convolutional features. We develop an algorithm for the nontrivial end-to-end training of this causal, cascaded structure. Our solution is a clean, single-step training framework and can be generalized to cascades that have more stages. We demonstrate state-of-the-art instance-aware semantic segmentation accuracy on PASCAL VOC. Meanwhile, our method takes only 360ms testing an image using VGG-16, which is two orders of magnitude faster than previous systems for this challenging problem. As a by product, our method also achieves compelling object detection results which surpass the competitive Fast/Faster R-CNN systems. The method described in this paper is the foundation of our submissions to the MS COCO 2015 segmentation competition, where we won the 1st place."
    },
    {
        "paperId": "21c99706bb26e9012bfb4d8d48009a3d45af59b2",
        "url": "https://www.semanticscholar.org/paper/21c99706bb26e9012bfb4d8d48009a3d45af59b2",
        "title": "Neural Module Networks",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2015,
        "citationCount": 1129,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1511.02799, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2112400",
                "name": "Jacob Andreas"
            },
            {
                "authorId": "34849128",
                "name": "Marcus Rohrbach"
            },
            {
                "authorId": "1753210",
                "name": "Trevor Darrell"
            },
            {
                "authorId": "38666915",
                "name": "D. Klein"
            }
        ],
        "abstract": "Visual question answering is fundamentally compositional in nature-a question like where is the dog? shares substructure with questions like what color is the dog? and where is the cat? This paper seeks to simultaneously exploit the representational capacity of deep networks and the compositional linguistic structure of questions. We describe a procedure for constructing and learning neural module networks, which compose collections of jointly-trained neural \"modules\" into deep networks for question answering. Our approach decomposes questions into their linguistic substructures, and uses these structures to dynamically instantiate modular networks (with reusable components for recognizing dogs, classifying colors, etc.). The resulting compound networks are jointly trained. We evaluate our approach on two challenging datasets for visual question answering, achieving state-of-the-art results on both the VQA natural image dataset and a new dataset of complex questions about abstract shapes."
    },
    {
        "paperId": "23ffaa0fe06eae05817f527a47ac3291077f9e58",
        "url": "https://www.semanticscholar.org/paper/23ffaa0fe06eae05817f527a47ac3291077f9e58",
        "title": "Rethinking the Inception Architecture for Computer Vision",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2015,
        "citationCount": 29753,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/1512.00567",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1512.00567, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2574060",
                "name": "Christian Szegedy"
            },
            {
                "authorId": "2657155",
                "name": "Vincent Vanhoucke"
            },
            {
                "authorId": "2054165706",
                "name": "Sergey Ioffe"
            },
            {
                "authorId": "1789737",
                "name": "Jonathon Shlens"
            },
            {
                "authorId": "3282833",
                "name": "Z. Wojna"
            }
        ],
        "abstract": "Convolutional networks are at the core of most state of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we are exploring ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21:2% top-1 and 5:6% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3:5% top-5 error and 17:3% top-1 error on the validation set and 3:6% top-5 error on the official test set."
    },
    {
        "paperId": "274e5454e22b8cd73f7110dc42e09b8a84ff5a82",
        "url": "https://www.semanticscholar.org/paper/274e5454e22b8cd73f7110dc42e09b8a84ff5a82",
        "title": "An improved deep learning architecture for person re-identification",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2015,
        "citationCount": 1251,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR.2015.7299016?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR.2015.7299016, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2061173337",
                "name": "Ejaz Ahmed"
            },
            {
                "authorId": "2111328101",
                "name": "Michael Jones"
            },
            {
                "authorId": "34749896",
                "name": "Tim K. Marks"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "2946c4e29b057d3543f09b89f547965895676d19",
        "url": "https://www.semanticscholar.org/paper/2946c4e29b057d3543f09b89f547965895676d19",
        "title": "Single image super-resolution from transformed self-exemplars",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2015,
        "citationCount": 3169,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR.2015.7299156?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR.2015.7299156, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3068086",
                "name": "Jia-Bin Huang"
            },
            {
                "authorId": "144541718",
                "name": "Abhishek Singh"
            },
            {
                "authorId": "145237406",
                "name": "N. Ahuja"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d",
        "url": "https://www.semanticscholar.org/paper/2c03df8b48bf3fa39054345bafabfeff15bfd11d",
        "title": "Deep Residual Learning for Image Recognition",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2015,
        "citationCount": 215984,
        "openAccessPdf": {
            "url": "https://repositorio.unal.edu.co/bitstream/unal/81443/1/98670607.2022.pdf",
            "status": "GREEN",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1512.03385, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "39353098",
                "name": "Kaiming He"
            },
            {
                "authorId": "1771551",
                "name": "X. Zhang"
            },
            {
                "authorId": "3080683",
                "name": "Shaoqing Ren"
            },
            {
                "authorId": null,
                "name": "Jian Sun"
            }
        ],
        "abstract": "Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8 deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation."
    },
    {
        "paperId": "2c1890864c1c2b750f48316dc8b650ba4772adc5",
        "url": "https://www.semanticscholar.org/paper/2c1890864c1c2b750f48316dc8b650ba4772adc5",
        "title": "Stacked Attention Networks for Image Question Answering",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2015,
        "citationCount": 1971,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1511.02274",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1511.02274, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "8387085",
                "name": "Zichao Yang"
            },
            {
                "authorId": "144137069",
                "name": "Xiaodong He"
            },
            {
                "authorId": "1800422",
                "name": "Jianfeng Gao"
            },
            {
                "authorId": "144718788",
                "name": "L. Deng"
            },
            {
                "authorId": "46234526",
                "name": "Alex Smola"
            }
        ],
        "abstract": "This paper presents stacked attention networks (SANs) that learn to answer natural language questions from images. SANs use semantic representation of a question as query to search for the regions in an image that are related to the answer. We argue that image question answering (QA) often requires multiple steps of reasoning. Thus, we develop a multiple-layer SAN in which we query an image multiple times to infer the answer progressively. Experiments conducted on four image QA data sets demonstrate that the proposed SANs significantly outperform previous state-of-the-art approaches. The visualization of the attention layers illustrates the progress that the SAN locates the relevant visual clues that lead to the answer of the question layer-by-layer."
    },
    {
        "paperId": "2ce63d77eecc35faef85a3b752a314c93a077ac9",
        "url": "https://www.semanticscholar.org/paper/2ce63d77eecc35faef85a3b752a314c93a077ac9",
        "title": "Learning Multi-domain Convolutional Neural Networks for Visual Tracking",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2015,
        "citationCount": 2573,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1510.07945",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1510.07945, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "34758272",
                "name": "Hyeonseob Nam"
            },
            {
                "authorId": "40030651",
                "name": "Bohyung Han"
            }
        ],
        "abstract": "We propose a novel visual tracking algorithm based on the representations from a discriminatively trained Convolutional Neural Network (CNN). Our algorithm pretrains a CNN using a large set of videos with tracking ground-truths to obtain a generic target representation. Our network is composed of shared layers and multiple branches of domain-specific layers, where domains correspond to individual training sequences and each branch is responsible for binary classification to identify target in each domain. We train each domain in the network iteratively to obtain generic target representations in the shared layers. When tracking a target in a new sequence, we construct a new network by combining the shared layers in the pretrained CNN with a new binary classification layer, which is updated online. Online tracking is performed by evaluating the candidate windows randomly sampled around the previous target state. The proposed algorithm illustrates outstanding performance in existing tracking benchmarks."
    },
    {
        "paperId": "2e61eb4a5c6fe6c0fdb36cfb84d460ee1524099f",
        "url": "https://www.semanticscholar.org/paper/2e61eb4a5c6fe6c0fdb36cfb84d460ee1524099f",
        "title": "A convolutional neural network cascade for face detection",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2015,
        "citationCount": 1312,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR.2015.7299170?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR.2015.7299170, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3131569",
                "name": "Haoxiang Li"
            },
            {
                "authorId": "145527707",
                "name": "Zhe L. Lin"
            },
            {
                "authorId": "1720987",
                "name": "Xiaohui Shen"
            },
            {
                "authorId": "145561604",
                "name": "Jonathan Brandt"
            },
            {
                "authorId": "144988571",
                "name": "G. Hua"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "31f9eb39d840821979e5df9f34a6e92dd9c879f2",
        "url": "https://www.semanticscholar.org/paper/31f9eb39d840821979e5df9f34a6e92dd9c879f2",
        "title": "Learning Deep Features for Discriminative Localization",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2015,
        "citationCount": 10155,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/1512.04150",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1512.04150, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "145291669",
                "name": "Bolei Zhou"
            },
            {
                "authorId": "2556428",
                "name": "A. Khosla"
            },
            {
                "authorId": "2677488",
                "name": "gata Lapedriza"
            },
            {
                "authorId": "143868587",
                "name": "A. Oliva"
            },
            {
                "authorId": "143805211",
                "name": "A. Torralba"
            }
        ],
        "abstract": "In this work, we revisit the global average pooling layer proposed in [13], and shed light on how it explicitly enables the convolutional neural network (CNN) to have remarkable localization ability despite being trained on imagelevel labels. While this technique was previously proposed as a means for regularizing training, we find that it actually builds a generic localizable deep representation that exposes the implicit attention of CNNs on an image. Despite the apparent simplicity of global average pooling, we are able to achieve 37.1% top-5 error for object localization on ILSVRC 2014 without training on any bounding box annotation. We demonstrate in a variety of experiments that our network is able to localize the discriminative image regions despite just being trained for solving classification task1."
    },
    {
        "paperId": "44e1ee7a63a01a76371d7070c132361a5ddd54a0",
        "url": "https://www.semanticscholar.org/paper/44e1ee7a63a01a76371d7070c132361a5ddd54a0",
        "title": "Structural-RNN: Deep Learning on Spatio-Temporal Graphs",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2015,
        "citationCount": 1151,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/1511.05298",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1511.05298, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1726066",
                "name": "Ashesh Jain"
            },
            {
                "authorId": "40029556",
                "name": "Amir Zamir"
            },
            {
                "authorId": "1702137",
                "name": "S. Savarese"
            },
            {
                "authorId": "1681995",
                "name": "Ashutosh Saxena"
            }
        ],
        "abstract": "Deep Recurrent Neural Network architectures, though remarkably capable at modeling sequences, lack an intuitive high-level spatio-temporal structure. That is while many problems in computer vision inherently have an underlying high-level structure and can benefit from it. Spatiotemporal graphs are a popular tool for imposing such high-level intuitions in the formulation of real world problems. In this paper, we propose an approach for combining the power of high-level spatio-temporal graphs and sequence learning success of Recurrent Neural Networks (RNNs). We develop a scalable method for casting an arbitrary spatio-temporal graph as a rich RNN mixture that is feedforward, fully differentiable, and jointly trainable. The proposed method is generic and principled as it can be used for transforming any spatio-temporal graph through employing a certain set of well defined steps. The evaluations of the proposed approach on a diverse set of problems, ranging from modeling human motion to object interactions, shows improvement over the state-of-the-art with a large margin. We expect this method to empower new approaches to problem formulation through high-level spatio-temporal graphs and Recurrent Neural Networks."
    },
    {
        "paperId": "52693bcf4627bdbc31734e7f3dbcf5ea7eef4d35",
        "url": "https://www.semanticscholar.org/paper/52693bcf4627bdbc31734e7f3dbcf5ea7eef4d35",
        "title": "DeepFool: A Simple and Accurate Method to Fool Deep Neural Networks",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2015,
        "citationCount": 5235,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1511.04599",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1511.04599, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1403182206",
                "name": "Seyed-Mohsen Moosavi-Dezfooli"
            },
            {
                "authorId": "33054064",
                "name": "Alhussein Fawzi"
            },
            {
                "authorId": "1703189",
                "name": "P. Frossard"
            }
        ],
        "abstract": "State-of-the-art deep neural networks have achieved impressive results on many image classification tasks. However, these same architectures have been shown to be unstable to small, well sought, perturbations of the images. Despite the importance of this phenomenon, no effective methods have been proposed to accurately compute the robustness of state-of-the-art deep classifiers to such perturbations on large-scale datasets. In this paper, we fill this gap and propose the DeepFool algorithm to efficiently compute perturbations that fool deep networks, and thus reliably quantify the robustness of these classifiers. Extensive experimental results show that our approach outperforms recent methods in the task of computing adversarial perturbations and making classifiers more robust."
    },
    {
        "paperId": "52d7eb0fbc3522434c13cc247549f74bb9609c5d",
        "url": "https://www.semanticscholar.org/paper/52d7eb0fbc3522434c13cc247549f74bb9609c5d",
        "title": "WIDER FACE: A Face Detection Benchmark",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2015,
        "citationCount": 1711,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/1511.06523",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1511.06523, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "92887925",
                "name": "Shuo Yang"
            },
            {
                "authorId": "47571885",
                "name": "Ping Luo"
            },
            {
                "authorId": "1717179",
                "name": "Chen Change Loy"
            },
            {
                "authorId": "50295995",
                "name": "Xiaoou Tang"
            }
        ],
        "abstract": "Face detection is one of the most studied topics in the computer vision community. Much of the progresses have been made by the availability of face detection benchmark datasets. We show that there is a gap between current face detection performance and the real world requirements. To facilitate future face detection research, we introduce the WIDER FACE dataset1, which is 10 times larger than existing datasets. The dataset contains rich annotations, including occlusions, poses, event categories, and face bounding boxes. Faces in the proposed dataset are extremely challenging due to large variations in scale, pose and occlusion, as shown in Fig. 1. Furthermore, we show that WIDER FACE dataset is an effective training source for face detection. We benchmark several representative detection systems, providing an overview of state-of-the-art performance and propose a solution to deal with large scale variation. Finally, we discuss common failure cases that worth to be further investigated."
    },
    {
        "paperId": "5418b2a482720e013d487a385c26fae0f017c6a6",
        "url": "https://www.semanticscholar.org/paper/5418b2a482720e013d487a385c26fae0f017c6a6",
        "title": "Beyond short snippets: Deep networks for video classification",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2015,
        "citationCount": 2420,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1503.08909",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1503.08909, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2340579",
                "name": "Joe Yue-Hei Ng"
            },
            {
                "authorId": "3308897",
                "name": "Matthew J. Hausknecht"
            },
            {
                "authorId": "2259154",
                "name": "Sudheendra Vijayanarasimhan"
            },
            {
                "authorId": "1689108",
                "name": "O. Vinyals"
            },
            {
                "authorId": "3089272",
                "name": "R. Monga"
            },
            {
                "authorId": "1805076",
                "name": "G. Toderici"
            }
        ],
        "abstract": "Convolutional neural networks (CNNs) have been extensively applied for image recognition problems giving state-of-the-art results on recognition, detection, segmentation and retrieval. In this work we propose and evaluate several deep neural network architectures to combine image information across a video over longer time periods than previously attempted. We propose two methods capable of handling full length videos. The first method explores various convolutional temporal feature pooling architectures, examining the various design choices which need to be made when adapting a CNN for this task. The second proposed method explicitly models the video as an ordered sequence of frames. For this purpose we employ a recurrent neural network that uses Long Short-Term Memory (LSTM) cells which are connected to the output of the underlying CNN. Our best networks exhibit significant performance improvements over previously published results on the Sports 1 million dataset (73.1% vs. 60.9%) and the UCF-101 datasets with (88.6% vs. 88.0%) and without additional optical flow information (82.6% vs. 73.0%)."
    },
    {
        "paperId": "5aa26299435bdf7db874ef1640a6c3b5a4a2c394",
        "url": "https://www.semanticscholar.org/paper/5aa26299435bdf7db874ef1640a6c3b5a4a2c394",
        "title": "FaceNet: A unified embedding for face recognition and clustering",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2015,
        "citationCount": 14204,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1503.03832",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1503.03832, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3302320",
                "name": "Florian Schroff"
            },
            {
                "authorId": "2741985",
                "name": "Dmitry Kalenichenko"
            },
            {
                "authorId": "2066819269",
                "name": "James Philbin"
            }
        ],
        "abstract": "Despite significant recent advances in the field of face recognition [10, 14, 15, 17], implementing face verification and recognition efficiently at scale presents serious challenges to current approaches. In this paper we present a system, called FaceNet, that directly learns a mapping from face images to a compact Euclidean space where distances directly correspond to a measure offace similarity. Once this space has been produced, tasks such as face recognition, verification and clustering can be easily implemented using standard techniques with FaceNet embeddings asfeature vectors. Our method uses a deep convolutional network trained to directly optimize the embedding itself, rather than an intermediate bottleneck layer as in previous deep learning approaches. To train, we use triplets of roughly aligned matching / non-matching face patches generated using a novel online triplet mining method. The benefit of our approach is much greater representational efficiency: we achieve state-of-the-artface recognition performance using only 128-bytes perface. On the widely used Labeled Faces in the Wild (LFW) dataset, our system achieves a new record accuracy of 99.63%. On YouTube Faces DB it achieves 95.12%. Our system cuts the error rate in comparison to the best published result [15] by 30% on both datasets."
    },
    {
        "paperId": "5d6ae67e569f974360b107060c23cbb8a13b0687",
        "url": "https://www.semanticscholar.org/paper/5d6ae67e569f974360b107060c23cbb8a13b0687",
        "title": "Learning from massive noisy labeled data for image classification",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2015,
        "citationCount": 1245,
        "openAccessPdf": {
            "url": "http://www.ee.cuhk.edu.hk/%7Exgwang/papers/xiaoXYHWcvpr15.pdf",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR.2015.7298885?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR.2015.7298885, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2014849645",
                "name": "Tong Xiao"
            },
            {
                "authorId": "2143749716",
                "name": "Tian Xia"
            },
            {
                "authorId": "2143686417",
                "name": "Yi Yang"
            },
            {
                "authorId": "48908475",
                "name": "Chang Huang"
            },
            {
                "authorId": "93768810",
                "name": "Xiaogang Wang"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "7c7899faf1512d2c7bbcb6fda5e881c79f8f135e",
        "url": "https://www.semanticscholar.org/paper/7c7899faf1512d2c7bbcb6fda5e881c79f8f135e",
        "title": "Supervised Discrete Hashing",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2015,
        "citationCount": 1141,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1503.01557, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "144618699",
                "name": "Fumin Shen"
            },
            {
                "authorId": "1780381",
                "name": "Chunhua Shen"
            },
            {
                "authorId": "46641573",
                "name": "W. Liu"
            },
            {
                "authorId": "1724393",
                "name": "Heng Tao Shen"
            }
        ],
        "abstract": "Recently, learning based hashing techniques have attracted broad research interests because they can support efficient storage and retrieval for high-dimensional data such as images, videos, documents, etc. However, a major difficulty of learning to hash lies in handling the discrete constraints imposed on the pursued hash codes, which typically makes hash optimizations very challenging (NP-hard in general). In this work, we propose a new supervised hashing framework, where the learning objective is to generate the optimal binary hash codes for linear classification. By introducing an auxiliary variable, we reformulate the objective such that it can be solved substantially efficiently by employing a regularization algorithm. One of the key steps in this algorithm is to solve a regularization sub-problem associated with the NP-hard binary optimization. We show that the sub-problem admits an analytical solution via cyclic coordinate descent. As such, a high-quality discrete solution can eventually be obtained in an efficient computing manner, therefore enabling to tackle massive datasets. We evaluate the proposed approach, dubbed Supervised Discrete Hashing (SDH), on four large image datasets and demonstrate its superiority to the state-of-the-art hashing methods in large-scale image retrieval."
    },
    {
        "paperId": "85ae705ef4353c6854f5be4a4664269d6317c66b",
        "url": "https://www.semanticscholar.org/paper/85ae705ef4353c6854f5be4a4664269d6317c66b",
        "title": "Image retrieval using scene graphs",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2015,
        "citationCount": 1159,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR.2015.7298990?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR.2015.7298990, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2115231104",
                "name": "Justin Johnson"
            },
            {
                "authorId": "145237361",
                "name": "Ranjay Krishna"
            },
            {
                "authorId": "144421225",
                "name": "Michael Stark"
            },
            {
                "authorId": "2040091191",
                "name": "Li-Jia Li"
            },
            {
                "authorId": "1760364",
                "name": "David A. Shamma"
            },
            {
                "authorId": "145879842",
                "name": "Michael S. Bernstein"
            },
            {
                "authorId": "48004138",
                "name": "Li Fei-Fei"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "884750937bb97e82c41316d80e5d104e0c0e4795",
        "url": "https://www.semanticscholar.org/paper/884750937bb97e82c41316d80e5d104e0c0e4795",
        "title": "Deep Metric Learning via Lifted Structured Feature Embedding",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2015,
        "citationCount": 1734,
        "openAccessPdf": {
            "url": "https://dspace.mit.edu/bitstream/1721.1/113397/1/Jegelka_Deep%20metric.pdf",
            "status": "GREEN",
            "license": "CCBYNCSA",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1511.06452, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2133680",
                "name": "Hyun Oh Song"
            },
            {
                "authorId": "144863550",
                "name": "Yu Xiang"
            },
            {
                "authorId": "2594093",
                "name": "S. Jegelka"
            },
            {
                "authorId": "1702137",
                "name": "S. Savarese"
            }
        ],
        "abstract": "Learning the distance metric between pairs of examples is of great importance for learning and visual recognition. With the remarkable success from the state of the art convolutional neural networks, recent works [1, 31] have shown promising results on discriminatively training the networks to learn semantic feature embeddings where similar examples are mapped close to each other and dissimilar examples are mapped farther apart. In this paper, we describe an algorithm for taking full advantage of the training batches in the neural network training by lifting the vector of pairwise distances within the batch to the matrix of pairwise distances. This step enables the algorithm to learn the state of the art feature embedding by optimizing a novel structured prediction objective on the lifted problem. Additionally, we collected Stanford Online Products dataset: 120k images of 23k classes of online products for metric learning. Our experiments on the CUB-200-2011 [37], CARS196 [19], and Stanford Online Products datasets demonstrate significant improvement over existing deep feature embedding methods on all experimented embedding sizes with the GoogLeNet [33] network. The source code and the dataset are available at: https://github.com/rksltnl/ Deep-Metric-Learning-CVPR16."
    },
    {
        "paperId": "940f7bb5c1f69c61e57b33b4a266c2d578faa012",
        "url": "https://www.semanticscholar.org/paper/940f7bb5c1f69c61e57b33b4a266c2d578faa012",
        "title": "Visual saliency based on multiscale deep features",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2015,
        "citationCount": 1299,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1503.08663",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1503.08663, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "144958813",
                "name": "Guanbin Li"
            },
            {
                "authorId": "1841911",
                "name": "Yizhou Yu"
            }
        ],
        "abstract": "Visual saliency is a fundamental problem in both cognitive and computational sciences, including computer vision. In this paper, we discover that a high-quality visual saliency model can be learned from multiscale features extracted using deep convolutional neural networks (CNNs), which have had many successes in visual recognition tasks. For learning such saliency models, we introduce a neural network architecture, which has fully connected layers on top of CNNs responsible for feature extraction at three different scales. We then propose a refinement method to enhance the spatial coherence of our saliency results. Finally, aggregating multiple saliency maps computed for different levels of image segmentation can further boost the performance, yielding saliency maps better than those generated from a single segmentation. To promote further research and evaluation of visual saliency models, we also construct a new large database of 4447 challenging images and their pixelwise saliency annotations. Experimental results demonstrate that our proposed method is capable of achieving state-of-the-art performance on all public benchmarks, improving the F-Measure by 5.0% and 13.2% respectively on the MSRA-B dataset and our new dataset (HKU-IS), and lowering the mean absolute error by 5.7% and 35.1% respectively on these two datasets."
    },
    {
        "paperId": "9f48616039cb21903132528c0be5348b3019db50",
        "url": "https://www.semanticscholar.org/paper/9f48616039cb21903132528c0be5348b3019db50",
        "title": "Attention to Scale: Scale-Aware Semantic Image Segmentation",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2015,
        "citationCount": 1371,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1511.03339",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1511.03339, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "34192119",
                "name": "Liang-Chieh Chen"
            },
            {
                "authorId": "2143686417",
                "name": "Yi Yang"
            },
            {
                "authorId": "152924487",
                "name": "Jiang Wang"
            },
            {
                "authorId": "1399905857",
                "name": "Wei Xu"
            },
            {
                "authorId": "145081362",
                "name": "A. Yuille"
            }
        ],
        "abstract": "Incorporating multi-scale features in fully convolutional neural networks (FCNs) has been a key element to achieving state-of-the-art performance on semantic image segmentation. One common way to extract multi-scale features is to feed multiple resized input images to a shared deep network and then merge the resulting features for pixelwise classification. In this work, we propose an attention mechanism that learns to softly weight the multi-scale features at each pixel location. We adapt a state-of-the-art semantic image segmentation model, which we jointly train with multi-scale input images and the attention model. The proposed attention model not only outperforms averageand max-pooling, but allows us to diagnostically visualize the importance of features at different positions and scales. Moreover, we show that adding extra supervision to the output at each scale is essential to achieving excellent performance when merging multi-scale features. We demonstrate the effectiveness of our model with extensive experiments on three challenging datasets, including PASCAL-Person-Part, PASCAL VOC 2012 and a subset of MS-COCO 2014."
    },
    {
        "paperId": "adc4e63b58cf4092420533fd877b8c29f8e2ec1d",
        "url": "https://www.semanticscholar.org/paper/adc4e63b58cf4092420533fd877b8c29f8e2ec1d",
        "title": "Inside-Outside Net: Detecting Objects in Context with Skip Pooling and Recurrent Neural Networks",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2015,
        "citationCount": 1252,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1512.04143",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1512.04143, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "144499674",
                "name": "Sean Bell"
            },
            {
                "authorId": "1699161",
                "name": "C. L. Zitnick"
            },
            {
                "authorId": "144374926",
                "name": "Kavita Bala"
            },
            {
                "authorId": "2983898",
                "name": "Ross B. Girshick"
            }
        ],
        "abstract": "It is well known that contextual and multi-scale representations are important for accurate visual recognition. In this paper we present the Inside-Outside Net (ION), an object detector that exploits information both inside and outside the region of interest. Contextual information outside the region of interest is integrated using spatial recurrent neural networks. Inside, we use skip pooling to extract information at multiple scales and levels of abstraction. Through extensive experiments we evaluate the design space and provide readers with an overview of what tricks of the trade are important. ION improves state-of-the-art on PASCAL VOC 2012 object detection from 73.9% to 77.9% mAP. On the new and more challenging MS COCO dataset, we improve state-of-the-art from 19.7% to 33.1% mAP. In the 2015 MS COCO Detection Challenge, our ION model won \"Best Student Entry\" and finished 3rd place overall. As intuition suggests, our detection results provide strong evidence that context and multi-scale representations improve small object detection."
    },
    {
        "paperId": "adcf9bdac7f05ba2bc003256a6794974aa571e0c",
        "url": "https://www.semanticscholar.org/paper/adcf9bdac7f05ba2bc003256a6794974aa571e0c",
        "title": "Learning to compare image patches via convolutional neural networks",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2015,
        "citationCount": 1464,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1504.03641",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1504.03641, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2134433",
                "name": "Sergey Zagoruyko"
            },
            {
                "authorId": "2505902",
                "name": "N. Komodakis"
            }
        ],
        "abstract": "In this paper we show how to learn directly from image data (i.e., without resorting to manually-designed features) a general similarity function for comparing image patches, which is a task of fundamental importance for many computer vision problems. To encode such a function, we opt for a CNN-based model that is trained to account for a wide variety of changes in image appearance. To that end, we explore and study multiple neural network architectures, which are specifically adapted to this task. We show that such an approach can significantly outperform the state-of-the-art on several problems and benchmark datasets."
    },
    {
        "paperId": "b0e52226bc29275698f35283060a97689b373490",
        "url": "https://www.semanticscholar.org/paper/b0e52226bc29275698f35283060a97689b373490",
        "title": "Cross-scene crowd counting via deep convolutional neural networks",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2015,
        "citationCount": 1159,
        "openAccessPdf": {
            "url": "http://www.ee.cuhk.edu.hk/%7Exgwang/papers/zhangLWYcvpr15.pdf",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR.2015.7298684?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR.2015.7298684, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2116343425",
                "name": "Cong Zhang"
            },
            {
                "authorId": "47893312",
                "name": "Hongsheng Li"
            },
            {
                "authorId": "93768810",
                "name": "Xiaogang Wang"
            },
            {
                "authorId": "1795291",
                "name": "Xiaokang Yang"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "b5f3e5d2912bedbcd9458952d664b08db6aed962",
        "url": "https://www.semanticscholar.org/paper/b5f3e5d2912bedbcd9458952d664b08db6aed962",
        "title": "Accurate Image Super-Resolution Using Very Deep Convolutional Networks",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2015,
        "citationCount": 6715,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1511.04587",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1511.04587, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3968500",
                "name": "Jiwon Kim"
            },
            {
                "authorId": "2119170990",
                "name": "Jung Kwon Lee"
            },
            {
                "authorId": "2135837",
                "name": "Kyoung Mu Lee"
            }
        ],
        "abstract": "We present a highly accurate single-image superresolution (SR) method. Our method uses a very deep convolutional network inspired by VGG-net used for ImageNet classification [19]. We find increasing our network depth shows a significant improvement in accuracy. Our final model uses 20 weight layers. By cascading small filters many times in a deep network structure, contextual information over large image regions is exploited in an efficient way. With very deep networks, however, convergence speed becomes a critical issue during training. We propose a simple yet effective training procedure. We learn residuals only and use extremely high learning rates (104 times higher than SRCNN [6]) enabled by adjustable gradient clipping. Our proposed method performs better than existing methods in accuracy and visual improvements in our results are easily noticeable."
    },
    {
        "paperId": "b73e2d40da901ad3da3813670a51c52803d7af7e",
        "url": "https://www.semanticscholar.org/paper/b73e2d40da901ad3da3813670a51c52803d7af7e",
        "title": "SUN RGB-D: A RGB-D scene understanding benchmark suite",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2015,
        "citationCount": 1968,
        "openAccessPdf": {
            "url": "http://vision.cs.princeton.edu/projects/2015/SUNrgbd/paper.pdf",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR.2015.7298655?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR.2015.7298655, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3340170",
                "name": "Shuran Song"
            },
            {
                "authorId": "39243526",
                "name": "Samuel P. Lichtenberg"
            },
            {
                "authorId": "40599257",
                "name": "Jianxiong Xiao"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "c10d25ca31df02571df8958d531995e7bbf6d0b3",
        "url": "https://www.semanticscholar.org/paper/c10d25ca31df02571df8958d531995e7bbf6d0b3",
        "title": "DeepCut: Joint Subset Partition and Labeling for Multi Person Pose Estimation",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2015,
        "citationCount": 1013,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/1511.06645",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1511.06645, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2299109",
                "name": "L. Pishchulin"
            },
            {
                "authorId": "3205238",
                "name": "Eldar Insafutdinov"
            },
            {
                "authorId": "1831081930",
                "name": "Siyu Tang"
            },
            {
                "authorId": "16576043",
                "name": "Bjoern Andres"
            },
            {
                "authorId": "1906895",
                "name": "Mykhaylo Andriluka"
            },
            {
                "authorId": "2871555",
                "name": "Peter Gehler"
            },
            {
                "authorId": "48920094",
                "name": "B. Schiele"
            }
        ],
        "abstract": "This paper considers the task of articulated human pose estimation of multiple people in real world images. We propose an approach that jointly solves the tasks of detection and pose estimation: it infers the number of persons in a scene, identifies occluded body parts, and disambiguates body parts between people in close proximity of each other. This joint formulation is in contrast to previous strategies, that address the problem by first detecting people and subsequently estimating their body pose. We propose a partitioning and labeling formulation of a set of body-part hypotheses generated with CNN-based part detectors. Our formulation, an instance of an integer linear program, implicitly performs non-maximum suppression on the set of part candidates and groups them to form configurations of body parts respecting geometric and appearance constraints. Experiments on four different datasets demonstrate state-of-the-art results for both single person and multi person pose estimation."
    },
    {
        "paperId": "d094fb0af5bc6a26fa9c27d638c4a3a0725d8b5c",
        "url": "https://www.semanticscholar.org/paper/d094fb0af5bc6a26fa9c27d638c4a3a0725d8b5c",
        "title": "Towards Open Set Deep Networks",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2015,
        "citationCount": 1612,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/1511.06233",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1511.06233, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3274223",
                "name": "Abhijit Bendale"
            },
            {
                "authorId": "32163276",
                "name": "T. Boult"
            }
        ],
        "abstract": "Deep networks have produced significant gains for various visual recognition problems, leading to high impact academic and commercial applications. Recent work in deep networks highlighted that it is easy to generate images that humans would never classify as a particular object class, yet networks classify such images high confidence as that given class - deep network are easily fooled with images humans do not consider meaningful. The closed set nature of deep networks forces them to choose from one of the known classes leading to such artifacts. Recognition in the real world is open set, i.e. the recognition system should reject unknown/unseen classes at test time. We present a methodology to adapt deep networks for open set recognition, by introducing a new model layer, OpenMax, which estimates the probability of an input being from an unknown class. A key element of estimating the unknown probability is adapting Meta-Recognition concepts to the activation patterns in the penultimate layer of the network. Open-Max allows rejection of \"fooling\" and unrelated open set images presented to the system, OpenMax greatly reduces the number of obvious errors made by a deep network. We prove that the OpenMax concept provides bounded open space risk, thereby formally providing an open set recognition solution. We evaluate the resulting open set deep networks using pre-trained networks from the Caffe Model-zoo on ImageNet 2012 validation data, and thousands of fooling and open set images. The proposed OpenMax model significantly outperforms open set recognition accuracy of basic deep networks as well as deep networks with thresholding of SoftMax probabilities."
    },
    {
        "paperId": "d3cb9bad655197b52932978dd8186b36c512bf92",
        "url": "https://www.semanticscholar.org/paper/d3cb9bad655197b52932978dd8186b36c512bf92",
        "title": "Quantized Convolutional Neural Networks for Mobile Devices",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2015,
        "citationCount": 1214,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/1512.06473",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1512.06473, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2321921828",
                "name": "Jiaxiang Wu"
            },
            {
                "authorId": "3259574",
                "name": "Cong Leng"
            },
            {
                "authorId": "2115768978",
                "name": "Yuhang Wang"
            },
            {
                "authorId": "2571792",
                "name": "Qinghao Hu"
            },
            {
                "authorId": "143949499",
                "name": "Jian Cheng"
            }
        ],
        "abstract": "Recently, convolutional neural networks (CNN) have demonstrated impressive performance in various computer vision tasks. However, high performance hardware is typically indispensable for the application of CNN models due to the high computation complexity, which prohibits their further extensions. In this paper, we propose an efficient framework, namely Quantized CNN, to simultaneously speed-up the computation and reduce the storage and memory overhead of CNN models. Both filter kernels in convolutional layers and weighting matrices in fully-connected layers are quantized, aiming at minimizing the estimation error of each layer's response. Extensive experiments on the ILSVRC-12 benchmark demonstrate 4 ~ 6 speed-up and 15 ~ 20 compression with merely one percentage loss of classification accuracy. With our quantized CNN model, even mobile devices can accurately classify images within one second."
    },
    {
        "paperId": "d7ce5665a72c0b607f484c1b448875f02ddfac3b",
        "url": "https://www.semanticscholar.org/paper/d7ce5665a72c0b607f484c1b448875f02ddfac3b",
        "title": "DenseCap: Fully Convolutional Localization Networks for Dense Captioning",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2015,
        "citationCount": 1212,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/1511.07571",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1511.07571, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2115231104",
                "name": "Justin Johnson"
            },
            {
                "authorId": "2354728",
                "name": "A. Karpathy"
            },
            {
                "authorId": "48004138",
                "name": "Li Fei-Fei"
            }
        ],
        "abstract": "We introduce the dense captioning task, which requires a computer vision system to both localize and describe salient regions in images in natural language. The dense captioning task generalizes object detection when the descriptions consist of a single word, and Image Captioning when one predicted region covers the full image. To address the localization and description task jointly we propose a Fully Convolutional Localization Network (FCLN) architecture that processes an image with a single, efficient forward pass, requires no external regions proposals, and can be trained end-to-end with a single round of optimization. The architecture is composed of a Convolutional Network, a novel dense localization layer, and Recurrent Neural Network language model that generates the label sequences. We evaluate our network on the Visual Genome dataset, which comprises 94,000 images and 4,100,000 region-grounded captions. We observe both speed and accuracy improvements over baselines based on current state of the art approaches in both generation and retrieval settings."
    },
    {
        "paperId": "df658828fb4146877f1031ec9d07052f7eb31186",
        "url": "https://www.semanticscholar.org/paper/df658828fb4146877f1031ec9d07052f7eb31186",
        "title": "Action recognition with trajectory-pooled deep-convolutional descriptors",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2015,
        "citationCount": 1187,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1505.04868",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1505.04868, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2109120086",
                "name": "Limin Wang"
            },
            {
                "authorId": "143970608",
                "name": "Y. Qiao"
            },
            {
                "authorId": "50295995",
                "name": "Xiaoou Tang"
            }
        ],
        "abstract": "Visual features are of vital importance for human action understanding in videos. This paper presents a new video representation, called trajectory-pooled deep-convolutional descriptor (TDD), which shares the merits of both hand-crafted features [31] and deep-learned features [24]. Specifically, we utilize deep architectures to learn discriminative convolutional feature maps, and conduct trajectory-constrained pooling to aggregate these convolutional features into effective descriptors. To enhance the robustness of TDDs, we design two normalization methods to transform convolutional feature maps, namely spatiotemporal normalization and channel normalization. The advantages of our features come from (i) TDDs are automatically learned and contain high discriminative capacity compared with those hand-crafted features; (ii) TDDs take account of the intrinsic characteristics of temporal dimension and introduce the strategies of trajectory-constrained sampling and pooling for aggregating deep-learned features. We conduct experiments on two challenging datasets: HMD-B51 and UCF101. Experimental results show that TDDs outperform previous hand-crafted features [31] and deep-learned features [24]. Our method also achieves superior performance to the state of the art on these datasets."
    },
    {
        "paperId": "e37f2fb8d6e675abbcda3bd09d586b9aaec26486",
        "url": "https://www.semanticscholar.org/paper/e37f2fb8d6e675abbcda3bd09d586b9aaec26486",
        "title": "DynamicFusion: Reconstruction and tracking of non-rigid scenes in real-time",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2015,
        "citationCount": 1051,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR.2015.7298631?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR.2015.7298631, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "50366818",
                "name": "Richard A. Newcombe"
            },
            {
                "authorId": "145197953",
                "name": "D. Fox"
            },
            {
                "authorId": "1679223",
                "name": "S. Seitz"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "e65142010431ffc089b272a1174214e00693e503",
        "url": "https://www.semanticscholar.org/paper/e65142010431ffc089b272a1174214e00693e503",
        "title": "Generation and Comprehension of Unambiguous Object Descriptions",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2015,
        "citationCount": 1552,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/1511.02283",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1511.02283, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "36010601",
                "name": "Junhua Mao"
            },
            {
                "authorId": "2136435893",
                "name": "Jonathan Huang"
            },
            {
                "authorId": "1726415",
                "name": "Alexander Toshev"
            },
            {
                "authorId": "3317152",
                "name": "Oana-Maria Camburu"
            },
            {
                "authorId": "145081362",
                "name": "A. Yuille"
            },
            {
                "authorId": "1702318",
                "name": "K. Murphy"
            }
        ],
        "abstract": "We propose a method that can generate an unambiguous description (known as a referring expression) of a specific object or region in an image, and which can also comprehend or interpret such an expression to infer which object is being described. We show that our method outperforms previous methods that generate descriptions of objects without taking into account other potentially ambiguous objects in the scene. Our model is inspired by recent successes of deep learning methods for image captioning, but while image captioning is difficult to evaluate, our task allows for easy objective evaluation. We also present a new large-scale dataset for referring expressions, based on MSCOCO. We have released the dataset and a toolbox for visualization and evaluation, see https://github.com/ mjhucla/Google_Refexp_toolbox."
    },
    {
        "paperId": "edf455c3b5b8d1c6337c72e39940125036354d03",
        "url": "https://www.semanticscholar.org/paper/edf455c3b5b8d1c6337c72e39940125036354d03",
        "title": "Object scene flow for autonomous vehicles",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2015,
        "citationCount": 2168,
        "openAccessPdf": {
            "url": "http://www.cvlibs.net/publications/Menze2015CVPR.pdf",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR.2015.7298925?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR.2015.7298925, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "101841672",
                "name": "Moritz Menze"
            },
            {
                "authorId": "47237027",
                "name": "Andreas Geiger"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "f8e79ac0ea341056ef20f2616628b3e964764cfd",
        "url": "https://www.semanticscholar.org/paper/f8e79ac0ea341056ef20f2616628b3e964764cfd",
        "title": "You Only Look Once: Unified, Real-Time Object Detection",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2015,
        "citationCount": 42306,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1506.02640",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1506.02640, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "40497777",
                "name": "Joseph Redmon"
            },
            {
                "authorId": "2038685",
                "name": "S. Divvala"
            },
            {
                "authorId": "2983898",
                "name": "Ross B. Girshick"
            },
            {
                "authorId": "143787583",
                "name": "Ali Farhadi"
            }
        ],
        "abstract": "We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is less likely to predict false positives on background. Finally, YOLO learns very general representations of objects. It outperforms other detection methods, including DPM and R-CNN, when generalizing from natural images to other domains like artwork."
    },
    {
        "paperId": "f971a22287ead6aa23ecd84a4afd8efca57cee3c",
        "url": "https://www.semanticscholar.org/paper/f971a22287ead6aa23ecd84a4afd8efca57cee3c",
        "title": "NetVLAD: CNN Architecture for Weakly Supervised Place Recognition",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2015,
        "citationCount": 2950,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/1511.07247",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1511.07247, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2299479",
                "name": "Relja Arandjelovi"
            },
            {
                "authorId": "3413968",
                "name": "Petr Gront"
            },
            {
                "authorId": "34395018",
                "name": "A. Torii"
            },
            {
                "authorId": "1758039",
                "name": "T. Pajdla"
            },
            {
                "authorId": "1782755",
                "name": "Josef Sivic"
            }
        ],
        "abstract": "We tackle the problem of large scale visual place recognition, where the task is to quickly and accurately recognize the location of a given query photograph. We present the following three principal contributions. First, we develop a convolutional neural network (CNN) architecture that is trainable in an end-to-end manner directly for the place recognition task. The main component of this architecture, NetVLAD, is a new generalized VLAD layer, inspired by the \"Vector of Locally Aggregated Descriptors\" image representation commonly used in image retrieval. The layer is readily pluggable into any CNN architecture and amenable to training via backpropagation. Second, we develop a training procedure, based on a new weakly supervised ranking loss, to learn parameters of the architecture in an end-to-end manner from images depicting the same places over time downloaded from Google Street View Time Machine. Finally, we show that the proposed architecture significantly outperforms non-learnt image representations and off-the-shelf CNN descriptors on two challenging place recognition benchmarks, and improves over current state of-the-art compact image representations on standard image retrieval benchmarks."
    },
    {
        "paperId": "0366b36006a6b37c673a42aad03ae77e8ef6ecda",
        "url": "https://www.semanticscholar.org/paper/0366b36006a6b37c673a42aad03ae77e8ef6ecda",
        "title": "Fully Convolutional Instance-Aware Semantic Segmentation",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2016,
        "citationCount": 1042,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1611.07709",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1611.07709, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2153682629",
                "name": "Yi Li"
            },
            {
                "authorId": "7217794",
                "name": "Haozhi Qi"
            },
            {
                "authorId": "3304536",
                "name": "Jifeng Dai"
            },
            {
                "authorId": "7807689",
                "name": "Xiangyang Ji"
            },
            {
                "authorId": "1732264",
                "name": "Yichen Wei"
            }
        ],
        "abstract": "We present the first fully convolutional end-to-end solution for instance-aware semantic segmentation task. It inherits all the merits of FCNs for semantic segmentation [29] and instance mask proposal [5]. It performs instance mask prediction and classification jointly. The underlying convolutional representation is fully shared between the two sub-tasks, as well as between all regions of interest. The network architecture is highly integrated and efficient. It achieves state-of-the-art performance in both accuracy and efficiency. It wins the COCO 2016 segmentation competition by a large margin. Code would be released at https://github.com/daijifeng001/TA-FCN."
    },
    {
        "paperId": "03a5b2aac53443e6078f0f63b35d4f95d6d54c5d",
        "url": "https://www.semanticscholar.org/paper/03a5b2aac53443e6078f0f63b35d4f95d6d54c5d",
        "title": "Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2016,
        "citationCount": 5680,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/1609.05158",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1609.05158, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "46810836",
                "name": "Wenzhe Shi"
            },
            {
                "authorId": "145372820",
                "name": "Jose Caballero"
            },
            {
                "authorId": "3108066",
                "name": "Ferenc Huszr"
            },
            {
                "authorId": "1853456",
                "name": "J. Totz"
            },
            {
                "authorId": "49931957",
                "name": "Andrew P. Aitken"
            },
            {
                "authorId": "2073899761",
                "name": "Rob Bishop"
            },
            {
                "authorId": "1717710",
                "name": "D. Rueckert"
            },
            {
                "authorId": "34627233",
                "name": "Zehan Wang"
            }
        ],
        "abstract": "Recently, several models based on deep neural networks have achieved great success in terms of both reconstruction accuracy and computational performance for single image super-resolution. In these methods, the low resolution (LR) input image is upscaled to the high resolution (HR) space using a single filter, commonly bicubic interpolation, before reconstruction. This means that the super-resolution (SR) operation is performed in HR space. We demonstrate that this is sub-optimal and adds computational complexity. In this paper, we present the first convolutional neural network (CNN) capable of real-time SR of 1080p videos on a single K2 GPU. To achieve this, we propose a novel CNN architecture where the feature maps are extracted in the LR space. In addition, we introduce an efficient sub-pixel convolution layer which learns an array of upscaling filters to upscale the final LR feature maps into the HR output. By doing so, we effectively replace the handcrafted bicubic filter in the SR pipeline with more complex upscaling filters specifically trained for each feature map, whilst also reducing the computational complexity of the overall SR operation. We evaluate the proposed approach using images and videos from publicly available datasets and show that it performs significantly better (+0.15dB on Images and +0.39dB on Videos) and is an order of magnitude faster than previous CNN-based methods."
    },
    {
        "paperId": "03eb382e04cca8cca743f7799070869954f1402a",
        "url": "https://www.semanticscholar.org/paper/03eb382e04cca8cca743f7799070869954f1402a",
        "title": "CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2016,
        "citationCount": 2656,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1612.06890",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1612.06890, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2115231104",
                "name": "Justin Johnson"
            },
            {
                "authorId": "73710317",
                "name": "B. Hariharan"
            },
            {
                "authorId": "1803520",
                "name": "L. Maaten"
            },
            {
                "authorId": "48004138",
                "name": "Li Fei-Fei"
            },
            {
                "authorId": "1699161",
                "name": "C. L. Zitnick"
            },
            {
                "authorId": "2983898",
                "name": "Ross B. Girshick"
            }
        ],
        "abstract": "When building artificial intelligence systems that can reason and answer questions about visual data, we need diagnostic tests to analyze our progress and discover short-comings. Existing benchmarks for visual question answering can help, but have strong biases that models can exploit to correctly answer questions without reasoning. They also conflate multiple sources of error, making it hard to pinpoint model weaknesses. We present a diagnostic dataset that tests a range of visual reasoning abilities. It contains minimal biases and has detailed annotations describing the kind of reasoning each question requires. We use this dataset to analyze a variety of modern visual reasoning systems, providing novel insights into their abilities and limitations."
    },
    {
        "paperId": "05e9e85b5137016c93d042170e82f77bb551a108",
        "url": "https://www.semanticscholar.org/paper/05e9e85b5137016c93d042170e82f77bb551a108",
        "title": "A Benchmark Dataset and Evaluation Methodology for Video Object Segmentation",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2016,
        "citationCount": 2189,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR.2016.85?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR.2016.85, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2942259",
                "name": "Federico Perazzi"
            },
            {
                "authorId": "1403171438",
                "name": "J. Pont-Tuset"
            },
            {
                "authorId": "2953855",
                "name": "B. McWilliams"
            },
            {
                "authorId": "1681236",
                "name": "L. Gool"
            },
            {
                "authorId": "2280075525",
                "name": "M. Gross"
            },
            {
                "authorId": "1388791172",
                "name": "A. Sorkine-Hornung"
            }
        ],
        "abstract": "Over the years, datasets and benchmarks have proven their fundamental importance in computer vision research, enabling targeted progress and objective comparisons in many fields. At the same time, legacy datasets may impend the evolution of a field due to saturated algorithm performance and the lack of contemporary, high quality data. In this work we present a new benchmark dataset and evaluation methodology for the area of video object segmentation. The dataset, named DAVIS (Densely Annotated VIdeo Segmentation), consists of fifty high quality, Full HD video sequences, spanning multiple occurrences of common video object segmentation challenges such as occlusions, motionblur and appearance changes. Each video is accompanied by densely annotated, pixel-accurate and per-frame ground truth segmentation. In addition, we provide a comprehensive analysis of several state-of-the-art segmentation approaches using three complementary metrics that measure the spatial extent of the segmentation, the accuracy of the silhouette contours and the temporal coherence. The results uncover strengths and weaknesses of current approaches, opening up promising directions for future works."
    },
    {
        "paperId": "091e4d3c85dc0a8212afea875cd3b162d273d46b",
        "url": "https://www.semanticscholar.org/paper/091e4d3c85dc0a8212afea875cd3b162d273d46b",
        "title": "NTU RGB+D: A Large Scale Dataset for 3D Human Activity Analysis",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2016,
        "citationCount": 2774,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/1604.02808",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1604.02808, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3000984",
                "name": "Amir Shahroudy"
            },
            {
                "authorId": "40940512",
                "name": "Jun Liu"
            },
            {
                "authorId": "2475944",
                "name": "T. Ng"
            },
            {
                "authorId": "2096527",
                "name": "G. Wang"
            }
        ],
        "abstract": "Recent approaches in depth-based human activity analysis achieved outstanding performance and proved the effectiveness of 3D representation for classification of action classes. Currently available depth-based and RGB+Dbased action recognition benchmarks have a number of limitations, including the lack of training samples, distinct class labels, camera views and variety of subjects. In this paper we introduce a large-scale dataset for RGB+D human action recognition with more than 56 thousand video samples and 4 million frames, collected from 40 distinct subjects. Our dataset contains 60 different action classes including daily, mutual, and health-related actions. In addition, we propose a new recurrent neural network structure to model the long-term temporal correlation of the features for each body part, and utilize them for better action classification. Experimental results show the advantages of applying deep learning methods over state-of-the-art handcrafted features on the suggested cross-subject and cross-view evaluation criteria for our dataset. The introduction of this large scale dataset will enable the community to apply, develop and adapt various data-hungry learning techniques for the task of depth-based and RGB+D-based human activity analysis."
    },
    {
        "paperId": "1031a69923b80ad01cf3fbb703d10757a80e699b",
        "url": "https://www.semanticscholar.org/paper/1031a69923b80ad01cf3fbb703d10757a80e699b",
        "title": "Pyramid Scene Parsing Network",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2016,
        "citationCount": 13412,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1612.01105",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1612.01105, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3459894",
                "name": "Hengshuang Zhao"
            },
            {
                "authorId": "1788070",
                "name": "Jianping Shi"
            },
            {
                "authorId": "50844674",
                "name": "Xiaojuan Qi"
            },
            {
                "authorId": "31843833",
                "name": "Xiaogang Wang"
            },
            {
                "authorId": "1729056",
                "name": "Jiaya Jia"
            }
        ],
        "abstract": "Scene parsing is challenging for unrestricted open vocabulary and diverse scenes. In this paper, we exploit the capability of global context information by different-region-based context aggregation through our pyramid pooling module together with the proposed pyramid scene parsing network (PSPNet). Our global prior representation is effective to produce good quality results on the scene parsing task, while PSPNet provides a superior framework for pixel-level prediction. The proposed approach achieves state-of-the-art performance on various datasets. It came first in ImageNet scene parsing challenge 2016, PASCAL VOC 2012 benchmark and Cityscapes benchmark. A single PSPNet yields the new record of mIoU accuracy 85.4% on PASCAL VOC 2012 and accuracy 80.2% on Cityscapes."
    },
    {
        "paperId": "16aa01ca0834a924c25faad5d8bfef3fd1acfcfe",
        "url": "https://www.semanticscholar.org/paper/16aa01ca0834a924c25faad5d8bfef3fd1acfcfe",
        "title": "Universal Adversarial Perturbations",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2016,
        "citationCount": 2703,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1610.08401",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1610.08401, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1403182206",
                "name": "Seyed-Mohsen Moosavi-Dezfooli"
            },
            {
                "authorId": "33054064",
                "name": "Alhussein Fawzi"
            },
            {
                "authorId": "145602557",
                "name": "Omar Fawzi"
            },
            {
                "authorId": "1703189",
                "name": "P. Frossard"
            }
        ],
        "abstract": "Given a state-of-the-art deep neural network classifier, we show the existence of a universal (image-agnostic) and very small perturbation vector that causes natural images to be misclassified with high probability. We propose a systematic algorithm for computing universal perturbations, and show that state-of-the-art deep neural networks are highly vulnerable to such perturbations, albeit being quasi-imperceptible to the human eye. We further empirically analyze these universal perturbations and show, in particular, that they generalize very well across neural networks. The surprising existence of universal perturbations reveals important geometric correlations among the high-dimensional decision boundary of classifiers. It further outlines potential security breaches with the existence of single directions in the input space that adversaries can possibly exploit to break a classifier on most natural images."
    },
    {
        "paperId": "1703631a938b397ba7e858161ce16448f6046d6f",
        "url": "https://www.semanticscholar.org/paper/1703631a938b397ba7e858161ce16448f6046d6f",
        "title": "iCaRL: Incremental Classifier and Representation Learning",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2016,
        "citationCount": 4320,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1611.07725",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1611.07725, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "8478422",
                "name": "Sylvestre-Alvise Rebuffi"
            },
            {
                "authorId": "144629422",
                "name": "Alexander Kolesnikov"
            },
            {
                "authorId": "2091487678",
                "name": "G. Sperl"
            },
            {
                "authorId": "1787591",
                "name": "Christoph H. Lampert"
            }
        ],
        "abstract": "A major open problem on the road to artificial intelligence is the development of incrementally learning systems that learn about more and more concepts over time from a stream of data. In this work, we introduce a new training strategy, iCaRL, that allows learning in such a class-incremental way: only the training data for a small number of classes has to be present at the same time and new classes can be added progressively. iCaRL learns strong classifiers and a data representation simultaneously. This distinguishes it from earlier works that were fundamentally limited to fixed data representations and therefore incompatible with deep learning architectures. We show by experiments on CIFAR-100 and ImageNet ILSVRC 2012 data that iCaRL can learn many classes incrementally over a long period of time where other strategies quickly fail."
    },
    {
        "paperId": "1b80416cc2b05954941ac7e7dcbcc358c10e5ace",
        "url": "https://www.semanticscholar.org/paper/1b80416cc2b05954941ac7e7dcbcc358c10e5ace",
        "title": "Visual Dialog",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2016,
        "citationCount": 1057,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1611.08669, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2113242192",
                "name": "Abhishek Das"
            },
            {
                "authorId": "2150275",
                "name": "Satwik Kottur"
            },
            {
                "authorId": "39855500",
                "name": "Khushi Gupta"
            },
            {
                "authorId": "1899992",
                "name": "Avi Singh"
            },
            {
                "authorId": "24508084",
                "name": "Deshraj Yadav"
            },
            {
                "authorId": "144915495",
                "name": "Jos M. F. Moura"
            },
            {
                "authorId": "153432684",
                "name": "Devi Parikh"
            },
            {
                "authorId": "1746610",
                "name": "Dhruv Batra"
            }
        ],
        "abstract": "We introduce the task of Visual Dialog, which requires an AI agent to hold a meaningful dialog with humans in natural, conversational language about visual content. Specifically, given an image, a dialog history, and a question about the image, the agent has to ground the question in image, infer context from history, and answer the question accurately. Visual Dialog is disentangled enough from a specific downstream task so as to serve as a general test of machine intelligence, while being grounded in vision enough to allow objective evaluation of individual responses and benchmark progress. We develop a novel two-person chat data-collection protocol to curate a large-scale Visual Dialog dataset (VisDial). VisDial contains 1 dialog (10 question-answer pairs) on ~140k images from the COCO dataset, with a total of ~1.4M dialog question-answer pairs. We introduce a family of neural encoder-decoder models for Visual Dialog with 3 encoders (Late Fusion, Hierarchical Recurrent Encoder and Memory Network) and 2 decoders (generative and discriminative), which outperform a number of sophisticated baselines. We propose a retrieval-based evaluation protocol for Visual Dialog where the AI agent is asked to sort a set of candidate answers and evaluated on metrics such as mean-reciprocal-rank of human response. We quantify gap between machine and human performance on the Visual Dialog task via human studies. Our dataset, code, and trained models will be released publicly at https://visualdialog.org. Putting it all together, we demonstrate the first visual chatbot!."
    },
    {
        "paperId": "210f258524deabc3d08cbbea4e4ca5c2a98f4846",
        "url": "https://www.semanticscholar.org/paper/210f258524deabc3d08cbbea4e4ca5c2a98f4846",
        "title": "Temporal Convolutional Networks for Action Segmentation and Detection",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2016,
        "citationCount": 1794,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/1611.05267",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1611.05267, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "98658818",
                "name": "Colin S. Lea"
            },
            {
                "authorId": "2055373985",
                "name": "Michael D. Flynn"
            },
            {
                "authorId": "144020730",
                "name": "R. Vidal"
            },
            {
                "authorId": "3207112",
                "name": "A. Reiter"
            },
            {
                "authorId": "1678633",
                "name": "Gregory Hager"
            }
        ],
        "abstract": "The ability to identify and temporally segment fine-grained human actions throughout a video is crucial for robotics, surveillance, education, and beyond. Typical approaches decouple this problem by first extracting local spatiotemporal features from video frames and then feeding them into a temporal classifier that captures high-level temporal patterns. We describe a class of temporal models, which we call Temporal Convolutional Networks (TCNs), that use a hierarchy of temporal convolutions to perform fine-grained action segmentation or detection. Our Encoder-Decoder TCN uses pooling and upsampling to efficiently capture long-range temporal patterns whereas our Dilated TCN uses dilated convolutions. We show that TCNs are capable of capturing action compositions, segment durations, and long-range dependencies, and are over a magnitude faster to train than competing LSTM-based Recurrent Neural Networks. We apply these models to three challenging fine-grained datasets and show large improvements over the state of the art."
    },
    {
        "paperId": "220ac48a22547a455d05f416e1fd22bbd0b0788d",
        "url": "https://www.semanticscholar.org/paper/220ac48a22547a455d05f416e1fd22bbd0b0788d",
        "title": "Unsupervised Pixel-Level Domain Adaptation with Generative Adversarial Networks",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2016,
        "citationCount": 1599,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1612.05424",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1612.05424, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2732737",
                "name": "Konstantinos Bousmalis"
            },
            {
                "authorId": "2286640",
                "name": "N. Silberman"
            },
            {
                "authorId": "35363891",
                "name": "David Dohan"
            },
            {
                "authorId": "1761978",
                "name": "D. Erhan"
            },
            {
                "authorId": "1707347",
                "name": "Dilip Krishnan"
            }
        ],
        "abstract": "Collecting well-annotated image datasets to train modern machine learning algorithms is prohibitively expensive for many tasks. One appealing alternative is rendering synthetic data where ground-truth annotations are generated automatically. Unfortunately, models trained purely on rendered images fail to generalize to real images. To address this shortcoming, prior work introduced unsupervised domain adaptation algorithms that have tried to either map representations between the two domains, or learn to extract features that are domain-invariant. In this work, we approach the problem in a new light by learning in an unsupervised manner a transformation in the pixel space from one domain to the other. Our generative adversarial network (GAN)-based method adapts source-domain images to appear as if drawn from the target domain. Our approach not only produces plausible samples, but also outperforms the state-of-the-art on a number of unsupervised domain adaptation scenarios by large margins. Finally, we demonstrate that the adaptation process generalizes to object classes unseen during training."
    },
    {
        "paperId": "2976605dc3b73377696537291d45f09f1ab1fbf5",
        "url": "https://www.semanticscholar.org/paper/2976605dc3b73377696537291d45f09f1ab1fbf5",
        "title": "Cross-Stitch Networks for Multi-task Learning",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2016,
        "citationCount": 1451,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/1604.03539",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1604.03539, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1806773",
                "name": "Ishan Misra"
            },
            {
                "authorId": "1781242",
                "name": "Abhinav Shrivastava"
            },
            {
                "authorId": "1726095131",
                "name": "A. Gupta"
            },
            {
                "authorId": "145670946",
                "name": "M. Hebert"
            }
        ],
        "abstract": "Multi-task learning in Convolutional Networks has displayed remarkable success in the field of recognition. This success can be largely attributed to learning shared representations from multiple supervisory tasks. However, existing multi-task approaches rely on enumerating multiple network architectures specific to the tasks at hand, that do not generalize. In this paper, we propose a principled approach to learn shared representations in ConvNets using multitask learning. Specifically, we propose a new sharing unit: \"cross-stitch\" unit. These units combine the activations from multiple networks and can be trained end-to-end. A network with cross-stitch units can learn an optimal combination of shared and task-specific representations. Our proposed method generalizes across multiple tasks and shows dramatically improved performance over baseline methods for categories with few training examples."
    },
    {
        "paperId": "2a94c84383ee3de5e6211d43d16e7de387f68878",
        "url": "https://www.semanticscholar.org/paper/2a94c84383ee3de5e6211d43d16e7de387f68878",
        "title": "Feature Pyramid Networks for Object Detection",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2016,
        "citationCount": 25137,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1612.03144",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1612.03144, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "33493200",
                "name": "Tsung-Yi Lin"
            },
            {
                "authorId": "3127283",
                "name": "Piotr Dollr"
            },
            {
                "authorId": "2983898",
                "name": "Ross B. Girshick"
            },
            {
                "authorId": "39353098",
                "name": "Kaiming He"
            },
            {
                "authorId": "1790580",
                "name": "Bharath Hariharan"
            },
            {
                "authorId": "50172592",
                "name": "Serge J. Belongie"
            }
        ],
        "abstract": "Feature pyramids are a basic component in recognition systems for detecting objects at different scales. But pyramid representations have been avoided in recent object detectors that are based on deep convolutional networks, partially because they are slow to compute and memory intensive. In this paper, we exploit the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost. A top-down architecture with lateral connections is developed for building high-level semantic feature maps at all scales. This architecture, called a Feature Pyramid Network (FPN), shows significant improvement as a generic feature extractor in several applications. Using a basic Faster R-CNN system, our method achieves state-of-the-art single-model results on the COCO detection benchmark without bells and whistles, surpassing all existing single-model entries including those from the COCO 2016 challenge winners. In addition, our method can run at 5 FPS on a GPU and thus is a practical and accurate solution to multi-scale object detection. Code will be made publicly available."
    },
    {
        "paperId": "2dc3b3eff8ded8914c8b536d05ee713ff0cdf3cd",
        "url": "https://www.semanticscholar.org/paper/2dc3b3eff8ded8914c8b536d05ee713ff0cdf3cd",
        "title": "Single-Image Crowd Counting via Multi-Column Convolutional Neural Network",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2016,
        "citationCount": 1958,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR.2016.70?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR.2016.70, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": null,
                "name": "Yingying Zhang"
            },
            {
                "authorId": "7533195",
                "name": "Desen Zhou"
            },
            {
                "authorId": "2211606460",
                "name": "Siqin Chen"
            },
            {
                "authorId": "1702868",
                "name": "Shenghua Gao"
            },
            {
                "authorId": "50032052",
                "name": "Yi Ma"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "376aad5815f808c82e518956a70091bf828dbd25",
        "url": "https://www.semanticscholar.org/paper/376aad5815f808c82e518956a70091bf828dbd25",
        "title": "Person Re-identification by Multi-Channel Parts-Based CNN with Improved Triplet Loss Function",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2016,
        "citationCount": 1304,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR.2016.149?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR.2016.149, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "145067864",
                "name": "De Cheng"
            },
            {
                "authorId": "144768792",
                "name": "Yihong Gong"
            },
            {
                "authorId": "3373601",
                "name": "Sanping Zhou"
            },
            {
                "authorId": "71563118",
                "name": "Jinjun Wang"
            },
            {
                "authorId": "122737130",
                "name": "N. Zheng"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "385ae5201434ac8d903f1f6bb1b0d420a1ef2c4f",
        "url": "https://www.semanticscholar.org/paper/385ae5201434ac8d903f1f6bb1b0d420a1ef2c4f",
        "title": "DeepFashion: Powering Robust Clothes Recognition and Retrieval with Rich Annotations",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2016,
        "citationCount": 1851,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR.2016.124?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR.2016.124, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2117940996",
                "name": "Ziwei Liu"
            },
            {
                "authorId": "47571885",
                "name": "Ping Luo"
            },
            {
                "authorId": "2146345714",
                "name": "Shi Qiu"
            },
            {
                "authorId": "31843833",
                "name": "Xiaogang Wang"
            },
            {
                "authorId": "50295995",
                "name": "Xiaoou Tang"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "3d1e82b69663758a1db87fbebed6525d23090146",
        "url": "https://www.semanticscholar.org/paper/3d1e82b69663758a1db87fbebed6525d23090146",
        "title": "ScribbleSup: Scribble-Supervised Convolutional Networks for Semantic Segmentation",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2016,
        "citationCount": 1073,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/1604.05144",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1604.05144, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "39870324",
                "name": "Di Lin"
            },
            {
                "authorId": "3304536",
                "name": "Jifeng Dai"
            },
            {
                "authorId": "1729056",
                "name": "Jiaya Jia"
            },
            {
                "authorId": "39353098",
                "name": "Kaiming He"
            },
            {
                "authorId": null,
                "name": "Jian Sun"
            }
        ],
        "abstract": "Large-scale data is of crucial importance for learning semantic segmentation models, but annotating per-pixel masks is a tedious and inefficient procedure. We note that for the topic of interactive image segmentation, scribbles are very widely used in academic research and commercial software, and are recognized as one of the most userfriendly ways of interacting. In this paper, we propose to use scribbles to annotate images, and develop an algorithm to train convolutional networks for semantic segmentation supervised by scribbles. Our algorithm is based on a graphical model that jointly propagates information from scribbles to unmarked pixels and learns network parameters. We present competitive object semantic segmentation results on the PASCAL VOC dataset by using scribbles as annotations. Scribbles are also favored for annotating stuff (e.g., water, sky, grass) that has no well-defined shape, and our method shows excellent results on the PASCALCONTEXT dataset thanks to extra inexpensive scribble annotations. Our scribble annotations on PASCAL VOC are available at http://research.microsoft.com/en-us/um/ people/jifdai/downloads/scribble_sup."
    },
    {
        "paperId": "400eb5386b13c32968fee796c71dec32aa754f1e",
        "url": "https://www.semanticscholar.org/paper/400eb5386b13c32968fee796c71dec32aa754f1e",
        "title": "Synthetic Data for Text Localisation in Natural Images",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2016,
        "citationCount": 1504,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1604.06646",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1604.06646, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2110759501",
                "name": "Ankush Gupta"
            },
            {
                "authorId": "1687524",
                "name": "A. Vedaldi"
            },
            {
                "authorId": "1688869",
                "name": "Andrew Zisserman"
            }
        ],
        "abstract": "In this paper we introduce a new method for text detection in natural images. The method comprises two contributions: First, a fast and scalable engine to generate synthetic images of text in clutter. This engine overlays synthetic text to existing background images in a natural way, accounting for the local 3D scene geometry. Second, we use the synthetic images to train a Fully-Convolutional Regression Network (FCRN) which efficiently performs text detection and bounding-box regression at all locations and multiple scales in an image. We discuss the relation of FCRN to the recently-introduced YOLO detector, as well as other end-to-end object detection systems based on deep learning. The resulting detection network significantly out performs current methods for text detection in natural images, achieving an F-measure of 84.2% on the standard ICDAR 2013 benchmark. Furthermore, it can process 15 images per second on a GPU."
    },
    {
        "paperId": "41d08fb733f3e50ac183490f84d6377dffccf350",
        "url": "https://www.semanticscholar.org/paper/41d08fb733f3e50ac183490f84d6377dffccf350",
        "title": "A Point Set Generation Network for 3D Object Reconstruction from a Single Image",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2016,
        "citationCount": 2481,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1612.00603",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1612.00603, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1934546",
                "name": "Haoqiang Fan"
            },
            {
                "authorId": "144914140",
                "name": "Hao Su"
            },
            {
                "authorId": "51352814",
                "name": "L. Guibas"
            }
        ],
        "abstract": "Generation of 3D data by deep neural network has been attracting increasing attention in the research community. The majority of extant works resort to regular representations such as volumetric grids or collection of images, however, these representations obscure the natural invariance of 3D shapes under geometric transformations, and also suffer from a number of other issues. In this paper we address the problem of 3D reconstruction from a single image, generating a straight-forward form of output &#x2013; point cloud coordinates. Along with this problem arises a unique and interesting issue, that the groundtruth shape for an input image may be ambiguous. Driven by this unorthordox output form and the inherent ambiguity in groundtruth, we design architecture, loss function and learning paradigm that are novel and effective. Our final solution is a conditional shape sampler, capable of predicting multiple plausible 3D point clouds from an input image. In experiments not only can our system outperform state-of-the-art methods on single image based 3D reconstruction benchmarks, but it also shows strong performance for 3D shape completion and promising ability in making multiple plausible predictions."
    },
    {
        "paperId": "4463dc4a32b948f0230f3b782cbfecaf1c9e5b1d",
        "url": "https://www.semanticscholar.org/paper/4463dc4a32b948f0230f3b782cbfecaf1c9e5b1d",
        "title": "Unsupervised Monocular Depth Estimation with Left-Right Consistency",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2016,
        "citationCount": 3115,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1609.03677",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1609.03677, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "31082236",
                "name": "Clment Godard"
            },
            {
                "authorId": "2918822",
                "name": "Oisin Mac Aodha"
            },
            {
                "authorId": "3309893",
                "name": "G. Brostow"
            }
        ],
        "abstract": "Learning based methods have shown very promising results for the task of depth estimation in single images. However, most existing approaches treat depth prediction as a supervised regression problem and as a result, require vast quantities of corresponding ground truth depth data for training. Just recording quality depth data in a range of environments is a challenging problem. In this paper, we innovate beyond existing approaches, replacing the use of explicit depth data during training with easier-to-obtain binocular stereo footage. We propose a novel training objective that enables our convolutional neural network to learn to perform single image depth estimation, despite the absence of ground truth depth data. Ex-ploiting epipolar geometry constraints, we generate disparity images by training our network with an image reconstruction loss. We show that solving for image reconstruction alone results in poor quality depth images. To overcome this problem, we propose a novel training loss that enforces consistency between the disparities produced relative to both the left and right images, leading to improved performance and robustness compared to existing approaches. Our method produces state of the art results for monocular depth estimation on the KITTI driving dataset, even outperforming supervised methods that have been trained with ground truth depth."
    },
    {
        "paperId": "4cdcf2ae5e1fafebd9b3613247a7b1962584da34",
        "url": "https://www.semanticscholar.org/paper/4cdcf2ae5e1fafebd9b3613247a7b1962584da34",
        "title": "Volumetric and Multi-view CNNs for Object Classification on 3D Data",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2016,
        "citationCount": 1619,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/1604.03265",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1604.03265, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "144329939",
                "name": "C. Qi"
            },
            {
                "authorId": "144914140",
                "name": "Hao Su"
            },
            {
                "authorId": "2209612",
                "name": "M. Niener"
            },
            {
                "authorId": "2208531",
                "name": "Angela Dai"
            },
            {
                "authorId": "3235234",
                "name": "Mengyuan Yan"
            },
            {
                "authorId": "51352814",
                "name": "L. Guibas"
            }
        ],
        "abstract": "3D shape models are becoming widely available and easier to capture, making available 3D information crucial for progress in object classification. Current state-of-theart methods rely on CNNs to address this problem. Recently, we witness two types of CNNs being developed: CNNs based upon volumetric representations versus CNNs based upon multi-view representations. Empirical results from these two types of CNNs exhibit a large gap, indicating that existing volumetric CNN architectures and approaches are unable to fully exploit the power of 3D representations. In this paper, we aim to improve both volumetric CNNs and multi-view CNNs according to extensive analysis of existing approaches. To this end, we introduce two distinct network architectures of volumetric CNNs. In addition, we examine multi-view CNNs, where we introduce multiresolution filtering in 3D. Overall, we are able to outperform current state-of-the-art methods for both volumetric CNNs and multi-view CNNs. We provide extensive experiments designed to evaluate underlying design choices, thus providing a better understanding of the space of methods available for object classification on 3D data."
    },
    {
        "paperId": "511ff60d4aa5ab2660552f02ee85692425912b46",
        "url": "https://www.semanticscholar.org/paper/511ff60d4aa5ab2660552f02ee85692425912b46",
        "title": "OctNet: Learning Deep 3D Representations at High Resolutions",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2016,
        "citationCount": 1556,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1611.05009",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1611.05009, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "40310317",
                "name": "Gernot Riegler"
            },
            {
                "authorId": "2736582",
                "name": "Ali O. Ulusoy"
            },
            {
                "authorId": "47237027",
                "name": "Andreas Geiger"
            }
        ],
        "abstract": "We present OctNet, a representation for deep learning with sparse 3D data. In contrast to existing models, our representation enables 3D convolutional networks which are both deep and high resolution. Towards this goal, we exploit the sparsity in the input data to hierarchically partition the space using a set of unbalanced octrees where each leaf node stores a pooled feature representation. This allows to focus memory allocation and computation to the relevant dense regions and enables deeper networks without compromising resolution. We demonstrate the utility of our OctNet representation by analyzing the impact of resolution on several 3D tasks including 3D object classification, orientation estimation and point cloud labeling."
    },
    {
        "paperId": "5694e46284460a648fe29117cbc55f6c9be3fa3c",
        "url": "https://www.semanticscholar.org/paper/5694e46284460a648fe29117cbc55f6c9be3fa3c",
        "title": "Densely Connected Convolutional Networks",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2016,
        "citationCount": 40912,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1608.06993",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1608.06993, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "143983679",
                "name": "Gao Huang"
            },
            {
                "authorId": "2109168016",
                "name": "Zhuang Liu"
            },
            {
                "authorId": "7446832",
                "name": "Kilian Q. Weinberger"
            }
        ],
        "abstract": "Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections&#x2014;one between each layer and its subsequent layer&#x2014;our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less memory and computation to achieve high performance. Code and pre-trained models are available at https://github.com/liuzhuang13/DenseNet."
    },
    {
        "paperId": "5b589d7564dbef07ec98b3248f58a481b4ca1395",
        "url": "https://www.semanticscholar.org/paper/5b589d7564dbef07ec98b3248f58a481b4ca1395",
        "title": "3DMatch: Learning Local Geometric Descriptors from RGB-D Reconstructions",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2016,
        "citationCount": 1119,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1603.08182",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1603.08182, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "38591293",
                "name": "Andy Zeng"
            },
            {
                "authorId": "3340170",
                "name": "Shuran Song"
            },
            {
                "authorId": "2209612",
                "name": "M. Niener"
            },
            {
                "authorId": "145002004",
                "name": "Matthew Fisher"
            },
            {
                "authorId": "40599257",
                "name": "Jianxiong Xiao"
            },
            {
                "authorId": "1807080",
                "name": "T. Funkhouser"
            }
        ],
        "abstract": "Matching local geometric features on real-world depth images is a challenging task due to the noisy, low-resolution, and incomplete nature of 3D scan data. These difficulties limit the performance of current state-of-art methods, which are typically based on histograms over geometric properties. In this paper, we present 3DMatch, a data-driven model that learns a local volumetric patch descriptor for establishing correspondences between partial 3D data. To amass training data for our model, we propose a self-supervised feature learning method that leverages the millions of correspondence labels found in existing RGB-D reconstructions. Experiments show that our descriptor is not only able to match local geometry in new scenes for reconstruction, but also generalize to different tasks and spatial scales (e.g. instance-level object model alignment for the Amazon Picking Challenge, and mesh surface correspondence). Results show that 3DMatch consistently outperforms other state-of-the-art approaches by a significant margin. Code, data, benchmarks, and pre-trained models are available online at http://3dmatch.cs.princeton.edu."
    },
    {
        "paperId": "5b6ec746d309b165f9f9def873a2375b6fb40f3d",
        "url": "https://www.semanticscholar.org/paper/5b6ec746d309b165f9f9def873a2375b6fb40f3d",
        "title": "Xception: Deep Learning with Depthwise Separable Convolutions",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2016,
        "citationCount": 16597,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1610.02357",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1610.02357, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1565641737",
                "name": "Franois Chollet"
            }
        ],
        "abstract": "We present an interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and significantly outperforms Inception V3 on a larger image classification dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameters as Inception V3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters."
    },
    {
        "paperId": "63333669bcf694aba2e1928f6060ab1d6a5161fe",
        "url": "https://www.semanticscholar.org/paper/63333669bcf694aba2e1928f6060ab1d6a5161fe",
        "title": "Training Region-Based Object Detectors with Online Hard Example Mining",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2016,
        "citationCount": 2577,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1604.03540",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1604.03540, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1781242",
                "name": "Abhinav Shrivastava"
            },
            {
                "authorId": "1726095131",
                "name": "A. Gupta"
            },
            {
                "authorId": "2983898",
                "name": "Ross B. Girshick"
            }
        ],
        "abstract": "The field of object detection has made significant advances riding on the wave of region-based ConvNets, but their training procedure still includes many heuristics and hyperparameters that are costly to tune. We present a simple yet surprisingly effective online hard example mining (OHEM) algorithm for training region-based ConvNet detectors. Our motivation is the same as it has always been - detection datasets contain an overwhelming number of easy examples and a small number of hard examples. Automatic selection of these hard examples can make training more effective and efficient. OHEM is a simple and intuitive algorithm that eliminates several heuristics and hyperparameters in common use. But more importantly, it yields consistent and significant boosts in detection performance on benchmarks like PASCAL VOC 2007 and 2012. Its effectiveness increases as datasets become larger and more difficult, as demonstrated by the results on the MS COCO dataset. Moreover, combined with complementary advances in the field, OHEM leads to state-of-the-art results of 78.9% and 76.3% mAP on PASCAL VOC 2007 and 2012 respectively."
    },
    {
        "paperId": "65b16da51891a6b98140d425804c8a0fd0299219",
        "url": "https://www.semanticscholar.org/paper/65b16da51891a6b98140d425804c8a0fd0299219",
        "title": "Deep Multi-scale Convolutional Neural Network for Dynamic Scene Deblurring",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2016,
        "citationCount": 2250,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/1612.02177",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1612.02177, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "40648435",
                "name": "Seungjun Nah"
            },
            {
                "authorId": "2111251935",
                "name": "Tae Hyun Kim"
            },
            {
                "authorId": "2135837",
                "name": "Kyoung Mu Lee"
            }
        ],
        "abstract": "Non-uniform blind deblurring for general dynamic scenes is a challenging computer vision problem as blurs arise not only from multiple object motions but also from camera shake, scene depth variation. To remove these complicated motion blurs, conventional energy optimization based methods rely on simple assumptions such that blur kernel is partially uniform or locally linear. Moreover, recent machine learning based methods also depend on synthetic blur datasets generated under these assumptions. This makes conventional deblurring methods fail to remove blurs where blur kernel is difficult to approximate or parameterize (e.g. object motion boundaries). In this work, we propose a multi-scale convolutional neural network that restores sharp images in an end-to-end manner where blur is caused by various sources. Together, we present multi-scale loss function that mimics conventional coarse-to-fine approaches. Furthermore, we propose a new large-scale dataset that provides pairs of realistic blurry image and the corresponding ground truth sharp image that are obtained by a high-speed camera. With the proposed model trained on this dataset, we demonstrate empirically that our method achieves the state-of-the-art performance in dynamic scene deblurring not only qualitatively, but also quantitatively."
    },
    {
        "paperId": "68cb9fce1e6af2740377494350b650533c9a29e1",
        "url": "https://www.semanticscholar.org/paper/68cb9fce1e6af2740377494350b650533c9a29e1",
        "title": "Learning from Simulated and Unsupervised Images through Adversarial Training",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2016,
        "citationCount": 1857,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1612.07828",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1612.07828, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1490900960",
                "name": "A. Shrivastava"
            },
            {
                "authorId": "1945962",
                "name": "Tomas Pfister"
            },
            {
                "authorId": "2577513",
                "name": "Oncel Tuzel"
            },
            {
                "authorId": "49158771",
                "name": "J. Susskind"
            },
            {
                "authorId": "2108465550",
                "name": "Wenda Wang"
            },
            {
                "authorId": "51138986",
                "name": "Russ Webb"
            }
        ],
        "abstract": "With recent progress in graphics, it has become more tractable to train models on synthetic images, potentially avoiding the need for expensive annotations. However, learning from synthetic images may not achieve the desired performance due to a gap between synthetic and real image distributions. To reduce this gap, we propose Simulated+Unsupervised (S+U) learning, where the task is to learn a model to improve the realism of a simulators output using unlabeled real data, while preserving the annotation information from the simulator. We develop a method for S+U learning that uses an adversarial network similar to Generative Adversarial Networks (GANs), but with synthetic images as inputs instead of random vectors. We make several key modifications to the standard GAN algorithm to preserve annotations, avoid artifacts, and stabilize training: (i) a self-regularization term, (ii) a local adversarial loss, and (iii) updating the discriminator using a history of refined images. We show that this enables generation of highly realistic images, which we demonstrate both qualitatively and with a user study. We quantitatively evaluate the generated images by training models for gaze estimation and hand pose estimation. We show a significant improvement over using synthetic images, and achieve state-of-the-art results on the MPIIGaze dataset without any labeled real data."
    },
    {
        "paperId": "6c8353697cdbb98dfba4f493875778c4286d3e3a",
        "url": "https://www.semanticscholar.org/paper/6c8353697cdbb98dfba4f493875778c4286d3e3a",
        "title": "Self-Critical Sequence Training for Image Captioning",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2016,
        "citationCount": 2035,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1612.00563",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1612.00563, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2071376",
                "name": "Steven J. Rennie"
            },
            {
                "authorId": "2293163",
                "name": "E. Marcheret"
            },
            {
                "authorId": "2211263",
                "name": "Youssef Mroueh"
            },
            {
                "authorId": "39320489",
                "name": "Jerret Ross"
            },
            {
                "authorId": "1782589",
                "name": "Vaibhava Goel"
            }
        ],
        "abstract": "Recently it has been shown that policy-gradient methods for reinforcement learning can be utilized to train deep end-to-end systems directly on non-differentiable metrics for the task at hand. In this paper we consider the problem of optimizing image captioning systems using reinforcement learning, and show that by carefully optimizing our systems using the test metrics of the MSCOCO task, significant gains in performance can be realized. Our systems are built using a new optimization approach that we call self-critical sequence training (SCST). SCST is a form of the popular REINFORCE algorithm that, rather than estimating a baseline to normalize the rewards and reduce variance, utilizes the output of its own test-time inference algorithm to normalize the rewards it experiences. Using this approach, estimating the reward signal (as actor-critic methods must do) and estimating normalization (as REINFORCE algorithms typically do) is avoided, while at the same time harmonizing the model with respect to its test-time inference procedure. Empirically we find that directly optimizing the CIDEr metric with SCST and greedy decoding at test-time is highly effective. Our results on the MSCOCO evaluation sever establish a new state-of-the-art on the task, improving the best result in terms of CIDEr from 104.9 to 114.7."
    },
    {
        "paperId": "6de27f256e97c014a95706c72af7fa651e8fa34a",
        "url": "https://www.semanticscholar.org/paper/6de27f256e97c014a95706c72af7fa651e8fa34a",
        "title": "Deep Joint Rain Detection and Removal from a Single Image",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2016,
        "citationCount": 1172,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/1609.07769",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1609.07769, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1898172",
                "name": "Wenhan Yang"
            },
            {
                "authorId": "1726720",
                "name": "R. Tan"
            },
            {
                "authorId": "33221685",
                "name": "Jiashi Feng"
            },
            {
                "authorId": "41127426",
                "name": "Jiaying Liu"
            },
            {
                "authorId": "35310979",
                "name": "Zongming Guo"
            },
            {
                "authorId": "143653681",
                "name": "Shuicheng Yan"
            }
        ],
        "abstract": "In this paper, we address a rain removal problem from a single image, even in the presence of heavy rain and rain streak accumulation. Our core ideas lie in our new rain image model and new deep learning architecture. We add a binary map that provides rain streak locations to an existing model, which comprises a rain streak layer and a background layer. We create a model consisting of a component representing rain streak accumulation (where individual streaks cannot be seen, and thus visually similar to mist or fog), and another component representing various shapes and directions of overlapping rain streaks, which usually happen in heavy rain. Based on the model, we develop a multi-task deep learning architecture that learns the binary rain streak map, the appearance of rain streaks, and the clean background, which is our ultimate output. The additional binary map is critically beneficial, since its loss function can provide additional strong information to the network. To handle rain streak accumulation (again, a phenomenon visually similar to mist or fog) and various shapes and directions of overlapping rain streaks, we propose a recurrent rain detection and removal network that removes rain streaks and clears up the rain accumulation iteratively and progressively. In each recurrence of our method, a new contextualized dilated network is developed to exploit regional contextual information and to produce better representations for rain detection. The evaluation on real images, particularly on heavy rain, shows the effectiveness of our models and architecture."
    },
    {
        "paperId": "6f321e268990e3e1a792d4bcf829600caab41e1e",
        "url": "https://www.semanticscholar.org/paper/6f321e268990e3e1a792d4bcf829600caab41e1e",
        "title": "CNN-RNN: A Unified Framework for Multi-label Image Classification",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2016,
        "citationCount": 1222,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1604.04573",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1604.04573, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "152924487",
                "name": "Jiang Wang"
            },
            {
                "authorId": "2143686417",
                "name": "Yi Yang"
            },
            {
                "authorId": "36010601",
                "name": "Junhua Mao"
            },
            {
                "authorId": "3109481",
                "name": "Zhiheng Huang"
            },
            {
                "authorId": "48908475",
                "name": "Chang Huang"
            },
            {
                "authorId": "40515617",
                "name": "W. Xu"
            }
        ],
        "abstract": "While deep convolutional neural networks (CNNs) have shown a great success in single-label image classification, it is important to note that real world images generally contain multiple labels, which could correspond to different objects, scenes, actions and attributes in an image. Traditional approaches to multi-label image classification learn independent classifiers for each category and employ ranking or thresholding on the classification results. These techniques, although working well, fail to explicitly exploit the label dependencies in an image. In this paper, we utilize recurrent neural networks (RNNs) to address this problem. Combined with CNNs, the proposed CNN-RNN framework learns a joint image-label embedding to characterize the semantic label dependency as well as the image-label relevance, and it can be trained end-to-end from scratch to integrate both information in a unified framework. Experimental results on public benchmark datasets demonstrate that the proposed architecture achieves better performance than the state-of-the-art multi-label classification models."
    },
    {
        "paperId": "7568d13a82f7afa4be79f09c295940e48ec6db89",
        "url": "https://www.semanticscholar.org/paper/7568d13a82f7afa4be79f09c295940e48ec6db89",
        "title": "Image Style Transfer Using Convolutional Neural Networks",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2016,
        "citationCount": 5498,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR.2016.265?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR.2016.265, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1891828",
                "name": "Leon A. Gatys"
            },
            {
                "authorId": "1746183",
                "name": "Alexander S. Ecker"
            },
            {
                "authorId": "1731199",
                "name": "M. Bethge"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "7b2ca78221fc59b40c122e3b230b8f552e856d12",
        "url": "https://www.semanticscholar.org/paper/7b2ca78221fc59b40c122e3b230b8f552e856d12",
        "title": "Non-local Image Dehazing",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2016,
        "citationCount": 1410,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR.2016.185?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR.2016.185, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "40219339",
                "name": "Dana Berman"
            },
            {
                "authorId": "2937314",
                "name": "T. Treibitz"
            },
            {
                "authorId": "1815078",
                "name": "S. Avidan"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "7d0effebfa4bed19b6ba41f3af5b7e5b6890de87",
        "url": "https://www.semanticscholar.org/paper/7d0effebfa4bed19b6ba41f3af5b7e5b6890de87",
        "title": "Context Encoders: Feature Learning by Inpainting",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2016,
        "citationCount": 5606,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/1604.07379",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1604.07379, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "38236002",
                "name": "Deepak Pathak"
            },
            {
                "authorId": "2562966",
                "name": "Philipp Krhenbhl"
            },
            {
                "authorId": "7408951",
                "name": "Jeff Donahue"
            },
            {
                "authorId": "1753210",
                "name": "Trevor Darrell"
            },
            {
                "authorId": "1763086",
                "name": "Alexei A. Efros"
            }
        ],
        "abstract": "We present an unsupervised visual feature learning algorithm driven by context-based pixel prediction. By analogy with auto-encoders, we propose Context Encoders - a convolutional neural network trained to generate the contents of an arbitrary image region conditioned on its surroundings. In order to succeed at this task, context encoders need to both understand the content of the entire image, as well as produce a plausible hypothesis for the missing part(s). When training context encoders, we have experimented with both a standard pixel-wise reconstruction loss, as well as a reconstruction plus an adversarial loss. The latter produces much sharper results because it can better handle multiple modes in the output. We found that a context encoder learns a representation that captures not just appearance but also the semantics of visual structures. We quantitatively demonstrate the effectiveness of our learned features for CNN pre-training on classification, detection, and segmentation tasks. Furthermore, context encoders can be used for semantic inpainting tasks, either stand-alone or as initialization for non-parametric methods."
    },
    {
        "paperId": "7d39d69b23424446f0400ef603b2e3e22d0309d6",
        "url": "https://www.semanticscholar.org/paper/7d39d69b23424446f0400ef603b2e3e22d0309d6",
        "title": "YOLO9000: Better, Faster, Stronger",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2016,
        "citationCount": 16993,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1612.08242",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1612.08242, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "40497777",
                "name": "Joseph Redmon"
            },
            {
                "authorId": "143787583",
                "name": "Ali Farhadi"
            }
        ],
        "abstract": "We introduce YOLO9000, a state-of-the-art, real-time object detection system that can detect over 9000 object categories. First we propose various improvements to the YOLO detection method, both novel and drawn from prior work. The improved model, YOLOv2, is state-of-the-art on standard detection tasks like PASCAL VOC and COCO. Using a novel, multi-scale training method the same YOLOv2 model can run at varying sizes, offering an easy tradeoff between speed and accuracy. At 67 FPS, YOLOv2 gets 76.8 mAP on VOC 2007. At 40 FPS, YOLOv2 gets 78.6 mAP, outperforming state-of-the-art methods like Faster RCNN with ResNet and SSD while still running significantly faster. Finally we propose a method to jointly train on object detection and classification. Using this method we train YOLO9000 simultaneously on the COCO detection dataset and the ImageNet classification dataset. Our joint training allows YOLO9000 to predict detections for object classes that dont have labelled detection data. We validate our approach on the ImageNet detection task. YOLO9000 gets 19.7 mAP on the ImageNet detection validation set despite only having detection data for 44 of the 200 classes. On the 156 classes not in COCO, YOLO9000 gets 16.0 mAP. YOLO9000 predicts detections for more than 9000 different object categories, all in real-time."
    },
    {
        "paperId": "864e7db59f2ccfec1ee9f6eba79566ac7b0634df",
        "url": "https://www.semanticscholar.org/paper/864e7db59f2ccfec1ee9f6eba79566ac7b0634df",
        "title": "Convolutional Pose Machines",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2016,
        "citationCount": 2857,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1602.00134",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1602.00134, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2797981",
                "name": "S. Wei"
            },
            {
                "authorId": "20569810",
                "name": "V. Ramakrishna"
            },
            {
                "authorId": "1733113",
                "name": "T. Kanade"
            },
            {
                "authorId": "1774867",
                "name": "Yaser Sheikh"
            }
        ],
        "abstract": "Pose Machines provide a sequential prediction framework for learning rich implicit spatial models. In this work we show a systematic design for how convolutional networks can be incorporated into the pose machine framework for learning image features and image-dependent spatial models for the task of pose estimation. The contribution of this paper is to implicitly model long-range dependencies between variables in structured prediction tasks such as articulated pose estimation. We achieve this by designing a sequential architecture composed of convolutional networks that directly operate on belief maps from previous stages, producing increasingly refined estimates for part locations, without the need for explicit graphical model-style inference. Our approach addresses the characteristic difficulty of vanishing gradients during training by providing a natural learning objective function that enforces intermediate supervision, thereby replenishing back-propagated gradients and conditioning the learning procedure. We demonstrate state-of-the-art performance and outperform competing methods on standard benchmarks including the MPII, LSP, and FLIC datasets."
    },
    {
        "paperId": "88513e738a95840de05a62f0e43d30a67b3c542e",
        "url": "https://www.semanticscholar.org/paper/88513e738a95840de05a62f0e43d30a67b3c542e",
        "title": "SCA-CNN: Spatial and Channel-Wise Attention in Convolutional Networks for Image Captioning",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2016,
        "citationCount": 1777,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1611.05594",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1611.05594, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "143891667",
                "name": "Long Chen"
            },
            {
                "authorId": "5462268",
                "name": "Hanwang Zhang"
            },
            {
                "authorId": "145974111",
                "name": "Jun Xiao"
            },
            {
                "authorId": "143982887",
                "name": "Liqiang Nie"
            },
            {
                "authorId": "2549731",
                "name": "Jian Shao"
            },
            {
                "authorId": "2157221163",
                "name": "Wei Liu"
            },
            {
                "authorId": "144078686",
                "name": "Tat-Seng Chua"
            }
        ],
        "abstract": "Visual attention has been successfully applied in structural prediction tasks such as visual captioning and question answering. Existing visual attention models are generally spatial, i.e., the attention is modeled as spatial probabilities that re-weight the last conv-layer feature map of a CNN encoding an input image. However, we argue that such spatial attention does not necessarily conform to the attention mechanism &#x2014; a dynamic feature extractor that combines contextual fixations over time, as CNN features are naturally spatial, channel-wise and multi-layer. In this paper, we introduce a novel convolutional neural network dubbed SCA-CNN that incorporates Spatial and Channel-wise Attentions in a CNN. In the task of image captioning, SCA-CNN dynamically modulates the sentence generation context in multi-layer feature maps, encoding where (i.e., attentive spatial locations at multiple layers) and what (i.e., attentive channels) the visual attention is. We evaluate the proposed SCA-CNN architecture on three benchmark image captioning datasets: Flickr8K, Flickr30K, and MSCOCO. It is consistently observed that SCA-CNN significantly outperforms state-of-the-art visual attention-based image captioning methods."
    },
    {
        "paperId": "8a05db7a75c65ee61c3ca7a6e5401b946166290d",
        "url": "https://www.semanticscholar.org/paper/8a05db7a75c65ee61c3ca7a6e5401b946166290d",
        "title": "Semantic Scene Completion from a Single Depth Image",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2016,
        "citationCount": 1339,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/1611.08974",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1611.08974, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3340170",
                "name": "Shuran Song"
            },
            {
                "authorId": "1807197",
                "name": "F. Yu"
            },
            {
                "authorId": "38591293",
                "name": "Andy Zeng"
            },
            {
                "authorId": "145830541",
                "name": "Angel X. Chang"
            },
            {
                "authorId": "2295141",
                "name": "M. Savva"
            },
            {
                "authorId": "1807080",
                "name": "T. Funkhouser"
            }
        ],
        "abstract": "This paper focuses on semantic scene completion, a task for producing a complete 3D voxel representation of volumetric occupancy and semantic labels for a scene from a single-view depth map observation. Previous work has considered scene completion and semantic labeling of depth maps separately. However, we observe that these two problems are tightly intertwined. To leverage the coupled nature of these two tasks, we introduce the semantic scene completion network (SSCNet), an end-to-end 3D convolutional network that takes a single depth image as input and simultaneously outputs occupancy and semantic labels for all voxels in the camera view frustum. Our network uses a dilation-based 3D context module to efficiently expand the receptive field and enable 3D context learning. To train our network, we construct SUNCG - a manually created largescale dataset of synthetic 3D scenes with dense volumetric annotations. Our experiments demonstrate that the joint model outperforms methods addressing each task in isolation and outperforms alternative approaches on the semantic scene completion task. The dataset and code is available at http://sscnet.cs.princeton.edu."
    },
    {
        "paperId": "8a3bf4d403a39ed33f0fa8cf78dc906d6130595f",
        "url": "https://www.semanticscholar.org/paper/8a3bf4d403a39ed33f0fa8cf78dc906d6130595f",
        "title": "Semantic Image Inpainting with Deep Generative Models",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2016,
        "citationCount": 1215,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1607.07539",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1607.07539, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "28919105",
                "name": "Raymond A. Yeh"
            },
            {
                "authorId": null,
                "name": "Chen Chen"
            },
            {
                "authorId": "33494814",
                "name": "Teck-Yian Lim"
            },
            {
                "authorId": "2068227",
                "name": "A. Schwing"
            },
            {
                "authorId": "1399115926",
                "name": "M. Hasegawa-Johnson"
            },
            {
                "authorId": "1834451",
                "name": "M. Do"
            }
        ],
        "abstract": "Semantic image inpainting is a challenging task where large missing regions have to be filled based on the available visual data. Existing methods which extract information from only a single image generally produce unsatisfactory results due to the lack of high level context. In this paper, we propose a novel method for semantic image inpainting, which generates the missing content by conditioning on the available data. Given a trained generative model, we search for the closest encoding of the corrupted image in the latent image manifold using our context and prior losses. This encoding is then passed through the generative model to infer the missing content. In our method, inference is possible irrespective of how the missing content is structured, while the state-of-the-art learning based method requires specific information about the holes in the training phase. Experiments on three datasets show that our method successfully predicts information in large missing regions and achieves pixel-level photorealism, significantly outperforming the state-of-the-art methods."
    },
    {
        "paperId": "8acbe90d5b852dadea7810345451a99608ee54c7",
        "url": "https://www.semanticscholar.org/paper/8acbe90d5b852dadea7810345451a99608ee54c7",
        "title": "Image-to-Image Translation with Conditional Adversarial Networks",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2016,
        "citationCount": 21354,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1611.07004",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1611.07004, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2094770",
                "name": "Phillip Isola"
            },
            {
                "authorId": "2436356",
                "name": "Jun-Yan Zhu"
            },
            {
                "authorId": "1822702",
                "name": "Tinghui Zhou"
            },
            {
                "authorId": "1763086",
                "name": "Alexei A. Efros"
            }
        ],
        "abstract": "We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Moreover, since the release of the pix2pix software associated with this paper, hundreds of twitter users have posted their own artistic experiments using our system. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without handengineering our loss functions either."
    },
    {
        "paperId": "91d331d2bdd5fc86400c40c497bcb4c741c652be",
        "url": "https://www.semanticscholar.org/paper/91d331d2bdd5fc86400c40c497bcb4c741c652be",
        "title": "Making Deep Neural Networks Robust to Label Noise: A Loss Correction Approach",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2016,
        "citationCount": 1585,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/1609.03683",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1609.03683, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "49047270",
                "name": "Giorgio Patrini"
            },
            {
                "authorId": "36360198",
                "name": "A. Rozza"
            },
            {
                "authorId": "2844480",
                "name": "A. Menon"
            },
            {
                "authorId": "1718786",
                "name": "R. Nock"
            },
            {
                "authorId": "14564042",
                "name": "Lizhen Qu"
            }
        ],
        "abstract": "We present a theoretically grounded approach to train deep neural networks, including recurrent networks, subject to class-dependent label noise. We propose two procedures for loss correction that are agnostic to both application domain and network architecture. They simply amount to at most a matrix inversion and multiplication, provided that we know the probability of each class being corrupted into another. We further show how one can estimate these probabilities, adapting a recent technique for noise estimation to the multi-class setting, and thus providing an end-to-end framework. Extensive experiments on MNIST, IMDB, CIFAR-10, CIFAR-100 and a large scale dataset of clothing images employing a diversity of architectures &#x2014; stacking dense, convolutional, pooling, dropout, batch normalization, word embedding, LSTM and residual layers &#x2014; demonstrate the noise robustness of our proposals. Incidentally, we also prove that, when ReLU is the only non-linearity, the loss curvature is immune to class-dependent label noise."
    },
    {
        "paperId": "9358d2ae944cfbdcb4b48e2e0c5f7ad97118b74e",
        "url": "https://www.semanticscholar.org/paper/9358d2ae944cfbdcb4b48e2e0c5f7ad97118b74e",
        "title": "The SYNTHIA Dataset: A Large Collection of Synthetic Images for Semantic Segmentation of Urban Scenes",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2016,
        "citationCount": 2363,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR.2016.352?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR.2016.352, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "51007566",
                "name": "G. Ros"
            },
            {
                "authorId": "7421202",
                "name": "Laura Sellart"
            },
            {
                "authorId": "2273472080",
                "name": "Joanna Materzynska"
            },
            {
                "authorId": "146560965",
                "name": "David Vzquez"
            },
            {
                "authorId": "144187725",
                "name": "Antonio M. Lpez"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "97e7c94a78ae17cfb90848c1cfca8c431082a7b2",
        "url": "https://www.semanticscholar.org/paper/97e7c94a78ae17cfb90848c1cfca8c431082a7b2",
        "title": "Learning Temporal Regularity in Video Sequences",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2016,
        "citationCount": 1248,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/1604.04574",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1604.04574, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2152302063",
                "name": "Mahmudul Hasan"
            },
            {
                "authorId": "119675494",
                "name": "Jonghyun Choi"
            },
            {
                "authorId": "144660077",
                "name": "J. Neumann"
            },
            {
                "authorId": "1404727582",
                "name": "A. Roy-Chowdhury"
            },
            {
                "authorId": "1693428",
                "name": "L. Davis"
            }
        ],
        "abstract": "Perceiving meaningful activities in a long video sequence is a challenging problem due to ambiguous definition of 'meaningfulness' as well as clutters in the scene. We approach this problem by learning a generative model for regular motion patterns (termed as regularity) using multiple sources with very limited supervision. Specifically, we propose two methods that are built upon the autoencoders for their ability to work with little to no supervision. We first leverage the conventional handcrafted spatio-temporal local features and learn a fully connected autoencoder on them. Second, we build a fully convolutional feed-forward autoencoder to learn both the local features and the classifiers as an end-to-end learning framework. Our model can capture the regularities from multiple datasets. We evaluate our methods in both qualitative and quantitative ways - showing the learned regularity of videos in various aspects and demonstrating competitive performance on anomaly detection datasets as an application."
    },
    {
        "paperId": "9d9aced120e530484609164c836da64548693484",
        "url": "https://www.semanticscholar.org/paper/9d9aced120e530484609164c836da64548693484",
        "title": "Convolutional Two-Stream Network Fusion for Video Action Recognition",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2016,
        "citationCount": 2720,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1604.06573",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1604.06573, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2366173928",
                "name": "Christoph Feichtenhofer"
            },
            {
                "authorId": "1718587",
                "name": "A. Pinz"
            },
            {
                "authorId": "1688869",
                "name": "Andrew Zisserman"
            }
        ],
        "abstract": "Recent applications of Convolutional Neural Networks (ConvNets) for human action recognition in videos have proposed different solutions for incorporating the appearance and motion information. We study a number of ways of fusing ConvNet towers both spatially and temporally in order to best take advantage of this spatio-temporal information. We make the following findings: (i) that rather than fusing at the softmax layer, a spatial and temporal network can be fused at a convolution layer without loss of performance, but with a substantial saving in parameters, (ii) that it is better to fuse such networks spatially at the last convolutional layer than earlier, and that additionally fusing at the class prediction layer can boost accuracy, finally (iii) that pooling of abstract convolutional features over spatiotemporal neighbourhoods further boosts performance. Based on these studies we propose a new ConvNet architecture for spatiotemporal fusion of video snippets, and evaluate its performance on standard benchmarks where this architecture achieves state-of-the-art results."
    },
    {
        "paperId": "9e880a4283a3a4e67304f54e00daf30cc535eeef",
        "url": "https://www.semanticscholar.org/paper/9e880a4283a3a4e67304f54e00daf30cc535eeef",
        "title": "3D Bounding Box Estimation Using Deep Learning and Geometry",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2016,
        "citationCount": 1056,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1612.00496",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1612.00496, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3040583",
                "name": "A. Mousavian"
            },
            {
                "authorId": "1838674",
                "name": "Dragomir Anguelov"
            },
            {
                "authorId": "48001135",
                "name": "John Flynn"
            },
            {
                "authorId": "1743020",
                "name": "J. Kosecka"
            }
        ],
        "abstract": "We present a method for 3D object detection and pose estimation from a single image. In contrast to current techniques that only regress the 3D orientation of an object, our method first regresses relatively stable 3D object properties using a deep convolutional neural network and then combines these estimates with geometric constraints provided by a 2D object bounding box to produce a complete 3D bounding box. The first network output estimates the 3D object orientation using a novel hybrid discrete-continuous loss, which significantly outperforms the L2 loss. The second output regresses the 3D object dimensions, which have relatively little variance compared to alternatives and can often be predicted for many object types. These estimates, combined with the geometric constraints on translation imposed by the 2D bounding box, enable us to recover a stable and accurate 3D object pose. We evaluate our method on the challenging KITTI object detection benchmark [2] both on the official metric of 3D orientation estimation and also on the accuracy of the obtained 3D bounding boxes. Although conceptually simple, our method outperforms more complex and computationally expensive approaches that leverage semantic segmentation, instance level segmentation and flat ground priors [4] and sub-category detection [23][24]. Our discrete-continuous loss also produces state of the art results for 3D viewpoint estimation on the Pascal 3D+ dataset[26]."
    },
    {
        "paperId": "9f4d7d622d1f7319cc511bfef661cd973e881a4c",
        "url": "https://www.semanticscholar.org/paper/9f4d7d622d1f7319cc511bfef661cd973e881a4c",
        "title": "Knowing When to Look: Adaptive Attention via a Visual Sentinel for Image Captioning",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2016,
        "citationCount": 1567,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1612.01887",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1612.01887, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "8553015",
                "name": "Jiasen Lu"
            },
            {
                "authorId": "2228109",
                "name": "Caiming Xiong"
            },
            {
                "authorId": "153432684",
                "name": "Devi Parikh"
            },
            {
                "authorId": "2166511",
                "name": "R. Socher"
            }
        ],
        "abstract": "Attention-based neural encoder-decoder frameworks have been widely adopted for image captioning. Most methods force visual attention to be active for every generated word. However, the decoder likely requires little to no visual information from the image to predict non-visual words such as the and of. Other words that may seem visual can often be predicted reliably just from the language model e.g., sign after behind a red stop or phone following talking on a cell. In this paper, we propose a novel adaptive attention model with a visual sentinel. At each time step, our model decides whether to attend to the image (and if so, to which regions) or to the visual sentinel. The model decides whether to attend to the image and where, in order to extract meaningful information for sequential word generation. We test our method on the COCO image captioning 2015 challenge dataset and Flickr30K. Our approach sets the new state-of-the-art by a significant margin."
    },
    {
        "paperId": "a312a573ef81793d56401e932ef6c9498791a3d1",
        "url": "https://www.semanticscholar.org/paper/a312a573ef81793d56401e932ef6c9498791a3d1",
        "title": "Speed/Accuracy Trade-Offs for Modern Convolutional Object Detectors",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2016,
        "citationCount": 2638,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1611.10012",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1611.10012, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2136435893",
                "name": "Jonathan Huang"
            },
            {
                "authorId": "1382126732",
                "name": "V. Rathod"
            },
            {
                "authorId": "1491624845",
                "name": "Chen Sun"
            },
            {
                "authorId": "2717876",
                "name": "Menglong Zhu"
            },
            {
                "authorId": "34786378",
                "name": "Anoop Korattikara Balan"
            },
            {
                "authorId": "50706340",
                "name": "A. Fathi"
            },
            {
                "authorId": "33091759",
                "name": "Ian S. Fischer"
            },
            {
                "authorId": "3282833",
                "name": "Z. Wojna"
            },
            {
                "authorId": "2157997231",
                "name": "Yang Song"
            },
            {
                "authorId": "1687120",
                "name": "S. Guadarrama"
            },
            {
                "authorId": "1702318",
                "name": "K. Murphy"
            }
        ],
        "abstract": "The goal of this paper is to serve as a guide for selecting a detection architecture that achieves the right speed/memory/accuracy balance for a given application and platform. To this end, we investigate various ways to trade accuracy for speed and memory usage in modern convolutional object detection systems. A number of successful systems have been proposed in recent years, but apples-toapples comparisons are difficult due to different base feature extractors (e.g., VGG, Residual Networks), different default image resolutions, as well as different hardware and software platforms. We present a unified implementation of the Faster R-CNN [30], R-FCN [6] and SSD [25] systems, which we view as meta-architectures and trace out the speed/accuracy trade-off curve created by using alternative feature extractors and varying other critical parameters such as image size within each of these meta-architectures. On one extreme end of this spectrum where speed and memory are critical, we present a detector that achieves real time speeds and can be deployed on a mobile device. On the opposite end in which accuracy is critical, we present a detector that achieves state-of-the-art performance measured on the COCO detection task."
    },
    {
        "paperId": "a87cc499cf101b3697cacc65094b4b6590e0d061",
        "url": "https://www.semanticscholar.org/paper/a87cc499cf101b3697cacc65094b4b6590e0d061",
        "title": "ECO: Efficient Convolution Operators for Tracking",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2016,
        "citationCount": 2407,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1611.09224",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1611.09224, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2488938",
                "name": "Martin Danelljan"
            },
            {
                "authorId": "49922196",
                "name": "Goutam Bhat"
            },
            {
                "authorId": "2358803",
                "name": "F. Khan"
            },
            {
                "authorId": "2228323",
                "name": "M. Felsberg"
            }
        ],
        "abstract": "In recent years, Discriminative Correlation Filter (DCF) based methods have significantly advanced the state-of-the-art in tracking. However, in the pursuit of ever increasing tracking performance, their characteristic speed and real-time capability have gradually faded. Further, the increasingly complex models, with massive number of trainable parameters, have introduced the risk of severe over-fitting. In this work, we tackle the key causes behind the problems of computational complexity and over-fitting, with the aim of simultaneously improving both speed and performance. We revisit the core DCF formulation and introduce: (i) a factorized convolution operator, which drastically reduces the number of parameters in the model, (ii) a compact generative model of the training sample distribution, that significantly reduces memory and time complexity, while providing better diversity of samples, (iii) a conservative model update strategy with improved robustness and reduced complexity. We perform comprehensive experiments on four benchmarks: VOT2016, UAV123, OTB-2015, and TempleColor. When using expensive deep features, our tracker provides a 20-fold speedup and achieves a 13.0% relative gain in Expected Average Overlap compared to the top ranked method [12] in the VOT2016 challenge. Moreover, our fast variant, using hand-crafted features, operates at 60 Hz on a single CPU, while obtaining 65.0% AUC on OTB-2015."
    },
    {
        "paperId": "a93d81c7f033f1e2b54d3288b60d214f55ccc010",
        "url": "https://www.semanticscholar.org/paper/a93d81c7f033f1e2b54d3288b60d214f55ccc010",
        "title": "Optical Flow Estimation Using a Spatial Pyramid Network",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2016,
        "citationCount": 1323,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1611.00850",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1611.00850, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1952002",
                "name": "Anurag Ranjan"
            },
            {
                "authorId": "2105795",
                "name": "Michael J. Black"
            }
        ],
        "abstract": "We learn to compute optical flow by combining a classical spatial-pyramid formulation with deep learning. This estimates large motions in a coarse-to-fine approach by warping one image of a pair at each pyramid level by the current flow estimate and computing an update to the flow. Instead of the standard minimization of an objective function at each pyramid level, we train one deep network per level to compute the flow update. Unlike the recent FlowNet approach, the networks do not need to deal with large motions, these are dealt with by the pyramid. This has several advantages. First, our Spatial Pyramid Network (SPyNet) is much simpler and 96% smaller than FlowNet in terms of model parameters. This makes it more efficient and appropriate for embedded applications. Second, since the flow at each pyramid level is small ("
    },
    {
        "paperId": "b8e2e9f3ba008e28257195ec69a00e07f260131d",
        "url": "https://www.semanticscholar.org/paper/b8e2e9f3ba008e28257195ec69a00e07f260131d",
        "title": "MSR-VTT: A Large Video Description Dataset for Bridging Video and Language",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2016,
        "citationCount": 2285,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR.2016.571?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR.2016.571, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2150637593",
                "name": "Jun Xu"
            },
            {
                "authorId": "144025741",
                "name": "Tao Mei"
            },
            {
                "authorId": "145690248",
                "name": "Ting Yao"
            },
            {
                "authorId": "145459057",
                "name": "Y. Rui"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "ba11b4feb04a472cb5e5962697ed6faa653dc647",
        "url": "https://www.semanticscholar.org/paper/ba11b4feb04a472cb5e5962697ed6faa653dc647",
        "title": "Face2Face: Real-Time Face Capture and Reenactment of RGB Videos",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2016,
        "citationCount": 2119,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2007.14808",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2007.14808, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "34105638",
                "name": "Justus Thies"
            },
            {
                "authorId": "1699058",
                "name": "M. Zollhfer"
            },
            {
                "authorId": "144140066",
                "name": "M. Stamminger"
            },
            {
                "authorId": "1680185",
                "name": "C. Theobalt"
            },
            {
                "authorId": "2209612",
                "name": "M. Niener"
            }
        ],
        "abstract": "We present a novel approach for real-time facial reenactment of a monocular target video sequence (e.g., Youtube video). The source sequence is also a monocular video stream, captured live with a commodity webcam. Our goal is to animate the facial expressions of the target video by a source actor and re-render the manipulated output video in a photo-realistic fashion. To this end, we first address the under-constrained problem of facial identity recovery from monocular video by non-rigid model-based bundling. At run time, we track facial expressions of both source and target video using a dense photometric consistency measure. Reenactment is then achieved by fast and efficient deformation transfer between source and target. The mouth interior that best matches the re-targeted expression is retrieved from the target sequence and warped to produce an accurate fit. Finally, we convincingly re-render the synthesized target face on top of the corresponding video stream such that it seamlessly blends with the real-world illumination. We demonstrate our method in a live setup, where Youtube videos are reenacted in real time."
    },
    {
        "paperId": "bf55591e09b58ea9ce8d66110d6d3000ee804bdd",
        "url": "https://www.semanticscholar.org/paper/bf55591e09b58ea9ce8d66110d6d3000ee804bdd",
        "title": "Image Captioning with Semantic Attention",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2016,
        "citationCount": 1753,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/1603.03925",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1603.03925, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "36610242",
                "name": "Quanzeng You"
            },
            {
                "authorId": "41151701",
                "name": "Hailin Jin"
            },
            {
                "authorId": "8056043",
                "name": "Zhaowen Wang"
            },
            {
                "authorId": "144823841",
                "name": "Chen Fang"
            },
            {
                "authorId": "33642939",
                "name": "Jiebo Luo"
            }
        ],
        "abstract": "Automatically generating a natural language description of an image has attracted interests recently both because of its importance in practical applications and because it connects two major artificial intelligence fields: computer vision and natural language processing. Existing approaches are either top-down, which start from a gist of an image and convert it into words, or bottom-up, which come up with words describing various aspects of an image and then combine them. In this paper, we propose a new algorithm that combines both approaches through a model of semantic attention. Our algorithm learns to selectively attend to semantic concept proposals and fuse them into hidden states and outputs of recurrent neural networks. The selection and fusion form a feedback connecting the top-down and bottom-up computation. We evaluate our algorithm on two public benchmarks: Microsoft COCO and Flickr30K. Experimental results show that our algorithm significantly outperforms the state-of-the-art approaches consistently across different evaluation metrics."
    },
    {
        "paperId": "c88dbaa5d8f4c915e286be5e38b5599038220493",
        "url": "https://www.semanticscholar.org/paper/c88dbaa5d8f4c915e286be5e38b5599038220493",
        "title": "Learning Deep Representation for Imbalanced Classification",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2016,
        "citationCount": 1046,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR.2016.580?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR.2016.580, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2150607621",
                "name": "Chen Huang"
            },
            {
                "authorId": "47002704",
                "name": "Yining Li"
            },
            {
                "authorId": "1717179",
                "name": "Chen Change Loy"
            },
            {
                "authorId": "50295995",
                "name": "Xiaoou Tang"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "c8c494ee5488fe20e0aa01bddf3fc4632086d654",
        "url": "https://www.semanticscholar.org/paper/c8c494ee5488fe20e0aa01bddf3fc4632086d654",
        "title": "The Cityscapes Dataset for Semantic Urban Scene Understanding",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2016,
        "citationCount": 12791,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1604.01685",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1604.01685, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2841796",
                "name": "Marius Cordts"
            },
            {
                "authorId": "144187309",
                "name": "Mohamed Omran"
            },
            {
                "authorId": "39940699",
                "name": "Sebastian Ramos"
            },
            {
                "authorId": "3393153",
                "name": "Timo Rehfeld"
            },
            {
                "authorId": "1765022",
                "name": "Markus Enzweiler"
            },
            {
                "authorId": "1798000",
                "name": "Rodrigo Benenson"
            },
            {
                "authorId": "145582788",
                "name": "Uwe Franke"
            },
            {
                "authorId": "145920814",
                "name": "S. Roth"
            },
            {
                "authorId": "48920094",
                "name": "B. Schiele"
            }
        ],
        "abstract": "Visual understanding of complex urban street scenes is an enabling factor for a wide range of applications. Object detection has benefited enormously from large-scale datasets, especially in the context of deep learning. For semantic urban scene understanding, however, no current dataset adequately captures the complexity of real-world urban scenes. To address this, we introduce Cityscapes, a benchmark suite and large-scale dataset to train and test approaches for pixel-level and instance-level semantic labeling. Cityscapes is comprised of a large, diverse set of stereo video sequences recorded in streets from 50 different cities. 5000 of these images have high quality pixel-level annotations, 20 000 additional images have coarse annotations to enable methods that leverage large volumes of weakly-labeled data. Crucially, our effort exceeds previous attempts in terms of dataset size, annotation richness, scene variability, and complexity. Our accompanying empirical study provides an in-depth analysis of the dataset characteristics, as well as a performance evaluation of several state-of-the-art approaches based on our benchmark."
    },
    {
        "paperId": "d997beefc0922d97202789d2ac307c55c2c52fba",
        "url": "https://www.semanticscholar.org/paper/d997beefc0922d97202789d2ac307c55c2c52fba",
        "title": "PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2016,
        "citationCount": 16315,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1612.00593",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1612.00593, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "144329939",
                "name": "C. Qi"
            },
            {
                "authorId": "144914140",
                "name": "Hao Su"
            },
            {
                "authorId": "2216377",
                "name": "Kaichun Mo"
            },
            {
                "authorId": "51352814",
                "name": "L. Guibas"
            }
        ],
        "abstract": "Point cloud is an important type of geometric data structure. Due to its irregular format, most researchers transform such data to regular 3D voxel grids or collections of images. This, however, renders data unnecessarily voluminous and causes issues. In this paper, we design a novel type of neural network that directly consumes point clouds, which well respects the permutation invariance of points in the input. Our network, named PointNet, provides a unified architecture for applications ranging from object classification, part segmentation, to scene semantic parsing. Though simple, PointNet is highly efficient and effective. Empirically, it shows strong performance on par or even better than state of the art. Theoretically, we provide analysis towards understanding of what the network has learnt and why the network is robust with respect to input perturbation and corruption."
    },
    {
        "paperId": "dc200ab22bf63e10e8b2af328a9e072d82cf75b7",
        "url": "https://www.semanticscholar.org/paper/dc200ab22bf63e10e8b2af328a9e072d82cf75b7",
        "title": "Multi-view 3D Object Detection Network for Autonomous Driving",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2016,
        "citationCount": 3025,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1611.07759",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1611.07759, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1847684",
                "name": "Xiaozhi Chen"
            },
            {
                "authorId": "46389698",
                "name": "Huimin Ma"
            },
            {
                "authorId": "2041852738",
                "name": "Ji Wan"
            },
            {
                "authorId": "38406638",
                "name": "Bo Li"
            },
            {
                "authorId": "2054145194",
                "name": "Tian Xia"
            }
        ],
        "abstract": "This paper aims at high-accuracy 3D object detection in autonomous driving scenario. We propose Multi-View 3D networks (MV3D), a sensory-fusion framework that takes both LIDAR point cloud and RGB images as input and predicts oriented 3D bounding boxes. We encode the sparse 3D point cloud with a compact multi-view representation. The network is composed of two subnetworks: one for 3D object proposal generation and another for multi-view feature fusion. The proposal network generates 3D candidate boxes efficiently from the birds eye view representation of 3D point cloud. We design a deep fusion scheme to combine region-wise features from multiple views and enable interactions between intermediate layers of different paths. Experiments on the challenging KITTI benchmark show that our approach outperforms the state-of-the-art by around 25% and 30% AP on the tasks of 3D localization and 3D detection. In addition, for 2D detection, our approach obtains 14.9% higher AP than the state-of-the-art on the hard data among the LIDAR-based methods."
    },
    {
        "paperId": "dc3f8c8513441915408ab0549e9ac5f2f2f31eec",
        "url": "https://www.semanticscholar.org/paper/dc3f8c8513441915408ab0549e9ac5f2f2f31eec",
        "title": "3D Semantic Parsing of Large-Scale Indoor Spaces",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2016,
        "citationCount": 1894,
        "openAccessPdf": {
            "url": "https://www.repository.cam.ac.uk/bitstreams/504ea6ba-c0e9-426b-98f2-7f7405bd7623/download",
            "status": "GREEN",
            "license": "mit",
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR.2016.170?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR.2016.170, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1798372",
                "name": "Iro Armeni"
            },
            {
                "authorId": "3114252",
                "name": "Ozan Sener"
            },
            {
                "authorId": "40029556",
                "name": "Amir Zamir"
            },
            {
                "authorId": "4910251",
                "name": "Helen Jiang"
            },
            {
                "authorId": "1770886",
                "name": "I. Brilakis"
            },
            {
                "authorId": "2113447583",
                "name": "Martin Fischer"
            },
            {
                "authorId": "1702137",
                "name": "S. Savarese"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "de4ee92cfad3734ca820d004bc9ee75fc9dcfbf4",
        "url": "https://www.semanticscholar.org/paper/de4ee92cfad3734ca820d004bc9ee75fc9dcfbf4",
        "title": "RefineNet: Multi-path Refinement Networks for High-Resolution Semantic Segmentation",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2016,
        "citationCount": 3036,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1611.06612",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1611.06612, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2604251",
                "name": "Guosheng Lin"
            },
            {
                "authorId": "34761498",
                "name": "Anton Milan"
            },
            {
                "authorId": "1780381",
                "name": "Chunhua Shen"
            },
            {
                "authorId": "145950884",
                "name": "I. Reid"
            }
        ],
        "abstract": "Recently, very deep convolutional neural networks (CNNs) have shown outstanding performance in object recognition and have also been the first choice for dense classification problems such as semantic segmentation. However, repeated subsampling operations like pooling or convolution striding in deep CNNs lead to a significant decrease in the initial image resolution. Here, we present RefineNet, a generic multi-path refinement network that explicitly exploits all the information available along the down-sampling process to enable high-resolution prediction using long-range residual connections. In this way, the deeper layers that capture high-level semantic features can be directly refined using fine-grained features from earlier convolutions. The individual components of RefineNet employ residual connections following the identity mapping mindset, which allows for effective end-to-end training. Further, we introduce chained residual pooling, which captures rich background context in an efficient manner. We carry out comprehensive experiments and set new state-of-the-art results on seven public datasets. In particular, we achieve an intersection-over-union score of 83.4 on the challenging PASCAL VOC 2012 dataset, which is the best reported result to date."
    },
    {
        "paperId": "df0c54fe61f0ffb9f0e36a17c2038d9a1964cba3",
        "url": "https://www.semanticscholar.org/paper/df0c54fe61f0ffb9f0e36a17c2038d9a1964cba3",
        "title": "Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2016,
        "citationCount": 11610,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/1609.04802",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1609.04802, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1779917",
                "name": "C. Ledig"
            },
            {
                "authorId": "2073063",
                "name": "Lucas Theis"
            },
            {
                "authorId": "3108066",
                "name": "Ferenc Huszr"
            },
            {
                "authorId": "145372820",
                "name": "Jose Caballero"
            },
            {
                "authorId": "49931957",
                "name": "Andrew P. Aitken"
            },
            {
                "authorId": "41203992",
                "name": "Alykhan Tejani"
            },
            {
                "authorId": "1853456",
                "name": "J. Totz"
            },
            {
                "authorId": "34627233",
                "name": "Zehan Wang"
            },
            {
                "authorId": "46810836",
                "name": "Wenzhe Shi"
            }
        ],
        "abstract": "Despite the breakthroughs in accuracy and speed of single image super-resolution using faster and deeper convolutional neural networks, one central problem remains largely unsolved: how do we recover the finer texture details when we super-resolve at large upscaling factors? The behavior of optimization-based super-resolution methods is principally driven by the choice of the objective function. Recent work has largely focused on minimizing the mean squared reconstruction error. The resulting estimates have high peak signal-to-noise ratios, but they are often lacking high-frequency details and are perceptually unsatisfying in the sense that they fail to match the fidelity expected at the higher resolution. In this paper, we present SRGAN, a generative adversarial network (GAN) for image super-resolution (SR). To our knowledge, it is the first framework capable of inferring photo-realistic natural images for 4x upscaling factors. To achieve this, we propose a perceptual loss function which consists of an adversarial loss and a content loss. The adversarial loss pushes our solution to the natural image manifold using a discriminator network that is trained to differentiate between the super-resolved images and original photo-realistic images. In addition, we use a content loss motivated by perceptual similarity instead of similarity in pixel space. Our deep residual network is able to recover photo-realistic textures from heavily downsampled images on public benchmarks. An extensive mean-opinion-score (MOS) test shows hugely significant gains in perceptual quality using SRGAN. The MOS scores obtained with SRGAN are closer to those of the original high-resolution images than to those obtained with any state-of-the-art method."
    },
    {
        "paperId": "dfad8f616bd2a05c8cae5f61060f743f966ece85",
        "url": "https://www.semanticscholar.org/paper/dfad8f616bd2a05c8cae5f61060f743f966ece85",
        "title": "Realtime Multi-person 2D Pose Estimation Using Part Affinity Fields",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2016,
        "citationCount": 7018,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1611.08050",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1611.08050, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "47060433",
                "name": "Zhe Cao"
            },
            {
                "authorId": "145386542",
                "name": "T. Simon"
            },
            {
                "authorId": "2797981",
                "name": "S. Wei"
            },
            {
                "authorId": "1774867",
                "name": "Yaser Sheikh"
            }
        ],
        "abstract": "We present an approach to efficiently detect the 2D pose of multiple people in an image. The approach uses a nonparametric representation, which we refer to as Part Affinity Fields (PAFs), to learn to associate body parts with individuals in the image. The architecture encodes global context, allowing a greedy bottom-up parsing step that maintains high accuracy while achieving realtime performance, irrespective of the number of people in the image. The architecture is designed to jointly learn part locations and their association via two branches of the same sequential prediction process. Our method placed first in the inaugural COCO 2016 keypoints challenge, and significantly exceeds the previous state-of-the-art result on the MPII Multi-Person benchmark, both in performance and efficiency."
    },
    {
        "paperId": "e11a020f0d2942d09127daf1ce7e658d3bf67291",
        "url": "https://www.semanticscholar.org/paper/e11a020f0d2942d09127daf1ce7e658d3bf67291",
        "title": "Social LSTM: Human Trajectory Prediction in Crowded Spaces",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2016,
        "citationCount": 3169,
        "openAccessPdf": {
            "url": "https://infoscience.epfl.ch/record/230265/files/CVPR16_N_LSTM.pdf",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR.2016.110?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR.2016.110, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3304525",
                "name": "Alexandre Alahi"
            },
            {
                "authorId": "2957685",
                "name": "Kratarth Goel"
            },
            {
                "authorId": "34066479",
                "name": "Vignesh Ramanathan"
            },
            {
                "authorId": "2364487",
                "name": "Alexandre Robicquet"
            },
            {
                "authorId": "48004138",
                "name": "Li Fei-Fei"
            },
            {
                "authorId": "1702137",
                "name": "S. Savarese"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "e7d0c37f4f3589a3b787f39e8307704da5ed8d6c",
        "url": "https://www.semanticscholar.org/paper/e7d0c37f4f3589a3b787f39e8307704da5ed8d6c",
        "title": "Structure-from-Motion Revisited",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2016,
        "citationCount": 6537,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR.2016.445?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR.2016.445, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3010882",
                "name": "Johannes L. Schnberger"
            },
            {
                "authorId": "40454588",
                "name": "Jan-Michael Frahm"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "e944b414e9f601a6008076bd43b91d382090adbc",
        "url": "https://www.semanticscholar.org/paper/e944b414e9f601a6008076bd43b91d382090adbc",
        "title": "VirtualWorlds as Proxy for Multi-object Tracking Analysis",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2016,
        "citationCount": 1149,
        "openAccessPdf": {
            "url": "https://elib.dlr.de/105154/1/Gaidon_Virtual_Worlds_as_CVPR_2016_paper.pdf",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1605.06457, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1799820",
                "name": "Adrien Gaidon"
            },
            {
                "authorId": "2116722100",
                "name": "Qiao Wang"
            },
            {
                "authorId": "3407519",
                "name": "Yohann Cabon"
            },
            {
                "authorId": "2286630",
                "name": "E. Vig"
            }
        ],
        "abstract": "Modern computer vision algorithms typically require expensive data acquisition and accurate manual labeling. In this work, we instead leverage the recent progress in computer graphics to generate fully labeled, dynamic, and photo-realistic proxy virtual worlds. We propose an efficient real-to-virtual world cloning method, and validate our approach by building and publicly releasing a new video dataset, called \"Virtual KITTI\", automatically labeled with accurate ground truth for object detection, tracking, scene and instance segmentation, depth, and optical flow. We provide quantitative experimental evidence suggesting that (i) modern deep learning algorithms pre-trained on real data behave similarly in real and virtual worlds, and (ii) pre-training on virtual data improves performance. As the gap between real and virtual worlds is small, virtual worlds enable measuring the impact of various weather and imaging conditions on recognition performance, all other things being equal. We show these factors may affect drastically otherwise high-performing deep models for tracking."
    },
    {
        "paperId": "edd846e76cacfba5be37da99c006e3ccc9b861b0",
        "url": "https://www.semanticscholar.org/paper/edd846e76cacfba5be37da99c006e3ccc9b861b0",
        "title": "FlowNet 2.0: Evolution of Optical Flow Estimation with Deep Networks",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2016,
        "citationCount": 3281,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1612.01925",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1612.01925, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "48105320",
                "name": "Eddy Ilg"
            },
            {
                "authorId": "153200643",
                "name": "N. Mayer"
            },
            {
                "authorId": "2872102",
                "name": "Tonmoy Saikia"
            },
            {
                "authorId": "2363337973",
                "name": "Margret Keuper"
            },
            {
                "authorId": "2841331",
                "name": "Alexey Dosovitskiy"
            },
            {
                "authorId": "1710872",
                "name": "T. Brox"
            }
        ],
        "abstract": "The FlowNet demonstrated that optical flow estimation can be cast as a learning problem. However, the state of the art with regard to the quality of the flow has still been defined by traditional methods. Particularly on small displacements and real-world data, FlowNet cannot compete with variational methods. In this paper, we advance the concept of end-to-end learning of optical flow and make it work really well. The large improvements in quality and speed are caused by three major contributions: first, we focus on the training data and show that the schedule of presenting data during training is very important. Second, we develop a stacked architecture that includes warping of the second image with intermediate optical flow. Third, we elaborate on small displacements by introducing a subnetwork specializing on small motions. FlowNet 2.0 is only marginally slower than the original FlowNet but decreases the estimation error by more than 50%. It performs on par with state-of-the-art methods, while running at interactive frame rates. Moreover, we present faster variants that allow optical flow computation at up to 140fps with accuracy matching the original FlowNet."
    },
    {
        "paperId": "f09f7888aa5aeaf88a2a44aea768d9a8747e97d2",
        "url": "https://www.semanticscholar.org/paper/f09f7888aa5aeaf88a2a44aea768d9a8747e97d2",
        "title": "Geometric Deep Learning on Graphs and Manifolds Using Mixture Model CNNs",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2016,
        "citationCount": 1905,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1611.08402",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1611.08402, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2500309",
                "name": "Federico Monti"
            },
            {
                "authorId": "1804261",
                "name": "Davide Boscaini"
            },
            {
                "authorId": "2426718",
                "name": "Jonathan Masci"
            },
            {
                "authorId": "1796150",
                "name": "E. Rodol"
            },
            {
                "authorId": "2064928589",
                "name": "Jan Svoboda"
            },
            {
                "authorId": "1732570",
                "name": "M. Bronstein"
            }
        ],
        "abstract": "Deep learning has achieved a remarkable performance breakthrough in several fields, most notably in speech recognition, natural language processing, and computer vision. In particular, convolutional neural network (CNN) architectures currently produce state-of-the-art performance on a variety of image analysis tasks such as object detection and recognition. Most of deep learning research has so far focused on dealing with 1D, 2D, or 3D Euclidean-structured data such as acoustic signals, images, or videos. Recently, there has been an increasing interest in geometric deep learning, attempting to generalize deep learning methods to non-Euclidean structured data such as graphs and manifolds, with a variety of applications from the domains of network analysis, computational social science, or computer graphics. In this paper, we propose a unified framework allowing to generalize CNN architectures to non-Euclidean domains (graphs and manifolds) and learn local, stationary, and compositional task-specific features. We show that various non-Euclidean CNN methods previously proposed in the literature can be considered as particular instances of our framework. We test the proposed method on standard tasks from the realms of image-, graph-and 3D shape analysis and show that it consistently outperforms previous approaches."
    },
    {
        "paperId": "f6e0856b4a9199fa968ac00da612a9407b5cb85c",
        "url": "https://www.semanticscholar.org/paper/f6e0856b4a9199fa968ac00da612a9407b5cb85c",
        "title": "Aggregated Residual Transformations for Deep Neural Networks",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2016,
        "citationCount": 11230,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/1611.05431",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1611.05431, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1817030",
                "name": "Saining Xie"
            },
            {
                "authorId": "2983898",
                "name": "Ross B. Girshick"
            },
            {
                "authorId": "3127283",
                "name": "Piotr Dollr"
            },
            {
                "authorId": "144035504",
                "name": "Z. Tu"
            },
            {
                "authorId": "39353098",
                "name": "Kaiming He"
            }
        ],
        "abstract": "We present a simple, highly modularized network architecture for image classification. Our network is constructed by repeating a building block that aggregates a set of transformations with the same topology. Our simple design results in a homogeneous, multi-branch architecture that has only a few hyper-parameters to set. This strategy exposes a new dimension, which we call cardinality (the size of the set of transformations), as an essential factor in addition to the dimensions of depth and width. On the ImageNet-1K dataset, we empirically show that even under the restricted condition of maintaining complexity, increasing cardinality is able to improve classification accuracy. Moreover, increasing cardinality is more effective than going deeper or wider when we increase the capacity. Our models, named ResNeXt, are the foundations of our entry to the ILSVRC 2016 classification task in which we secured 2nd place. We further investigate ResNeXt on an ImageNet-5K set and the COCO detection set, also showing better results than its ResNet counterpart. The code and models are publicly available online."
    },
    {
        "paperId": "000178cd12c8a6e5da8215b6365fae03c20fd18d",
        "url": "https://www.semanticscholar.org/paper/000178cd12c8a6e5da8215b6365fae03c20fd18d",
        "title": "End-to-End Representation Learning for Correlation Filter Based Tracking",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2017,
        "citationCount": 1449,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1704.06036",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1704.06036, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1881617",
                "name": "Jack Valmadre"
            },
            {
                "authorId": "2271057",
                "name": "Luca Bertinetto"
            },
            {
                "authorId": "143848064",
                "name": "Joo F. Henriques"
            },
            {
                "authorId": "1687524",
                "name": "A. Vedaldi"
            },
            {
                "authorId": "143635540",
                "name": "Philip H. S. Torr"
            }
        ],
        "abstract": "The Correlation Filter is an algorithm that trains a linear template to discriminate between images and their translations. It is well suited to object tracking because its formulation in the Fourier domain provides a fast solution, enabling the detector to be re-trained once per frame. Previous works that use the Correlation Filter, however, have adopted features that were either manually designed or trained for a different task. This work is the first to overcome this limitation by interpreting the Correlation Filter learner, which has a closed-form solution, as a differentiable layer in a deep neural network. This enables learning deep features that are tightly coupled to the Correlation Filter. Experiments illustrate that our method has the important practical benefit of allowing lightweight architectures to achieve state-of-the-art performance at high framerates."
    },
    {
        "paperId": "0410659b6a311b281d10e0e44abce9b1c06be462",
        "url": "https://www.semanticscholar.org/paper/0410659b6a311b281d10e0e44abce9b1c06be462",
        "title": "A Gift from Knowledge Distillation: Fast Optimization, Network Minimization and Transfer Learning",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2017,
        "citationCount": 1636,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR.2017.754?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR.2017.754, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3249661",
                "name": "Junho Yim"
            },
            {
                "authorId": "50001046",
                "name": "Donggyu Joo"
            },
            {
                "authorId": "2072617049",
                "name": "JiHoon Bae"
            },
            {
                "authorId": "1769295",
                "name": "Junmo Kim"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "1a39bb2caa151d15efd6718f3a80d9f4bff95af2",
        "url": "https://www.semanticscholar.org/paper/1a39bb2caa151d15efd6718f3a80d9f4bff95af2",
        "title": "Dynamic Edge-Conditioned Filters in Convolutional Neural Networks on Graphs",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2017,
        "citationCount": 1310,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1704.02901",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1704.02901, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3451689",
                "name": "M. Simonovsky"
            },
            {
                "authorId": "2505902",
                "name": "N. Komodakis"
            }
        ],
        "abstract": "A number of problems can be formulated as prediction on graph-structured data. In this work, we generalize the convolution operator from regular grids to arbitrary graphs while avoiding the spectral domain, which allows us to handle graphs of varying size and connectivity. To move beyond a simple diffusion, filter weights are conditioned on the specific edge labels in the neighborhood of a vertex. Together with the proper choice of graph coarsening, we explore constructing deep neural networks for graph classification. In particular, we demonstrate the generality of our formulation in point cloud classification, where we set the new state of the art, and on a graph classification dataset, where we outperform other deep learning approaches."
    },
    {
        "paperId": "29ad37a306732676a41f25e05960ff0adbca631e",
        "url": "https://www.semanticscholar.org/paper/29ad37a306732676a41f25e05960ff0adbca631e",
        "title": "Removing Rain from Single Images via a Deep Detail Network",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2017,
        "citationCount": 1146,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR.2017.186?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR.2017.186, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3061449",
                "name": "Xueyang Fu"
            },
            {
                "authorId": "49024793",
                "name": "Jiabin Huang"
            },
            {
                "authorId": "2667724",
                "name": "Delu Zeng"
            },
            {
                "authorId": "1950637",
                "name": "Yue Huang"
            },
            {
                "authorId": "2713947",
                "name": "Xinghao Ding"
            },
            {
                "authorId": "143855009",
                "name": "J. Paisley"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "2a5667702b0f1ff77dde8fb3e2e10d4e05e8de9d",
        "url": "https://www.semanticscholar.org/paper/2a5667702b0f1ff77dde8fb3e2e10d4e05e8de9d",
        "title": "Scene Parsing through ADE20K Dataset",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2017,
        "citationCount": 3539,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR.2017.544?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR.2017.544, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "145291669",
                "name": "Bolei Zhou"
            },
            {
                "authorId": "47940821",
                "name": "Hang Zhao"
            },
            {
                "authorId": "143872936",
                "name": "Xavier Puig"
            },
            {
                "authorId": "37895334",
                "name": "S. Fidler"
            },
            {
                "authorId": "2063937685",
                "name": "Adela Barriuso"
            },
            {
                "authorId": "143805211",
                "name": "A. Torralba"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "345afa0e85cb2f5cb438ae44027499ff2c392409",
        "url": "https://www.semanticscholar.org/paper/345afa0e85cb2f5cb438ae44027499ff2c392409",
        "title": "Adversarial Discriminative Domain Adaptation",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2017,
        "citationCount": 5024,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1702.05464",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1702.05464, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2368132",
                "name": "Eric Tzeng"
            },
            {
                "authorId": "50196944",
                "name": "Judy Hoffman"
            },
            {
                "authorId": "2903226",
                "name": "Kate Saenko"
            },
            {
                "authorId": "1753210",
                "name": "Trevor Darrell"
            }
        ],
        "abstract": "Adversarial learning methods are a promising approach to training robust deep networks, and can generate complex samples across diverse domains. They can also improve recognition despite the presence of domain shift or dataset bias: recent adversarial approaches to unsupervised domain adaptation reduce the difference between the training and test domain distributions and thus improve generalization performance. However, while generative adversarial networks (GANs) show compelling visualizations, they are not optimal on discriminative tasks and can be limited to smaller shifts. On the other hand, discriminative approaches can handle larger domain shifts, but impose tied weights on the model and do not exploit a GAN-based loss. In this work, we first outline a novel generalized framework for adversarial adaptation, which subsumes recent state-of-the-art approaches as special cases, and use this generalized view to better relate prior approaches. We then propose a previously unexplored instance of our general framework which combines discriminative modeling, untied weight sharing, and a GAN loss, which we call Adversarial Discriminative Domain Adaptation (ADDA). We show that ADDA is more effective yet considerably simpler than competing domain-adversarial methods, and demonstrate the promise of our approach by exceeding state-of-the-art unsupervised adaptation results on standard domain adaptation tasks as well as a difficult cross-modality object classification task."
    },
    {
        "paperId": "34b73c1aa158b892bbe41705b4ae5bf01ecaea86",
        "url": "https://www.semanticscholar.org/paper/34b73c1aa158b892bbe41705b4ae5bf01ecaea86",
        "title": "Scene Graph Generation by Iterative Message Passing",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2017,
        "citationCount": 1323,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1701.02426",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1701.02426, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2068265",
                "name": "Danfei Xu"
            },
            {
                "authorId": "2117748",
                "name": "Yuke Zhu"
            },
            {
                "authorId": "2144103",
                "name": "C. Choy"
            },
            {
                "authorId": "48004138",
                "name": "Li Fei-Fei"
            }
        ],
        "abstract": "Understanding a visual scene goes beyond recognizing individual objects in isolation. Relationships between objects also constitute rich semantic information about the scene. In this work, we explicitly model the objects and their relationships using scene graphs, a visually-grounded graphical structure of an image. We propose a novel end-to-end model that generates such structured scene representation from an input image. Our key insight is that the graph generation problem can be formulated as message passing between the primal node graph and its dual edge graph. Our joint inference model can take advantage of contextual cues to make better predictions on objects and their relationships. The experiments show that our model significantly outperforms previous methods on the Visual Genome dataset as well as support relation inference in NYU Depth V2 dataset."
    },
    {
        "paperId": "3617ccfec4bed2d8ac15d0ad1a35b589d9b270cb",
        "url": "https://www.semanticscholar.org/paper/3617ccfec4bed2d8ac15d0ad1a35b589d9b270cb",
        "title": "Large Kernel Matters  Improve Semantic Segmentation by Global Convolutional Network",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2017,
        "citationCount": 1569,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1703.02719",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1703.02719, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2113567716",
                "name": "Chao Peng"
            },
            {
                "authorId": "50875121",
                "name": "Xiangyu Zhang"
            },
            {
                "authorId": "2116565951",
                "name": "Gang Yu"
            },
            {
                "authorId": "1873167",
                "name": "G. Luo"
            },
            {
                "authorId": null,
                "name": "Jian Sun"
            }
        ],
        "abstract": "One of recent trends [31, 32, 14] in network architecture design is stacking small filters (e.g., 1x1 or 3x3) in the entire network because the stacked small filters is more efficient than a large kernel, given the same computational complexity. However, in the field of semantic segmentation, where we need to perform dense per-pixel prediction, we find that the large kernel (and effective receptive field) plays an important role when we have to perform the classification and localization tasks simultaneously. Following our design principle, we propose a Global Convolutional Network to address both the classification and localization issues for the semantic segmentation. We also suggest a residual-based boundary refinement to further refine the object boundaries. Our approach achieves state-of-art performance on two public benchmarks and significantly outperforms previous results, 82.2% (vs 80.2%) on PASCAL VOC 2012 dataset and 76.9% (vs 71.8%) on Cityscapes dataset."
    },
    {
        "paperId": "3abf64d10a5d9a426d864bcfd68daed370d6904c",
        "url": "https://www.semanticscholar.org/paper/3abf64d10a5d9a426d864bcfd68daed370d6904c",
        "title": "Unsupervised Learning of Depth and Ego-Motion from Video",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2017,
        "citationCount": 2771,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/1704.07813",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1704.07813, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1822702",
                "name": "Tinghui Zhou"
            },
            {
                "authorId": "144735785",
                "name": "Matthew A. Brown"
            },
            {
                "authorId": "1830653",
                "name": "Noah Snavely"
            },
            {
                "authorId": "35238678",
                "name": "D. Lowe"
            }
        ],
        "abstract": "We present an unsupervised learning framework for the task of monocular depth and camera motion estimation from unstructured video sequences. In common with recent work [10, 14, 16], we use an end-to-end learning approach with view synthesis as the supervisory signal. In contrast to the previous work, our method is completely unsupervised, requiring only monocular video sequences for training. Our method uses single-view depth and multiview pose networks, with a loss based on warping nearby views to the target using the computed depth and pose. The networks are thus coupled by the loss during training, but can be applied independently at test time. Empirical evaluation on the KITTI dataset demonstrates the effectiveness of our approach: 1) monocular depth performs comparably with supervised methods that use either ground-truth pose or depth for training, and 2) pose estimation performs favorably compared to established SLAM systems under comparable input settings."
    },
    {
        "paperId": "4901eaa5a3b8cca41e4bb664ef0446d6118bd87c",
        "url": "https://www.semanticscholar.org/paper/4901eaa5a3b8cca41e4bb664ef0446d6118bd87c",
        "title": "Beyond Triplet Loss: A Deep Quadruplet Network for Person Re-identification",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2017,
        "citationCount": 1191,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1704.01719",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1704.01719, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "6961334",
                "name": "Weihua Chen"
            },
            {
                "authorId": "1810827",
                "name": "Xiaotang Chen"
            },
            {
                "authorId": "2155240973",
                "name": "Jianguo Zhang"
            },
            {
                "authorId": "2887871",
                "name": "Kaiqi Huang"
            }
        ],
        "abstract": "Person re-identification (ReID) is an important task in wide area video surveillance which focuses on identifying people across different cameras. Recently, deep learning networks with a triplet loss become a common framework for person ReID. However, the triplet loss pays main attentions on obtaining correct orders on the training set. It still suffers from a weaker generalization capability from the training set to the testing set, thus resulting in inferior performance. In this paper, we design a quadruplet loss, which can lead to the model output with a larger inter-class variation and a smaller intra-class variation compared to the triplet loss. As a result, our model has a better generalization ability and can achieve a higher performance on the testing set. In particular, a quadruplet deep network using a margin-based online hard negative mining is proposed based on the quadruplet loss for the person ReID. In extensive experiments, the proposed network outperforms most of the state-of-the-art algorithms on representative datasets which clearly demonstrates the effectiveness of our proposed method."
    },
    {
        "paperId": "55c0113534c62b7f3f238210cf501b42d91cc33a",
        "url": "https://www.semanticscholar.org/paper/55c0113534c62b7f3f238210cf501b42d91cc33a",
        "title": "Hand Keypoint Detection in Single Images Using Multiview Bootstrapping",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2017,
        "citationCount": 1171,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/1704.07809",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1704.07809, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "145386542",
                "name": "T. Simon"
            },
            {
                "authorId": "7996087",
                "name": "H. Joo"
            },
            {
                "authorId": "1711695",
                "name": "I. Matthews"
            },
            {
                "authorId": "1774867",
                "name": "Yaser Sheikh"
            }
        ],
        "abstract": "We present an approach that uses a multi-camera system to train fine-grained detectors for keypoints that are prone to occlusion, such as the joints of a hand. We call this procedure multiview bootstrapping: first, an initial keypoint detector is used to produce noisy labels in multiple views of the hand. The noisy detections are then triangulated in 3D using multiview geometry or marked as outliers. Finally, the reprojected triangulations are used as new labeled training data to improve the detector. We repeat this process, generating more labeled data in each iteration. We derive a result analytically relating the minimum number of views to achieve target true and false positive rates for a given detector. The method is used to train a hand keypoint detector for single images. The resulting keypoint detector runs in realtime on RGB images and has accuracy comparable to methods that use depth sensors. The single view detector, triangulated over multiple views, enables 3D markerless hand motion capture with complex object interactions."
    },
    {
        "paperId": "58b6bd06ea58c367c64286126ba14128b45041b8",
        "url": "https://www.semanticscholar.org/paper/58b6bd06ea58c367c64286126ba14128b45041b8",
        "title": "ChestX-Ray8: Hospital-Scale Chest X-Ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2017,
        "citationCount": 2997,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1705.02315",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1705.02315, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2026596",
                "name": "Xiaosong Wang"
            },
            {
                "authorId": "2256805957",
                "name": "Yifan Peng"
            },
            {
                "authorId": "50706692",
                "name": "Le Lu"
            },
            {
                "authorId": "2237807642",
                "name": "Zhiyong Lu"
            },
            {
                "authorId": "3774191",
                "name": "M. Bagheri"
            },
            {
                "authorId": "2257812443",
                "name": "Ronald M. Summers"
            }
        ],
        "abstract": "The chest X-ray is one of the most commonly accessible radiological examinations for screening and diagnosis of many lung diseases. A tremendous number of X-ray imaging studies accompanied by radiological reports are accumulated and stored in many modern hospitals Picture Archiving and Communication Systems (PACS). On the other side, it is still an open question how this type of hospital-size knowledge database containing invaluable imaging informatics (i.e., loosely labeled) can be used to facilitate the data-hungry deep learning paradigms in building truly large-scale high precision computer-aided diagnosis (CAD) systems. In this paper, we present a new chest X-ray database, namely ChestX-ray8, which comprises 108,948 frontal-view X-ray images of 32,717 unique patients with the text-mined eight disease image labels (where each image can have multi-labels), from the associated radiological reports using natural language processing. Importantly, we demonstrate that these commonly occurring thoracic diseases can be detected and even spatially-located via a unified weakly-supervised multi-label image classification and disease localization framework, which is validated using our proposed dataset. Although the initial quantitative results are promising as reported, deep convolutional neural network based reading chest X-rays (i.e., recognizing and locating the common disease patterns trained with only image-level labels) remains a strenuous task for fully-automated high precision CAD systems."
    },
    {
        "paperId": "5f1630b4485027eb99ae59b745372ef1f3699c16",
        "url": "https://www.semanticscholar.org/paper/5f1630b4485027eb99ae59b745372ef1f3699c16",
        "title": "EAST: An Efficient and Accurate Scene Text Detector",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2017,
        "citationCount": 1604,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1704.03155",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1704.03155, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2148927556",
                "name": "Xinyu Zhou"
            },
            {
                "authorId": "2146721",
                "name": "C. Yao"
            },
            {
                "authorId": "2075348632",
                "name": "He Wen"
            },
            {
                "authorId": "47905836",
                "name": "Yuzhi Wang"
            },
            {
                "authorId": "35132667",
                "name": "Shuchang Zhou"
            },
            {
                "authorId": "2416953",
                "name": "Weiran He"
            },
            {
                "authorId": "1387852255",
                "name": "Jiajun Liang"
            }
        ],
        "abstract": "Previous approaches for scene text detection have already achieved promising performances across various benchmarks. However, they usually fall short when dealing with challenging scenarios, even when equipped with deep neural network models, because the overall performance is determined by the interplay of multiple stages and components in the pipelines. In this work, we propose a simple yet powerful pipeline that yields fast and accurate text detection in natural scenes. The pipeline directly predicts words or text lines of arbitrary orientations and quadrilateral shapes in full images, eliminating unnecessary intermediate steps (e.g., candidate aggregation and word partitioning), with a single neural network. The simplicity of our pipeline allows concentrating efforts on designing loss functions and neural network architecture. Experiments on standard datasets including ICDAR 2015, COCO-Text and MSRA-TD500 demonstrate that the proposed algorithm significantly outperforms state-of-the-art methods in terms of both accuracy and efficiency. On the ICDAR 2015 dataset, the proposed algorithm achieves an F-score of 0.7820 at 13.2fps at 720p resolution."
    },
    {
        "paperId": "661778ccef17289a07e3eb0ba2343b851762213e",
        "url": "https://www.semanticscholar.org/paper/661778ccef17289a07e3eb0ba2343b851762213e",
        "title": "Learning from Synthetic Humans",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2017,
        "citationCount": 1019,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1701.01370",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1701.01370, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2397485103",
                "name": "Gl Varol"
            },
            {
                "authorId": "143881914",
                "name": "J. Romero"
            },
            {
                "authorId": "2071700418",
                "name": "Xavier Martin"
            },
            {
                "authorId": "1892850",
                "name": "Naureen Mahmood"
            },
            {
                "authorId": "2105795",
                "name": "Michael J. Black"
            },
            {
                "authorId": "143991676",
                "name": "I. Laptev"
            },
            {
                "authorId": "2462253",
                "name": "C. Schmid"
            }
        ],
        "abstract": "Estimating human pose, shape, and motion from images and videos are fundamental challenges with many applications. Recent advances in 2D human pose estimation use large amounts of manually-labeled training data for learning convolutional neural networks (CNNs). Such data is time consuming to acquire and difficult to extend. Moreover, manual labeling of 3D pose, depth and motion is impractical. In this work we present SURREAL (Synthetic hUmans foR REAL tasks): a new large-scale dataset with synthetically-generated but realistic images of people rendered from 3D sequences of human motion capture data. We generate more than 6 million frames together with ground truth pose, depth maps, and segmentation masks. We show that CNNs trained on our synthetic dataset allow for accurate human depth estimation and human part segmentation in real RGB images. Our results and the new dataset open up new possibilities for advancing person analysis using cheap and large-scale synthetic data."
    },
    {
        "paperId": "6f34b9a4a0e2ee90e86ed720dc26cc6ba9da8df0",
        "url": "https://www.semanticscholar.org/paper/6f34b9a4a0e2ee90e86ed720dc26cc6ba9da8df0",
        "title": "Dilated Residual Networks",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2017,
        "citationCount": 1743,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1705.09914",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1705.09914, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1807197",
                "name": "F. Yu"
            },
            {
                "authorId": "145231047",
                "name": "V. Koltun"
            },
            {
                "authorId": "1807080",
                "name": "T. Funkhouser"
            }
        ],
        "abstract": "Convolutional networks for image classification progressively reduce resolution until the image is represented by tiny feature maps in which the spatial structure of the scene is no longer discernible. Such loss of spatial acuity can limit image classification accuracy and complicate the transfer of the model to downstream applications that require detailed scene understanding. These problems can be alleviated by dilation, which increases the resolution of output feature maps without reducing the receptive field of individual neurons. We show that dilated residual networks (DRNs) outperform their non-dilated counterparts in image classification without increasing the models depth or complexity. We then study gridding artifacts introduced by dilation, develop an approach to removing these artifacts (degridding), and show that this further increases the performance of DRNs. In addition, we show that the accuracy advantage of DRNs is further magnified in downstream applications such as object localization and semantic segmentation."
    },
    {
        "paperId": "714733b70475649320513fa6fa030bd01dd4cd37",
        "url": "https://www.semanticscholar.org/paper/714733b70475649320513fa6fa030bd01dd4cd37",
        "title": "Reliable Crowdsourcing and Deep Locality-Preserving Learning for Expression Recognition in the Wild",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2017,
        "citationCount": 1197,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR.2017.277?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR.2017.277, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2144502570",
                "name": "Shan Li"
            },
            {
                "authorId": "1774956",
                "name": "Weihong Deng"
            },
            {
                "authorId": "8491162",
                "name": "Junping Du"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "744464cd6fa8341633cd3b5d378faab18a3b543a",
        "url": "https://www.semanticscholar.org/paper/744464cd6fa8341633cd3b5d378faab18a3b543a",
        "title": "Network Dissection: Quantifying Interpretability of Deep Visual Representations",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2017,
        "citationCount": 1664,
        "openAccessPdf": {
            "url": "https://dspace.mit.edu/bitstream/1721.1/124985/2/final-network-dissection.pdf",
            "status": "GREEN",
            "license": "CCBYNCSA",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1704.05796, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "144159726",
                "name": "David Bau"
            },
            {
                "authorId": "145291669",
                "name": "Bolei Zhou"
            },
            {
                "authorId": "2556428",
                "name": "A. Khosla"
            },
            {
                "authorId": "143868587",
                "name": "A. Oliva"
            },
            {
                "authorId": "143805211",
                "name": "A. Torralba"
            }
        ],
        "abstract": "We propose a general framework called Network Dissection for quantifying the interpretability of latent representations of CNNs by evaluating the alignment between individual hidden units and a set of semantic concepts. Given any CNN model, the proposed method draws on a data set of concepts to score the semantics of hidden units at each intermediate convolutional layer. The units with semantics are labeled across a broad range of visual concepts including objects, parts, scenes, textures, materials, and colors. We use the proposed method to test the hypothesis that interpretability is an axis-independent property of the representation space, then we apply the method to compare the latent representations of various networks when trained to solve different classification problems. We further analyze the effect of training iterations, compare networks trained with different initializations, and measure the effect of dropout and batch normalization on the interpretability of deep visual representations. We demonstrate that the proposed method can shed light on characteristics of CNN models and training methods that go beyond measurements of their discriminative power."
    },
    {
        "paperId": "76d2422cf5db5a6a96977fc42dcdd92422c27c21",
        "url": "https://www.semanticscholar.org/paper/76d2422cf5db5a6a96977fc42dcdd92422c27c21",
        "title": "Re-ranking Person Re-identification with k-Reciprocal Encoding",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2017,
        "citationCount": 1589,
        "openAccessPdf": {
            "url": "https://opus.lib.uts.edu.au/bitstream/10453/118077/4/6FE4BDE0-2335-4423-BA25-603F4FFF0AF2.pdf",
            "status": "GREEN",
            "license": "other-oa",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1701.08398, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2069512103",
                "name": "Zhun Zhong"
            },
            {
                "authorId": "144802394",
                "name": "Liang Zheng"
            },
            {
                "authorId": "38187621",
                "name": "Donglin Cao"
            },
            {
                "authorId": "8086812",
                "name": "Shaozi Li"
            }
        ],
        "abstract": "When considering person re-identification (re-ID) as a retrieval process, re-ranking is a critical step to improve its accuracy. Yet in the re-ID community, limited effort has been devoted to re-ranking, especially those fully automatic, unsupervised solutions. In this paper, we propose a k-reciprocal encoding method to re-rank the re-ID results. Our hypothesis is that if a gallery image is similar to the probe in the k-reciprocal nearest neighbors, it is more likely to be a true match. Specifically, given an image, a k-reciprocal feature is calculated by encoding its k-reciprocal nearest neighbors into a single vector, which is used for re-ranking under the Jaccard distance. The final distance is computed as the combination of the original distance and the Jaccard distance. Our re-ranking method does not require any human interaction or any labeled data, so it is applicable to large-scale datasets. Experiments on the large-scale Market-1501, CUHK03, MARS, and PRW datasets confirm the effectiveness of our method."
    },
    {
        "paperId": "77d30cf9a34fb6b50979c6a68863099da9a060ad",
        "url": "https://www.semanticscholar.org/paper/77d30cf9a34fb6b50979c6a68863099da9a060ad",
        "title": "Residual Attention Network for Image Classification",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2017,
        "citationCount": 3530,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1704.06904",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1704.06904, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1682816",
                "name": "Fei Wang"
            },
            {
                "authorId": "2152153832",
                "name": "Mengqing Jiang"
            },
            {
                "authorId": "2005026919",
                "name": "C. Qian"
            },
            {
                "authorId": "92887925",
                "name": "Shuo Yang"
            },
            {
                "authorId": "2133092242",
                "name": "Cheng Li"
            },
            {
                "authorId": "2108970782",
                "name": "Honggang Zhang"
            },
            {
                "authorId": "31843833",
                "name": "Xiaogang Wang"
            },
            {
                "authorId": "50295995",
                "name": "Xiaoou Tang"
            }
        ],
        "abstract": "In this work, we propose Residual Attention Network, a convolutional neural network using attention mechanism which can incorporate with state-of-art feed forward network architecture in an end-to-end training fashion. Our Residual Attention Network is built by stacking Attention Modules which generate attention-aware features. The attention-aware features from different modules change adaptively as layers going deeper. Inside each Attention Module, bottom-up top-down feedforward structure is used to unfold the feedforward and feedback attention process into a single feedforward process. Importantly, we propose attention residual learning to train very deep Residual Attention Networks which can be easily scaled up to hundreds of layers. Extensive analyses are conducted on CIFAR-10 and CIFAR-100 datasets to verify the effectiveness of every module mentioned above. Our Residual Attention Network achieves state-of-the-art object recognition performance on three benchmark datasets including CIFAR-10 (3.90% error), CIFAR-100 (20.45% error) and ImageNet (4.8% single model and single crop, top-5 error). Note that, our method achieves 0.6% top-1 accuracy improvement with 46% trunk depth and 69% forward FLOPs comparing to ResNet-200. The experiment also demonstrates that our network is robust against noisy labels."
    },
    {
        "paperId": "7bcebd481bb1161843efdd135e4ba59dfac4b61c",
        "url": "https://www.semanticscholar.org/paper/7bcebd481bb1161843efdd135e4ba59dfac4b61c",
        "title": "Age Progression/Regression by Conditional Adversarial Autoencoder",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2017,
        "citationCount": 1225,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1702.08423",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1702.08423, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1786391",
                "name": "Zhifei Zhang"
            },
            {
                "authorId": "144404428",
                "name": "Yang Song"
            },
            {
                "authorId": "1698645",
                "name": "H. Qi"
            }
        ],
        "abstract": "If I provide you a face image of mine (without telling you the actual age when I took the picture) and a large amount of face images that I crawled (containing labeled faces of different ages but not necessarily paired), can you show me what I would look like when I am 80 or what I was like when I was 5? The answer is probably a No. Most existing face aging works attempt to learn the transformation between age groups and thus would require the paired samples as well as the labeled query image. In this paper, we look at the problem from a generative modeling perspective such that no paired samples is required. In addition, given an unlabeled image, the generative model can directly produce the image with desired age attribute. We propose a conditional adversarial autoencoder (CAAE) that learns a face manifold, traversing on which smooth age progression and regression can be realized simultaneously. In CAAE, the face is first mapped to a latent vector through a convolutional encoder, and then the vector is projected to the face manifold conditional on age through a deconvolutional generator. The latent vector preserves personalized face features (i.e., personality) and the age condition controls progression vs. regression. Two adversarial networks are imposed on the encoder and generator, respectively, forcing to generate more photo-realistic faces. Experimental results demonstrate the appealing performance and flexibility of the proposed framework by comparing with the state-of-the-art and ground truth."
    },
    {
        "paperId": "7f065002afcb90240a41f05f138269c5675b9805",
        "url": "https://www.semanticscholar.org/paper/7f065002afcb90240a41f05f138269c5675b9805",
        "title": "On Human Motion Prediction Using Recurrent Neural Networks",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2017,
        "citationCount": 1004,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1705.02445",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1705.02445, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2109650391",
                "name": "Julieta Martinez"
            },
            {
                "authorId": "2105795",
                "name": "Michael J. Black"
            },
            {
                "authorId": "143881914",
                "name": "J. Romero"
            }
        ],
        "abstract": "Human motion modelling is a classical problem at the intersection of graphics and computer vision, with applications spanning human-computer interaction, motion synthesis, and motion prediction for virtual and augmented reality. Following the success of deep learning methods in several computer vision tasks, recent work has focused on using deep recurrent neural networks (RNNs) to model human motion, with the goal of learning time-dependent representations that perform tasks such as short-term motion prediction and long-term human motion synthesis. We examine recent work, with a focus on the evaluation methodologies commonly used in the literature, and show that, surprisingly, state of the art performance can be achieved by a simple baseline that does not attempt to model motion at all. We investigate this result, and analyze recent RNN methods by looking at the architectures, loss functions, and training procedures used in state-of-the-art approaches. We propose three changes to the standard RNN models typically used for human motion, which results in a simple and scalable RNN architecture that obtains state-of-the-art performance on human motion prediction."
    },
    {
        "paperId": "864ad6b8f2778bf7238e8a983551edc2072f08a9",
        "url": "https://www.semanticscholar.org/paper/864ad6b8f2778bf7238e8a983551edc2072f08a9",
        "title": "DESIRE: Distant Future Prediction in Dynamic Scenes with Interacting Agents",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2017,
        "citationCount": 1052,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1704.04394",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1704.04394, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2702448",
                "name": "Namhoon Lee"
            },
            {
                "authorId": "17132791",
                "name": "Wongun Choi"
            },
            {
                "authorId": "2998590",
                "name": "Paul Vernaza"
            },
            {
                "authorId": "2144103",
                "name": "C. Choy"
            },
            {
                "authorId": "143635540",
                "name": "Philip H. S. Torr"
            },
            {
                "authorId": "2099305",
                "name": "Manmohan Chandraker"
            }
        ],
        "abstract": "We introduce a Deep Stochastic IOC RNN Encoder-decoder framework, DESIRE, for the task of future predictions of multiple interacting agents in dynamic scenes. DESIRE effectively predicts future locations of objects in multiple scenes by 1) accounting for the multi-modal nature of the future prediction (i.e., given the same context, future may vary), 2) foreseeing the potential future outcomes and make a strategic prediction based on that, and 3) reasoning not only from the past motion history, but also from the scene context as well as the interactions among the agents. DESIRE achieves these in a single end-to-end trainable neural network model, while being computationally efficient. The model first obtains a diverse set of hypothetical future prediction samples employing a conditional variational auto-encoder, which are ranked and refined by the following RNN scoring-regression module. Samples are scored by accounting for accumulated future rewards, which enables better long-term strategic decisions similar to IOC frameworks. An RNN scene context fusion module jointly captures past motion histories, the semantic scene context and interactions among multiple agents. A feedback mechanism iterates over the ranking and refinement to further boost the prediction accuracy. We evaluate our model on two publicly available datasets: KITTI and Stanford Drone Dataset. Our experiments show that the proposed model significantly improves the prediction accuracy compared to other baseline methods."
    },
    {
        "paperId": "88d346374f17189cebef7394ae5d39492443df89",
        "url": "https://www.semanticscholar.org/paper/88d346374f17189cebef7394ae5d39492443df89",
        "title": "Deep Laplacian Pyramid Networks for Fast and Accurate Super-Resolution",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2017,
        "citationCount": 2686,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/1704.03915",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1704.03915, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2268189",
                "name": "Wei-Sheng Lai"
            },
            {
                "authorId": "3068086",
                "name": "Jia-Bin Huang"
            },
            {
                "authorId": "145237406",
                "name": "N. Ahuja"
            },
            {
                "authorId": "1715634",
                "name": "Ming-Hsuan Yang"
            }
        ],
        "abstract": "Convolutional neural networks have recently demonstrated high-quality reconstruction for single-image super-resolution. In this paper, we propose the Laplacian Pyramid Super-Resolution Network (LapSRN) to progressively reconstruct the sub-band residuals of high-resolution images. At each pyramid level, our model takes coarse-resolution feature maps as input, predicts the high-frequency residuals, and uses transposed convolutions for upsampling to the finer level. Our method does not require the bicubic interpolation as the pre-processing step and thus dramatically reduces the computational complexity. We train the proposed LapSRN with deep supervision using a robust Charbonnier loss function and achieve high-quality reconstruction. Furthermore, our network generates multi-scale predictions in one feed-forward pass through the progressive reconstruction, thereby facilitates resource-aware applications. Extensive quantitative and qualitative evaluations on benchmark datasets show that the proposed algorithm performs favorably against the state-of-the-art methods in terms of speed and accuracy."
    },
    {
        "paperId": "a0ac9d4b0b02f6eb3a5188624e87e63e5eae6709",
        "url": "https://www.semanticscholar.org/paper/a0ac9d4b0b02f6eb3a5188624e87e63e5eae6709",
        "title": "Look Closer to See Better: Recurrent Attention Convolutional Neural Network for Fine-Grained Image Recognition",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2017,
        "citationCount": 1242,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR.2017.476?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR.2017.476, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3247966",
                "name": "Jianlong Fu"
            },
            {
                "authorId": "28331771",
                "name": "Heliang Zheng"
            },
            {
                "authorId": "144025741",
                "name": "Tao Mei"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "a55970013b984f344dfbbbba677d89dce0ba5f81",
        "url": "https://www.semanticscholar.org/paper/a55970013b984f344dfbbbba677d89dce0ba5f81",
        "title": "Image Super-Resolution via Deep Recursive Residual Network",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2017,
        "citationCount": 2128,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR.2017.298?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR.2017.298, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "144970872",
                "name": "Ying Tai"
            },
            {
                "authorId": "51460259",
                "name": "Jian Yang"
            },
            {
                "authorId": "2108960018",
                "name": "Xiaoming Liu"
            }
        ],
        "abstract": "Recently, Convolutional Neural Network (CNN) based models have achieved great success in Single Image Super-Resolution (SISR). Owing to the strength of deep networks, these CNN models learn an effective nonlinear mapping from the low-resolution input image to the high-resolution target image, at the cost of requiring enormous parameters. This paper proposes a very deep CNN model (up to 52 convolutional layers) named Deep Recursive Residual Network (DRRN) that strives for deep yet concise networks. Specifically, residual learning is adopted, both in global and local manners, to mitigate the difficulty of training very deep networks, recursive learning is used to control the model parameters while increasing the depth. Extensive benchmark evaluation shows that DRRN significantly outperforms state of the art in SISR, while utilizing far fewer parameters. Code is available at https://github.com/tyshiwo/DRRN_CVPR17."
    },
    {
        "paperId": "b61a3f8b80bbd44f24544dc915f52fd30bbdf485",
        "url": "https://www.semanticscholar.org/paper/b61a3f8b80bbd44f24544dc915f52fd30bbdf485",
        "title": "Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2017,
        "citationCount": 8966,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/1705.07750",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1705.07750, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "35681810",
                "name": "Joo Carreira"
            },
            {
                "authorId": "1688869",
                "name": "Andrew Zisserman"
            }
        ],
        "abstract": "The paucity of videos in current action classification datasets (UCF-101 and HMDB-51) has made it difficult to identify good video architectures, as most methods obtain similar performance on existing small-scale benchmarks. This paper re-evaluates state-of-the-art architectures in light of the new Kinetics Human Action Video dataset. Kinetics has two orders of magnitude more data, with 400 human action classes and over 400 clips per class, and is collected from realistic, challenging YouTube videos. We provide an analysis on how current architectures fare on the task of action classification on this dataset and how much performance improves on the smaller benchmark datasets after pre-training on Kinetics. We also introduce a new Two-Stream Inflated 3D ConvNet (I3D) that is based on 2D ConvNet inflation: filters and pooling kernels of very deep image classification ConvNets are expanded into 3D, making it possible to learn seamless spatio-temporal feature extractors from video while leveraging successful ImageNet architecture designs and even their parameters. We show that, after pre-training on Kinetics, I3D models considerably improve upon the state-of-the-art in action classification, reaching 80.2% on HMDB-51 and 97.9% on UCF-101."
    },
    {
        "paperId": "b8ebda42e272d3617375118542d4675a0c0e501d",
        "url": "https://www.semanticscholar.org/paper/b8ebda42e272d3617375118542d4675a0c0e501d",
        "title": "Deep Hashing Network for Unsupervised Domain Adaptation",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2017,
        "citationCount": 2329,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/1706.07522",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1706.07522, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3151995",
                "name": "Hemanth Venkateswara"
            },
            {
                "authorId": "2072459779",
                "name": "Jos Eusbio"
            },
            {
                "authorId": "2471253",
                "name": "Shayok Chakraborty"
            },
            {
                "authorId": "1743991",
                "name": "S. Panchanathan"
            }
        ],
        "abstract": "In recent years, deep neural networks have emerged as a dominant machine learning tool for a wide variety of application domains. However, training a deep neural network requires a large amount of labeled data, which is an expensive process in terms of time, labor and human expertise. Domain adaptation or transfer learning algorithms address this challenge by leveraging labeled data in a different, but related source domain, to develop a model for the target domain. Further, the explosive growth of digital data has posed a fundamental challenge concerning its storage and retrieval. Due to its storage and retrieval efficiency, recent years have witnessed a wide application of hashing in a variety of computer vision applications. In this paper, we first introduce a new dataset, Office-Home, to evaluate domain adaptation algorithms. The dataset contains images of a variety of everyday objects from multiple domains. We then propose a novel deep learning framework that can exploit labeled source data and unlabeled target data to learn informative hash codes, to accurately classify unseen target data. To the best of our knowledge, this is the first research effort to exploit the feature learning capabilities of deep neural networks to learn representative hash codes to address the domain adaptation problem. Our extensive empirical studies on multiple transfer tasks corroborate the usefulness of the framework in learning efficient hash codes which outperform existing competitive baselines for unsupervised domain adaptation."
    },
    {
        "paperId": "bd8f77b7d3b9d272f7a68defc1412f73e5ac3135",
        "url": "https://www.semanticscholar.org/paper/bd8f77b7d3b9d272f7a68defc1412f73e5ac3135",
        "title": "SphereFace: Deep Hypersphere Embedding for Face Recognition",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2017,
        "citationCount": 2969,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/1704.08063",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1704.08063, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "36326884",
                "name": "Weiyang Liu"
            },
            {
                "authorId": "145357606",
                "name": "Yandong Wen"
            },
            {
                "authorId": "1751019",
                "name": "Zhiding Yu"
            },
            {
                "authorId": "2150655769",
                "name": "Ming Li"
            },
            {
                "authorId": "1681921",
                "name": "B. Raj"
            },
            {
                "authorId": "1779453",
                "name": "Le Song"
            }
        ],
        "abstract": "This paper addresses deep face recognition (FR) problem under open-set protocol, where ideal face features are expected to have smaller maximal intra-class distance than minimal inter-class distance under a suitably chosen metric space. However, few existing algorithms can effectively achieve this criterion. To this end, we propose the angular softmax (A-Softmax) loss that enables convolutional neural networks (CNNs) to learn angularly discriminative features. Geometrically, A-Softmax loss can be viewed as imposing discriminative constraints on a hypersphere manifold, which intrinsically matches the prior that faces also lie on a manifold. Moreover, the size of angular margin can be quantitatively adjusted by a parameter m. We further derive specific m to approximate the ideal feature criterion. Extensive analysis and experiments on Labeled Face in the Wild (LFW), Youtube Faces (YTF) and MegaFace Challenge 1 show the superiority of A-Softmax loss in FR tasks."
    },
    {
        "paperId": "bf9aee2857c39cfcc8f468aa93c81f48e2453d89",
        "url": "https://www.semanticscholar.org/paper/bf9aee2857c39cfcc8f468aa93c81f48e2453d89",
        "title": "Learning to Detect Salient Objects with Image-Level Supervision",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2017,
        "citationCount": 1177,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR.2017.404?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR.2017.404, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2118354131",
                "name": "Lijun Wang"
            },
            {
                "authorId": "153176123",
                "name": "Huchuan Lu"
            },
            {
                "authorId": null,
                "name": "Yifan Wang"
            },
            {
                "authorId": "40117581",
                "name": "Mengyang Feng"
            },
            {
                "authorId": "40562844",
                "name": "D. Wang"
            },
            {
                "authorId": "1714354",
                "name": "Baocai Yin"
            },
            {
                "authorId": "144526777",
                "name": "Xiang Ruan"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "dc8861b4ab6799be542829ae1ace13f23cf807cd",
        "url": "https://www.semanticscholar.org/paper/dc8861b4ab6799be542829ae1ace13f23cf807cd",
        "title": "Learning Deep CNN Denoiser Prior for Image Restoration",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2017,
        "citationCount": 2010,
        "openAccessPdf": {
            "url": "http://ira.lib.polyu.edu.hk/bitstream/10397/105643/1/Zhang_Learning_Deep_Cnn.pdf",
            "status": "GREEN",
            "license": "other-oa",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1704.03264, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "144110274",
                "name": "K. Zhang"
            },
            {
                "authorId": "1724520",
                "name": "W. Zuo"
            },
            {
                "authorId": "2476317",
                "name": "Shuhang Gu"
            },
            {
                "authorId": "36685537",
                "name": "Lei Zhang"
            }
        ],
        "abstract": "Model-based optimization methods and discriminative learning methods have been the two dominant strategies for solving various inverse problems in low-level vision. Typically, those two kinds of methods have their respective merits and drawbacks, e.g., model-based optimization methods are flexible for handling different inverse problems but are usually time-consuming with sophisticated priors for the purpose of good performance, in the meanwhile, discriminative learning methods have fast testing speed but their application range is greatly restricted by the specialized task. Recent works have revealed that, with the aid of variable splitting techniques, denoiser prior can be plugged in as a modular part of model-based optimization methods to solve other inverse problems (e.g., deblurring). Such an integration induces considerable advantage when the denoiser is obtained via discriminative learning. However, the study of integration with fast discriminative denoiser prior is still lacking. To this end, this paper aims to train a set of fast and effective CNN (convolutional neural network) denoisers and integrate them into model-based optimization method to solve other inverse problems. Experimental results demonstrate that the learned set of denoisers can not only achieve promising Gaussian denoising results but also can be used as prior to deliver good performance for various low-level vision applications."
    },
    {
        "paperId": "e52e37cd91366f07df1f98e88f87010f494dd16e",
        "url": "https://www.semanticscholar.org/paper/e52e37cd91366f07df1f98e88f87010f494dd16e",
        "title": "ScanNet: Richly-Annotated 3D Reconstructions of Indoor Scenes",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2017,
        "citationCount": 4856,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1702.04405",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1702.04405, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2208531",
                "name": "Angela Dai"
            },
            {
                "authorId": "145830541",
                "name": "Angel X. Chang"
            },
            {
                "authorId": "2295141",
                "name": "M. Savva"
            },
            {
                "authorId": "49034306",
                "name": "Maciej Halber"
            },
            {
                "authorId": "1807080",
                "name": "T. Funkhouser"
            },
            {
                "authorId": "2209612",
                "name": "M. Niener"
            }
        ],
        "abstract": "A key requirement for leveraging supervised deep learning methods is the availability of large, labeled datasets. Unfortunately, in the context of RGB-D scene understanding, very little data is available &#x2013; current datasets cover a small range of scene views and have limited semantic annotations. To address this issue, we introduce ScanNet, an RGB-D video dataset containing 2.5M views in 1513 scenes annotated with 3D camera poses, surface reconstructions, and semantic segmentations. To collect this data, we designed an easy-to-use and scalable RGB-D capture system that includes automated surface reconstruction and crowdsourced semantic annotation. We show that using this data helps achieve state-of-the-art performance on several 3D scene understanding tasks, including 3D object classification, semantic voxel labeling, and CAD model retrieval."
    },
    {
        "paperId": "1365b4a286e607a4902ef11c84a1f309719d946c",
        "url": "https://www.semanticscholar.org/paper/1365b4a286e607a4902ef11c84a1f309719d946c",
        "title": "ADVENT: Adversarial Entropy Minimization for Domain Adaptation in Semantic Segmentation",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2018,
        "citationCount": 1422,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1811.12833, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2142754750",
                "name": "Tuan-Hung Vu"
            },
            {
                "authorId": "3451474",
                "name": "Himalaya Jain"
            },
            {
                "authorId": "48326034",
                "name": "Max Bucher"
            },
            {
                "authorId": "51021910",
                "name": "M. Cord"
            },
            {
                "authorId": "144565371",
                "name": "P. Prez"
            }
        ],
        "abstract": "Semantic segmentation is a key problem for many computer vision tasks. While approaches based on convolutional neural networks constantly break new records on different benchmarks, generalizing well to diverse testing environments remains a major challenge. In numerous real-world applications, there is indeed a large gap between data distributions in train and test domains, which results in severe performance loss at run-time. In this work, we address the task of unsupervised domain adaptation in semantic segmentation with losses based on the entropy of the pixel-wise predictions. To this end, we propose two novel, complementary methods using (i) entropy loss and (ii) adversarial loss respectively. We demonstrate state-of-the-art performance in semantic segmentation on two challenging synthetic-2-real set-ups and show that the approach can also be used for detection."
    },
    {
        "paperId": "171ab935a731a9719d443a77fa6bcb30249b3498",
        "url": "https://www.semanticscholar.org/paper/171ab935a731a9719d443a77fa6bcb30249b3498",
        "title": "3D Human Pose Estimation in Video With Temporal Convolutions and Semi-Supervised Training",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2018,
        "citationCount": 1145,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1811.11742",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1811.11742, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "41018093",
                "name": "Dario Pavllo"
            },
            {
                "authorId": "2366173924",
                "name": "Christoph Feichtenhofer"
            },
            {
                "authorId": "2529182",
                "name": "David Grangier"
            },
            {
                "authorId": "2325985",
                "name": "Michael Auli"
            }
        ],
        "abstract": "In this work, we demonstrate that 3D poses in video can be effectively estimated with a fully convolutional model based on dilated temporal convolutions over 2D keypoints. We also introduce back-projection, a simple and effective semi-supervised training method that leverages unlabeled video data. We start with predicted 2D keypoints for unlabeled video, then estimate 3D poses and finally back-project to the input 2D keypoints. In the supervised setting, our fully-convolutional model outperforms the previous best result from the literature by 6 mm mean per-joint position error on Human3.6M, corresponding to an error reduction of 11%, and the model also shows significant improvements on HumanEva-I. Moreover, experiments with back-projection show that it comfortably outperforms previous state-of-the-art results in semi-supervised settings where labeled data is scarce. Code and models are available at https://github.com/facebookresearch/VideoPose3D"
    },
    {
        "paperId": "29309743870c825f9645a4803af727402462e513",
        "url": "https://www.semanticscholar.org/paper/29309743870c825f9645a4803af727402462e513",
        "title": "Bag of Tricks for Image Classification with Convolutional Neural Networks",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2018,
        "citationCount": 1524,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1812.01187",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1812.01187, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2118328320",
                "name": "Tong He"
            },
            {
                "authorId": "2117991985",
                "name": "Zhi Zhang"
            },
            {
                "authorId": "2119077209",
                "name": "Hang Zhang"
            },
            {
                "authorId": "48806269",
                "name": "Zhongyue Zhang"
            },
            {
                "authorId": "2369548",
                "name": "Junyuan Xie"
            },
            {
                "authorId": "2112143972",
                "name": "Mu Li"
            }
        ],
        "abstract": "Much of the recent progress made in image classification research can be credited to training procedure refinements, such as changes in data augmentations and optimization methods. In the literature, however, most refinements are either briefly mentioned as implementation details or only visible in source code. In this paper, we will examine a collection of such refinements and empirically evaluate their impact on the final model accuracy through ablation study. We will show that, by combining these refinements together, we are able to improve various CNN models significantly. For example, we raise ResNet-50's top-1 validation accuracy from 75.3% to 79.29% on ImageNet. We will also demonstrate that improvement on image classification accuracy leads to better transfer learning performance in other application domains such as object detection and semantic segmentation."
    },
    {
        "paperId": "2e689bdce24cf3644432505ce2783f03a1445ed2",
        "url": "https://www.semanticscholar.org/paper/2e689bdce24cf3644432505ce2783f03a1445ed2",
        "title": "Occupancy Networks: Learning 3D Reconstruction in Function Space",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2018,
        "citationCount": 3195,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1812.03828",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1812.03828, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "8226549",
                "name": "L. Mescheder"
            },
            {
                "authorId": "52211220",
                "name": "Michael Oechsle"
            },
            {
                "authorId": "145048708",
                "name": "Michael Niemeyer"
            },
            {
                "authorId": "2388416",
                "name": "Sebastian Nowozin"
            },
            {
                "authorId": "47237027",
                "name": "Andreas Geiger"
            }
        ],
        "abstract": "With the advent of deep neural networks, learning-based approaches for 3D reconstruction have gained popularity. However, unlike for images, in 3D there is no canonical representation which is both computationally and memory efficient yet allows for representing high-resolution geometry of arbitrary topology. Many of the state-of-the-art learning-based 3D reconstruction approaches can hence only represent very coarse 3D geometry or are limited to a restricted domain. In this paper, we propose Occupancy Networks, a new representation for learning-based 3D reconstruction methods. Occupancy networks implicitly represent the 3D surface as the continuous decision boundary of a deep neural network classifier. In contrast to existing approaches, our representation encodes a description of the 3D output at infinite resolution without excessive memory footprint. We validate that our representation can efficiently encode 3D structure and can be inferred from various kinds of input. Our experiments demonstrate competitive results, both qualitatively and quantitatively, for the challenging tasks of 3D reconstruction from single images, noisy point clouds and coarse discrete voxel grids. We believe that occupancy networks will become a useful tool in a wide variety of learning-based 3D tasks."
    },
    {
        "paperId": "3bb322718d64a34b91b29c8230c5978de5d7fb7a",
        "url": "https://www.semanticscholar.org/paper/3bb322718d64a34b91b29c8230c5978de5d7fb7a",
        "title": "PointPillars: Fast Encoders for Object Detection From Point Clouds",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2018,
        "citationCount": 4067,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1812.05784",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1812.05784, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "33242383",
                "name": "Alex H. Lang"
            },
            {
                "authorId": "22254044",
                "name": "Sourabh Vora"
            },
            {
                "authorId": "3078154",
                "name": "Holger Caesar"
            },
            {
                "authorId": "2131343647",
                "name": "Lubing Zhou"
            },
            {
                "authorId": "2109811170",
                "name": "Jiong Yang"
            },
            {
                "authorId": "3258919",
                "name": "Oscar Beijbom"
            }
        ],
        "abstract": "Object detection in point clouds is an important aspect of many robotics applications such as autonomous driving. In this paper, we consider the problem of encoding a point cloud into a format appropriate for a downstream detection pipeline. Recent literature suggests two types of encoders; fixed encoders tend to be fast but sacrifice accuracy, while encoders that are learned from data are more accurate, but slower. In this work, we propose PointPillars, a novel encoder which utilizes PointNets to learn a representation of point clouds organized in vertical columns (pillars). While the encoded features can be used with any standard 2D convolutional detection architecture, we further propose a lean downstream network. Extensive experimentation shows that PointPillars outperforms previous encoders with respect to both speed and accuracy by a large margin. Despite only using lidar, our full detection pipeline significantly outperforms the state of the art, even among fusion methods, with respect to both the 3D and birds eye view KITTI benchmarks. This detection performance is achieved while running at 62 Hz: a 2 - 4 fold runtime improvement. A faster version of our method matches the state of the art at 105 Hz. These benchmarks suggest that PointPillars is an appropriate encoding for object detection in point clouds."
    },
    {
        "paperId": "45532bffbfbb5553da0b2d0844e95a1b37e59147",
        "url": "https://www.semanticscholar.org/paper/45532bffbfbb5553da0b2d0844e95a1b37e59147",
        "title": "FBNet: Hardware-Aware Efficient ConvNet Design via Differentiable Neural Architecture Search",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2018,
        "citationCount": 1377,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1812.03443",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1812.03443, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3130257",
                "name": "Bichen Wu"
            },
            {
                "authorId": "4527324",
                "name": "Xiaoliang Dai"
            },
            {
                "authorId": "2918780",
                "name": "Peizhao Zhang"
            },
            {
                "authorId": "2146020227",
                "name": "Yanghan Wang"
            },
            {
                "authorId": "2075373634",
                "name": "Fei Sun"
            },
            {
                "authorId": "2115877352",
                "name": "Yiming Wu"
            },
            {
                "authorId": "39402399",
                "name": "Yuandong Tian"
            },
            {
                "authorId": "48682997",
                "name": "Pter Vajda"
            },
            {
                "authorId": "39978391",
                "name": "Yangqing Jia"
            },
            {
                "authorId": "1732330",
                "name": "K. Keutzer"
            }
        ],
        "abstract": "Designing accurate and efficient ConvNets for mobile devices is challenging because the design space is combinatorially large. Due to this, previous neural architecture search (NAS) methods are computationally expensive. ConvNet architecture optimality depends on factors such as input resolution and target devices. However, existing approaches are too resource demanding for case-by-case redesigns. Also, previous work focuses primarily on reducing FLOPs, but FLOP count does not always reflect actual latency. To address these, we propose a differentiable neural architecture search (DNAS) framework that uses gradient-based methods to optimize ConvNet architectures, avoiding enumerating and training individual architectures separately as in previous methods. FBNets (Facebook-Berkeley-Nets), a family of models discovered by DNAS surpass state-of-the-art models both designed manually and generated automatically. FBNet-B achieves 74.1% top-1 accuracy on ImageNet with 295M FLOPs and 23.1 ms latency on a Samsung S8 phone, 2.4x smaller and 1.5x faster than MobileNetV2-1.3 with similar accuracy. Despite higher accuracy and lower latency than MnasNet, we estimate FBNet-B's search cost is 420x smaller than MnasNet's, at only 216 GPU-hours. Searched for different resolutions and channel sizes, FBNets achieve 1.5% to 6.4% higher accuracy than MobileNetV2. The smallest FBNet achieves 50.2% accuracy and 2.9 ms latency (345 frames per second) on a Samsung S8. Over a Samsung-optimized FBNet, the iPhone-X-optimized model achieves a 1.4x speedup on an iPhone X. FBNet models are open-sourced at https://github. com/facebookresearch/mobile-vision."
    },
    {
        "paperId": "5ee147684b06ffc4db0f6326e0cba017d12ceff3",
        "url": "https://www.semanticscholar.org/paper/5ee147684b06ffc4db0f6326e0cba017d12ceff3",
        "title": "PointConv: Deep Convolutional Networks on 3D Point Clouds",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2018,
        "citationCount": 1767,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1811.07246",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1811.07246, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "47203460",
                "name": "Wenxuan Wu"
            },
            {
                "authorId": "2539841",
                "name": "Zhongang Qi"
            },
            {
                "authorId": "3141988",
                "name": "Fuxin Li"
            }
        ],
        "abstract": "Unlike images which are represented in regular dense grids, 3D point clouds are irregular and unordered, hence applying convolution on them can be difficult. In this paper, we extend the dynamic filter to a new convolution operation, named PointConv. PointConv can be applied on point clouds to build deep convolutional networks. We treat convolution kernels as nonlinear functions of the local coordinates of 3D points comprised of weight and density functions. With respect to a given point, the weight functions are learned with multi-layer perceptron networks and the density functions through kernel density estimation. A novel reformulation is proposed for efficiently computing the weight functions, which allowed us to dramatically scale up the network and significantly improve its performance. The learned convolution kernel can be used to compute translation-invariant and permutation-invariant convolution on any point set in the 3D space. Besides, PointConv can also be used as deconvolution operators to propagate features from a subsampled point cloud back to its original resolution. Experiments on ModelNet40, ShapeNet, and ScanNet show that deep convolutional neural networks built on PointConv are able to achieve state-of-the-art on challenging semantic segmentation benchmarks on 3D point clouds. Besides, our experiments converting CIFAR-10 into a point cloud showed that networks built on PointConv can match the performance of convolutional networks in 2D images of a similar structure."
    },
    {
        "paperId": "619cf9d39abb93fe1ab17921c163fc5734ac1e70",
        "url": "https://www.semanticscholar.org/paper/619cf9d39abb93fe1ab17921c163fc5734ac1e70",
        "title": "End-To-End Multi-Task Learning With Attention",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2018,
        "citationCount": 1228,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1803.10704, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "48641424",
                "name": "Shikun Liu"
            },
            {
                "authorId": "144596772",
                "name": "Edward Johns"
            },
            {
                "authorId": "2052135690",
                "name": "A. Davison"
            }
        ],
        "abstract": "We propose a novel multi-task learning architecture, which allows learning of task-specific feature-level attention. Our design, the Multi-Task Attention Network (MTAN), consists of a single shared network containing a global feature pool, together with a soft-attention module for each task. These modules allow for learning of task-specific features from the global features, whilst simultaneously allowing for features to be shared across different tasks. The architecture can be trained end-to-end and can be built upon any feed-forward neural network, is simple to implement, and is parameter efficient. We evaluate our approach on a variety of datasets, across both image-to-image predictions and image classification tasks. We show that our architecture is state-of-the-art in multi-task learning compared to existing methods, and is also less sensitive to various weighting schemes in the multi-task loss function. Code is available at https://github.com/lorenmt/mtan."
    },
    {
        "paperId": "693c97ecedb0a84539b7162c95e89fa3cd84ca73",
        "url": "https://www.semanticscholar.org/paper/693c97ecedb0a84539b7162c95e89fa3cd84ca73",
        "title": "MnasNet: Platform-Aware Neural Architecture Search for Mobile",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2018,
        "citationCount": 3273,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1807.11626",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1807.11626, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "120805419",
                "name": "Mingxing Tan"
            },
            {
                "authorId": null,
                "name": "Bo Chen"
            },
            {
                "authorId": "34320634",
                "name": "Ruoming Pang"
            },
            {
                "authorId": "2053781980",
                "name": "Vijay Vasudevan"
            },
            {
                "authorId": "2827616",
                "name": "Quoc V. Le"
            }
        ],
        "abstract": "Designing convolutional neural networks (CNN) for mobile devices is challenging because mobile models need to be small and fast, yet still accurate. Although significant efforts have been dedicated to design and improve mobile CNNs on all dimensions, it is very difficult to manually balance these trade-offs when there are so many architectural possibilities to consider. In this paper, we propose an automated mobile neural architecture search (MNAS) approach, which explicitly incorporate model latency into the main objective so that the search can identify a model that achieves a good trade-off between accuracy and latency. Unlike previous work, where latency is considered via another, often inaccurate proxy (e.g., FLOPS), our approach directly measures real-world inference latency by executing the model on mobile phones. To further strike the right balance between flexibility and search space size, we propose a novel factorized hierarchical search space that encourages layer diversity throughout the network. Experimental results show that our approach consistently outperforms state-of-the-art mobile CNN models across multiple vision tasks. On the ImageNet classification task, our MnasNet achieves 75.2% top-1 accuracy with 78ms latency on a Pixel phone, which is 1.8 faster than MobileNetV2 with 0.5% higher accuracy and 2.3 faster than NASNet with 1.2% higher accuracy. Our MnasNet also achieves better mAP quality than MobileNets for COCO object detection. Code is at https://github.com/tensorflow/tpu/tree/master/models/official/mnasnet."
    },
    {
        "paperId": "6fc0e648ef6002fe3507107f4f3637c76d54477f",
        "url": "https://www.semanticscholar.org/paper/6fc0e648ef6002fe3507107f4f3637c76d54477f",
        "title": "On the Continuity of Rotation Representations in Neural Networks",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2018,
        "citationCount": 1533,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1812.07035",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1812.07035, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": null,
                "name": "Yi Zhou"
            },
            {
                "authorId": "2496412",
                "name": "Connelly Barnes"
            },
            {
                "authorId": "2054975",
                "name": "Jingwan Lu"
            },
            {
                "authorId": "1768964",
                "name": "Jimei Yang"
            },
            {
                "authorId": "79482877",
                "name": "Hao Li"
            }
        ],
        "abstract": "In neural networks, it is often desirable to work with various representations of the same space. For example, 3D rotations can be represented with quaternions or Euler angles. In this paper, we advance a definition of a continuous representation, which can be helpful for training deep neural networks. We relate this to topological concepts such as homeomorphism and embedding. We then investigate what are continuous and discontinuous representations for 2D, 3D, and n-dimensional rotations. We demonstrate that for 3D rotations, all representations are discontinuous in the real Euclidean spaces of four or fewer dimensions. Thus, widely used representations such as quaternions and Euler angles are discontinuous and difficult for neural networks to learn. We show that the 3D rotations have continuous representations in 5D and 6D, which are more suitable for learning. We also present continuous representations for the general case of the n-dimensional rotation group SO(n). While our main focus is on rotations, we also show that our constructions apply to other groups such as the orthogonal group and similarity transforms. We finally present empirical results, which show that our continuous rotation representations outperform discontinuous ones for several practical problems in graphics and vision, including a simple autoencoder sanity test, a rotation estimator for 3D point clouds, and an inverse kinematics solver for 3D human poses."
    },
    {
        "paperId": "7ce6eca495909de2ffa0b6d9c16993757208764e",
        "url": "https://www.semanticscholar.org/paper/7ce6eca495909de2ffa0b6d9c16993757208764e",
        "title": "PointRCNN: 3D Object Proposal Generation and Detection From Point Cloud",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2018,
        "citationCount": 2715,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1812.04244",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1812.04244, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2072683588",
                "name": "Shaoshuai Shi"
            },
            {
                "authorId": "93768810",
                "name": "Xiaogang Wang"
            },
            {
                "authorId": "47893312",
                "name": "Hongsheng Li"
            }
        ],
        "abstract": "In this paper, we propose PointRCNN for 3D object detection from raw point cloud. The whole framework is composed of two stages: stage-1 for the bottom-up 3D proposal generation and stage-2 for refining proposals in the canonical coordinates to obtain the final detection results. Instead of generating proposals from RGB image or projecting point cloud to bird's view or voxels as previous methods do, our stage-1 sub-network directly generates a small number of high-quality 3D proposals from point cloud in a bottom-up manner via segmenting the point cloud of the whole scene into foreground points and background. The stage-2 sub-network transforms the pooled points of each proposal to canonical coordinates to learn better local spatial features, which is combined with global semantic features of each point learned in stage-1 for accurate box refinement and confidence prediction. Extensive experiments on the 3D detection benchmark of KITTI dataset show that our proposed architecture outperforms state-of-the-art methods with remarkable margins by using only point cloud as input. The code is available at https://github.com/sshaoshuai/PointRCNN."
    },
    {
        "paperId": "852bad998bc2a1c8c86314da3b5b5a162a76d500",
        "url": "https://www.semanticscholar.org/paper/852bad998bc2a1c8c86314da3b5b5a162a76d500",
        "title": "Toward Convolutional Blind Denoising of Real Photographs",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2018,
        "citationCount": 1023,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1807.04686",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1807.04686, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "145089091",
                "name": "Shi Guo"
            },
            {
                "authorId": "2371501",
                "name": "Zifei Yan"
            },
            {
                "authorId": "144110274",
                "name": "K. Zhang"
            },
            {
                "authorId": "1724520",
                "name": "W. Zuo"
            },
            {
                "authorId": "2152828733",
                "name": "Lei Zhang"
            }
        ],
        "abstract": "While deep convolutional neural networks (CNNs) have achieved impressive success in image denoising with additive white Gaussian noise (AWGN), their performance remains limited on real-world noisy photographs. The main reason is that their learned models are easy to overfit on the simplified AWGN model which deviates severely from the complicated real-world noise model. In order to improve the generalization ability of deep CNN denoisers, we suggest training a convolutional blind denoising network (CBDNet) with more realistic noise model and real-world noisy-clean image pairs. On the one hand, both signal-dependent noise and in-camera signal processing pipeline is considered to synthesize realistic noisy images. On the other hand, real-world noisy photographs and their nearly noise-free counterparts are also included to train our CBDNet. To further provide an interactive strategy to rectify denoising result conveniently, a noise estimation subnetwork with asymmetric learning to suppress under-estimation of noise level is embedded into CBDNet. Extensive experimental results on three datasets of real-world noisy pho- tographs clearly demonstrate the superior performance of CBDNet over state-of-the-arts in terms of quantitative met- rics and visual quality. The code has been made available at https://github.com/GuoShi28/CBDNet."
    },
    {
        "paperId": "8a8cfa45b4c0d071fbffa091c02670b19c94b693",
        "url": "https://www.semanticscholar.org/paper/8a8cfa45b4c0d071fbffa091c02670b19c94b693",
        "title": "Do Better ImageNet Models Transfer Better?",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2018,
        "citationCount": 1438,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1805.08974",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1805.08974, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "40464924",
                "name": "Simon Kornblith"
            },
            {
                "authorId": "1789737",
                "name": "Jonathon Shlens"
            },
            {
                "authorId": "2827616",
                "name": "Quoc V. Le"
            }
        ],
        "abstract": "Transfer learning is a cornerstone of computer vision, yet little work has been done to evaluate the relationship between architecture and transfer. An implicit hypothesis in modern computer vision research is that models that perform better on ImageNet necessarily perform better on other vision tasks. However, this hypothesis has never been systematically tested. Here, we compare the performance of 16 classification networks on 12 image classification datasets. We find that, when networks are used as fixed feature extractors or fine-tuned, there is a strong correlation between ImageNet accuracy and transfer accuracy (r = 0.99 and 0.96, respectively). In the former setting, we find that this relationship is very sensitive to the way in which networks are trained on ImageNet; many common forms of regularization slightly improve ImageNet accuracy but yield features that are much worse for transfer learning. Additionally, we find that, on two small fine-grained image classification datasets, pretraining on ImageNet provides minimal benefits, indicating the learned features from ImageNet do not transfer well to fine-grained tasks. Together, our results show that ImageNet architectures generalize well across datasets, but ImageNet features are less general than previously suggested."
    },
    {
        "paperId": "8c328bc048449131d98b545eaa3c99f8356c5620",
        "url": "https://www.semanticscholar.org/paper/8c328bc048449131d98b545eaa3c99f8356c5620",
        "title": "Pseudo-LiDAR From Visual Depth Estimation: Bridging the Gap in 3D Object Detection for Autonomous Driving",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2018,
        "citationCount": 1100,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/1812.07179",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1812.07179, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "47906756",
                "name": "Yan Wang"
            },
            {
                "authorId": "38784892",
                "name": "Wei-Lun Chao"
            },
            {
                "authorId": "47230666",
                "name": "Divyansh Garg"
            },
            {
                "authorId": "1790580",
                "name": "Bharath Hariharan"
            },
            {
                "authorId": "143903367",
                "name": "M. Campbell"
            },
            {
                "authorId": "7446832",
                "name": "Kilian Q. Weinberger"
            }
        ],
        "abstract": "3D object detection is an essential task in autonomous driving. Recent techniques excel with highly accurate detection rates, provided the 3D input data is obtained from precise but expensive LiDAR technology. Approaches based on cheaper monocular or stereo imagery data have, until now, resulted in drastically lower accuracies --- a gap that is commonly attributed to poor image-based depth estimation. However, in this paper we argue that it is not the quality of the data but its representation that accounts for the majority of the difference. Taking the inner workings of convolutional neural networks into consideration, we propose to convert image-based depth maps to pseudo-LiDAR representations --- essentially mimicking the LiDAR signal. With this representation we can apply different existing LiDAR-based detection algorithms. On the popular KITTI benchmark, our approach achieves impressive improvements over the existing state-of-the-art in image-based performance --- raising the detection accuracy of objects within the 30m range from the previous state-of-the-art of 22% to an unprecedented 74%. At the time of submission our algorithm holds the highest entry on the KITTI 3D object detection leaderboard for stereo-image-based approaches."
    },
    {
        "paperId": "900ab48d25b44c076e31224b7befa503d9550c53",
        "url": "https://www.semanticscholar.org/paper/900ab48d25b44c076e31224b7befa503d9550c53",
        "title": "LaSOT: A High-Quality Benchmark for Large-Scale Single Object Tracking",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2018,
        "citationCount": 1409,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1809.07845",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1809.07845, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "143911163",
                "name": "Heng Fan"
            },
            {
                "authorId": "2110865252",
                "name": "Liting Lin"
            },
            {
                "authorId": "2152891553",
                "name": "Fan Yang"
            },
            {
                "authorId": "49829905",
                "name": "Peng Chu"
            },
            {
                "authorId": "2056296884",
                "name": "Ge Deng"
            },
            {
                "authorId": "2116617939",
                "name": "Sijia Yu"
            },
            {
                "authorId": "81225235",
                "name": "Hexin Bai"
            },
            {
                "authorId": "121983569",
                "name": "Yong Xu"
            },
            {
                "authorId": "2686363",
                "name": "Chunyuan Liao"
            },
            {
                "authorId": "1805398",
                "name": "Haibin Ling"
            }
        ],
        "abstract": "In this paper, we present LaSOT, a high-quality benchmark for Large-scale Single Object Tracking. LaSOT consists of 1,400 sequences with more than 3.5M frames in total. Each frame in these sequences is carefully and manually annotated with a bounding box, making LaSOT the largest, to the best of our knowledge, densely annotated tracking benchmark. The average video length of LaSOT is more than 2,500 frames, and each sequence comprises various challenges deriving from the wild where target objects may disappear and re-appear again in the view. By releasing LaSOT, we expect to provide the community with a large-scale dedicated benchmark with high quality for both the training of deep trackers and the veritable evaluation of tracking algorithms. Moreover, considering the close connections of visual appearance and natural language, we enrich LaSOT by providing additional language specification, aiming at encouraging the exploration of natural linguistic feature for tracking. A thorough experimental evaluation of 35 tracking algorithms on LaSOT is presented with detailed analysis, and the results demonstrate that there is still a big room for improvements."
    },
    {
        "paperId": "97ef575d7049ba0f1aeba4453a6dcd5475567630",
        "url": "https://www.semanticscholar.org/paper/97ef575d7049ba0f1aeba4453a6dcd5475567630",
        "title": "From Coarse to Fine: Robust Hierarchical Localization at Large Scale",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2018,
        "citationCount": 1040,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1812.03506",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1812.03506, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "51435497",
                "name": "Paul-Edouard Sarlin"
            },
            {
                "authorId": "2586813",
                "name": "Csar Cadena"
            },
            {
                "authorId": "1720483",
                "name": "R. Siegwart"
            },
            {
                "authorId": "2886224",
                "name": "Marcin Dymczyk"
            }
        ],
        "abstract": "Robust and accurate visual localization is a fundamental capability for numerous applications, such as autonomous driving, mobile robotics, or augmented reality. It remains, however, a challenging task, particularly for large-scale environments and in presence of significant appearance changes. State-of-the-art methods not only struggle with such scenarios, but are often too resource intensive for certain real-time applications. In this paper we propose HF-Net, a hierarchical localization approach based on a monolithic CNN that simultaneously predicts local features and global descriptors for accurate 6-DoF localization. We exploit the coarse-to-fine localization paradigm: we first perform a global retrieval to obtain location hypotheses and only later match local features within those candidate places. This hierarchical approach incurs significant runtime savings and makes our system suitable for real-time operation. By leveraging learned descriptors, our method achieves remarkable localization robustness across large variations of appearance and sets a new state-of-the-art on two challenging benchmarks for large-scale localization."
    },
    {
        "paperId": "987b2db58fbe0bda771f11a046cd23de1ce92b39",
        "url": "https://www.semanticscholar.org/paper/987b2db58fbe0bda771f11a046cd23de1ce92b39",
        "title": "Deformable ConvNets V2: More Deformable, Better Results",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2018,
        "citationCount": 2368,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1811.11168",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1811.11168, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2578924",
                "name": "Xizhou Zhu"
            },
            {
                "authorId": "1823518756",
                "name": "Han Hu"
            },
            {
                "authorId": "145676588",
                "name": "Stephen Lin"
            },
            {
                "authorId": "3304536",
                "name": "Jifeng Dai"
            }
        ],
        "abstract": "The superior performance of Deformable Convolutional Networks arises from its ability to adapt to the geometric variations of objects. Through an examination of its adaptive behavior, we observe that while the spatial support for its neural features conforms more closely than regular ConvNets to object structure, this support may nevertheless extend well beyond the region of interest, causing features to be influenced by irrelevant image content. To address this problem, we present a reformulation of Deformable ConvNets that improves its ability to focus on pertinent image regions, through increased modeling power and stronger training. The modeling power is enhanced through a more comprehensive integration of deformable convolution within the network, and by introducing a modulation mechanism that expands the scope of deformation modeling. To effectively harness this enriched modeling capability, we guide network training via a proposed feature mimicking scheme that helps the network to learn features that reflect the object focus and classification power of R-CNN features. With the proposed contributions, this new version of Deformable ConvNets yields significant performance gains over the original model and produces leading results on the COCO benchmark for object detection and instance segmentation."
    },
    {
        "paperId": "ad655c25e052fa4eeed53421344aca6f239c4c9d",
        "url": "https://www.semanticscholar.org/paper/ad655c25e052fa4eeed53421344aca6f239c4c9d",
        "title": "Dual Attention Network for Scene Segmentation",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2018,
        "citationCount": 5755,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1809.02983",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1809.02983, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "145504601",
                "name": "J. Fu"
            },
            {
                "authorId": "46701354",
                "name": "J. Liu"
            },
            {
                "authorId": "115563890",
                "name": "Haijie Tian"
            },
            {
                "authorId": "49597404",
                "name": "Zhiwei Fang"
            },
            {
                "authorId": "1694235",
                "name": "Hanqing Lu"
            }
        ],
        "abstract": "In this paper, we address the scene segmentation task by capturing rich contextual dependencies based on the self-attention mechanism. Unlike previous works that capture contexts by multi-scale features fusion, we propose a Dual Attention Networks (DANet) to adaptively integrate local features with their global dependencies. Specifically, we append two types of attention modules on top of traditional dilated FCN, which model the semantic interdependencies in spatial and channel dimensions respectively. The position attention module selectively aggregates the features at each position by a weighted sum of the features at all positions. Similar features would be related to each other regardless of their distances. Meanwhile, the channel attention module selectively emphasizes interdependent channel maps by integrating associated features among all channel maps. We sum the outputs of the two attention modules to further improve feature representation which contributes to more precise segmentation results. We achieve new state-of-the-art segmentation performance on three challenging scene segmentation datasets, i.e., Cityscapes, PASCAL Context and COCO Stuff dataset. In particular, a Mean IoU score of 81.5% on Cityscapes test set is achieved without using coarse data."
    },
    {
        "paperId": "b763d492b88c079459e67c28959d616a3f1a572d",
        "url": "https://www.semanticscholar.org/paper/b763d492b88c079459e67c28959d616a3f1a572d",
        "title": "SoPhie: An Attentive GAN for Predicting Paths Compliant to Social and Physical Constraints",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2018,
        "citationCount": 1007,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1806.01482",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1806.01482, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "145759966",
                "name": "Amir Sadeghian"
            },
            {
                "authorId": "13622184",
                "name": "Vineet Kosaraju"
            },
            {
                "authorId": "51283807",
                "name": "A. Sadeghian"
            },
            {
                "authorId": "145564847",
                "name": "N. Hirose"
            },
            {
                "authorId": "1702137",
                "name": "S. Savarese"
            }
        ],
        "abstract": "This paper addresses the problem of path prediction for multiple interacting agents in a scene, which is a crucial step for many autonomous platforms such as self-driving cars and social robots. We present SoPhie; an interpretable framework based on Generative Adversarial Network (GAN), which leverages two sources of information, the path history of all the agents in a scene, and the scene context information, using images of the scene. To predict a future path for an agent, both physical and social information must be leveraged. Previous work has not been successful to jointly model physical and social interactions. Our approach blends a social attention mechanism with physical attention that helps the model to learn where to look in a large scene and extract the most salient parts of the image relevant to the path. Whereas, the social attention component aggregates information across the different agent interactions and extracts the most important trajectory information from the surrounding neighbors. SoPhie also takes advantage of GAN to generates more realistic samples and to capture the uncertain nature of the future paths by modeling its distribution. All these mechanisms enable our approach to predict socially and physically plausible paths for the agents and to achieve state-of-the-art performance on several different trajectory forecasting benchmarks."
    },
    {
        "paperId": "bb5bc0acea8d452a7999c512127b4f7b3acf8a6d",
        "url": "https://www.semanticscholar.org/paper/bb5bc0acea8d452a7999c512127b4f7b3acf8a6d",
        "title": "Filter Pruning via Geometric Median for Deep Convolutional Neural Networks Acceleration",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2018,
        "citationCount": 1172,
        "openAccessPdf": {
            "url": "https://opus.lib.uts.edu.au/bitstream/10453/141030/3/2019_CVPR_submitted_PFGM.pdf",
            "status": "GREEN",
            "license": "other-oa",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1811.00250, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2145997740",
                "name": "Yang He"
            },
            {
                "authorId": "144303230",
                "name": "Ping Liu"
            },
            {
                "authorId": "2274572919",
                "name": "Ziwei Wang"
            },
            {
                "authorId": "2111296371",
                "name": "Zhilan Hu"
            },
            {
                "authorId": "2048438762",
                "name": "Yi Yang"
            }
        ],
        "abstract": "Previous works utilized smaller-norm-less-important criterion to prune filters with smaller norm values in a convolutional neural network. In this paper, we analyze this norm-based criterion and point out that its effectiveness depends on two requirements that are not always met: (1) the norm deviation of the filters should be large; (2) the minimum norm of the filters should be small. To solve this problem, we propose a novel filter pruning method, namely Filter Pruning via Geometric Median (FPGM), to compress the model regardless of those two requirements. Unlike previous methods, FPGM compresses CNN models by pruning filters with redundancy, rather than those withrelatively less importance. When applied to two image classification benchmarks, our method validates its usefulness and strengths. Notably, on CIFAR-10, FPGM reduces more than 52% FLOPs on ResNet-110 with even 2.69% relative accuracy improvement. Moreover, on ILSVRC-2012, FPGM reduces more than 42% FLOPs on ResNet-101 without top-5 accuracy drop, which has advanced the state-of-the-art. Code is publicly available on GitHub: https://github.com/he-y/filter-pruning-geometric-median"
    },
    {
        "paperId": "be2cafced16bb8834bdd322a0a512142c8d05388",
        "url": "https://www.semanticscholar.org/paper/be2cafced16bb8834bdd322a0a512142c8d05388",
        "title": "Noise2Void - Learning Denoising From Single Noisy Images",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2018,
        "citationCount": 1281,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1811.10980",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1811.10980, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2903785",
                "name": "Alexander Krull"
            },
            {
                "authorId": "32290934",
                "name": "T. Buchholz"
            },
            {
                "authorId": "1773294",
                "name": "Florian Jug"
            }
        ],
        "abstract": "The field of image denoising is currently dominated by discriminative deep learning methods that are trained on pairs of noisy input and clean target images. Recently it has been shown that such methods can also be trained without clean targets. Instead, independent pairs of noisy images can be used, in an approach known as Noise2Noise (N2N). Here, we introduce Noise2Void (N2V), a training scheme that takes this idea one step further. It does not require noisy image pairs, nor clean target images. Consequently, N2V allows us to train directly on the body of data to be denoised and can therefore be applied when other methods cannot. Especially interesting is the application to biomedical image data, where the acquisition of training targets, clean or noisy, is frequently not possible. We compare the performance of N2V to approaches that have either clean target images and/or noisy image pairs available. Intuitively, N2V cannot be expected to outperform methods that have more information available during training. Still, we observe that the denoising performance of Noise2Void drops in moderation and compares favorably to training-free denoising methods."
    },
    {
        "paperId": "c3294425af6e2c059835ec7f0dca7290b48a8faf",
        "url": "https://www.semanticscholar.org/paper/c3294425af6e2c059835ec7f0dca7290b48a8faf",
        "title": "Learning Implicit Fields for Generative Shape Modeling",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2018,
        "citationCount": 1755,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1812.02822",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1812.02822, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2406577",
                "name": "Zhiqin Chen"
            },
            {
                "authorId": "39497427",
                "name": "Hao Zhang"
            }
        ],
        "abstract": "We advocate the use of implicit fields for learning generative models of shapes and introduce an implicit field decoder, called IM-NET, for shape generation, aimed at improving the visual quality of the generated shapes. An implicit field assigns a value to each point in 3D space, so that a shape can be extracted as an iso-surface. IM-NET is trained to perform this assignment by means of a binary classifier. Specifically, it takes a point coordinate, along with a feature vector encoding a shape, and outputs a value which indicates whether the point is outside the shape or not. By replacing conventional decoders by our implicit decoder for representation learning (via IM-AE) and shape generation (via IM-GAN), we demonstrate superior results for tasks such as generative shape modeling, interpolation, and single-view 3D reconstruction, particularly in terms of visual quality. Code and supplementary material are available at https://github.com/czq142857/implicit-decoder."
    },
    {
        "paperId": "ca235ce0decdb4f80024a429a20ae4437ceae09e",
        "url": "https://www.semanticscholar.org/paper/ca235ce0decdb4f80024a429a20ae4437ceae09e",
        "title": "ArcFace: Additive Angular Margin Loss for Deep Face Recognition",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2018,
        "citationCount": 7094,
        "openAccessPdf": {
            "url": "http://spiral.imperial.ac.uk/bitstream/10044/1/69953/2/1801.07698.pdf",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR.2019.00482?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR.2019.00482, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3234063",
                "name": "Jiankang Deng"
            },
            {
                "authorId": "3007274",
                "name": "J. Guo"
            },
            {
                "authorId": "1776444",
                "name": "S. Zafeiriou"
            }
        ],
        "abstract": "One of the main challenges in feature learning using Deep Convolutional Neural Networks (DCNNs) for large-scale face recognition is the design of appropriate loss functions that can enhance the discriminative power. Centre loss penalises the distance between deep features and their corresponding class centres in the Euclidean space to achieve intra-class compactness. SphereFace assumes that the linear transformation matrix in the last fully connected layer can be used as a representation of the class centres in the angular space and therefore penalises the angles between deep features and their corresponding weights in a multiplicative way. Recently, a popular line of research is to incorporate margins in well-established loss functions in order to maximise face class separability. In this paper, we propose an Additive Angular Margin Loss (ArcFace) to obtain highly discriminative features for face recognition. The proposed ArcFace has a clear geometric interpretation due to its exact correspondence to geodesic distance on a hypersphere. We present arguably the most extensive experimental evaluation against all recent state-of-the-art face recognition methods on ten face recognition benchmarks which includes a new large-scale image database with trillions of pairs and a large-scale video dataset. We show that ArcFace consistently outperforms the state of the art and can be easily implemented with negligible computational overhead. To facilitate future research, the code has been made available."
    },
    {
        "paperId": "ceb2ebef0b41e31c1a21b28c2734123900c005e2",
        "url": "https://www.semanticscholar.org/paper/ceb2ebef0b41e31c1a21b28c2734123900c005e2",
        "title": "A Style-Based Generator Architecture for Generative Adversarial Networks",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2018,
        "citationCount": 12050,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1812.04948",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1812.04948, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2976930",
                "name": "Tero Karras"
            },
            {
                "authorId": "36436218",
                "name": "S. Laine"
            },
            {
                "authorId": "1761103",
                "name": "Timo Aila"
            }
        ],
        "abstract": "We propose an alternative generator architecture for generative adversarial networks, borrowing from style transfer literature. The new architecture leads to an automatically learned, unsupervised separation of high-level attributes (e.g., pose and identity when trained on human faces) and stochastic variation in the generated images (e.g., freckles, hair), and it enables intuitive, scale-specific control of the synthesis. The new generator improves the state-of-the-art in terms of traditional distribution quality metrics, leads to demonstrably better interpolation properties, and also better disentangles the latent factors of variation. To quantify interpolation quality and disentanglement, we propose two new, automated methods that are applicable to any generator architecture. Finally, we introduce a new, highly varied and high-quality dataset of human faces."
    },
    {
        "paperId": "d1a4135a2edd1af8a1e501109bbf7c2c720f10f8",
        "url": "https://www.semanticscholar.org/paper/d1a4135a2edd1af8a1e501109bbf7c2c720f10f8",
        "title": "SiamRPN++: Evolution of Siamese Visual Tracking With Very Deep Networks",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2018,
        "citationCount": 2069,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1812.11703",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1812.11703, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "71788673",
                "name": "Bo Li"
            },
            {
                "authorId": "39533001",
                "name": "Wei Wu"
            },
            {
                "authorId": "145805403",
                "name": "Qiang Wang"
            },
            {
                "authorId": "2896212",
                "name": "Fangyi Zhang"
            },
            {
                "authorId": "1757173",
                "name": "Junliang Xing"
            },
            {
                "authorId": "1721677",
                "name": "Junjie Yan"
            }
        ],
        "abstract": "Siamese network based trackers formulate tracking as convolutional feature cross-correlation between target template and searching region. However, Siamese trackers still have accuracy gap compared with state-of-the-art algorithms and they cannot take advantage of feature from deep networks, such as ResNet-50 or deeper. In this work we prove the core reason comes from the lack of strict translation invariance. By comprehensive theoretical analysis and experimental validations, we break this restriction through a simple yet effective spatial aware sampling strategy and successfully train a ResNet-driven Siamese tracker with significant performance gain. Moreover, we propose a new model architecture to perform depth-wise and layer-wise aggregations, which not only further improves the accuracy but also reduces the model size. We conduct extensive ablation studies to demonstrate the effectiveness of the proposed tracker, which obtains currently the best results on four large tracking benchmarks, including OTB2015, VOT2018, UAV123, and LaSOT. Our model will be released to facilitate further studies based on this problem."
    },
    {
        "paperId": "d58e13f7e5e06440c9470a9101ccbb1bfd91b5a1",
        "url": "https://www.semanticscholar.org/paper/d58e13f7e5e06440c9470a9101ccbb1bfd91b5a1",
        "title": "Fast Online Object Tracking and Segmentation: A Unifying Approach",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2018,
        "citationCount": 1287,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1812.05050",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1812.05050, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "145805403",
                "name": "Qiang Wang"
            },
            {
                "authorId": "48459110",
                "name": "Li Zhang"
            },
            {
                "authorId": "2271057",
                "name": "Luca Bertinetto"
            },
            {
                "authorId": "40506509",
                "name": "Weiming Hu"
            },
            {
                "authorId": "143635540",
                "name": "Philip H. S. Torr"
            }
        ],
        "abstract": "In this paper we illustrate how to perform both visual object tracking and semi-supervised video object segmentation, in real-time, with a single simple approach. Our method, dubbed SiamMask, improves the offline training procedure of popular fully-convolutional Siamese approaches for object tracking by augmenting their loss with a binary segmentation task. Once trained, SiamMask solely relies on a single bounding box initialisation and operates online, producing class-agnostic object segmentation masks and rotated bounding boxes at 55 frames per second. Despite its simplicity, versatility and fast speed, our strategy allows us to establish a new state-of-the-art among real-time trackers on VOT-2018, while at the same time demonstrating competitive performance and the best speed for the semi-supervised video object segmentation task on DAVIS-2016 and DAVIS-2017."
    },
    {
        "paperId": "d74169a8fd2f90a06480d1d583d0ae5e980ea951",
        "url": "https://www.semanticscholar.org/paper/d74169a8fd2f90a06480d1d583d0ae5e980ea951",
        "title": "ATOM: Accurate Tracking by Overlap Maximization",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2018,
        "citationCount": 1220,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1811.07628",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1811.07628, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2488938",
                "name": "Martin Danelljan"
            },
            {
                "authorId": "49922196",
                "name": "Goutam Bhat"
            },
            {
                "authorId": "2358803",
                "name": "F. Khan"
            },
            {
                "authorId": "2228323",
                "name": "M. Felsberg"
            }
        ],
        "abstract": "While recent years have witnessed astonishing improvements in visual tracking robustness, the advancements in tracking accuracy have been limited. As the focus has been directed towards the development of powerful classifiers, the problem of accurate target state estimation has been largely overlooked. In fact, most trackers resort to a simple multi-scale search in order to estimate the target bounding box. We argue that this approach is fundamentally limited since target estimation is a complex task, requiring high-level knowledge about the object. We address this problem by proposing a novel tracking architecture, consisting of dedicated target estimation and classification components. High level knowledge is incorporated into the target estimation through extensive offline learning. Our target estimation component is trained to predict the overlap between the target object and an estimated bounding box. By carefully integrating target-specific information, our approach achieves previously unseen bounding box accuracy. We further introduce a classification component that is trained online to guarantee high discriminative power in the presence of distractors. Our final tracking framework sets a new state-of-the-art on five challenging benchmarks. On the new large-scale TrackingNet dataset, our tracker ATOM achieves a relative gain of 15% over the previous best approach, while running at over 30 FPS. Code and models are available at https://github.com/visionml/pytracking."
    },
    {
        "paperId": "d8d680aea59295c020b9d53d78dd8d954a876845",
        "url": "https://www.semanticscholar.org/paper/d8d680aea59295c020b9d53d78dd8d954a876845",
        "title": "Meta-Transfer Learning for Few-Shot Learning",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2018,
        "citationCount": 1185,
        "openAccessPdf": {
            "url": "https://ink.library.smu.edu.sg/sis_research/4447",
            "status": "GREEN",
            "license": "CCBYNCND",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1812.02391, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "32222907",
                "name": "Qianru Sun"
            },
            {
                "authorId": "3035233",
                "name": "Yaoyao Liu"
            },
            {
                "authorId": "144078686",
                "name": "Tat-Seng Chua"
            },
            {
                "authorId": "48920094",
                "name": "B. Schiele"
            }
        ],
        "abstract": "Meta-learning has been proposed as a framework to address the challenging few-shot learning setting. The key idea is to leverage a large number of similar few-shot tasks in order to learn how to adapt a base-learner to a new task for which only a few labeled samples are available. As deep neural networks (DNNs) tend to overfit using a few samples only, meta-learning typically uses shallow neural networks (SNNs), thus limiting its effectiveness. In this paper we propose a novel few-shot learning method called meta-transfer learning (MTL) which learns to adapt a deep NN for few shot learning tasks. Specifically, \"meta\" refers to training multiple tasks, and \"transfer\" is achieved by learning scaling and shifting functions of DNN weights for each task. In addition, we introduce the hard task (HT) meta-batch scheme as an effective learning curriculum for MTL. We conduct experiments using (5-class, 1-shot) and (5-class, 5-shot) recognition tasks on two challenging few-shot learning benchmarks: miniImageNet and Fewshot-CIFAR100. Extensive comparisons to related works validate that our meta-transfer learning approach trained with the proposed HT meta-batch scheme achieves top performance. An ablation study also shows that both components contribute to fast convergence and high accuracy."
    },
    {
        "paperId": "dce916351ef589afa7a63452648dd8acba931e92",
        "url": "https://www.semanticscholar.org/paper/dce916351ef589afa7a63452648dd8acba931e92",
        "title": "Panoptic Segmentation",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2018,
        "citationCount": 1599,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1801.00868, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "144843400",
                "name": "Alexander Kirillov"
            },
            {
                "authorId": "39353098",
                "name": "Kaiming He"
            },
            {
                "authorId": "2983898",
                "name": "Ross B. Girshick"
            },
            {
                "authorId": "1756036",
                "name": "C. Rother"
            },
            {
                "authorId": "3127283",
                "name": "Piotr Dollr"
            }
        ],
        "abstract": "We propose and study a task we name panoptic segmentation (PS). Panoptic segmentation unifies the typically distinct tasks of semantic segmentation (assign a class label to each pixel) and instance segmentation (detect and segment each object instance). The proposed task requires generating a coherent scene segmentation that is rich and complete, an important step toward real-world vision systems. While early work in computer vision addressed related image/scene parsing tasks, these are not currently popular, possibly due to lack of appropriate metrics or associated recognition challenges. To address this, we propose a novel panoptic quality (PQ) metric that captures performance for all classes (stuff and things) in an interpretable and unified manner. Using the proposed metric, we perform a rigorous study of both human and machine performance for PS on three existing datasets, revealing interesting insights about the task. The aim of our work is to revive the interest of the community in a more unified view of image segmentation. For more analysis and up-to-date results, please check the arXiv version of the paper: {\\small\\url{https://arxiv.org/abs/1801.00868}}."
    },
    {
        "paperId": "e48f36aacb72adb74cef077c87d2351121124137",
        "url": "https://www.semanticscholar.org/paper/e48f36aacb72adb74cef077c87d2351121124137",
        "title": "Two-Stream Adaptive Graph Convolutional Networks for Skeleton-Based Action Recognition",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2018,
        "citationCount": 1675,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1805.07694",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR.2019.01230?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR.2019.01230, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "19284184",
                "name": "Lei Shi"
            },
            {
                "authorId": "40382978",
                "name": "Yifan Zhang"
            },
            {
                "authorId": "143949499",
                "name": "Jian Cheng"
            },
            {
                "authorId": "1694235",
                "name": "Hanqing Lu"
            }
        ],
        "abstract": "In skeleton-based action recognition, graph convolutional networks (GCNs), which model the human body skeletons as spatiotemporal graphs, have achieved remarkable performance. However, in existing GCN-based methods, the topology of the graph is set manually, and it is fixed over all layers and input samples. This may not be optimal for the hierarchical GCN and diverse samples in action recognition tasks. In addition, the second-order information (the lengths and directions of bones) of the skeleton data, which is naturally more informative and discriminative for action recognition, is rarely investigated in existing methods. In this work, we propose a novel two-stream adaptive graph convolutional network (2s-AGCN) for skeleton-based action recognition. The topology of the graph in our model can be either uniformly or individually learned by the BP algorithm in an end-to-end manner. This data-driven method increases the flexibility of the model for graph construction and brings more generality to adapt to various data samples. Moreover, a two-stream framework is proposed to model both the first-order and the second-order information simultaneously, which shows notable improvement for the recognition accuracy. Extensive experiments on the two large-scale datasets, NTU-RGBD and Kinetics-Skeleton, demonstrate that the performance of our model exceeds the state-of-the-art with a significant margin."
    },
    {
        "paperId": "e65c2b0feddfe4c89e9955ca9b5ece6ef416628f",
        "url": "https://www.semanticscholar.org/paper/e65c2b0feddfe4c89e9955ca9b5ece6ef416628f",
        "title": "BDD100K: A Diverse Driving Dataset for Heterogeneous Multitask Learning",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2018,
        "citationCount": 2571,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/1805.04687",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1805.04687, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1807197",
                "name": "F. Yu"
            },
            {
                "authorId": "2149051402",
                "name": "Haofeng Chen"
            },
            {
                "authorId": "2153688131",
                "name": "Xin Wang"
            },
            {
                "authorId": "19282988",
                "name": "Wenqi Xian"
            },
            {
                "authorId": "50580380",
                "name": "Yingying Chen"
            },
            {
                "authorId": "32324034",
                "name": "Fangchen Liu"
            },
            {
                "authorId": "8309711",
                "name": "Vashisht Madhavan"
            },
            {
                "authorId": "1753210",
                "name": "Trevor Darrell"
            }
        ],
        "abstract": "Datasets drive vision progress, yet existing driving datasets are impoverished in terms of visual content and supported tasks to study multitask learning for autonomous driving. Researchers are usually constrained to study a small set of problems on one dataset, while real-world computer vision applications require performing tasks of various complexities. We construct BDD100K, the largest driving video dataset with 100K videos and 10 tasks to evaluate the exciting progress of image recognition algorithms on autonomous driving. The dataset possesses geographic, environmental, and weather diversity, which is useful for training models that are less likely to be surprised by new conditions. Based on this diverse dataset, we build a benchmark for heterogeneous multitask learning and study how to solve the tasks together. Our experiments show that special training strategies are needed for existing models to perform such heterogeneous tasks. BDD100K opens the door for future studies in this important venue."
    },
    {
        "paperId": "f78a911f516625d6b7b76a9a33c1eb14613341c4",
        "url": "https://www.semanticscholar.org/paper/f78a911f516625d6b7b76a9a33c1eb14613341c4",
        "title": "Improving Transferability of Adversarial Examples With Input Diversity",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2018,
        "citationCount": 1289,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1803.06978",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1803.06978, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3011497",
                "name": "Cihang Xie"
            },
            {
                "authorId": "2852303",
                "name": "Zhishuai Zhang"
            },
            {
                "authorId": null,
                "name": "Jianyu Wang"
            },
            {
                "authorId": "7743268",
                "name": "Yuyin Zhou"
            },
            {
                "authorId": "145888238",
                "name": "Zhou Ren"
            },
            {
                "authorId": "145081362",
                "name": "A. Yuille"
            }
        ],
        "abstract": "Though CNNs have achieved the state-of-the-art performance on various vision tasks, they are vulnerable to adversarial examples --- crafted by adding human-imperceptible perturbations to clean images. However, most of the existing adversarial attacks only achieve relatively low success rates under the challenging black-box setting, where the attackers have no knowledge of the model structure and parameters. To this end, we propose to improve the transferability of adversarial examples by creating diverse input patterns. Instead of only using the original images to generate adversarial examples, our method applies random transformations to the input images at each iteration. Extensive experiments on ImageNet show that the proposed attack method can generate adversarial examples that transfer much better to different networks than existing baselines. By evaluating our method against top defense solutions and official baselines from NIPS 2017 adversarial competition, the enhanced attack reaches an average success rate of 73.0%, which outperforms the top-1 attack submission in the NIPS competition by a large margin of 6.6%. We hope that our proposed attack strategy can serve as a strong benchmark baseline for evaluating the robustness of networks to adversaries and the effectiveness of different defense methods in the future. Code is available at https://github.com/cihangxie/DI-2-FGSM."
    },
    {
        "paperId": "0170bb0b524df2c81b5adc3062c6001a2eb34c96",
        "url": "https://www.semanticscholar.org/paper/0170bb0b524df2c81b5adc3062c6001a2eb34c96",
        "title": "Self-Supervised Learning of Pretext-Invariant Representations",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2019,
        "citationCount": 1556,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1912.01991",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1912.01991, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1806773",
                "name": "Ishan Misra"
            },
            {
                "authorId": "1803520",
                "name": "L. Maaten"
            }
        ],
        "abstract": "The goal of self-supervised learning from images is to construct image representations that are semantically meaningful via pretext tasks that do not require semantic annotations. Many pretext tasks lead to representations that are covariant with image transformations. We argue that, instead, semantic representations ought to be invariant under such transformations. Specifically, we develop Pretext-Invariant Representation Learning (PIRL, pronounced as `pearl') that learns invariant representations based on pretext tasks. We use PIRL with a commonly used pretext task that involves solving jigsaw puzzles. We find that PIRL substantially improves the semantic quality of the learned image representations. Our approach sets a new state-of-the-art in self-supervised learning from images on several popular benchmarks for self-supervised learning. Despite being unsupervised, PIRL outperforms supervised pre-training in learning image representations for object detection. Altogether, our results demonstrate the potential of self-supervised representations with good invariance properties."
    },
    {
        "paperId": "01c9fd4c011a580e106dd4a8d4e86299acac8564",
        "url": "https://www.semanticscholar.org/paper/01c9fd4c011a580e106dd4a8d4e86299acac8564",
        "title": "CNN-Generated Images Are Surprisingly Easy to Spot for Now",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2019,
        "citationCount": 1233,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1912.11035",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1912.11035, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "12782331",
                "name": "Sheng-Yu Wang"
            },
            {
                "authorId": "39231399",
                "name": "Oliver Wang"
            },
            {
                "authorId": "2844849",
                "name": "Richard Zhang"
            },
            {
                "authorId": "144956994",
                "name": "Andrew Owens"
            },
            {
                "authorId": "1763086",
                "name": "Alexei A. Efros"
            }
        ],
        "abstract": "In this work we ask whether it is possible to create a \"universal\" detector for telling apart real images from these generated by a CNN, regardless of architecture or dataset used. To test this, we collect a dataset consisting of fake images generated by 11 different CNN-based image generator models, chosen to span the space of commonly used architectures today (ProGAN, StyleGAN, BigGAN, CycleGAN, StarGAN, GauGAN, DeepFakes, cascaded refinement networks, implicit maximum likelihood estimation, second-order attention super-resolution, seeing-in-the-dark). We demonstrate that, with careful pre- and post-processing and data augmentation, a standard image classifier trained on only one specific CNN generator (ProGAN) is able to generalize surprisingly well to unseen architectures, datasets, and training methods (including the just released StyleGAN2). Our findings suggest the intriguing possibility that today's CNN-generated images share some common systematic flaws, preventing them from achieving realistic image synthesis."
    },
    {
        "paperId": "0f736d2067ee9c950b876f14521268c6009e67d6",
        "url": "https://www.semanticscholar.org/paper/0f736d2067ee9c950b876f14521268c6009e67d6",
        "title": "Relational Knowledge Distillation",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2019,
        "citationCount": 1695,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1904.05068",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1904.05068, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "107950764",
                "name": "Wonpyo Park"
            },
            {
                "authorId": "2111840349",
                "name": "Dongju Kim"
            },
            {
                "authorId": "144574822",
                "name": "Yan Lu"
            },
            {
                "authorId": "72643925",
                "name": "Minsu Cho"
            }
        ],
        "abstract": "Knowledge distillation aims at transferring knowledge acquired in one model (a teacher) to another model (a student) that is typically smaller. Previous approaches can be expressed as a form of training the student to mimic output activations of individual data examples represented by the teacher. We introduce a novel approach, dubbed relational knowledge distillation (RKD), that transfers mutual relations of data examples instead. For concrete realizations of RKD, we propose distance-wise and angle-wise distillation losses that penalize structural differences in relations. Experiments conducted on different tasks show that the proposed method improves educated student models with a significant margin. In particular for metric learning, it allows students to outperform their teachers' performance, achieving the state of the arts on standard benchmark datasets."
    },
    {
        "paperId": "14fdc18d9c164e5b0d6d946b3238c04e81921358",
        "url": "https://www.semanticscholar.org/paper/14fdc18d9c164e5b0d6d946b3238c04e81921358",
        "title": "Analyzing and Improving the Image Quality of StyleGAN",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2019,
        "citationCount": 6565,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1912.04958",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1912.04958, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2976930",
                "name": "Tero Karras"
            },
            {
                "authorId": "36436218",
                "name": "S. Laine"
            },
            {
                "authorId": "1907688",
                "name": "M. Aittala"
            },
            {
                "authorId": "1454226629",
                "name": "Janne Hellsten"
            },
            {
                "authorId": "49244945",
                "name": "J. Lehtinen"
            },
            {
                "authorId": "1761103",
                "name": "Timo Aila"
            }
        ],
        "abstract": "The style-based GAN architecture (StyleGAN) yields state-of-the-art results in data-driven unconditional generative image modeling. We expose and analyze several of its characteristic artifacts, and propose changes in both model architecture and training methods to address them. In particular, we redesign the generator normalization, revisit progressive growing, and regularize the generator to encourage good conditioning in the mapping from latent codes to images. In addition to improving image quality, this path length regularizer yields the additional benefit that the generator becomes significantly easier to invert. This makes it possible to reliably attribute a generated image to a particular network. We furthermore visualize how well the generator utilizes its output resolution, and identify a capacity problem, motivating us to train larger models for additional quality improvements. Overall, our improved model redefines the state of the art in unconditional image modeling, both in terms of existing distribution quality metrics as well as perceived image quality."
    },
    {
        "paperId": "1954a5ef37030002574f1b000cc1192f3bd4cad1",
        "url": "https://www.semanticscholar.org/paper/1954a5ef37030002574f1b000cc1192f3bd4cad1",
        "title": "4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural Networks",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2019,
        "citationCount": 2106,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1904.08755",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1904.08755, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2144103",
                "name": "C. Choy"
            },
            {
                "authorId": "39813007",
                "name": "JunYoung Gwak"
            },
            {
                "authorId": "1702137",
                "name": "S. Savarese"
            }
        ],
        "abstract": "In many robotics and VR/AR applications, 3D-videos are readily-available input sources (a sequence of depth images, or LIDAR scans). However, in many cases, the 3D-videos are processed frame-by-frame either through 2D convnets or 3D perception algorithms. In this work, we propose 4-dimensional convolutional neural networks for spatio-temporal perception that can directly process such 3D-videos using high-dimensional convolutions. For this, we adopt sparse tensors and propose generalized sparse convolutions that encompass all discrete convolutions. To implement the generalized sparse convolution, we create an open-source auto-differentiation library for sparse tensors that provides extensive functions for high-dimensional convolutional neural networks. We create 4D spatio-temporal convolutional neural networks using the library and validate them on various 3D semantic segmentation benchmarks and proposed 4D datasets for 3D-video perception. To overcome challenges in 4D space, we propose the hybrid kernel, a special case of the generalized sparse convolution, and trilateral-stationary conditional random fields that enforce spatio-temporal consistency in the 7D space-time-chroma space. Experimentally, we show that a convolutional neural network with only generalized 3D sparse convolutions can outperform 2D or 2D-3D hybrid methods by a large margin. Also, we show that on 3D-videos, 4D spatio-temporal convolutional neural networks are robust to noise and outperform the 3D convolutional neural network."
    },
    {
        "paperId": "199123c3a93521a397ee4f2ae8c852b0b252f5ba",
        "url": "https://www.semanticscholar.org/paper/199123c3a93521a397ee4f2ae8c852b0b252f5ba",
        "title": "PV-RCNN: Point-Voxel Feature Set Abstraction for 3D Object Detection",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2019,
        "citationCount": 2081,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1912.13192",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1912.13192, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2072683588",
                "name": "Shaoshuai Shi"
            },
            {
                "authorId": "46181112",
                "name": "Chaoxu Guo"
            },
            {
                "authorId": "2108810714",
                "name": "Li Jiang"
            },
            {
                "authorId": "40072288",
                "name": "Zhe Wang"
            },
            {
                "authorId": "1788070",
                "name": "Jianping Shi"
            },
            {
                "authorId": "31843833",
                "name": "Xiaogang Wang"
            },
            {
                "authorId": "47893312",
                "name": "Hongsheng Li"
            }
        ],
        "abstract": "We present a novel and high-performance 3D object detection framework, named PointVoxel-RCNN (PV-RCNN), for accurate 3D object detection from point clouds. Our proposed method deeply integrates both 3D voxel Convolutional Neural Network (CNN) and PointNet-based set abstraction to learn more discriminative point cloud features. It takes advantages of efficient learning and high-quality proposals of the 3D voxel CNN and the flexible receptive fields of the PointNet-based networks. Specifically, the proposed framework summarizes the 3D scene with a 3D voxel CNN into a small set of keypoints via a novel voxel set abstraction module to save follow-up computations and also to encode representative scene features. Given the high-quality 3D proposals generated by the voxel CNN, the RoI-grid pooling is proposed to abstract proposal-specific features from the keypoints to the RoI-grid points via keypoint set abstraction. Compared with conventional pooling operations, the RoI-grid feature points encode much richer context information for accurately estimating object confidences and locations. Extensive experiments on both the KITTI dataset and the Waymo Open dataset show that our proposed PV-RCNN surpasses state-of-the-art 3D detection methods with remarkable margins."
    },
    {
        "paperId": "1f6d30772a94d978c9f81e2f7c1f4b0bdec117dd",
        "url": "https://www.semanticscholar.org/paper/1f6d30772a94d978c9f81e2f7c1f4b0bdec117dd",
        "title": "Large Scale Incremental Learning",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2019,
        "citationCount": 1412,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1905.13260",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1905.13260, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2119299240",
                "name": "Yue Wu"
            },
            {
                "authorId": "2109306087",
                "name": "Yinpeng Chen"
            },
            {
                "authorId": "29957038",
                "name": "Lijuan Wang"
            },
            {
                "authorId": "3105254",
                "name": "Yuancheng Ye"
            },
            {
                "authorId": "2145253136",
                "name": "Zicheng Liu"
            },
            {
                "authorId": "3133575",
                "name": "Yandong Guo"
            },
            {
                "authorId": "46956675",
                "name": "Y. Fu"
            }
        ],
        "abstract": "Modern machine learning suffers from \\textit{catastrophic forgetting} when learning new classes incrementally. The performance dramatically degrades due to the missing data of old classes. Incremental learning methods have been proposed to retain the knowledge acquired from the old classes, by using knowledge distilling and keeping a few exemplars from the old classes. However, these methods struggle to \\textbf{scale up to a large number of classes}. We believe this is because of the combination of two factors: (a) the data imbalance between the old and new classes, and (b) the increasing number of visually similar classes. Distinguishing between an increasing number of visually similar classes is particularly challenging, when the training data is unbalanced. We propose a simple and effective method to address this data imbalance issue. We found that the last fully connected layer has a strong bias towards the new classes, and this bias can be corrected by a linear model. With two bias parameters, our method performs remarkably well on two large datasets: ImageNet (1000 classes) and MS-Celeb-1M (10000 classes), outperforming the state-of-the-art algorithms by 11.1\\% and 13.2\\% respectively."
    },
    {
        "paperId": "20ba55ee3229db5cb190a00e788c59f08d2a767d",
        "url": "https://www.semanticscholar.org/paper/20ba55ee3229db5cb190a00e788c59f08d2a767d",
        "title": "Self-Training With Noisy Student Improves ImageNet Classification",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2019,
        "citationCount": 2605,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/1911.04252",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1911.04252, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1912046",
                "name": "Qizhe Xie"
            },
            {
                "authorId": "144547315",
                "name": "E. Hovy"
            },
            {
                "authorId": "1707242",
                "name": "Minh-Thang Luong"
            },
            {
                "authorId": "2827616",
                "name": "Quoc V. Le"
            }
        ],
        "abstract": "We present a simple self-training method that achieves 88.4% top-1 accuracy on ImageNet, which is 2.0% better than the state-of-the-art model that requires 3.5B weakly labeled Instagram images. On robustness test sets, it improves ImageNet-A top-1 accuracy from 61.0% to 83.7%, reduces ImageNet-C mean corruption error from 45.7 to 28.3, and reduces ImageNet-P mean flip rate from 27.8 to 12.2. To achieve this result, we first train an EfficientNet model on labeled ImageNet images and use it as a teacher to generate pseudo labels on 300M unlabeled images. We then train a larger EfficientNet as a student model on the combination of labeled and pseudo labeled images. We iterate this process by putting back the student as the teacher. During the generation of the pseudo labels, the teacher is not noised so that the pseudo labels are as accurate as possible. However, during the learning of the student, we inject noise such as dropout, stochastic depth and data augmentation via RandAugment to the student so that the student generalizes better than the teacher."
    },
    {
        "paperId": "21248bcc81539e7cd1ef83b3b184768603f6f247",
        "url": "https://www.semanticscholar.org/paper/21248bcc81539e7cd1ef83b3b184768603f6f247",
        "title": "Hybrid Task Cascade for Instance Segmentation",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2019,
        "citationCount": 1433,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1901.07518",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1901.07518, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "152568027",
                "name": "Kai Chen"
            },
            {
                "authorId": "49968574",
                "name": "Jiangmiao Pang"
            },
            {
                "authorId": null,
                "name": "Jiaqi Wang"
            },
            {
                "authorId": "2112718966",
                "name": "Yu Xiong"
            },
            {
                "authorId": "2108536754",
                "name": "Xiaoxiao Li"
            },
            {
                "authorId": "2115306116",
                "name": "Shuyang Sun"
            },
            {
                "authorId": "66165285",
                "name": "Wansen Feng"
            },
            {
                "authorId": "2117940996",
                "name": "Ziwei Liu"
            },
            {
                "authorId": "1788070",
                "name": "Jianping Shi"
            },
            {
                "authorId": "3001348",
                "name": "Wanli Ouyang"
            },
            {
                "authorId": "1717179",
                "name": "Chen Change Loy"
            },
            {
                "authorId": "1807606",
                "name": "Dahua Lin"
            }
        ],
        "abstract": "Cascade is a classic yet powerful architecture that has boosted performance on various tasks. However, how to introduce cascade to instance segmentation remains an open question. A simple combination of Cascade R-CNN and Mask R-CNN only brings limited gain. In exploring a more effective approach, we find that the key to a successful instance segmentation cascade is to fully leverage the reciprocal relationship between detection and segmentation. In this work, we propose a new framework, Hybrid Task Cascade (HTC), which differs in two important aspects: (1) instead of performing cascaded refinement on these two tasks separately, it interweaves them for a joint multi-stage processing; (2) it adopts a fully convolutional branch to provide spatial context, which can help distinguishing hard foreground from cluttered background. Overall, this framework can learn more discriminative features progressively while integrating complementary features together in each stage. Without bells and whistles, a single HTC obtains 38.4% and 1.5% improvement over a strong Cascade Mask R-CNN baseline on MSCOCO dataset. Moreover, our overall system achieves 48.6 mask AP on the test-challenge split, ranking 1st in the COCO 2018 Challenge Object Detection Task. Code is available at https://github.com/open-mmlab/mmdetection."
    },
    {
        "paperId": "21de3a36cb51adc205fad8a1d3d69118891dc3dd",
        "url": "https://www.semanticscholar.org/paper/21de3a36cb51adc205fad8a1d3d69118891dc3dd",
        "title": "AutoAugment: Learning Augmentation Strategies From Data",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2019,
        "citationCount": 2437,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR.2019.00020?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR.2019.00020, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "8132903",
                "name": "E. D. Cubuk"
            },
            {
                "authorId": "2368067",
                "name": "Barret Zoph"
            },
            {
                "authorId": "30415265",
                "name": "Dandelion Man"
            },
            {
                "authorId": "2053781980",
                "name": "Vijay Vasudevan"
            },
            {
                "authorId": "2827616",
                "name": "Quoc V. Le"
            }
        ],
        "abstract": "Data augmentation is an effective technique for improving the accuracy of modern image classifiers. However, current data augmentation implementations are manually designed. In this paper, we describe a simple procedure called AutoAugment to automatically search for improved data augmentation policies. In our implementation, we have designed a search space where a policy consists of many sub-policies, one of which is randomly chosen for each image in each mini-batch. A sub-policy consists of two operations, each operation being an image processing function such as translation, rotation, or shearing, and the probabilities and magnitudes with which the functions are applied. We use a search algorithm to find the best policy such that the neural network yields the highest validation accuracy on a target dataset. Our method achieves state-of-the-art accuracy on CIFAR-10, CIFAR-100, SVHN, and ImageNet (without additional data). On ImageNet, we attain a Top-1 accuracy of 83.5% which is 0.4% better than the previous record of 83.1%. On CIFAR-10, we achieve an error rate of 1.5%, which is 0.6% better than the previous state-of-the-art. Augmentation policies we find are transferable between datasets. The policy learned on ImageNet transfers well to achieve significant improvements on other datasets, such as Oxford Flowers, Caltech-101, Oxford-IIT Pets, FGVC Aircraft, and Stanford Cars."
    },
    {
        "paperId": "28ad018c39d1578bea84e7cedf94459e3dbe1e70",
        "url": "https://www.semanticscholar.org/paper/28ad018c39d1578bea84e7cedf94459e3dbe1e70",
        "title": "OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2019,
        "citationCount": 1358,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1906.00067",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1906.00067, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "35789996",
                "name": "Kenneth Marino"
            },
            {
                "authorId": "143887493",
                "name": "Mohammad Rastegari"
            },
            {
                "authorId": "143787583",
                "name": "Ali Farhadi"
            },
            {
                "authorId": "3012475",
                "name": "Roozbeh Mottaghi"
            }
        ],
        "abstract": "Visual Question Answering (VQA) in its ideal form lets us study reasoning in the joint space of vision and language and serves as a proxy for the AI task of scene understanding. However, most VQA benchmarks to date are focused on questions such as simple counting, visual attributes, and object detection that do not require reasoning or knowledge beyond what is in the image. In this paper, we address the task of knowledge-based visual question answering and provide a benchmark, called OK-VQA, where the image content is not sufficient to answer the questions, encouraging methods that rely on external knowledge resources. Our new dataset includes more than 14,000 questions that require external knowledge to answer. We show that the performance of the state-of-the-art VQA models degrades drastically in this new setting. Our analysis shows that our knowledge-based VQA task is diverse, difficult, and large compared to previous knowledge-based VQA datasets. We hope that this dataset enables researchers to open up new avenues for research in this domain."
    },
    {
        "paperId": "300d08e8f5c310c2b194b7eb94398e480994d5cc",
        "url": "https://www.semanticscholar.org/paper/300d08e8f5c310c2b194b7eb94398e480994d5cc",
        "title": "Celeb-DF: A Large-Scale Challenging Dataset for DeepFake Forensics",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2019,
        "citationCount": 1406,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1909.12962",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1909.12962, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2778359",
                "name": "Yuezun Li"
            },
            {
                "authorId": "2150440804",
                "name": "Xin Yang"
            },
            {
                "authorId": "145021633",
                "name": "Pu Sun"
            },
            {
                "authorId": "144097734",
                "name": "H. Qi"
            },
            {
                "authorId": "1794837",
                "name": "Siwei Lyu"
            }
        ],
        "abstract": "AI-synthesized face-swapping videos, commonly known as DeepFakes, is an emerging problem threatening the trustworthiness of online information. The need to develop and evaluate DeepFake detection algorithms calls for datasets of DeepFake videos. However, current DeepFake datasets suffer from low visual quality and do not resemble DeepFake videos circulated on the Internet. We present a new large-scale challenging DeepFake video dataset, Celeb-DF, which contains 5,639 high-quality DeepFake videos of celebrities generated using improved synthesis process. We conduct a comprehensive evaluation of DeepFake detection methods and datasets to demonstrate the escalated level of challenges posed by Celeb-DF."
    },
    {
        "paperId": "32a69681c103807704f71b838454c7924ceec5ce",
        "url": "https://www.semanticscholar.org/paper/32a69681c103807704f71b838454c7924ceec5ce",
        "title": "Libra R-CNN: Towards Balanced Learning for Object Detection",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2019,
        "citationCount": 1425,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1904.02701",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1904.02701, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "49968574",
                "name": "Jiangmiao Pang"
            },
            {
                "authorId": "152568027",
                "name": "Kai Chen"
            },
            {
                "authorId": "1788070",
                "name": "Jianping Shi"
            },
            {
                "authorId": "46854729",
                "name": "H. Feng"
            },
            {
                "authorId": "3001348",
                "name": "Wanli Ouyang"
            },
            {
                "authorId": "1807606",
                "name": "Dahua Lin"
            }
        ],
        "abstract": "Compared with model architectures, the training process, which is also crucial to the success of detectors, has received relatively less attention in object detection. In this work, we carefully revisit the standard training practice of detectors, and find that the detection performance is often limited by the imbalance during the training process, which generally consists in three levels - sample level, feature level, and objective level. To mitigate the adverse effects caused thereby, we propose Libra R-CNN, a simple but effective framework towards balanced learning for object detection. It integrates three novel components: IoU-balanced sampling, balanced feature pyramid, and balanced L1 loss, respectively for reducing the imbalance at sample, feature, and objective level. Benefitted from the overall balanced design, Libra R-CNN significantly improves the detection performance. Without bells and whistles, it achieves 2.5 points and 2.0 points higher Average Precision (AP) than FPN Faster R-CNN and RetinaNet respectively on MSCOCO."
    },
    {
        "paperId": "347e837b1aa03c9d17c69a522929000f0a0f0a51",
        "url": "https://www.semanticscholar.org/paper/347e837b1aa03c9d17c69a522929000f0a0f0a51",
        "title": "SuperGlue: Learning Feature Matching With Graph Neural Networks",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2019,
        "citationCount": 2390,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/1911.11763",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1911.11763, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "51435497",
                "name": "Paul-Edouard Sarlin"
            },
            {
                "authorId": "3422291",
                "name": "Daniel DeTone"
            },
            {
                "authorId": "3045340",
                "name": "Tomasz Malisiewicz"
            },
            {
                "authorId": "39863668",
                "name": "Andrew Rabinovich"
            }
        ],
        "abstract": "This paper introduces SuperGlue, a neural network that matches two sets of local features by jointly finding correspondences and rejecting non-matchable points. Assignments are estimated by solving a differentiable optimal transport problem, whose costs are predicted by a graph neural network. We introduce a flexible context aggregation mechanism based on attention, enabling SuperGlue to reason about the underlying 3D scene and feature assignments jointly. Compared to traditional, hand-designed heuristics, our technique learns priors over geometric transformations and regularities of the 3D world through end-to-end training from image pairs. SuperGlue outperforms other learned approaches and achieves state-of-the-art results on the task of pose estimation in challenging real-world indoor and outdoor environments. The proposed method performs matching in real-time on a modern GPU and can be readily integrated into modern SfM or SLAM systems. The code and trained weights are publicly available at github.com/magicleap/SuperGluePretrainedNetwork."
    },
    {
        "paperId": "3aa681914a7da79f7d7293f51a058eefe61c8bb7",
        "url": "https://www.semanticscholar.org/paper/3aa681914a7da79f7d7293f51a058eefe61c8bb7",
        "title": "MVTec AD  A Comprehensive Real-World Dataset for Unsupervised Anomaly Detection",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2019,
        "citationCount": 1746,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR.2019.00982?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR.2019.00982, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "33609329",
                "name": "Paul Bergmann"
            },
            {
                "authorId": "34194478",
                "name": "Michael Fauser"
            },
            {
                "authorId": "23633923",
                "name": "David Sattlegger"
            },
            {
                "authorId": "36494513",
                "name": "C. Steger"
            }
        ],
        "abstract": "The detection of anomalous structures in natural image data is of utmost importance for numerous tasks in the eld of computer vision. The development of methods for unsupervised anomaly detection requires data on which to train and evaluate new approaches and ideas. We introduce the MVTec Anomaly Detection (MVTec AD) dataset containing 5354 high-resolution color images of different object and texture categories. It contains normal, i.e., defect-free, images intended for training and images with anomalies intended for testing. The anomalies manifest themselves in the form of over 70 different types of defects such as scratches, dents, contaminations, and various structural changes. In addition, we provide pixel-precise ground truth regions for all anomalies. We also conduct a thorough evaluation of current state-of-the-art unsupervised anomaly detection methods based on deep architectures such as convolutional autoencoders, generative adversarial networks, and feature descriptors using pre-trained convolutional neural networks, as well as classical computer vision methods. This initial benchmark indicates that there is considerable room for improvement. To the best of our knowledge, this is the rst comprehensive, multi-object, multi-defect dataset for anomaly detection that provides pixel-accurate ground truth regions and focuses on real-world applications."
    },
    {
        "paperId": "41c67d04be2d1632c0d3b0880c21c9fe797cdab8",
        "url": "https://www.semanticscholar.org/paper/41c67d04be2d1632c0d3b0880c21c9fe797cdab8",
        "title": "EfficientDet: Scalable and Efficient Object Detection",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2019,
        "citationCount": 6199,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1911.09070",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1911.09070, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "120805419",
                "name": "Mingxing Tan"
            },
            {
                "authorId": "34320634",
                "name": "Ruoming Pang"
            },
            {
                "authorId": "2827616",
                "name": "Quoc V. Le"
            }
        ],
        "abstract": "Model efficiency has become increasingly important in computer vision. In this paper, we systematically study neural network architecture design choices for object detection and propose several key optimizations to improve efficiency. First, we propose a weighted bi-directional feature pyramid network (BiFPN), which allows easy and fast multi-scale feature fusion; Second, we propose a compound scaling method that uniformly scales the resolution, depth, and width for all backbone, feature network, and box/class prediction networks at the same time. Based on these optimizations and EfficientNet backbones, we have developed a new family of object detectors, called EfficientDet, which consistently achieve much better efficiency than prior art across a wide spectrum of resource constraints. In particular, with single-model and single-scale, our EfficientDetD7 achieves state-of-the-art 52.2 AP on COCO test-dev with 52M parameters and 325B FLOPs, being 4x  9x smaller and using 13x  42x fewer FLOPs than previous detector."
    },
    {
        "paperId": "45557cc70cd6989ab6b03e5aeb787e34299099f7",
        "url": "https://www.semanticscholar.org/paper/45557cc70cd6989ab6b03e5aeb787e34299099f7",
        "title": "Natural Adversarial Examples",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2019,
        "citationCount": 1733,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1907.07174",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1907.07174, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3422872",
                "name": "Dan Hendrycks"
            },
            {
                "authorId": "2074109526",
                "name": "Kevin Zhao"
            },
            {
                "authorId": "104444594",
                "name": "Steven Basart"
            },
            {
                "authorId": "5164568",
                "name": "J. Steinhardt"
            },
            {
                "authorId": "143711382",
                "name": "D. Song"
            }
        ],
        "abstract": "We introduce two challenging datasets that reliably cause machine learning model performance to substantially degrade. The datasets are collected with a simple adversarial filtration technique to create datasets with limited spurious cues. Our datasets real-world, unmodified examples transfer to various unseen models reliably, demonstrating that computer vision models have shared weaknesses. The first dataset is called IMAGENET-A and is like the ImageNet test set, but it is far more challenging for existing models. We also curate an adversarial out-of-distribution detection dataset called IMAGENET-O, which is the first out-of-distribution detection dataset created for ImageNet models. On IMAGENET-A a DenseNet-121 obtains around 2% accuracy, an accuracy drop of approximately 90%, and its out-of-distribution detection performance on IMAGENET-O is near random chance levels. We find that existing data augmentation techniques hardly boost performance, and using other public training datasets provides improvements that are limited. However, we find that improvements to computer vision architectures provide a promising path towards robust models."
    },
    {
        "paperId": "47c92620b4b966cb320a8f8ea5b8dfc8065e8fa4",
        "url": "https://www.semanticscholar.org/paper/47c92620b4b966cb320a8f8ea5b8dfc8065e8fa4",
        "title": "PointRend: Image Segmentation As Rendering",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2019,
        "citationCount": 1015,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1912.08193",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1912.08193, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "144843400",
                "name": "Alexander Kirillov"
            },
            {
                "authorId": "98264506",
                "name": "Yuxin Wu"
            },
            {
                "authorId": "39353098",
                "name": "Kaiming He"
            },
            {
                "authorId": "2983898",
                "name": "Ross B. Girshick"
            }
        ],
        "abstract": "We present a new method for efficient high-quality image segmentation of objects and scenes. By analogizing classical computer graphics methods for efficient rendering with over- and undersampling challenges faced in pixel labeling tasks, we develop a unique perspective of image segmentation as a rendering problem. From this vantage, we present the PointRend (Point-based Rendering) neural network module: a module that performs point-based segmentation predictions at adaptively selected locations based on an iterative subdivision algorithm. PointRend can be flexibly applied to both instance and semantic segmentation tasks by building on top of existing state-of-the-art models. While many concrete implementations of the general idea are possible, we show that a simple design already achieves excellent results. Qualitatively, PointRend outputs crisp object boundaries in regions that are over-smoothed by previous methods. Quantitatively, PointRend yields significant gains on COCO and Cityscapes, for both instance and semantic segmentation. PointRend's efficiency enables output resolutions that are otherwise impractical in terms of memory or computation compared to existing approaches. Code has been made available at https://github.com/facebookresearch/detectron2/tree/master/projects/PointRend."
    },
    {
        "paperId": "4be4707aba8d622a0553aa159dc92ae7f9af9c5e",
        "url": "https://www.semanticscholar.org/paper/4be4707aba8d622a0553aa159dc92ae7f9af9c5e",
        "title": "Expressive Body Capture: 3D Hands, Face, and Body From a Single Image",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2019,
        "citationCount": 2092,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1904.05866, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2829330",
                "name": "G. Pavlakos"
            },
            {
                "authorId": "52022007",
                "name": "Vasileios Choutas"
            },
            {
                "authorId": "2066023556",
                "name": "N. Ghorbani"
            },
            {
                "authorId": "1780750",
                "name": "Timo Bolkart"
            },
            {
                "authorId": "144261075",
                "name": "Ahmed A. A. Osman"
            },
            {
                "authorId": "1940674",
                "name": "Dimitrios Tzionas"
            },
            {
                "authorId": "2105795",
                "name": "Michael J. Black"
            }
        ],
        "abstract": "To facilitate the analysis of human actions, interactions and emotions, we compute a 3D model of human body pose, hand pose, and facial expression from a single monocular image. To achieve this, we use thousands of 3D scans to train a new, unified, 3D model of the human body, SMPL-X, that extends SMPL with fully articulated hands and an expressive face. Learning to regress the parameters of SMPL-X directly from images is challenging without paired images and 3D ground truth. Consequently, we follow the approach of SMPLify, which estimates 2D features and then optimizes model parameters to fit the features. We improve on SMPLify in several significant ways: (1) we detect 2D features corresponding to the face, hands, and feet and fit the full SMPL-X model to these; (2) we train a new neural network pose prior using a large MoCap dataset; (3) we define a new interpenetration penalty that is both fast and accurate; (4) we automatically detect gender and the appropriate body models (male, female, or neutral); (5) our PyTorch implementation achieves a speedup of more than 8x over Chumpy. We use the new method, SMPLify-X, to fit SMPL-X to both controlled images and images in the wild. We evaluate 3D accuracy on a new curated dataset comprising 100 images with pseudo ground-truth. This is a step towards automatic expressive human capture from monocular RGB data. The models, code, and data are available for research purposes at https://smpl-x.is.tue.mpg.de."
    },
    {
        "paperId": "4ccd95612be1b970f64871e6c132cd01269d8ad9",
        "url": "https://www.semanticscholar.org/paper/4ccd95612be1b970f64871e6c132cd01269d8ad9",
        "title": "Learning a Unified Classifier Incrementally via Rebalancing",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2019,
        "citationCount": 1279,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR.2019.00092?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR.2019.00092, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3408031",
                "name": "Saihui Hou"
            },
            {
                "authorId": "47420039",
                "name": "Xinyu Pan"
            },
            {
                "authorId": "1717179",
                "name": "Chen Change Loy"
            },
            {
                "authorId": "3238613",
                "name": "Zilei Wang"
            },
            {
                "authorId": "1807606",
                "name": "Dahua Lin"
            }
        ],
        "abstract": "Conventionally, deep neural networks are trained offline, relying on a large dataset prepared in advance. This paradigm is often challenged in real-world applications, e.g. online services that involve continuous streams of incoming data. Recently, incremental learning receives increasing attention, and is considered as a promising solution to the practical challenges mentioned above. However, it has been observed that incremental learning is subject to a fundamental difficulty -- catastrophic forgetting, namely adapting a model to new data often results in severe performance degradation on previous tasks or classes. Our study reveals that the imbalance between previous and new data is a crucial cause to this problem. In this work, we develop a new framework for incrementally learning a unified classifier, e.g. a classifier that treats both old and new classes uniformly. Specifically, we incorporate three components, cosine normalization, less-forget constraint, and inter-class separation, to mitigate the adverse effects of the imbalance. Experiments show that the proposed method can effectively rebalance the training process, thus obtaining superior performance compared to the existing methods. On CIFAR-100 and ImageNet, our method can reduce the classification errors by more than 6% and 13% respectively, under the incremental setting of 10 phases."
    },
    {
        "paperId": "54036f43acc6c9b49b334270c7237217685f52fb",
        "url": "https://www.semanticscholar.org/paper/54036f43acc6c9b49b334270c7237217685f52fb",
        "title": "Class-Balanced Loss Based on Effective Number of Samples",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2019,
        "citationCount": 2688,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1901.05555",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1901.05555, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "50355189",
                "name": "Yin Cui"
            },
            {
                "authorId": "51502783",
                "name": "Menglin Jia"
            },
            {
                "authorId": "33493200",
                "name": "Tsung-Yi Lin"
            },
            {
                "authorId": "144404428",
                "name": "Yang Song"
            },
            {
                "authorId": "50172592",
                "name": "Serge J. Belongie"
            }
        ],
        "abstract": "With the rapid increase of large-scale, real-world datasets, it becomes critical to address the problem of long-tailed data distribution (i.e., a few classes account for most of the data, while most classes are under-represented). Existing solutions typically adopt class re-balancing strategies such as re-sampling and re-weighting based on the number of observations for each class. In this work, we argue that as the number of samples increases, the additional benefit of a newly added data point will diminish. We introduce a novel theoretical framework to measure data overlap by associating with each sample a small neighboring region rather than a single point. The effective number of samples is defined as the volume of samples and can be calculated by a simple formula $(1-\\beta^{n})/(1-\\beta)$, where $n$ is the number of samples and $\\beta \\in [0,1)$ is a hyperparameter. We design a re-weighting scheme that uses the effective number of samples for each class to re-balance the loss, thereby yielding a class-balanced loss. Comprehensive experiments are conducted on artificially induced long-tailed CIFAR datasets and large-scale datasets including ImageNet and iNaturalist. Our results show that when trained with the proposed class-balanced loss, the network is able to achieve significant performance gains on long-tailed datasets."
    },
    {
        "paperId": "5474ddca920f59c4ec3c243345a5b9248e64065b",
        "url": "https://www.semanticscholar.org/paper/5474ddca920f59c4ec3c243345a5b9248e64065b",
        "title": "StarGAN v2: Diverse Image Synthesis for Multiple Domains",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2019,
        "citationCount": 1942,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1912.01865",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1912.01865, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "30187096",
                "name": "Yunjey Choi"
            },
            {
                "authorId": "2847986",
                "name": "Youngjung Uh"
            },
            {
                "authorId": "8351571",
                "name": "Jaejun Yoo"
            },
            {
                "authorId": "2577039",
                "name": "Jung-Woo Ha"
            }
        ],
        "abstract": "A good image-to-image translation model should learn a mapping between different visual domains while satisfying the following properties: 1) diversity of generated images and 2) scalability over multiple domains. Existing methods address either of the issues, having limited diversity or multiple models for all domains. We propose StarGAN v2, a single framework that tackles both and shows significantly improved results over the baselines. Experiments on CelebA-HQ and a new animal faces dataset (AFHQ) validate our superiority in terms of visual quality, diversity, and scalability. To better assess image-to-image translation models, we release AFHQ, high-quality animal faces with large inter- and intra-domain differences. The code, pretrained models, and dataset are available at https://github.com/clovaai/stargan-v2."
    },
    {
        "paperId": "587110558b1744b912108dd0b74970f131317e0d",
        "url": "https://www.semanticscholar.org/paper/587110558b1744b912108dd0b74970f131317e0d",
        "title": "RandLA-Net: Efficient Semantic Segmentation of Large-Scale Point Clouds",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2019,
        "citationCount": 1780,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1911.11236",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1911.11236, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "8163866",
                "name": "Qingyong Hu"
            },
            {
                "authorId": "2119658026",
                "name": "Bo Yang"
            },
            {
                "authorId": "11246861",
                "name": "Linhai Xie"
            },
            {
                "authorId": "145895122",
                "name": "Stefano Rosa"
            },
            {
                "authorId": "2366316759",
                "name": "Yulan Guo"
            },
            {
                "authorId": "2108346706",
                "name": "Zhihua Wang"
            },
            {
                "authorId": "3641238",
                "name": "A. Trigoni"
            },
            {
                "authorId": "34401562",
                "name": "A. Markham"
            }
        ],
        "abstract": "We study the problem of efficient semantic segmentation for large-scale 3D point clouds. By relying on expensive sampling techniques or computationally heavy pre/post-processing steps, most existing approaches are only able to be trained and operate over small-scale point clouds. In this paper, we introduce RandLA-Net, an efficient and lightweight neural architecture to directly infer per-point semantics for large-scale point clouds. The key to our approach is to use random point sampling instead of more complex point selection approaches. Although remarkably computation and memory efficient, random sampling can discard key features by chance. To overcome this, we introduce a novel local feature aggregation module to progressively increase the receptive field for each 3D point, thereby effectively preserving geometric details. Extensive experiments show that our RandLA-Net can process 1 million points in a single pass with up to 200x faster than existing approaches. Moreover, our RandLA-Net clearly surpasses state-of-the-art approaches for semantic segmentation on two large-scale benchmarks Semantic3D and SemanticKITTI."
    },
    {
        "paperId": "6303bac53abd725c3b458190a6abe389a4a1e72d",
        "url": "https://www.semanticscholar.org/paper/6303bac53abd725c3b458190a6abe389a4a1e72d",
        "title": "Deep High-Resolution Representation Learning for Human Pose Estimation",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2019,
        "citationCount": 4723,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1902.09212",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1902.09212, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "143819050",
                "name": "Ke Sun"
            },
            {
                "authorId": "144025674",
                "name": "Bin Xiao"
            },
            {
                "authorId": "1718355",
                "name": "Dong Liu"
            },
            {
                "authorId": "1688516",
                "name": "Jingdong Wang"
            }
        ],
        "abstract": "In this paper, we are interested in the human pose estimation problem with a focus on learning reliable high-resolution representations. Most existing methods recover high-resolution representations from low-resolution representations produced by a high-to-low resolution network. Instead, our proposed network maintains high-resolution representations through the whole process. We start from a high-resolution subnetwork as the first stage, gradually add high-to-low resolution subnetworks one by one to form more stages, and connect the mutli-resolution subnetworks in parallel. We conduct repeated multi-scale fusions such that each of the high-to-low resolution representations receives information from other parallel representations over and over, leading to rich high-resolution representations. As a result, the predicted keypoint heatmap is potentially more accurate and spatially more precise. We empirically demonstrate the effectiveness of our network through the superior pose estimation results over two benchmark datasets: the COCO keypoint detection dataset and the MPII Human Pose dataset. In addition, we show the superiority of our network in pose tracking on the PoseTrack dataset. The code and models have been publicly available at https://github.com/leoxiaobin/deep-high-resolution-net.pytorch."
    },
    {
        "paperId": "73c07e0a998576bb9d9409e5eed713788c0be037",
        "url": "https://www.semanticscholar.org/paper/73c07e0a998576bb9d9409e5eed713788c0be037",
        "title": "Large-Scale Long-Tailed Recognition in an Open World",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2019,
        "citationCount": 1308,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1904.05160",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1904.05160, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2117940996",
                "name": "Ziwei Liu"
            },
            {
                "authorId": "89139426",
                "name": "Zhongqi Miao"
            },
            {
                "authorId": "31818765",
                "name": "Xiaohang Zhan"
            },
            {
                "authorId": "2035768213",
                "name": "Jiayun Wang"
            },
            {
                "authorId": "40206014",
                "name": "Boqing Gong"
            },
            {
                "authorId": "2107881808",
                "name": "Stella X. Yu"
            }
        ],
        "abstract": "Real world data often have a long-tailed and open-ended distribution. A practical recognition system must classify among majority and minority classes, generalize from a few known instances, and acknowledge novelty upon a never seen instance. We define Open Long-Tailed Recognition (OLTR) as learning from such naturally distributed data and optimizing the classification accuracy over a balanced test set which include head, tail, and open classes. OLTR must handle imbalanced classification, few-shot learning, and open-set recognition in one integrated algorithm, whereas existing classification approaches focus only on one aspect and deliver poorly over the entire class spectrum. The key challenges are how to share visual knowledge between head and tail classes and how to reduce confusion between tail and open classes. We develop an integrated OLTR algorithm that maps an image to a feature space such that visual concepts can easily relate to each other based on a learned metric that respects the closed-world classification while acknowledging the novelty of the open world. Our so-called dynamic meta-embedding combines a direct image feature and an associated memory feature, with the feature norm indicating the familiarity to known classes. On three large-scale OLTR datasets we curate from object-centric ImageNet, scene-centric Places, and face-centric MS1M data, our method consistently outperforms the state-of-the-art. Our code, datasets, and models enable future OLTR research and are publicly available at \\url{https://liuziwei7.github.io/projects/LongTail.html}."
    },
    {
        "paperId": "79ed40c04328d8e1600a3a31004b4efb843dbab1",
        "url": "https://www.semanticscholar.org/paper/79ed40c04328d8e1600a3a31004b4efb843dbab1",
        "title": "DenseFusion: 6D Object Pose Estimation by Iterative Dense Fusion",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2019,
        "citationCount": 1079,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1901.04780",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1901.04780, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2109119431",
                "name": "Chen Wang"
            },
            {
                "authorId": "2068265",
                "name": "Danfei Xu"
            },
            {
                "authorId": "2117748",
                "name": "Yuke Zhu"
            },
            {
                "authorId": "1382655067",
                "name": "Roberto Martn-Martn"
            },
            {
                "authorId": "2336035401",
                "name": "Cewu Lu"
            },
            {
                "authorId": "48004138",
                "name": "Li Fei-Fei"
            },
            {
                "authorId": "1702137",
                "name": "S. Savarese"
            }
        ],
        "abstract": "A key technical challenge in performing 6D object pose estimation from RGB-D image is to fully leverage the two complementary data sources. Prior works either extract information from the RGB image and depth separately or use costly post-processing steps, limiting their performances in highly cluttered scenes and real-time applications. In this work, we present DenseFusion, a generic framework for estimating 6D pose of a set of known objects from RGB-D images. DenseFusion is a heterogeneous architecture that processes the two data sources individually and uses a novel dense fusion network to extract pixel-wise dense feature embedding, from which the pose is estimated. Furthermore, we integrate an end-to-end iterative pose refinement procedure that further improves the pose estimation while achieving near real-time inference. Our experiments show that our method outperforms state-of-the-art approaches in two datasets, YCB-Video and LineMOD. We also deploy our proposed method to a real robot to grasp and manipulate objects based on the estimated pose."
    },
    {
        "paperId": "814b70cd133f97ef039bcc44124d9344dd8b3f64",
        "url": "https://www.semanticscholar.org/paper/814b70cd133f97ef039bcc44124d9344dd8b3f64",
        "title": "Actional-Structural Graph Convolutional Networks for Skeleton-Based Action Recognition",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2019,
        "citationCount": 1007,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1904.12659",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1904.12659, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "51515065",
                "name": "Maosen Li"
            },
            {
                "authorId": "145552439",
                "name": "Siheng Chen"
            },
            {
                "authorId": "2144230222",
                "name": "Xu Chen"
            },
            {
                "authorId": "2108035782",
                "name": "Ya Zhang"
            },
            {
                "authorId": "2108846176",
                "name": "Yanfeng Wang"
            },
            {
                "authorId": "1400120070",
                "name": "Qi Tian"
            }
        ],
        "abstract": "Action recognition with skeleton data has recently attracted much attention in computer vision. Previous studies are mostly based on fixed skeleton graphs, only capturing local physical dependencies among joints, which may miss implicit joint correlations. To capture richer dependencies, we introduce an encoder-decoder structure, called A-link inference module, to capture action-specific latent dependencies, i.e. actional links, directly from actions. We also extend the existing skeleton graphs to represent higher-order dependencies, i.e. structural links. Combing the two types of links into a generalized skeleton graph, We further propose the actional-structural graph convolution network (AS-GCN), which stacks actional-structural graph convolution and temporal convolution as a basic building block, to learn both spatial and temporal features for action recognition. A future pose prediction head is added in parallel to the recognition head to help capture more detailed action patterns through self-supervision. We validate AS-GCN in action recognition using two skeleton data sets, NTU-RGB+D and Kinetics. The proposed AS-GCN achieves consistently large improvement compared to the state-of-the-art methods. As a side product, AS-GCN also shows promising results for future pose prediction."
    },
    {
        "paperId": "8406903fd2f0eb25349bf071ccfaae3947e2a9cd",
        "url": "https://www.semanticscholar.org/paper/8406903fd2f0eb25349bf071ccfaae3947e2a9cd",
        "title": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2019,
        "citationCount": 3581,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1912.04838",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1912.04838, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2075416446",
                "name": "Pei Sun"
            },
            {
                "authorId": "34699489",
                "name": "Henrik Kretzschmar"
            },
            {
                "authorId": "1404332584",
                "name": "Xerxes Dotiwalla"
            },
            {
                "authorId": "1453874781",
                "name": "Aurelien Chouard"
            },
            {
                "authorId": "1453886710",
                "name": "Vijaysai Patnaik"
            },
            {
                "authorId": "46842220",
                "name": "P. Tsui"
            },
            {
                "authorId": "2117140730",
                "name": "James Guo"
            },
            {
                "authorId": "2118860832",
                "name": "Yin Zhou"
            },
            {
                "authorId": "34858254",
                "name": "Yuning Chai"
            },
            {
                "authorId": "1388817146",
                "name": "Benjamin Caine"
            },
            {
                "authorId": "2053781980",
                "name": "Vijay Vasudevan"
            },
            {
                "authorId": "143911112",
                "name": "Wei Han"
            },
            {
                "authorId": "2020608",
                "name": "Jiquan Ngiam"
            },
            {
                "authorId": "47940821",
                "name": "Hang Zhao"
            },
            {
                "authorId": "2065297282",
                "name": "Aleksei Timofeev"
            },
            {
                "authorId": "10664203",
                "name": "S. Ettinger"
            },
            {
                "authorId": "1453887949",
                "name": "Maxim Krivokon"
            },
            {
                "authorId": "2054606886",
                "name": "A. Gao"
            },
            {
                "authorId": null,
                "name": "Aditya Joshi"
            },
            {
                "authorId": "2153639166",
                "name": "Yu Zhang"
            },
            {
                "authorId": "1789737",
                "name": "Jonathon Shlens"
            },
            {
                "authorId": "2111317372",
                "name": "Zhifeng Chen"
            },
            {
                "authorId": "1838674",
                "name": "Dragomir Anguelov"
            }
        ],
        "abstract": "The research community has increasing interest in autonomous driving research, despite the resource intensity of obtaining representative real world data. Existing self-driving datasets are limited in the scale and variation of the environments they capture, even though generalization within and between operating regions is crucial to the over-all viability of the technology. In an effort to help align the research communitys contributions with real-world self-driving problems, we introduce a new large scale, high quality, diverse dataset. Our new dataset consists of 1150 scenes that each span 20 seconds, consisting of well synchronized and calibrated high quality LiDAR and camera data captured across a range of urban and suburban geographies. It is 15x more diverse than the largest camera+LiDAR dataset available based on our proposed diversity metric. We exhaustively annotated this data with 2D (camera image) and 3D (LiDAR) bounding boxes, with consistent identifiers across frames. Finally, we provide strong baselines for 2D as well as 3D detection and tracking tasks. We further study the effects of dataset size and generalization across geographies on 3D detection methods. Find data, code and more up-to-date information at http://www.waymo.com/open."
    },
    {
        "paperId": "889c81b4d7b7ed43a3f69f880ea60b0572e02e27",
        "url": "https://www.semanticscholar.org/paper/889c81b4d7b7ed43a3f69f880ea60b0572e02e27",
        "title": "Generalized Intersection Over Union: A Metric and a Loss for Bounding Box Regression",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2019,
        "citationCount": 5104,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1902.09630",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1902.09630, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "73768948",
                "name": "S. H. Rezatofighi"
            },
            {
                "authorId": "39282796",
                "name": "Nathan Tsoi"
            },
            {
                "authorId": "39813007",
                "name": "JunYoung Gwak"
            },
            {
                "authorId": "145759966",
                "name": "Amir Sadeghian"
            },
            {
                "authorId": "145950884",
                "name": "I. Reid"
            },
            {
                "authorId": "1702137",
                "name": "S. Savarese"
            }
        ],
        "abstract": "Intersection over Union (IoU) is the most popular evaluation metric used in the object detection benchmarks. However, there is a gap between optimizing the commonly used distance losses for regressing the parameters of a bounding box and maximizing this metric value. The optimal objective for a metric is the metric itself. In the case of axis-aligned 2D bounding boxes, it can be shown that IoU can be directly used as a regression loss. However, IoU has a plateau making it infeasible to optimize in the case of non-overlapping bounding boxes. In this paper, we address the this weakness by introducing a generalized version of IoU as both a new loss and a new metric. By incorporating this generalized IoU ( GIoU) as a loss into the state-of-the art object detection frameworks, we show a consistent improvement on their performance using both the standard, IoU based, and new, GIoU based, performance measures on popular object detection benchmarks such as PASCAL VOC and MS COCO."
    },
    {
        "paperId": "8cb34cbdcf65c23ef98430441b14a648c4e8d992",
        "url": "https://www.semanticscholar.org/paper/8cb34cbdcf65c23ef98430441b14a648c4e8d992",
        "title": "ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2019,
        "citationCount": 5113,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1910.03151",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1910.03151, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "49110790",
                "name": "Qilong Wang"
            },
            {
                "authorId": "1387668229",
                "name": "Banggu Wu"
            },
            {
                "authorId": "2055691681",
                "name": "Peng Fei Zhu"
            },
            {
                "authorId": "40426020",
                "name": "P. Li"
            },
            {
                "authorId": "1724520",
                "name": "W. Zuo"
            },
            {
                "authorId": "144281199",
                "name": "Q. Hu"
            }
        ],
        "abstract": "Recently, channel attention mechanism has demonstrated to offer great potential in improving the performance of deep convolutional neural networks (CNNs). However, most existing methods dedicate to developing more sophisticated attention modules for achieving better performance, which inevitably increase model complexity. To overcome the paradox of performance and complexity trade-off, this paper proposes an Efficient Channel Attention (ECA) module, which only involves a handful of parameters while bringing clear performance gain. By dissecting the channel attention module in SENet, we empirically show avoiding dimensionality reduction is important for learning channel attention, and appropriate cross-channel interaction can preserve performance while significantly decreasing model complexity. Therefore, we propose a local cross-channel interaction strategy without dimensionality reduction, which can be efficiently implemented via 1D convolution. Furthermore, we develop a method to adaptively select kernel size of 1D convolution, determining coverage of local cross-channel interaction. The proposed ECA module is both efficient and effective, e.g., the parameters and computations of our modules against backbone of ResNet50 are 80 vs. 24.37M and 4.7e-4 GFlops vs. 3.86 GFlops, respectively, and the performance boost is more than 2% in terms of Top-1 accuracy. We extensively evaluate our ECA module on image classification, object detection and instance segmentation with backbones of ResNets and MobileNetV2. The experimental results show our module is more efficient while performing favorably against its counterparts."
    },
    {
        "paperId": "8e66c7e494476eb0dee846349df1bd705ceac6c3",
        "url": "https://www.semanticscholar.org/paper/8e66c7e494476eb0dee846349df1bd705ceac6c3",
        "title": "Argoverse: 3D Tracking and Forecasting With Rich Maps",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2019,
        "citationCount": 1487,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1911.02620",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1911.02620, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "153045055",
                "name": "Ming-Fang Chang"
            },
            {
                "authorId": "2056002474",
                "name": "John Lambert"
            },
            {
                "authorId": "3430745",
                "name": "Patsorn Sangkloy"
            },
            {
                "authorId": "2112714006",
                "name": "Jagjeet Singh"
            },
            {
                "authorId": "40657086",
                "name": "Sawomir Bk"
            },
            {
                "authorId": "40451702",
                "name": "Andrew Hartnett"
            },
            {
                "authorId": "144877660",
                "name": "De Wang"
            },
            {
                "authorId": "47890108",
                "name": "Peter Carr"
            },
            {
                "authorId": "1820249",
                "name": "S. Lucey"
            },
            {
                "authorId": "1770537",
                "name": "Deva Ramanan"
            },
            {
                "authorId": "48966748",
                "name": "James Hays"
            }
        ],
        "abstract": "We present Argoverse, a dataset designed to support autonomous vehicle perception tasks including 3D tracking and motion forecasting. Argoverse includes sensor data collected by a fleet of autonomous vehicles in Pittsburgh and Miami as well as 3D tracking annotations, 300k extracted interesting vehicle trajectories, and rich semantic maps. The sensor data consists of 360 degree images from 7 cameras with overlapping fields of view, forward-facing stereo imagery, 3D point clouds from long range LiDAR, and 6-DOF pose. Our 290km of mapped lanes contain rich geometric and semantic metadata which are not currently available in any public dataset. All data is released under a Creative Commons license at Argoverse.org. In baseline experiments, we use map information such as lane direction, driveable area, and ground height to improve the accuracy of 3D object tracking. We use 3D object tracking to mine for more than 300k interesting vehicle trajectories to create a trajectory forecasting benchmark. Motion forecasting experiments ranging in complexity from classical methods (k-NN) to LSTMs demonstrate that using detailed vector maps with lane-level information substantially reduces prediction error. Our tracking and forecasting experiments represent only a superficial exploration of the potential of rich maps in robotic perception. We hope that Argoverse will enable the research community to explore these problems in greater depth."
    },
    {
        "paperId": "9a2d83ed7d7cc647421e976d8669b023974fff67",
        "url": "https://www.semanticscholar.org/paper/9a2d83ed7d7cc647421e976d8669b023974fff67",
        "title": "MaskGAN: Towards Diverse and Interactive Facial Image Manipulation",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2019,
        "citationCount": 1204,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1907.11922",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1907.11922, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "50520553",
                "name": "Cheng-Han Lee"
            },
            {
                "authorId": "2117940996",
                "name": "Ziwei Liu"
            },
            {
                "authorId": "50789827",
                "name": "Lingyun Wu"
            },
            {
                "authorId": "47571885",
                "name": "Ping Luo"
            }
        ],
        "abstract": "Facial image manipulation has achieved great progress in recent years. However, previous methods either operate on a predefined set of face attributes or leave users little freedom to interactively manipulate images. To overcome these drawbacks, we propose a novel framework termed MaskGAN, enabling diverse and interactive face manipulation. Our key insight is that semantic masks serve as a suitable intermediate representation for flexible face manipulation with fidelity preservation. MaskGAN has two main components: 1) Dense Mapping Network (DMN) and 2) Editing Behavior Simulated Training (EBST). Specifically, DMN learns style mapping between a free-form user modified mask and a target image, enabling diverse generation results. EBST models the user editing behavior on the source mask, making the overall framework more robust to various manipulated inputs. Specifically, it introduces dual-editing consistency as the auxiliary supervision signal. To facilitate extensive studies, we construct a large-scale high-resolution face dataset with fine-grained mask annotations named CelebAMask-HQ. MaskGAN is comprehensively evaluated on two challenging tasks: attribute transfer and style copy, demonstrating superior performance over other state-of-the-art methods. The code, models, and dataset are available at https://github.com/switchablenorms/CelebAMask-HQ."
    },
    {
        "paperId": "9e475a514f54665478aac6038c262e5a6bac5e64",
        "url": "https://www.semanticscholar.org/paper/9e475a514f54665478aac6038c262e5a6bac5e64",
        "title": "nuScenes: A Multimodal Dataset for Autonomous Driving",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2019,
        "citationCount": 7148,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1903.11027",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1903.11027, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3078154",
                "name": "Holger Caesar"
            },
            {
                "authorId": "88740363",
                "name": "Varun Bankiti"
            },
            {
                "authorId": "33242383",
                "name": "Alex H. Lang"
            },
            {
                "authorId": "22254044",
                "name": "Sourabh Vora"
            },
            {
                "authorId": "1754854",
                "name": "Venice Erin Liong"
            },
            {
                "authorId": "2149106173",
                "name": "Qiang Xu"
            },
            {
                "authorId": "2064366548",
                "name": "Anush Krishnan"
            },
            {
                "authorId": "1959810281",
                "name": "Yuxin Pan"
            },
            {
                "authorId": "46718993",
                "name": "G. Baldan"
            },
            {
                "authorId": "3258919",
                "name": "Oscar Beijbom"
            }
        ],
        "abstract": "Robust detection and tracking of objects is crucial for the deployment of autonomous vehicle technology. Image based benchmark datasets have driven development in computer vision tasks such as object detection, tracking and segmentation of agents in the environment. Most autonomous vehicles, however, carry a combination of cameras and range sensors such as lidar and radar. As machine learning based methods for detection and tracking become more prevalent, there is a need to train and evaluate such methods on datasets containing range sensor data along with images. In this work we present nuTonomy scenes (nuScenes), the first dataset to carry the full autonomous vehicle sensor suite: 6 cameras, 5 radars and 1 lidar, all with full 360 degree field of view. nuScenes comprises 1000 scenes, each 20s long and fully annotated with 3D bounding boxes for 23 classes and 8 attributes. It has 7x as many annotations and 100x as many images as the pioneering KITTI dataset. We define novel 3D detection and tracking metrics. We also provide careful dataset analysis as well as baselines for lidar and image based detection and tracking. Data, development kit and more information are available online."
    },
    {
        "paperId": "a1a19aaddf57c0546357d890d9269092ba0afb26",
        "url": "https://www.semanticscholar.org/paper/a1a19aaddf57c0546357d890d9269092ba0afb26",
        "title": "Semantic Image Synthesis With Spatially-Adaptive Normalization",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2019,
        "citationCount": 2962,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1903.07291",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1903.07291, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2071929129",
                "name": "Taesung Park"
            },
            {
                "authorId": "39793900",
                "name": "Ming-Yu Liu"
            },
            {
                "authorId": "2195314",
                "name": "Ting-Chun Wang"
            },
            {
                "authorId": "2436356",
                "name": "Jun-Yan Zhu"
            }
        ],
        "abstract": "We propose spatially-adaptive normalization, a simple but effective layer for synthesizing photorealistic images given an input semantic layout. Previous methods directly feed the semantic layout as input to the network, forcing the network to memorize the information throughout all the layers. Instead, we propose using the input layout for modulating the activations in normalization layers through a spatially-adaptive, learned affine transformation. Experiments on several challenging datasets demonstrate the superiority of our method compared to existing approaches, regarding both visual fidelity and alignment with input layouts. Finally, our model allows users to easily control the style and content of image synthesis results as well as create multi-modal results. Code is available upon publication."
    },
    {
        "paperId": "a4cc0701170331a1fd0e58bad962bd7f39f5efc9",
        "url": "https://www.semanticscholar.org/paper/a4cc0701170331a1fd0e58bad962bd7f39f5efc9",
        "title": "GhostNet: More Features From Cheap Operations",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2019,
        "citationCount": 3524,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1911.11907",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1911.11907, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3826388",
                "name": "Kai Han"
            },
            {
                "authorId": "2108702980",
                "name": "Yunhe Wang"
            },
            {
                "authorId": "2056267684",
                "name": "Qi Tian"
            },
            {
                "authorId": "2148899357",
                "name": "Jianyuan Guo"
            },
            {
                "authorId": "1691522",
                "name": "Chunjing Xu"
            },
            {
                "authorId": null,
                "name": "Chang Xu"
            }
        ],
        "abstract": "Deploying convolutional neural networks (CNNs) on embedded devices is difficult due to the limited memory and computation resources. The redundancy in feature maps is an important characteristic of those successful CNNs, but has rarely been investigated in neural architecture design. This paper proposes a novel Ghost module to generate more feature maps from cheap operations. Based on a set of intrinsic feature maps, we apply a series of linear transformations with cheap cost to generate many ghost feature maps that could fully reveal information underlying intrinsic features. The proposed Ghost module can be taken as a plug-and-play component to upgrade existing convolutional neural networks. Ghost bottlenecks are designed to stack Ghost modules, and then the lightweight GhostNet can be easily established. Experiments conducted on benchmarks demonstrate that the proposed Ghost module is an impressive alternative of convolution layers in baseline models, and our GhostNet can achieve higher recognition performance (e.g. 75.7% top-1 accuracy) than MobileNetV3 with similar computational cost on the ImageNet ILSVRC-2012 classification dataset. Code is available at https://github.com/huawei-noah/ghostnet."
    },
    {
        "paperId": "a6f4917d043494d2ebaebe6b65cb35e6a07fda41",
        "url": "https://www.semanticscholar.org/paper/a6f4917d043494d2ebaebe6b65cb35e6a07fda41",
        "title": "Importance Estimation for Neural Network Pruning",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2019,
        "citationCount": 1040,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1906.10771",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1906.10771, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2824500",
                "name": "Pavlo Molchanov"
            },
            {
                "authorId": "36508529",
                "name": "Arun Mallya"
            },
            {
                "authorId": "2342481",
                "name": "Stephen Tyree"
            },
            {
                "authorId": "145134808",
                "name": "I. Frosio"
            },
            {
                "authorId": "2273651410",
                "name": "Jan Kautz"
            }
        ],
        "abstract": "Structural pruning of neural network parameters reduces computational, energy, and memory transfer costs during inference. We propose a novel method that estimates the contribution of a neuron (filter) to the final loss and iteratively removes those with smaller scores. We describe two variations of our method using the first and second-order Taylor expansions to approximate a filter's contribution. Both methods scale consistently across any network layer without requiring per-layer sensitivity analysis and can be applied to any kind of layer, including skip connections. For modern networks trained on ImageNet, we measured experimentally a high (>93%) correlation between the contribution computed by our methods and a reliable estimate of the true importance. Pruning with the proposed methods led to an improvement over state-of-the-art in terms of accuracy, FLOPs, and parameter reduction. On ResNet-101, we achieve a 40% FLOPS reduction by removing 30% of the parameters, with a loss of 0.02% in the top-1 accuracy on ImageNet."
    },
    {
        "paperId": "a7ac99d7cf3f568ab1a741392144b646b856ae0c",
        "url": "https://www.semanticscholar.org/paper/a7ac99d7cf3f568ab1a741392144b646b856ae0c",
        "title": "GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2019,
        "citationCount": 2637,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1902.09506",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR.2019.00686?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR.2019.00686, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "152951058",
                "name": "Drew A. Hudson"
            },
            {
                "authorId": "144783904",
                "name": "Christopher D. Manning"
            }
        ],
        "abstract": "We introduce GQA, a new dataset for real-world visual reasoning and compositional question answering, seeking to address key shortcomings of previous VQA datasets. We have developed a strong and robust question engine that leverages Visual Genome scene graph structures to create 22M diverse reasoning questions, which all come with functional programs that represent their semantics. We use the programs to gain tight control over the answer distribution and present a new tunable smoothing technique to mitigate question biases. Accompanying the dataset is a suite of new metrics that evaluate essential qualities such as consistency, grounding and plausibility. A careful analysis is performed for baselines as well as state-of-the-art models, providing fine-grained results for different question types and topologies. Whereas a blind LSTM obtains a mere 42.1%, and strong VQA models achieve 54.1%, human performance tops at 89.3%, offering ample opportunity for new research to explore. We hope GQA will provide an enabling resource for the next generation of models with enhanced robustness, improved consistency, and deeper semantic understanding of vision and language."
    },
    {
        "paperId": "a84906dbd4d6640f918d0b6ed2a7313dda0d55f1",
        "url": "https://www.semanticscholar.org/paper/a84906dbd4d6640f918d0b6ed2a7313dda0d55f1",
        "title": "Panoptic Feature Pyramid Networks",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2019,
        "citationCount": 1427,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/1901.02446",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1901.02446, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "144843400",
                "name": "Alexander Kirillov"
            },
            {
                "authorId": "2983898",
                "name": "Ross B. Girshick"
            },
            {
                "authorId": "39353098",
                "name": "Kaiming He"
            },
            {
                "authorId": "3127283",
                "name": "Piotr Dollr"
            }
        ],
        "abstract": "The recently introduced panoptic segmentation task has renewed our community's interest in unifying the tasks of instance segmentation (for thing classes) and semantic segmentation (for stuff classes). However, current state-of-the-art methods for this joint task use separate and dissimilar networks for instance and semantic segmentation, without performing any shared computation. In this work, we aim to unify these methods at the architectural level, designing a single network for both tasks. Our approach is to endow Mask R-CNN, a popular instance segmentation method, with a semantic segmentation branch using a shared Feature Pyramid Network (FPN) backbone. Surprisingly, this simple baseline not only remains effective for instance segmentation, but also yields a lightweight, top-performing method for semantic segmentation. In this work, we perform a detailed study of this minimally extended version of Mask R-CNN with FPN, which we refer to as Panoptic FPN, and show it is a robust and accurate baseline for both tasks. Given its effectiveness and conceptual simplicity, we hope our method can serve as a strong baseline and aid future research in panoptic segmentation."
    },
    {
        "paperId": "add2f205338d70e10ce5e686df4a690e2851bdfc",
        "url": "https://www.semanticscholar.org/paper/add2f205338d70e10ce5e686df4a690e2851bdfc",
        "title": "Momentum Contrast for Unsupervised Visual Representation Learning",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2019,
        "citationCount": 13902,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1911.05722",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1911.05722, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "39353098",
                "name": "Kaiming He"
            },
            {
                "authorId": "146884473",
                "name": "Haoqi Fan"
            },
            {
                "authorId": "98264506",
                "name": "Yuxin Wu"
            },
            {
                "authorId": "1817030",
                "name": "Saining Xie"
            },
            {
                "authorId": "2983898",
                "name": "Ross B. Girshick"
            }
        ],
        "abstract": "We present Momentum Contrast (MoCo) for unsupervised visual representation learning. From a perspective on contrastive learning as dictionary look-up, we build a dynamic dictionary with a queue and a moving-averaged encoder. This enables building a large and consistent dictionary on-the-fly that facilitates contrastive unsupervised learning. MoCo provides competitive results under the common linear protocol on ImageNet classification. More importantly, the representations learned by MoCo transfer well to downstream tasks. MoCo can outperform its supervised pre-training counterpart in 7 detection/segmentation tasks on PASCAL VOC, COCO, and other datasets, sometimes surpassing it by large margins. This suggests that the gap between unsupervised and supervised representation learning has been largely closed in many vision tasks."
    },
    {
        "paperId": "aeef4a512aecb76ca04f102ec8972fe8dbce71e7",
        "url": "https://www.semanticscholar.org/paper/aeef4a512aecb76ca04f102ec8972fe8dbce71e7",
        "title": "Learning RoI Transformer for Oriented Object Detection in Aerial Images",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2019,
        "citationCount": 1006,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR.2019.00296?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR.2019.00296, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2023933935",
                "name": "Jian Ding"
            },
            {
                "authorId": "2685089",
                "name": "Nan Xue"
            },
            {
                "authorId": "2072952146",
                "name": "Yang Long"
            },
            {
                "authorId": "51280933",
                "name": "Guisong Xia"
            },
            {
                "authorId": "2457942",
                "name": "Qikai Lu"
            }
        ],
        "abstract": "Object detection in aerial images is an active yet challenging task in computer vision because of the birds-eye view perspective, the highly complex backgrounds, and the variant appearances of objects. Especially when detecting densely packed objects in aerial images, methods relying on horizontal proposals for common object detection often introduce mismatches between the Region of Interests (RoIs) and objects. This leads to the common misalignment between the final object classification confidence and localization accuracy. In this paper, we propose a RoI Transformer to address these problems. The core idea of RoI Transformer is to apply spatial transformations on RoIs and learn the transformation parameters under the supervision of oriented bounding box (OBB) annotations. RoI Transformer is with lightweight and can be easily embedded into detectors for oriented object detection. Simply apply the RoI Transformer to light head RCNN has achieved state-of-the-art performances on two common and challenging aerial datasets, i.e., DOTA and HRSC2016, with a neglectable reduction to detection speed. Our RoI Transformer exceeds the deformable Position Sensitive RoI pooling when oriented bounding-box annotations are available. Extensive experiments have also validated the flexibility and effectiveness of our RoI Transformer."
    },
    {
        "paperId": "af1f7739283bdbd2b7a94903041f6d6afd991907",
        "url": "https://www.semanticscholar.org/paper/af1f7739283bdbd2b7a94903041f6d6afd991907",
        "title": "Towards VQA Models That Can Read",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2019,
        "citationCount": 1692,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1904.08920",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1904.08920, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "50286460",
                "name": "Amanpreet Singh"
            },
            {
                "authorId": "144223091",
                "name": "Vivek Natarajan"
            },
            {
                "authorId": "144826412",
                "name": "Meet Shah"
            },
            {
                "authorId": "2116341314",
                "name": "Yu Jiang"
            },
            {
                "authorId": "39717886",
                "name": "Xinlei Chen"
            },
            {
                "authorId": "1746610",
                "name": "Dhruv Batra"
            },
            {
                "authorId": "153432684",
                "name": "Devi Parikh"
            },
            {
                "authorId": "34849128",
                "name": "Marcus Rohrbach"
            }
        ],
        "abstract": "Studies have shown that a dominant class of questions asked by visually impaired users on images of their surroundings involves reading text in the image. But todays VQA models can not read! Our paper takes a first step towards addressing this problem. First, we introduce a new TextVQA dataset to facilitate progress on this important problem. Existing datasets either have a small proportion of questions about text (e.g., the VQA dataset) or are too small (e.g., the VizWiz dataset). TextVQA contains 45,336 questions on 28,408 images that require reasoning about text to answer. Second, we introduce a novel model architecture that reads text in the image, reasons about it in the context of the image and the question, and predicts an answer which might be a deduction based on the text and the image or composed of the strings found in the image. Consequently, we call our approach Look, Read, Reason & Answer (LoRRA). We show that LoRRA outperforms existing state-of-the-art VQA models on our TextVQA dataset. We find that the gap between human performance and machine performance is significantly larger on TextVQA than on VQA 2.0, suggesting that TextVQA is well-suited to benchmark progress along directions complementary to VQA 2.0."
    },
    {
        "paperId": "b1e245a304de66f6c6dcc8f5fb2254dab94de7d8",
        "url": "https://www.semanticscholar.org/paper/b1e245a304de66f6c6dcc8f5fb2254dab94de7d8",
        "title": "Multi-Label Image Recognition With Graph Convolutional Networks",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2019,
        "citationCount": 1137,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1904.03582",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1904.03582, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2111607364",
                "name": "Zhao-Min Chen"
            },
            {
                "authorId": "2126047",
                "name": "Xiu-Shen Wei"
            },
            {
                "authorId": "2155299843",
                "name": "Peng Wang"
            },
            {
                "authorId": "1720424",
                "name": "Yanwen Guo"
            }
        ],
        "abstract": "The task of multi-label image recognition is to predict a set of object labels that present in an image. As objects normally co-occur in an image, it is desirable to model the label dependencies to improve the recognition performance. To capture and explore such important dependencies, we propose a multi-label classification model based on Graph Convolutional Network (GCN). The model builds a directed graph over the object labels, where each node (label) is represented by word embeddings of a label, and GCN is learned to map this label graph into a set of inter-dependent object classifiers. These classifiers are applied to the image descriptors extracted by another sub-net, enabling the whole network to be end-to-end trainable. Furthermore, we propose a novel re-weighted scheme to create an effective label correlation matrix to guide information propagation among the nodes in GCN. Experiments on two multi-label image recognition datasets show that our approach obviously outperforms other existing state-of-the-art methods. In addition, visualization analyses reveal that the classifiers learned by our model maintain meaningful semantic topology."
    },
    {
        "paperId": "b5375995ab8d679a581ffcc2f2e8d3777d60324b",
        "url": "https://www.semanticscholar.org/paper/b5375995ab8d679a581ffcc2f2e8d3777d60324b",
        "title": "NAS-FPN: Learning Scalable Feature Pyramid Architecture for Object Detection",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2019,
        "citationCount": 1628,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1904.07392",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1904.07392, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1898210",
                "name": "Golnaz Ghiasi"
            },
            {
                "authorId": "33493200",
                "name": "Tsung-Yi Lin"
            },
            {
                "authorId": "34320634",
                "name": "Ruoming Pang"
            },
            {
                "authorId": "2827616",
                "name": "Quoc V. Le"
            }
        ],
        "abstract": "Current state-of-the-art convolutional architectures for object detection are manually designed. Here we aim to learn a better architecture of feature pyramid network for object detection. We adopt Neural Architecture Search and discover a new feature pyramid architecture in a novel scalable search space covering all cross-scale connections. The discovered architecture, named NAS-FPN, consists of a combination of top-down and bottom-up connections to fuse features across scales. NAS-FPN, combined with various backbone models in the RetinaNet framework, achieves better accuracy and latency tradeoff compared to state-of-the-art object detection models. NAS-FPN improves mobile detection accuracy by 2 AP compared to state-of-the-art SSDLite with MobileNetV2 model in [32] and achieves 48.3 AP which surpasses Mask R-CNN [10] detection accuracy with less computation time."
    },
    {
        "paperId": "cebd4ab4ab52be88b26d976aa7d4fb35cc19c2a2",
        "url": "https://www.semanticscholar.org/paper/cebd4ab4ab52be88b26d976aa7d4fb35cc19c2a2",
        "title": "BASNet: Boundary-Aware Salient Object Detection",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2019,
        "citationCount": 1333,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR.2019.00766?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR.2019.00766, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "144522278",
                "name": "Xuebin Qin"
            },
            {
                "authorId": null,
                "name": "Zichen Zhang"
            },
            {
                "authorId": "50418713",
                "name": "Chenyang Huang"
            },
            {
                "authorId": "2114088409",
                "name": "Chao Gao"
            },
            {
                "authorId": "153597007",
                "name": "Masood Dehghan"
            },
            {
                "authorId": "3160299",
                "name": "Martin Jgersand"
            }
        ],
        "abstract": "Deep Convolutional Neural Networks have been adopted for salient object detection and achieved the state-of-the-art performance. Most of the previous works however focus on region accuracy but not on the boundary quality. In this paper, we propose a predict-refine architecture, BASNet, and a new hybrid loss for Boundary-Aware Salient object detection. Specifically, the architecture is composed of a densely supervised Encoder-Decoder network and a residual refinement module, which are respectively in charge of saliency prediction and saliency map refinement. The hybrid loss guides the network to learn the transformation between the input image and the ground truth in a three-level hierarchy -- pixel-, patch- and map- level -- by fusing Binary Cross Entropy (BCE), Structural SIMilarity (SSIM) and Intersection-over-Union (IoU) losses. Equipped with the hybrid loss, the proposed predict-refine architecture is able to effectively segment the salient object regions and accurately predict the fine structures with clear boundaries. Experimental results on six public datasets show that our method outperforms the state-of-the-art methods both in terms of regional and boundary evaluation measures. Our method runs at over 25 fps on a single GPU. The code is available at: https://github.com/NathanUA/BASNet."
    },
    {
        "paperId": "d4e54ea33c097cc0a741200d45053e78f5d5ba9c",
        "url": "https://www.semanticscholar.org/paper/d4e54ea33c097cc0a741200d45053e78f5d5ba9c",
        "title": "Face X-Ray for More General Face Forgery Detection",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2019,
        "citationCount": 1026,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1912.13458",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1912.13458, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "49192783",
                "name": "Lingzhi Li"
            },
            {
                "authorId": "9324504",
                "name": "Jianmin Bao"
            },
            {
                "authorId": "2146320438",
                "name": "Ting Zhang"
            },
            {
                "authorId": "102320235",
                "name": "Hao Yang"
            },
            {
                "authorId": "47514557",
                "name": "Dong Chen"
            },
            {
                "authorId": "1716835",
                "name": "Fang Wen"
            },
            {
                "authorId": "2261753424",
                "name": "B. Guo"
            }
        ],
        "abstract": "In this paper we propose a novel image representation called face X-ray for detecting forgery in face images. The face X-ray of an input face image is a greyscale image that reveals whether the input image can be decomposed into the blending of two images from different sources. It does so by showing the blending boundary for a forged image and the absence of blending for a real image. We observe that most existing face manipulation methods share a common step: blending the altered face into an existing background image. For this reason, face X-ray provides an effective way for detecting forgery generated by most existing face manipulation algorithms. Face X-ray is general in the sense that it only assumes the existence of a blending step and does not rely on any knowledge of the artifacts associated with a specific face manipulation technique. Indeed, the algorithm for computing face X-ray can be trained without fake images generated by any of the state-of-the-art face manipulation methods. Extensive experiments show that face X-ray remains effective when applied to forgery generated by unseen face manipulation techniques, while most existing face forgery detection or deepfake detection algorithms experience a significant performance drop."
    },
    {
        "paperId": "d8d89a0a1eca983512247af701a9e5596c903a16",
        "url": "https://www.semanticscholar.org/paper/d8d89a0a1eca983512247af701a9e5596c903a16",
        "title": "Interpreting the Latent Space of GANs for Semantic Face Editing",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2019,
        "citationCount": 1207,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1907.10786",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1907.10786, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2117687899",
                "name": "Yujun Shen"
            },
            {
                "authorId": "4398255",
                "name": "Jinjin Gu"
            },
            {
                "authorId": "50295995",
                "name": "Xiaoou Tang"
            },
            {
                "authorId": "145291669",
                "name": "Bolei Zhou"
            }
        ],
        "abstract": "Despite the recent advance of Generative Adversarial Networks (GANs) in high-fidelity image synthesis, there lacks enough understanding of how GANs are able to map a latent code sampled from a random distribution to a photo-realistic image. Previous work assumes the latent space learned by GANs follows a distributed representation but observes the vector arithmetic phenomenon. In this work, we propose a novel framework, called InterFaceGAN, for semantic face editing by interpreting the latent semantics learned by GANs. In this framework, we conduct a detailed study on how different semantics are encoded in the latent space of GANs for face synthesis. We find that the latent code of well-trained generative models actually learns a disentangled representation after linear transformations. We explore the disentanglement between various semantics and manage to decouple some entangled semantics with subspace projection, leading to more precise control of facial attributes. Besides manipulating gender, age, expression, and the presence of eyeglasses, we can even vary the face pose as well as fix the artifacts accidentally generated by GAN models. The proposed method is further applied to achieve real image manipulation when combined with GAN inversion methods or some encoder-involved models. Extensive results suggest that learning to synthesize faces spontaneously brings a disentangled and controllable facial attribute representation."
    },
    {
        "paperId": "db160e36aec4b43cc0651039eb1fc1e63527b090",
        "url": "https://www.semanticscholar.org/paper/db160e36aec4b43cc0651039eb1fc1e63527b090",
        "title": "Bridging the Gap Between Anchor-Based and Anchor-Free Detection via Adaptive Training Sample Selection",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2019,
        "citationCount": 1883,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1912.02424",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1912.02424, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "145599121",
                "name": "Shifeng Zhang"
            },
            {
                "authorId": "143700133",
                "name": "Cheng Chi"
            },
            {
                "authorId": "2110253898",
                "name": "Yongqiang Yao"
            },
            {
                "authorId": "145754448",
                "name": "Zhen Lei"
            },
            {
                "authorId": "1390908654",
                "name": "Stan Z. Li"
            }
        ],
        "abstract": "Object detection has been dominated by anchor-based detectors for several years. Recently, anchor-free detectors have become popular due to the proposal of FPN and Focal Loss. In this paper, we first point out that the essential difference between anchor-based and anchor-free detection is actually how to define positive and negative training samples, which leads to the performance gap between them. If they adopt the same definition of positive and negative samples during training, there is no obvious difference in the final performance, no matter regressing from a box or a point. This shows that how to select positive and negative training samples is important for current object detectors. Then, we propose an Adaptive Training Sample Selection (ATSS) to automatically select positive and negative samples according to statistical characteristics of object. It significantly improves the performance of anchor-based and anchor-free detectors and bridges the gap between them. Finally, we discuss the necessity of tiling multiple anchors per location on the image to detect objects. Extensive experiments conducted on MS COCO support our aforementioned analysis and conclusions. With the newly introduced ATSS, we improve state-of-the-art detectors by a large margin to 50.7% AP without introducing any overhead. The code is available at https://github.com/sfzhang15/ATSS."
    },
    {
        "paperId": "dd81523b9accdf1c13cd37f76b22ab27d84b7a42",
        "url": "https://www.semanticscholar.org/paper/dd81523b9accdf1c13cd37f76b22ab27d84b7a42",
        "title": "DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2019,
        "citationCount": 4197,
        "openAccessPdf": {
            "url": "",
            "status": "CLOSED",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1901.05103, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2148838020",
                "name": "Jeong Joon Park"
            },
            {
                "authorId": "47686265",
                "name": "Peter R. Florence"
            },
            {
                "authorId": "20128275",
                "name": "Julian Straub"
            },
            {
                "authorId": "50366818",
                "name": "Richard A. Newcombe"
            },
            {
                "authorId": "3289186",
                "name": "S. Lovegrove"
            }
        ],
        "abstract": "Computer graphics, 3D computer vision and robotics communities have produced multiple approaches to representing 3D geometry for rendering and reconstruction. These provide trade-offs across fidelity, efficiency and compression capabilities. In this work, we introduce DeepSDF, a learned continuous Signed Distance Function (SDF) representation of a class of shapes that enables high quality shape representation, interpolation and completion from partial and noisy 3D input data. DeepSDF, like its classical counterpart, represents a shape's surface by a continuous volumetric field: the magnitude of a point in the field represents the distance to the surface boundary and the sign indicates whether the region is inside (-) or outside (+) of the shape, hence our representation implicitly encodes a shape's boundary as the zero-level-set of the learned function while explicitly representing the classification of space as being part of the shapes interior or not. While classical SDF's both in analytical or discretized voxel form typically represent the surface of a single shape, DeepSDF can represent an entire class of shapes. Furthermore, we show state-of-the-art performance for learned 3D shape representation and completion while reducing the model size by an order of magnitude compared with previous work."
    },
    {
        "paperId": "decf55f42e15b3ce56c6bd14eb14906cec29b7ab",
        "url": "https://www.semanticscholar.org/paper/decf55f42e15b3ce56c6bd14eb14906cec29b7ab",
        "title": "VIBE: Video Inference for Human Body Pose and Shape Estimation",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2019,
        "citationCount": 1050,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1912.05656",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1912.05656, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "51131930",
                "name": "Muhammed Kocabas"
            },
            {
                "authorId": "51174183",
                "name": "Nikos Athanasiou"
            },
            {
                "authorId": "2105795",
                "name": "Michael J. Black"
            }
        ],
        "abstract": "Human motion is fundamental to understanding behavior. Despite progress on single-image 3D pose and shape estimation, existing video-based state-of-the-art methods fail to produce accurate and natural motion sequences due to a lack of ground-truth 3D motion data for training. To address this problem, we propose \"Video Inference for Body Pose and Shape Estimation'' (VIBE), which makes use of an existing large-scale motion capture dataset (AMASS) together with unpaired, in-the-wild, 2D keypoint annotations. Our key novelty is an adversarial learning framework that leverages AMASS to discriminate between real human motions and those produced by our temporal pose and shape regression networks. We define a novel temporal network architecture with a self-attention mechanism and show that adversarial training, at the sequence level, produces kinematically plausible motion sequences without in-the-wild ground-truth 3D labels. We perform extensive experimentation to analyze the importance of motion and demonstrate the effectiveness of VIBE on challenging 3D pose estimation datasets, achieving state-of-the-art performance. Code and pretrained models are available at https://github.com/mkocabas/VIBE"
    },
    {
        "paperId": "dfc66041b88ccdfc21ae816e0ecee9e8d1ef4731",
        "url": "https://www.semanticscholar.org/paper/dfc66041b88ccdfc21ae816e0ecee9e8d1ef4731",
        "title": "Differentiable Volumetric Rendering: Learning Implicit 3D Representations Without 3D Supervision",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2019,
        "citationCount": 1062,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1912.07372",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1912.07372, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "145048708",
                "name": "Michael Niemeyer"
            },
            {
                "authorId": "8226549",
                "name": "L. Mescheder"
            },
            {
                "authorId": "52211220",
                "name": "Michael Oechsle"
            },
            {
                "authorId": "47237027",
                "name": "Andreas Geiger"
            }
        ],
        "abstract": "Learning-based 3D reconstruction methods have shown impressive results. However, most methods require 3D supervision which is often hard to obtain for real-world datasets. Recently, several works have proposed differentiable rendering techniques to train reconstruction models from RGB images. Unfortunately, these approaches are currently restricted to voxel- and mesh-based representations, suffering from discretization or low resolution. In this work, we propose a differentiable rendering formulation for implicit shape and texture representations. Implicit representations have recently gained popularity as they represent shape and texture continuously. Our key insight is that depth gradients can be derived analytically using the concept of implicit differentiation. This allows us to learn implicit shape and texture representations directly from RGB images. We experimentally show that our single-view reconstructions rival those learned with full 3D supervision. Moreover, we find that our method can be used for multi-view 3D reconstruction, directly resulting in watertight meshes."
    },
    {
        "paperId": "f4838839719cf96951ade45a221700341f57c4d7",
        "url": "https://www.semanticscholar.org/paper/f4838839719cf96951ade45a221700341f57c4d7",
        "title": "Auto-DeepLab: Hierarchical Neural Architecture Search for Semantic Image Segmentation",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2019,
        "citationCount": 1020,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1901.02985",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1901.02985, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "50557601",
                "name": "Chenxi Liu"
            },
            {
                "authorId": "34192119",
                "name": "Liang-Chieh Chen"
            },
            {
                "authorId": "3302320",
                "name": "Florian Schroff"
            },
            {
                "authorId": "2595180",
                "name": "Hartwig Adam"
            },
            {
                "authorId": "2052830834",
                "name": "Wei Hua"
            },
            {
                "authorId": "145081362",
                "name": "A. Yuille"
            },
            {
                "authorId": "48004138",
                "name": "Li Fei-Fei"
            }
        ],
        "abstract": "Recently, Neural Architecture Search (NAS) has successfully identified neural network architectures that exceed human designed ones on large-scale image classification. In this paper, we study NAS for semantic image segmentation. Existing works often focus on searching the repeatable cell structure, while hand-designing the outer network structure that controls the spatial resolution changes. This choice simplifies the search space, but becomes increasingly problematic for dense image prediction which exhibits a lot more network level architectural variations. Therefore, we propose to search the network level structure in addition to the cell level structure, which forms a hierarchical architecture search space. We present a network level search space that includes many popular designs, and develop a formulation that allows efficient gradient-based architecture search (3 P100 GPU days on Cityscapes images). We demonstrate the effectiveness of the proposed method on the challenging Cityscapes, PASCAL VOC 2012, and ADE20K datasets. Auto-DeepLab, our architecture searched specifically for semantic image segmentation, attains state-of-the-art performance without any ImageNet pretraining."
    },
    {
        "paperId": "f902a64f7d08aaa6bfca7463e8729952ddc6134e",
        "url": "https://www.semanticscholar.org/paper/f902a64f7d08aaa6bfca7463e8729952ddc6134e",
        "title": "LVIS: A Dataset for Large Vocabulary Instance Segmentation",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2019,
        "citationCount": 1598,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1908.03195",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1908.03195, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2337947220",
                "name": "Agrim Gupta"
            },
            {
                "authorId": "3127283",
                "name": "Piotr Dollr"
            },
            {
                "authorId": "2983898",
                "name": "Ross B. Girshick"
            }
        ],
        "abstract": "Progress on object detection is enabled by datasets that focus the research communitys attention on open challenges. This process led us from simple images to complex scenes and from bounding boxes to segmentation masks. In this work, we introduce LVIS (pronounced el-vis): a new dataset for Large Vocabulary Instance Segmentation. We plan to collect 2.2 million high-quality instance segmentation masks for over 1000 entry-level object categories in 164k images. Due to the Zipfian distribution of categories in natural images, LVIS naturally has a long tail of categories with few training samples. Given that state-of-the-art deep learning methods for object detection perform poorly in the low-sample regime, we believe that our dataset poses an important and exciting new scientific challenge. LVIS is available at http://www.lvisdataset.org."
    },
    {
        "paperId": "fb7972f30812c7dd056d7943c3e3f00af022d607",
        "url": "https://www.semanticscholar.org/paper/fb7972f30812c7dd056d7943c3e3f00af022d607",
        "title": "Dynamic Convolution: Attention Over Convolution Kernels",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2019,
        "citationCount": 1145,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1912.03458",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1912.03458, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2109306087",
                "name": "Yinpeng Chen"
            },
            {
                "authorId": "3386593",
                "name": "Xiyang Dai"
            },
            {
                "authorId": "2152968847",
                "name": "Mengchen Liu"
            },
            {
                "authorId": "49025801",
                "name": "Dongdong Chen"
            },
            {
                "authorId": "145347147",
                "name": "Lu Yuan"
            },
            {
                "authorId": "2145253136",
                "name": "Zicheng Liu"
            }
        ],
        "abstract": "Light-weight convolutional neural networks (CNNs) suffer performance degradation as their low computational budgets constrain both the depth (number of convolution layers) and the width (number of channels) of CNNs, resulting in limited representation capability. To address this issue, we present Dynamic Convolution, a new design that increases model complexity without increasing the network depth or width. Instead of using a single convolution kernel per layer, dynamic convolution aggregates multiple parallel convolution kernels dynamically based upon their attentions, which are input dependent. Assembling multiple kernels is not only computationally efficient due to the small kernel size, but also has more representation power since these kernels are aggregated in a non-linear way via attention. By simply using dynamic convolution for the state-of-the-art architecture MobileNetV3-Small, the top-1 accuracy of ImageNet classification is boosted by 2.9% with only 4% additional FLOPs and 2.9 AP gain is achieved on COCO keypoint detection."
    },
    {
        "paperId": "fb8cf663a71bf31f59557a35d36aaf8c465b50af",
        "url": "https://www.semanticscholar.org/paper/fb8cf663a71bf31f59557a35d36aaf8c465b50af",
        "title": "Selective Kernel Networks",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2019,
        "citationCount": 2443,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1903.06586",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1903.06586, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2144439048",
                "name": "Xiang Li"
            },
            {
                "authorId": "71074736",
                "name": "Wenhai Wang"
            },
            {
                "authorId": "2109753669",
                "name": "Xiaolin Hu"
            },
            {
                "authorId": "51460259",
                "name": "Jian Yang"
            }
        ],
        "abstract": "In standard Convolutional Neural Networks (CNNs), the receptive fields of artificial neurons in each layer are designed to share the same size. It is well-known in the neuroscience community that the receptive field size of visual cortical neurons are modulated by the stimulus, which has been rarely considered in constructing CNNs. We propose a dynamic selection mechanism in CNNs that allows each neuron to adaptively adjust its receptive field size based on multiple scales of input information. A building block called Selective Kernel (SK) unit is designed, in which multiple branches with different kernel sizes are fused using softmax attention that is guided by the information in these branches. Different attentions on these branches yield different sizes of the effective receptive fields of neurons in the fusion layer. Multiple SK units are stacked to a deep network termed Selective Kernel Networks (SKNets). On the ImageNet and CIFAR benchmarks, we empirically show that SKNet outperforms the existing state-of-the-art architectures with lower model complexity. Detailed analyses show that the neurons in SKNet can capture target objects with different scales, which verifies the capability of neurons for adaptively adjusting their receptive field sizes according to the input. The code and models are available at https://github.com/implus/SKNet."
    },
    {
        "paperId": "fc437af6204008647ea49f81058d5fdaddf75ead",
        "url": "https://www.semanticscholar.org/paper/fc437af6204008647ea49f81058d5fdaddf75ead",
        "title": "Meta-Learning With Differentiable Convex Optimization",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2019,
        "citationCount": 1385,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1904.03758",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1904.03758, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2668978",
                "name": "Kwonjoon Lee"
            },
            {
                "authorId": "35208858",
                "name": "Subhransu Maji"
            },
            {
                "authorId": "2529423",
                "name": "Avinash Ravichandran"
            },
            {
                "authorId": "1715959",
                "name": "Stefano Soatto"
            }
        ],
        "abstract": "Many meta-learning approaches for few-shot learning rely on simple base learners such as nearest-neighbor classifiers. However, even in the few-shot regime, discriminatively trained linear predictors can offer better generalization. We propose to use these predictors as base learners to learn representations for few-shot learning and show they offer better tradeoffs between feature size and performance across a range of few-shot recognition benchmarks. Our objective is to learn feature embeddings that generalize well under a linear classification rule for novel categories. To efficiently solve the objective, we exploit two properties of linear classifiers: implicit differentiation of the optimality conditions of the convex problem and the dual formulation of the optimization problem. This allows us to use high-dimensional embeddings with improved generalization at a modest increase in computational overhead. Our approach, named MetaOptNet, achieves state-of-the-art performance on miniImageNet, tieredImageNet, CIFAR-FS, and FC100 few-shot learning benchmarks."
    },
    {
        "paperId": "fc9c52f55ffe0e860b1bb4222fe86cce60c05551",
        "url": "https://www.semanticscholar.org/paper/fc9c52f55ffe0e860b1bb4222fe86cce60c05551",
        "title": "Meshed-Memory Transformer for Image Captioning",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2019,
        "citationCount": 1020,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1912.08226",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1912.08226, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3468983",
                "name": "Marcella Cornia"
            },
            {
                "authorId": "2054511289",
                "name": "Matteo Stefanini"
            },
            {
                "authorId": "1843795",
                "name": "L. Baraldi"
            },
            {
                "authorId": "1741922",
                "name": "R. Cucchiara"
            }
        ],
        "abstract": "Transformer-based architectures represent the state of the art in sequence modeling tasks like machine translation and language understanding. Their applicability to multi-modal contexts like image captioning, however, is still largely under-explored. With the aim of filling this gap, we present M - a Meshed Transformer with Memory for Image Captioning. The architecture improves both the image encoding and the language generation steps: it learns a multi-level representation of the relationships between image regions integrating learned a priori knowledge, and uses a mesh-like connectivity at decoding stage to exploit low- and high-level features. Experimentally, we investigate the performance of the M Transformer and different fully-attentive models in comparison with recurrent ones. When tested on COCO, our proposal achieves a new state of the art in single-model and ensemble configurations on the \"Karpathy\" test split and on the online test server. We also assess its performances when describing objects unseen in the training set. Trained models and code for reproducing the experiments are publicly available at: https://github.com/aimagelab/meshed-memory-transformer."
    },
    {
        "paperId": "fd2a0a326db4f034fe22340c20b7bacd9a14c3d6",
        "url": "https://www.semanticscholar.org/paper/fd2a0a326db4f034fe22340c20b7bacd9a14c3d6",
        "title": "Second-Order Attention Network for Single Image Super-Resolution",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2019,
        "citationCount": 1682,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR.2019.01132?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR.2019.01132, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2830263",
                "name": "Tao Dai"
            },
            {
                "authorId": "22275884",
                "name": "Jianrui Cai"
            },
            {
                "authorId": "46867176",
                "name": "Yongbing Zhang"
            },
            {
                "authorId": "3085483",
                "name": "Shutao Xia"
            },
            {
                "authorId": "1452981772",
                "name": "Lei Zhang"
            }
        ],
        "abstract": "Recently, deep convolutional neural networks (CNNs) have been widely explored in single image super-resolution (SISR) and obtained remarkable performance. However, most of the existing CNN-based SISR methods mainly focus on wider or deeper architecture design, neglecting to explore the feature correlations of intermediate layers, hence hindering the representational power of CNNs. To address this issue, in this paper, we propose a second-order attention network (SAN) for more powerful feature expression and feature correlation learning. Specifically, a novel train- able second-order channel attention (SOCA) module is developed to adaptively rescale the channel-wise features by using second-order feature statistics for more discriminative representations. Furthermore, we present a non-locally enhanced residual group (NLRG) structure, which not only incorporates non-local operations to capture long-distance spatial contextual information, but also contains repeated local-source residual attention groups (LSRAG) to learn increasingly abstract feature representations. Experimental results demonstrate the superiority of our SAN network over state-of-the-art SISR methods in terms of both quantitative metrics and visual quality."
    },
    {
        "paperId": "0e23d2f14e7e56e81538f4a63e11689d8ac1eb9d",
        "url": "https://www.semanticscholar.org/paper/0e23d2f14e7e56e81538f4a63e11689d8ac1eb9d",
        "title": "Exploring Simple Siamese Representation Learning",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2020,
        "citationCount": 4646,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2011.10566",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2011.10566, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "39717886",
                "name": "Xinlei Chen"
            },
            {
                "authorId": "39353098",
                "name": "Kaiming He"
            }
        ],
        "abstract": "Siamese networks have become a common structure in various recent models for unsupervised visual representation learning. These models maximize the similarity between two augmentations of one image, subject to certain conditions for avoiding collapsing solutions. In this paper, we report surprising empirical results that simple Siamese networks can learn meaningful representations even using none of the following: (i) negative sample pairs, (ii) large batches, (iii) momentum encoders. Our experiments show that collapsing solutions do exist for the loss and structure, but a stop-gradient operation plays an essential role in preventing collapsing. We provide a hypothesis on the implication of stop-gradient, and further show proof-of-concept experiments verifying it. Our \"SimSiam\" method achieves competitive results on ImageNet and downstream tasks. We hope this simple baseline will motivate people to rethink the roles of Siamese architectures for unsupervised representation learning. Code is made available.1"
    },
    {
        "paperId": "126d36f17cae3e5210a6f62e5c6a23ddec0ef350",
        "url": "https://www.semanticscholar.org/paper/126d36f17cae3e5210a6f62e5c6a23ddec0ef350",
        "title": "Scaled-YOLOv4: Scaling Cross Stage Partial Network",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2020,
        "citationCount": 1351,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2011.08036",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2011.08036, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2423297",
                "name": "Chien-Yao Wang"
            },
            {
                "authorId": "1651204675",
                "name": "Alexey Bochkovskiy"
            },
            {
                "authorId": "1704678",
                "name": "H. Liao"
            }
        ],
        "abstract": "We show that the YOLOv4 object detection neural network based on the CSP approach, scales both up and down and is applicable to small and large networks while maintaining optimal speed and accuracy. We propose a network scaling approach that modifies not only the depth, width, resolution, but also structure of the network. YOLOv4-large model achieves state-of-the-art results: 55.5% AP (73.4% AP50) for the MS COCO dataset at a speed of ~ 16 FPS on Tesla V100, while with the test time augmentation, YOLOv4-large achieves 56.0% AP (73.3 AP50). To the best of our knowledge, this is currently the highest accuracy on the COCO dataset among any published work. The YOLOv4-tiny model achieves 22.0% AP (42.0% AP50) at a speed of ~443 FPS on RTX 2080Ti, while by using TensorRT, batch size = 4 and FP16-precision the YOLOv4-tiny achieves 1774 FPS."
    },
    {
        "paperId": "22d40963e633e1b4af4a9fefda68e1b8dc96ba63",
        "url": "https://www.semanticscholar.org/paper/22d40963e633e1b4af4a9fefda68e1b8dc96ba63",
        "title": "Center-based 3D Object Detection and Tracking",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2020,
        "citationCount": 1992,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2006.11275",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2006.11275, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2068568214",
                "name": "Tianwei Yin"
            },
            {
                "authorId": "1390555183",
                "name": "Xingyi Zhou"
            },
            {
                "authorId": "2562966",
                "name": "Philipp Krhenbhl"
            }
        ],
        "abstract": "Three-dimensional objects are commonly represented as 3D boxes in a point-cloud. This representation mimics the well-studied image-based 2D bounding-box detection but comes with additional challenges. Objects in a 3D world do not follow any particular orientation, and box-based detectors have difficulties enumerating all orientations or fitting an axis-aligned bounding box to rotated objects. In this paper, we instead propose to represent, detect, and track 3D objects as points. Our framework, CenterPoint, first detects centers of objects using a keypoint detector and regresses to other attributes, including 3D size, 3D orientation, and velocity. In a second stage, it refines these estimates using additional point features on the object. In CenterPoint, 3D object tracking simplifies to greedy closest-point matching. The resulting detection and tracking algorithm is simple, efficient, and effective. CenterPoint achieved state-of-the-art performance on the nuScenes benchmark for both 3D detection and tracking, with 65.5 NDS and 63.8 AMOTA for a single model. On the Waymo Open Dataset, Center-Point outperforms all previous single model methods by a large margin and ranks first among all Lidar-only submissions. The code and pretrained models are available at https://github.com/tianweiy/CenterPoint."
    },
    {
        "paperId": "2709167f1c3a03fa5b970a665ea48ed243aab582",
        "url": "https://www.semanticscholar.org/paper/2709167f1c3a03fa5b970a665ea48ed243aab582",
        "title": "Designing Network Design Spaces",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2020,
        "citationCount": 1959,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2003.13678",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2003.13678, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "30407997",
                "name": "Ilija Radosavovic"
            },
            {
                "authorId": "26610238",
                "name": "Raj Prateek Kosaraju"
            },
            {
                "authorId": "2983898",
                "name": "Ross B. Girshick"
            },
            {
                "authorId": "39353098",
                "name": "Kaiming He"
            },
            {
                "authorId": "3127283",
                "name": "Piotr Dollr"
            }
        ],
        "abstract": "In this work, we present a new network design paradigm. Our goal is to help advance the understanding of network design and discover design principles that generalize across settings. Instead of focusing on designing individual network instances, we design network design spaces that parametrize populations of networks. The overall process is analogous to classic manual design of networks, but elevated to the design space level. Using our methodology we explore the structure aspect of network design and arrive at a low-dimensional design space consisting of simple, regular networks that we call RegNet. The core insight of the RegNet parametrization is surprisingly simple: widths and depths of good networks can be explained by a quantized linear function. We analyze the RegNet design space and arrive at interesting findings that do not match the current practice of network design. The RegNet design space provides simple and fast networks that work well across a wide range of flop regimes. Under comparable training settings and flops, the RegNet models outperform the popular EfficientNet models while being up to 5x faster on GPUs."
    },
    {
        "paperId": "4365f51fc270c55005adb794002685078a6fca1d",
        "url": "https://www.semanticscholar.org/paper/4365f51fc270c55005adb794002685078a6fca1d",
        "title": "pixelNeRF: Neural Radiance Fields from One or Few Images",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2020,
        "citationCount": 2021,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2012.02190",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2012.02190, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2024370854",
                "name": "Alex Yu"
            },
            {
                "authorId": "31541718",
                "name": "Vickie Ye"
            },
            {
                "authorId": "7638730",
                "name": "Matthew Tancik"
            },
            {
                "authorId": "20615377",
                "name": "Angjoo Kanazawa"
            }
        ],
        "abstract": "We propose pixelNeRF, a learning framework that predicts a continuous neural scene representation conditioned on one or few input images. The existing approach for constructing neural radiance fields [27] involves optimizing the representation to every scene independently, requiring many calibrated views and significant compute time. We take a step towards resolving these shortcomings by introducing an architecture that conditions a NeRF on image inputs in a fully convolutional manner. This allows the network to be trained across multiple scenes to learn a scene prior, enabling it to perform novel view synthesis in a feed-forward manner from a sparse set of views (as few as one). Leveraging the volume rendering approach of NeRF, our model can be trained directly from images with no explicit 3D supervision. We conduct extensive experiments on ShapeNet benchmarks for single image novel view synthesis tasks with held-out objects as well as entire unseen categories. We further demonstrate the flexibility of pixelNeRF by demonstrating it on multi-object ShapeNet scenes and real scenes from the DTU dataset. In all cases, pixelNeRF outperforms current state-of-the-art baselines for novel view synthesis and single image 3D reconstruction. For the video and code, please visit the project website:https://alexyu.net/pixelnerf."
    },
    {
        "paperId": "47f7ec3d0a5e6e83b6768ece35206a94dc81919c",
        "url": "https://www.semanticscholar.org/paper/47f7ec3d0a5e6e83b6768ece35206a94dc81919c",
        "title": "Taming Transformers for High-Resolution Image Synthesis",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2020,
        "citationCount": 3742,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2012.09841",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2012.09841, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "35175531",
                "name": "Patrick Esser"
            },
            {
                "authorId": "1660819540",
                "name": "Robin Rombach"
            },
            {
                "authorId": "1796707",
                "name": "B. Ommer"
            }
        ],
        "abstract": "Designed to learn long-range interactions on sequential data, transformers continue to show state-of-the-art results on a wide variety of tasks. In contrast to CNNs, they contain no inductive bias that prioritizes local interactions. This makes them expressive, but also computationally infeasible for long sequences, such as high-resolution images. We demonstrate how combining the effectiveness of the inductive bias of CNNs with the expressivity of transformers enables them to model and thereby synthesize high-resolution images. We show how to (i) use CNNs to learn a context-rich vocabulary of image constituents, and in turn (ii) utilize transformers to efficiently model their composition within high-resolution images. Our approach is readily applied to conditional synthesis tasks, where both non-spatial information, such as object classes, and spatial information, such as segmentations, can control the generated image. In particular, we present the first results on semantically-guided synthesis of megapixel images with transformers. Project page at https://git.io/JLlvY."
    },
    {
        "paperId": "4cc32db67ff82cf1aa160631c35bb315c5add749",
        "url": "https://www.semanticscholar.org/paper/4cc32db67ff82cf1aa160631c35bb315c5add749",
        "title": "Encoding in Style: a StyleGAN Encoder for Image-to-Image Translation",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2020,
        "citationCount": 1227,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2008.00951",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2008.00951, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2511847",
                "name": "Elad Richardson"
            },
            {
                "authorId": "1850630812",
                "name": "Yuval Alaluf"
            },
            {
                "authorId": "2819477",
                "name": "Or Patashnik"
            },
            {
                "authorId": "1702992975",
                "name": "Yotam Nitzan"
            },
            {
                "authorId": "1920175",
                "name": "Yaniv Azar"
            },
            {
                "authorId": "151221072",
                "name": "Stav Shapiro"
            },
            {
                "authorId": "1388323541",
                "name": "D. Cohen-Or"
            }
        ],
        "abstract": "We present a generic image-to-image translation framework, pixel2style2pixel (pSp). Our pSp framework is based on a novel encoder network that directly generates a series of style vectors which are fed into a pretrained StyleGAN generator, forming the extended $\\mathcal{W} + $ latent space. We first show that our encoder can directly embed real images into $\\mathcal{W} + $, with no additional optimization. Next, we propose utilizing our encoder to directly solve image-to-image translation tasks, defining them as encoding problems from some input domain into the latent domain. By deviating from the standard \"invert first, edit later\" methodology used with previous StyleGAN encoders, our approach can handle a variety of tasks even when the input image is not represented in the StyleGAN domain. We show that solving translation tasks through StyleGAN significantly simplifies the training process, as no adversary is required, has better support for solving tasks without pixel-to-pixel correspondence, and inherently supports multi-modal synthesis via the resampling of styles. Finally, we demonstrate the potential of our framework on a variety of facial image-to-image translation tasks, even when compared to state-of-the-art solutions designed specifically for a single task, and further show that it can be extended beyond the human facial domain. Code is available at https://github.com/eladrich/pixel2style2pixel."
    },
    {
        "paperId": "5df999a1077524c9178b43c84f80492f8aaa5a7e",
        "url": "https://www.semanticscholar.org/paper/5df999a1077524c9178b43c84f80492f8aaa5a7e",
        "title": "Learned Image Compression With Discretized Gaussian Mixture Likelihoods and Attention Modules",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2020,
        "citationCount": 1099,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2001.01568",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2001.01568, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "71418271",
                "name": "Zhengxue Cheng"
            },
            {
                "authorId": "3294614",
                "name": "Heming Sun"
            },
            {
                "authorId": "153168747",
                "name": "Masaru Takeuchi"
            },
            {
                "authorId": "1788294",
                "name": "J. Katto"
            }
        ],
        "abstract": "Image compression is a fundamental research field and many well-known compression standards have been developed for many decades. Recently, learned compression methods exhibit a fast development trend with promising results. However, there is still a performance gap between learned compression algorithms and reigning compression standards, especially in terms of widely used PSNR metric. In this paper, we explore the remaining redundancy of recent learned compression algorithms. We have found accurate entropy models for rate estimation largely affect the optimization of network parameters and thus affect the rate-distortion performance. Therefore, in this paper, we propose to use discretized Gaussian Mixture Likelihoods to parameterize the distributions of latent codes, which can achieve a more accurate and flexible entropy model. Besides, we take advantage of recent attention modules and incorporate them into network architecture to enhance the performance. Experimental results demonstrate our proposed method achieves a state-of-the-art performance compared to existing learned compression methods on both Kodak and high-resolution datasets. To our knowledge our approach is the first work to achieve comparable performance with latest compression standard Versatile Video Coding (VVC) regarding PSNR. More importantly, our approach generates more visually pleasant results when optimized by MS-SSIM."
    },
    {
        "paperId": "691eddbfaebbc71f6a12d3c99d5c155042459434",
        "url": "https://www.semanticscholar.org/paper/691eddbfaebbc71f6a12d3c99d5c155042459434",
        "title": "NeRF in the Wild: Neural Radiance Fields for Unconstrained Photo Collections",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2020,
        "citationCount": 1712,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2008.02268",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2008.02268, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1401885873",
                "name": "Ricardo Martin-Brualla"
            },
            {
                "authorId": "2066028918",
                "name": "Noha Radwan"
            },
            {
                "authorId": "2283034",
                "name": "Mehdi S. M. Sajjadi"
            },
            {
                "authorId": "50329510",
                "name": "J. Barron"
            },
            {
                "authorId": "2841331",
                "name": "Alexey Dosovitskiy"
            },
            {
                "authorId": "40620532",
                "name": "Daniel Duckworth"
            }
        ],
        "abstract": "We present a learning-based method for synthesizing novel views of complex scenes using only unstructured collections of in-the-wild photographs. We build on Neural Radiance Fields (NeRF), which uses the weights of a multi-layer perceptron to model the density and color of a scene as a function of 3D coordinates. While NeRF works well on images of static subjects captured under controlled settings, it is incapable of modeling many ubiquitous, real-world phenomena in uncontrolled images, such as variable illumination or transient occluders. We introduce a series of extensions to NeRF to address these issues, thereby enabling accurate reconstructions from unstructured image collections taken from the internet. We apply our system, dubbed NeRF-W, to internet photo collections of famous landmarks, and demonstrate temporally consistent novel view renderings that are significantly closer to photorealism than the prior state of the art."
    },
    {
        "paperId": "694bdf6e5906992dad2987a3cc8d1a176de691c9",
        "url": "https://www.semanticscholar.org/paper/694bdf6e5906992dad2987a3cc8d1a176de691c9",
        "title": "D-NeRF: Neural Radiance Fields for Dynamic Scenes",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2020,
        "citationCount": 1756,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2011.13961",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2011.13961, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "49107901",
                "name": "Albert Pumarola"
            },
            {
                "authorId": "3425624",
                "name": "Enric Corona"
            },
            {
                "authorId": "1403428213",
                "name": "Gerard Pons-Moll"
            },
            {
                "authorId": "1397181875",
                "name": "F. Moreno-Noguer"
            }
        ],
        "abstract": "Neural rendering techniques combining machine learning with geometric reasoning have arisen as one of the most promising approaches for synthesizing novel views of a scene from a sparse set of images. Among these, stands out the Neural radiance fields (NeRF) [31], which trains a deep network to map 5D input coordinates (representing spatial location and viewing direction) into a volume density and view-dependent emitted radiance. However, despite achieving an unprecedented level of photorealism on the generated images, NeRF is only applicable to static scenes, where the same spatial location can be queried from different images. In this paper we introduce D-NeRF, a method that extends neural radiance fields to a dynamic domain, allowing to reconstruct and render novel images of objects under rigid and non-rigid motions from a single camera moving around the scene. For this purpose we consider time as an additional input to the system, and split the learning process in two main stages: one that encodes the scene into a canonical space and another that maps this canonical representation into the deformed scene at a particular time. Both mappings are simultaneously learned using fully-connected networks. Once the networks are trained, D-NeRF can render novel images, controlling both the camera view and the time variable, and thus, the object movement. We demonstrate the effectiveness of our approach on scenes with objects under rigid, articulated and non-rigid motions. Code, model weights and the dynamic scenes dataset will be available at [1]."
    },
    {
        "paperId": "6d5f423164cd5ef9324281652987c8a65009e98e",
        "url": "https://www.semanticscholar.org/paper/6d5f423164cd5ef9324281652987c8a65009e98e",
        "title": "Sparse R-CNN: End-to-End Object Detection with Learnable Proposals",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2020,
        "citationCount": 1327,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2011.12450",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2011.12450, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2075416446",
                "name": "Pei Sun"
            },
            {
                "authorId": "1807746211",
                "name": "Rufeng Zhang"
            },
            {
                "authorId": "2146417774",
                "name": "Yi Jiang"
            },
            {
                "authorId": "145868988",
                "name": "Tao Kong"
            },
            {
                "authorId": "1490695028",
                "name": "Chenfeng Xu"
            },
            {
                "authorId": "144267500",
                "name": "Wei Zhan"
            },
            {
                "authorId": "1680165",
                "name": "M. Tomizuka"
            },
            {
                "authorId": "143900005",
                "name": "Lei Li"
            },
            {
                "authorId": "51305314",
                "name": "Zehuan Yuan"
            },
            {
                "authorId": "1906061249",
                "name": "Changhu Wang"
            },
            {
                "authorId": "47571885",
                "name": "Ping Luo"
            }
        ],
        "abstract": "We present Sparse R-CNN, a purely sparse method for object detection in images. Existing works on object detection heavily rely on dense object candidates, such as k anchor boxes pre-defined on all grids of image feature map of size H  W. In our method, however, a fixed sparse set of learned object proposals, total length of N, are provided to object recognition head to perform classification and location. By eliminating HWk (up to hundreds of thousands) hand-designed object candidates to N (e.g. 100) learnable proposals, Sparse R-CNN completely avoids all efforts related to object candidates design and many-to-one label assignment. More importantly, final predictions are directly output without non-maximum suppression post-procedure. Sparse R-CNN demonstrates accuracy, run-time and training convergence performance on par with the well-established detector baselines on the challenging COCO dataset, e.g., achieving 45.0 AP in standard 3 training schedule and running at 22 fps using ResNet-50 FPN model. We hope our work could inspire re-thinking the convention of dense prior in object detectors. The code is available at: https://github.com/PeizeSun/SparseR-CNN."
    },
    {
        "paperId": "6f6f73e69ee0d9d5d7d088bb882db1851d98175a",
        "url": "https://www.semanticscholar.org/paper/6f6f73e69ee0d9d5d7d088bb882db1851d98175a",
        "title": "Pre-Trained Image Processing Transformer",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2020,
        "citationCount": 2018,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2012.00364",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2012.00364, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2118023932",
                "name": "Hanting Chen"
            },
            {
                "authorId": "2108702980",
                "name": "Yunhe Wang"
            },
            {
                "authorId": "50412584",
                "name": "Tianyu Guo"
            },
            {
                "authorId": null,
                "name": "Chang Xu"
            },
            {
                "authorId": "2115656092",
                "name": "Yiping Deng"
            },
            {
                "authorId": "2125024057",
                "name": "Zhenhua Liu"
            },
            {
                "authorId": "2217676532",
                "name": "Siwei Ma"
            },
            {
                "authorId": "1691522",
                "name": "Chunjing Xu"
            },
            {
                "authorId": "2004428678",
                "name": "Chao Xu"
            },
            {
                "authorId": "2153706048",
                "name": "Wen Gao"
            }
        ],
        "abstract": "As the computing power of modern hardware is increasing strongly, pre-trained deep learning models (e.g., BERT, GPT-3) learned on large-scale datasets have shown their effectiveness over conventional methods. The big progress is mainly contributed to the representation ability of transformer and its variant architectures. In this paper, we study the low-level computer vision task (e.g., denoising, super-resolution and deraining) and develop a new pre-trained model, namely, image processing transformer (IPT). To maximally excavate the capability of transformer, we present to utilize the well-known ImageNet benchmark for generating a large amount of corrupted image pairs. The IPT model is trained on these images with multi-heads and multi-tails. In addition, the contrastive learning is introduced for well adapting to different image processing tasks. The pre-trained model can therefore efficiently employed on desired task after fine-tuning. With only one pre-trained model, IPT outperforms the current state-of-the-art methods on various low-level benchmarks. Code is available at https://github.com/huawei-noah/Pretrained-IPT and https://gitee.com/mindspore/mindspore/tree/master/model_zoo/research/cv/IPT"
    },
    {
        "paperId": "908cca0abefc35acc38033603714fbb1bcadc49d",
        "url": "https://www.semanticscholar.org/paper/908cca0abefc35acc38033603714fbb1bcadc49d",
        "title": "X3D: Expanding Architectures for Efficient Video Recognition",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2020,
        "citationCount": 1213,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2004.04730",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2004.04730, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2366173928",
                "name": "Christoph Feichtenhofer"
            }
        ],
        "abstract": "This paper presents X3D, a family of efficient video networks that progressively expand a tiny 2D image classification architecture along multiple network axes, in space, time, width and depth. Inspired by feature selection methods in machine learning, a simple stepwise network expansion approach is employed that expands a single axis in each step, such that good accuracy to complexity trade-off is achieved. To expand X3D to a specific target complexity, we perform progressive forward expansion followed by backward contraction. X3D achieves state-of-the-art performance while requiring 4.8x and 5.5x fewer multiply-adds and parameters for similar accuracy as previous work. Our most surprising finding is that networks with high spatiotemporal resolution can perform well, while being extremely light in terms of network width and parameters. We report competitive accuracy at unprecedented efficiency on video classification and detection benchmarks. Code is available at: https://github.com/facebookresearch/SlowFast."
    },
    {
        "paperId": "9717f7d873859ca0080caf944f7a2d9b456be121",
        "url": "https://www.semanticscholar.org/paper/9717f7d873859ca0080caf944f7a2d9b456be121",
        "title": "AdaBins: Depth Estimation Using Adaptive Bins",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2020,
        "citationCount": 1034,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2011.14141",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2011.14141, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2258961408",
                "name": "Shariq Farooq Bhat"
            },
            {
                "authorId": "2092067",
                "name": "Ibraheem Alhashim"
            },
            {
                "authorId": "1798011",
                "name": "Peter Wonka"
            }
        ],
        "abstract": "We address the problem of estimating a high quality dense depth map from a single RGB input image. We start out with a baseline encoder-decoder convolutional neural network architecture and pose the question of how the global processing of information can help improve overall depth estimation. To this end, we propose a transformer-based architecture block that divides the depth range into bins whose center value is estimated adaptively per image. The final depth values are estimated as linear combinations of the bin centers. We call our new building block AdaBins. Our results show a decisive improvement over the state-of-the-art on several popular depth datasets across all metrics. We also validate the effectiveness of the proposed block with an ablation study and provide the code and corresponding pre-trained weights of the new state-of-the-art model."
    },
    {
        "paperId": "a2a784f1f24bd400b9ef69f7e0cf7071530d4866",
        "url": "https://www.semanticscholar.org/paper/a2a784f1f24bd400b9ef69f7e0cf7071530d4866",
        "title": "Zero-Reference Deep Curve Estimation for Low-Light Image Enhancement",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2020,
        "citationCount": 1799,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2001.06826",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2001.06826, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "18158517",
                "name": "Chunle Guo"
            },
            {
                "authorId": "2698424",
                "name": "Chongyi Li"
            },
            {
                "authorId": "2910007",
                "name": "Jichang Guo"
            },
            {
                "authorId": "1717179",
                "name": "Chen Change Loy"
            },
            {
                "authorId": "40630305",
                "name": "Junhui Hou"
            },
            {
                "authorId": "1687386",
                "name": "S. Kwong"
            },
            {
                "authorId": "3409475",
                "name": "Runmin Cong"
            }
        ],
        "abstract": "The paper presents a novel method, Zero-Reference Deep Curve Estimation (Zero-DCE), which formulates light enhancement as a task of image-specific curve estimation with a deep network. Our method trains a lightweight deep network, DCE-Net, to estimate pixel-wise and high-order curves for dynamic range adjustment of a given image. The curve estimation is specially designed, considering pixel value range, monotonicity, and differentiability. Zero-DCE is appealing in its relaxed assumption on reference images, i.e., it does not require any paired or unpaired data during training. This is achieved through a set of carefully formulated non-reference loss functions, which implicitly measure the enhancement quality and drive the learning of the network. Our method is efficient as image enhancement can be achieved by an intuitive and simple nonlinear curve mapping. Despite its simplicity, we show that it generalizes well to diverse lighting conditions. Extensive experiments on various benchmarks demonstrate the advantages of our method over state-of-the-art methods qualitatively and quantitatively. Furthermore, the potential benefits of our Zero-DCE to face detection in the dark are discussed."
    },
    {
        "paperId": "b963890072ce4a9139c9937266b716bc7a6191f8",
        "url": "https://www.semanticscholar.org/paper/b963890072ce4a9139c9937266b716bc7a6191f8",
        "title": "Simple Copy-Paste is a Strong Data Augmentation Method for Instance Segmentation",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2020,
        "citationCount": 1145,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2012.07177",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2012.07177, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1898210",
                "name": "Golnaz Ghiasi"
            },
            {
                "authorId": "50355189",
                "name": "Yin Cui"
            },
            {
                "authorId": "41207614",
                "name": "A. Srinivas"
            },
            {
                "authorId": null,
                "name": "Rui Qian"
            },
            {
                "authorId": "33493200",
                "name": "Tsung-Yi Lin"
            },
            {
                "authorId": "8132903",
                "name": "E. D. Cubuk"
            },
            {
                "authorId": "2827616",
                "name": "Quoc V. Le"
            },
            {
                "authorId": "2368067",
                "name": "Barret Zoph"
            }
        ],
        "abstract": "Building instance segmentation models that are data-efficient and can handle rare object categories is an important challenge in computer vision. Leveraging data augmentations is a promising direction towards addressing this challenge. Here, we perform a systematic study of the Copy-Paste augmentation (e.g., [13], [12]) for instance segmentation where we randomly paste objects onto an image. Prior studies on Copy-Paste relied on modeling the surrounding visual context for pasting the objects. However, we find that the simple mechanism of pasting objects randomly is good enough and can provide solid gains on top of strong baselines. Furthermore, we show Copy-Paste is additive with semi-supervised methods that leverage extra data through pseudo labeling (e.g. self-training). On COCO instance segmentation, we achieve 49.1 mask AP and 57.3 box AP, an improvement of +0.6 mask AP and +1.5 box AP over the previous state-of-the-art. We further demonstrate that Copy-Paste can lead to significant improvements on the LVIS benchmark. Our baseline model outperforms the LVIS 2020 Challenge winning entry by +3.6 mask AP on rare categories.1"
    },
    {
        "paperId": "bb8656979f38d95062ba55640c1be65535f57c6a",
        "url": "https://www.semanticscholar.org/paper/bb8656979f38d95062ba55640c1be65535f57c6a",
        "title": "GIRAFFE: Representing Scenes as Compositional Generative Neural Feature Fields",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2020,
        "citationCount": 1031,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2011.12100",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2011.12100, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "145048708",
                "name": "Michael Niemeyer"
            },
            {
                "authorId": "47237027",
                "name": "Andreas Geiger"
            }
        ],
        "abstract": "Deep generative models allow for photorealistic image synthesis at high resolutions. But for many applications, this is not enough: content creation also needs to be controllable. While several recent works investigate how to disentangle underlying factors of variation in the data, most of them operate in 2D and hence ignore that our world is three-dimensional. Further, only few works consider the compositional nature of scenes. Our key hypothesis is that incorporating a compositional 3D scene representation into the generative model leads to more controllable image synthesis. Representing scenes as compositional generative neural feature fields allows us to disentangle one or multiple objects from the background as well as individual objects shapes and appearances while learning from unstructured and unposed image collections without any additional supervision. Combining this scene representation with a neural rendering pipeline yields a fast and realistic image synthesis model. As evidenced by our experiments, our model is able to disentangle individual objects and allows for translating and rotating them in the scene as well as changing the camera pose."
    },
    {
        "paperId": "c31c55db4d962527fd2e1366d173ba3f9a5fb0ed",
        "url": "https://www.semanticscholar.org/paper/c31c55db4d962527fd2e1366d173ba3f9a5fb0ed",
        "title": "3DSSD: Point-Based 3D Single Stage Object Detector",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2020,
        "citationCount": 1094,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2002.10187",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2002.10187, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2109711592",
                "name": "Zetong Yang"
            },
            {
                "authorId": "89359468",
                "name": "Yanan Sun"
            },
            {
                "authorId": "25059098",
                "name": "Shu Liu"
            },
            {
                "authorId": "1729056",
                "name": "Jiaya Jia"
            }
        ],
        "abstract": "Prevalence of voxel-based 3D single-stage detectors contrast with underexplored point-based methods. In this paper, we present a lightweight point-based 3D single stage object detector 3DSSD to achieve decent balance of accuracy and efficiency. In this paradigm, all upsampling layers and the refinement stage, which are indispensable in all existing point-based methods, are abandoned. We instead propose a fusion sampling strategy in downsampling process to make detection on less representative points feasible. A delicate box prediction network, including a candidate generation layer and an anchor-free regression head with a 3D center-ness assignment strategy, is developed to meet the demand of high accuracy and speed. Our 3DSSD paradigm is an elegant single-stage anchor-free one. We evaluate it on widely used KITTI dataset and more challenging nuScenes dataset. Our method outperforms all state-of-the-art voxel-based single-stage methods by a large margin, and even yields comparable performance with two-stage point-based methods, with amazing inference speed of 25+ FPS, 2x faster than former state-of-the-art point-based methods."
    },
    {
        "paperId": "d29430adccb805ab57b349afa8553954347b3197",
        "url": "https://www.semanticscholar.org/paper/d29430adccb805ab57b349afa8553954347b3197",
        "title": "Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2020,
        "citationCount": 3381,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2012.15840",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2012.15840, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "40895011",
                "name": "Sixiao Zheng"
            },
            {
                "authorId": "31727033",
                "name": "Jiachen Lu"
            },
            {
                "authorId": "3459894",
                "name": "Hengshuang Zhao"
            },
            {
                "authorId": "2171228",
                "name": "Xiatian Zhu"
            },
            {
                "authorId": "79454234",
                "name": "Zekun Luo"
            },
            {
                "authorId": "2628601",
                "name": "Yabiao Wang"
            },
            {
                "authorId": "35782003",
                "name": "Yanwei Fu"
            },
            {
                "authorId": "1384556269",
                "name": "Jianfeng Feng"
            },
            {
                "authorId": "145406421",
                "name": "T. Xiang"
            },
            {
                "authorId": "143635540",
                "name": "Philip H. S. Torr"
            },
            {
                "authorId": "48459110",
                "name": "Li Zhang"
            }
        ],
        "abstract": "Most recent semantic segmentation methods adopt a fully-convolutional network (FCN) with an encoder-decoder architecture. The encoder progressively reduces the spatial resolution and learns more abstract/semantic visual concepts with larger receptive fields. Since context modeling is critical for segmentation, the latest efforts have been focused on increasing the receptive field, through either dilated/atrous convolutions or inserting attention modules. However, the encoder-decoder based FCN architecture remains unchanged. In this paper, we aim to provide an alternative perspective by treating semantic segmentation as a sequence-to-sequence prediction task. Specifically, we deploy a pure transformer (i.e., without convolution and resolution reduction) to encode an image as a sequence of patches. With the global context modeled in every layer of the transformer, this encoder can be combined with a simple decoder to provide a powerful segmentation model, termed SEgmentation TRansformer (SETR). Extensive experiments show that SETR achieves new state of the art on ADE20K (50.28% mIoU), Pascal Context (55.83% mIoU) and competitive results on Cityscapes. Particularly, we achieve the first position in the highly competitive ADE20K test server leaderboard on the day of submission."
    },
    {
        "paperId": "f15aecec2a5672beaae77853ea0eea560505df8e",
        "url": "https://www.semanticscholar.org/paper/f15aecec2a5672beaae77853ea0eea560505df8e",
        "title": "FDA: Fourier Domain Adaptation for Semantic Segmentation",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2020,
        "citationCount": 1029,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2004.05498",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2004.05498, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2881401",
                "name": "Yanchao Yang"
            },
            {
                "authorId": "1715959",
                "name": "Stefano Soatto"
            }
        ],
        "abstract": "We describe a simple method for unsupervised domain adaptation, whereby the discrepancy between the source and target distributions is reduced by swapping the low-frequency spectrum of one with the other. We illustrate the method in semantic segmentation, where densely annotated images are aplenty in one domain (synthetic data), but difficult to obtain in another (real images). Current state-of-the-art methods are complex, some requiring adversarial optimization to render the backbone of a neural network invariant to the discrete domain selection variable. Our method does not require any training to perform the domain alignment, just a simple Fourier Transform and its inverse. Despite its simplicity, it achieves state-of-the-art performance in the current benchmarks, when integrated into a relatively standard semantic segmentation model. Our results indicate that even simple procedures can discount nuisance variability in the data that more sophisticated methods struggle to learn away."
    },
    {
        "paperId": "16f2d2f2b8103ed0c4a4e6f339a21247e58c5e78",
        "url": "https://www.semanticscholar.org/paper/16f2d2f2b8103ed0c4a4e6f339a21247e58c5e78",
        "title": "Bottleneck Transformers for Visual Recognition",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2021,
        "citationCount": 1118,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2101.11605",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2101.11605, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "41207614",
                "name": "A. Srinivas"
            },
            {
                "authorId": "33493200",
                "name": "Tsung-Yi Lin"
            },
            {
                "authorId": "3877127",
                "name": "Niki Parmar"
            },
            {
                "authorId": "1789737",
                "name": "Jonathon Shlens"
            },
            {
                "authorId": "1689992",
                "name": "P. Abbeel"
            },
            {
                "authorId": "1630664874",
                "name": "Ashish Vaswani"
            }
        ],
        "abstract": "We present BoTNet, a conceptually simple yet powerful backbone architecture that incorporates self-attention for multiple computer vision tasks including image classification, object detection and instance segmentation. By just replacing the spatial convolutions with global self-attention in the final three bottleneck blocks of a ResNet and no other changes, our approach improves upon the baselines significantly on instance segmentation and object detection while also reducing the parameters, with minimal overhead in latency. Through the design of BoTNet, we also point out how ResNet bottleneck blocks with self-attention can be viewed as Transformer blocks. Without any bells and whistles, BoTNet achieves 44.4% Mask AP and 49.7% Box AP on the COCO Instance Segmentation benchmark using the Mask R-CNN framework; surpassing the previous best published single model and single scale results of ResNeSt [67] evaluated on the COCO validation set. Finally, we present a simple adaptation of the BoTNet design for image classification, resulting in models that achieve a strong performance of 84.7% top-1 accuracy on the ImageNet benchmark while being up to 1.64x faster in \"compute\"1 time than the popular EfficientNet models on TPU-v3 hardware. We hope our simple and effective approach will serve as a strong baseline for future research in self-attention models for vision.2"
    },
    {
        "paperId": "1e88d5afe19aea324d33541f60a90b7036894c32",
        "url": "https://www.semanticscholar.org/paper/1e88d5afe19aea324d33541f60a90b7036894c32",
        "title": "Restormer: Efficient Transformer for High-Resolution Image Restoration",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2021,
        "citationCount": 3183,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2111.09881",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2111.09881, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3323621",
                "name": "Syed Waqas Zamir"
            },
            {
                "authorId": "153150198",
                "name": "Aditya Arora"
            },
            {
                "authorId": "152973423",
                "name": "Salman Hameed Khan"
            },
            {
                "authorId": "145684318",
                "name": "Munawar Hayat"
            },
            {
                "authorId": "2358803",
                "name": "F. Khan"
            },
            {
                "authorId": "37144787",
                "name": "Ming-Hsuan Yang"
            }
        ],
        "abstract": "Since convolutional neural networks (CNNs) perform well at learning generalizable image priors from large-scale data, these models have been extensively applied to image restoration and related tasks. Recently, another class of neural architectures, Transformers, have shown significant performance gains on natural language and high-level vision tasks. While the Transformer model mitigates the shortcomings of CNNs (i.e., limited receptive field and inadaptability to input content), its computational complexity grows quadratically with the spatial resolution, therefore making it infeasible to apply to most image restoration tasks involving high-resolution images. In this work, we propose an efficient Transformer model by making several key designs in the building blocks (multi-head attention and feed-forward network) such that it can capture long-range pixel interactions, while still remaining applicable to large images. Our model, named Restoration Transformer (Restormer), achieves state-of-the-art results on several image restoration tasks, including image deraining, single-image motion deblurring, defocus deblurring (single-image and dual-pixel data), and image denoising (Gaussian grayscale/color denoising, and real image denoising). The source code and pre-trained models are available at https://github.com/swz30/Restormer."
    },
    {
        "paperId": "23ad8fc48530ce366f8192dfb48d0f7df1dba277",
        "url": "https://www.semanticscholar.org/paper/23ad8fc48530ce366f8192dfb48d0f7df1dba277",
        "title": "Towards Total Recall in Industrial Anomaly Detection",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2021,
        "citationCount": 1280,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2106.08265",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2106.08265, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "77340962",
                "name": "Karsten Roth"
            },
            {
                "authorId": "3407972",
                "name": "Latha Pemula"
            },
            {
                "authorId": "144522839",
                "name": "J. Zepeda"
            },
            {
                "authorId": "1707625",
                "name": "B. Scholkopf"
            },
            {
                "authorId": "1710872",
                "name": "T. Brox"
            },
            {
                "authorId": "2871555",
                "name": "Peter Gehler"
            }
        ],
        "abstract": "Being able to spot defective parts is a critical component in large-scale industrial manufacturing. A particular challenge that we address in this work is the cold-start problem: fit a model using nominal (non-defective) example images only. While handcrafted solutions per class are possible, the goal is to build systems that work well simultaneously on many different tasks automatically. The best peforming approaches combine embeddings from ImageNet models with an outlier detection model. In this paper, we extend on this line of work and propose PatchCore, which uses a maximally representative memory bank of nominal patch-features. PatchCore offers competitive inference times while achieving state-of-the-art performance for both detection and localization. On the challenging, widely used MVTec AD benchmark PatchCore achieves an image-level anomaly detection AUROC score of up to 99.6%, more than halving the error compared to the next best competitor. We further report competitive results on two additional datasets and also find competitive results in the few samples regime. Code: github.com/amazon-research/patchcore-inspection."
    },
    {
        "paperId": "2835951fabf12804e17d5a525b2be2bee70e7910",
        "url": "https://www.semanticscholar.org/paper/2835951fabf12804e17d5a525b2be2bee70e7910",
        "title": "Uformer: A General U-Shaped Transformer for Image Restoration",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2021,
        "citationCount": 1854,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2106.03106",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2106.03106, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2108160372",
                "name": "Zhendong Wang"
            },
            {
                "authorId": "30176430",
                "name": "Xiaodong Cun"
            },
            {
                "authorId": "9324504",
                "name": "Jianmin Bao"
            },
            {
                "authorId": "2144167531",
                "name": "Jianzhuang Liu"
            }
        ],
        "abstract": "In this paper, we present Uformer, an effective and efficient Transformer-based architecture for image restoration, in which we build a hierarchical encoder-decoder network using the Transformer block. In Uformer, there are two core designs. First, we introduce a novel locally-enhanced window (LeWin) Transformer block, which performs non-overlapping window-based self-attention instead of global self-attention. It significantly reduces the computational complexity on high resolution feature map while capturing local context. Second, we propose a learnable multi-scale restoration modulator in the form of a multi-scale spatial bias to adjust features in multiple layers of the Uformer decoder. Our modulator demonstrates superior capability for restoring details for various image restoration tasks while introducing marginal extra parameters and computational cost. Powered by these two designs, Uformer enjoys a high capability for capturing both local and global dependencies for image restoration. To evaluate our approach, extensive experiments are conducted on several image restoration tasks, including image denoising, motion deblurring, defocus deblurring and deraining. Without bells and whistles, our Uformer achieves superior or comparable performance compared with the state-of-the-art algorithms. The code and models are available at https://github.com/ZhendongWang6/Uformer."
    },
    {
        "paperId": "2a805d0e1b067444a554c5169d189fa1f649f411",
        "url": "https://www.semanticscholar.org/paper/2a805d0e1b067444a554c5169d189fa1f649f411",
        "title": "Scaling Vision Transformers",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2021,
        "citationCount": 1303,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2106.04560",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2106.04560, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2743563",
                "name": "Xiaohua Zhai"
            },
            {
                "authorId": "144629422",
                "name": "Alexander Kolesnikov"
            },
            {
                "authorId": "2815290",
                "name": "N. Houlsby"
            },
            {
                "authorId": "39611591",
                "name": "Lucas Beyer"
            }
        ],
        "abstract": "Attention-based neural networks such as the Vision Transformer (ViT) have recently attained state-of-the-art results on many computer vision benchmarks. Scale is a primary ingredient in attaining excellent results, therefore, understanding a model's scaling properties is a key to designing future generations effectively. While the laws for scaling Transformer language models have been studied, it is unknown how Vision Transformers scale. To address this, we scale ViT models and data, both up and down, and characterize the relationships between error rate, data, and compute. Along the way, we refine the architecture and training of ViT, reducing memory consumption and increasing accuracy of the resulting models. As a result, we successfully train a ViT model with two billion parameters, which attains a new state-of-the-art on ImageNet of 90.45% top-1 accuracy. The model also performs well for few-shot transfer, for example, reaching 84.86% top-1 accuracy on ImageNet with only 10 examples per class."
    },
    {
        "paperId": "2b8088253e2378fce001a090fe923b81e8dedf25",
        "url": "https://www.semanticscholar.org/paper/2b8088253e2378fce001a090fe923b81e8dedf25",
        "title": "RepVGG: Making VGG-style ConvNets Great Again",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2021,
        "citationCount": 2061,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2101.03697",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2101.03697, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "4827513",
                "name": "Xiaohan Ding"
            },
            {
                "authorId": "1771551",
                "name": "X. Zhang"
            },
            {
                "authorId": "2068605434",
                "name": "Ningning Ma"
            },
            {
                "authorId": "151481622",
                "name": "Jungong Han"
            },
            {
                "authorId": "38329336",
                "name": "Guiguang Ding"
            },
            {
                "authorId": "2032184078",
                "name": "Jian Sun"
            }
        ],
        "abstract": "We present a simple but powerful architecture of convolutional neural network, which has a VGG-like inference-time body composed of nothing but a stack of 3  3 convolution and ReLU, while the training-time model has a multi-branch topology. Such decoupling of the training-time and inference-time architecture is realized by a structural re-parameterization technique so that the model is named RepVGG. On ImageNet, RepVGG reaches over 80% top-1 accuracy, which is the first time for a plain model, to the best of our knowledge. On NVIDIA 1080Ti GPU, RepVGG models run 83% faster than ResNet-50 or 101% faster than ResNet-101 with higher accuracy and show favorable accuracy-speed trade-off compared to the state-of-the-art models like EfficientNet and RegNet. The code and trained models are available at https://github.com/megvii-model/RepVGG."
    },
    {
        "paperId": "394be105b87e9bfe72c20efe6338de10604e1a11",
        "url": "https://www.semanticscholar.org/paper/394be105b87e9bfe72c20efe6338de10604e1a11",
        "title": "Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2021,
        "citationCount": 1348,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2102.08981",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2102.08981, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2059199",
                "name": "Soravit Changpinyo"
            },
            {
                "authorId": "46756891",
                "name": "P. Sharma"
            },
            {
                "authorId": "2066767241",
                "name": "Nan Ding"
            },
            {
                "authorId": "1737285",
                "name": "Radu Soricut"
            }
        ],
        "abstract": "The availability of large-scale image captioning and visual question answering datasets has contributed significantly to recent successes in vision-and-language pretraining. However, these datasets are often collected with overrestrictive requirements inherited from their original target tasks (e.g., image caption generation), which limit the resulting dataset scale and diversity. We take a step further in pushing the limits of vision-and-language pretraining data by relaxing the data collection pipeline used in Conceptual Captions 3M (CC3M) [54] and introduce the Conceptual 12M (CC12M), a dataset with 12 million image-text pairs specifically meant to be used for visionand-language pre-training. We perform an analysis of this dataset and benchmark its effectiveness against CC3M on multiple downstream tasks with an emphasis on long-tail visual recognition. Our results clearly illustrate the benefit of scaling up pre-training data for vision-and-language tasks, as indicated by the new state-of-the-art results on both the nocaps and Conceptual Captions benchmarks.1"
    },
    {
        "paperId": "4f7eb65f8d3c1eeb97e30f7ac68977ff16e1e942",
        "url": "https://www.semanticscholar.org/paper/4f7eb65f8d3c1eeb97e30f7ac68977ff16e1e942",
        "title": "Direct Voxel Grid Optimization: Super-fast Convergence for Radiance Fields Reconstruction",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2021,
        "citationCount": 1256,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2111.11215",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2111.11215, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2118830989",
                "name": "Cheng Sun"
            },
            {
                "authorId": "2152349515",
                "name": "Min Sun"
            },
            {
                "authorId": "1803730",
                "name": "Hwann-Tzong Chen"
            }
        ],
        "abstract": "We present a super-fast convergence approach to reconstructing the per-scene radiance field from a set of images that capture the scene with known poses. This task, which is often applied to novel view synthesis, is recently revolution-ized by Neural Radiance Field (NeRF) for its state-of-the-art quality and fiexibility. However, NeRF and its variants require a lengthy training time ranging from hours to days for a single scene. In contrast, our approach achieves NeRF-comparable quality and converges rapidly from scratch in less than 15 minutes with a single GPU. We adopt a representation consisting of a density voxel grid for scene geometry and a feature voxel grid with a shallow network for complex view-dependent appearance. Modeling with explicit and discretized volume representations is not new, but we propose two simple yet non-trivial techniques that contribute to fast convergence speed and high-quality output. First, we introduce the post-activation interpolation on voxel density, which is capable of producing sharp surfaces in lower grid resolution. Second, direct voxel density optimization is prone to suboptimal geometry solutions, so we robustify the optimization process by imposing several priors. Finally, evaluation on five inward-facing benchmarks shows that our method matches, if not surpasses, NeRF's quality, yet it only takes about 15 minutes to train from scratch for a new scene. Code: https://github.com/sunset1995/DirectVoxGO."
    },
    {
        "paperId": "5341b412383c43f4a693ad63ec4489e3ec7688c8",
        "url": "https://www.semanticscholar.org/paper/5341b412383c43f4a693ad63ec4489e3ec7688c8",
        "title": "Grounded Language-Image Pre-training",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2021,
        "citationCount": 1370,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2112.03857",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2112.03857, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "32562635",
                "name": "Liunian Harold Li"
            },
            {
                "authorId": "9325940",
                "name": "Pengchuan Zhang"
            },
            {
                "authorId": "49724177",
                "name": "Haotian Zhang"
            },
            {
                "authorId": "120157163",
                "name": "Jianwei Yang"
            },
            {
                "authorId": "2109737569",
                "name": "Chunyuan Li"
            },
            {
                "authorId": "1828787912",
                "name": "Yiwu Zhong"
            },
            {
                "authorId": "29957038",
                "name": "Lijuan Wang"
            },
            {
                "authorId": "145347147",
                "name": "Lu Yuan"
            },
            {
                "authorId": "2152828578",
                "name": "Lei Zhang"
            },
            {
                "authorId": "145159381",
                "name": "Jenq-Neng Hwang"
            },
            {
                "authorId": "2782886",
                "name": "Kai-Wei Chang"
            },
            {
                "authorId": "48441311",
                "name": "Jianfeng Gao"
            }
        ],
        "abstract": "This paper presents a grounded language-image pretraining (GLIP) model for learning object-level, language-aware, and semantic-rich visual representations. GLIP unifies object detection and phrase grounding for pre-training. The unification brings two benefits: 1) it allows GLIP to learn from both detection and grounding data to improve both tasks and bootstrap a good grounding model; 2) GLIP can leverage massive image-text pairs by generating grounding boxes in a self-training fashion, making the learned representations semantic-rich. In our experiments, we pre-train GLIP on 27M grounding data, including 3M human-annotated and 24M web-crawled image-text pairs. The learned representations demonstrate strong zero-shot and few-shot transferability to various object-level recognition tasks. 1) When directly evaluated on COCO and LVIS (without seeing any images in COCO during pre-training), GLIP achieves 49.8 AP and 26.9 AP, respectively, surpassing many supervised baselines.11Supervised baselines on COCO object detection: Faster-RCNN w/ ResNet50 (40.2) or ResNet101 (42.0), and DyHead w/ Swin-Tiny (49.7). 2) After fine-tuned on COCO, GLIP achieves 60.8 AP on val and 61.5 AP on test-dev, surpassing prior SoTA. 3) When transferred to 13 downstream object detection tasks, a 1-shot GLIP rivals with a fully-supervised Dynamic Head. Code will be released at https://github.com/microsoft/GLIP."
    },
    {
        "paperId": "57150ca7d793d6f784cf82da1c349edf7beb6bc2",
        "url": "https://www.semanticscholar.org/paper/57150ca7d793d6f784cf82da1c349edf7beb6bc2",
        "title": "MetaFormer is Actually What You Need for Vision",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2021,
        "citationCount": 1175,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2111.11418",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2111.11418, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "23476952",
                "name": "Weihao Yu"
            },
            {
                "authorId": "2109495772",
                "name": "Romy Mi Luo"
            },
            {
                "authorId": "2109890021",
                "name": "Pan Zhou"
            },
            {
                "authorId": "39927579",
                "name": "Chenyang Si"
            },
            {
                "authorId": "46432827",
                "name": "Yichen Zhou"
            },
            {
                "authorId": "48631088",
                "name": "Xinchao Wang"
            },
            {
                "authorId": "1698982",
                "name": "Jiashi Feng"
            },
            {
                "authorId": "143653681",
                "name": "Shuicheng Yan"
            }
        ],
        "abstract": "Transformers have shown great potential in computer vision tasks. A common belief is their attention-based token mixer module contributes most to their competence. However, recent works show the attention-based module in transformers can be replaced by spatial MLPs and the resulted models still perform quite well. Based on this observation, we hypothesize that the general architecture of the transformers, instead of the specific token mixer module, is more essential to the model's performance. To verify this, we deliberately replace the attention module in transformers with an embarrassingly simple spatial pooling operator to conduct only basic token mixing. Surprisingly, we observe that the derived model, termed as PoolFormer, achieves competitive performance on multiple computer vision tasks. For example, on ImageNet-1K, PoolFormer achieves 82.1 % top-1 accuracy, surpassing well-tuned vision transformer/MLP-like baselines DeiT-B/ResMLP-B24 by 0.3%/1.1% accuracy with 35%/52% fewer parameters and 49%/61% fewer MACs. The effectiveness of Pool-Former verifies our hypothesis and urges us to initiate the concept of MetaFormer, a general architecture abstracted from transformers without specifying the token mixer. Based on the extensive experiments, we argue that MetaFormer is the key player in achieving superior results for recent transformer and MLP-like models on vision tasks. This work calls for more future research dedicated to improving MetaFormer instead of focusing on the token mixer modules. Additionally, our proposed PoolFormer could serve as a starting baseline for future MetaFormer architecture design."
    },
    {
        "paperId": "6351ebb4a3287f5f3e1273464b3b91e5df5a16d7",
        "url": "https://www.semanticscholar.org/paper/6351ebb4a3287f5f3e1273464b3b91e5df5a16d7",
        "title": "Masked Autoencoders Are Scalable Vision Learners",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2021,
        "citationCount": 9957,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2111.06377",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2111.06377, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2058350112",
                "name": "Kaiming He"
            },
            {
                "authorId": "39717886",
                "name": "Xinlei Chen"
            },
            {
                "authorId": "1817030",
                "name": "Saining Xie"
            },
            {
                "authorId": "2366569281",
                "name": "Yanghao Li"
            },
            {
                "authorId": "2065731243",
                "name": "Piotr Doll'ar"
            },
            {
                "authorId": "2983898",
                "name": "Ross B. Girshick"
            }
        ],
        "abstract": "This paper shows that masked autoencoders (MAE) are scalable self-supervised learners for computer vision. Our MAE approach is simple: we mask random patches of the input image and reconstruct the missing pixels. It is based on two core designs. First, we develop an asymmetric encoder-decoder architecture, with an encoder that operates only on the visible subset of patches (without mask tokens), along with a lightweight decoder that reconstructs the original image from the latent representation and mask tokens. Second, we find that masking a high proportion of the input image, e.g., 75%, yields a nontrivial and meaningful self-supervisory task. Coupling these two designs enables us to train large models efficiently and effectively: we accelerate training (by 3 or more) and improve accuracy. Our scalable approach allows for learning high-capacity models that generalize well: e.g., a vanilla ViT-Huge model achieves the best accuracy (87.8%) among methods that use only ImageNet-1K data. Transfer performance in downstream tasks outperforms supervised pretraining and shows promising scaling behavior."
    },
    {
        "paperId": "63c74d15940af1af9b386b5762e4445e54c73719",
        "url": "https://www.semanticscholar.org/paper/63c74d15940af1af9b386b5762e4445e54c73719",
        "title": "VinVL: Revisiting Visual Representations in Vision-Language Models",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2021,
        "citationCount": 1025,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2101.00529",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR46437.2021.00553?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR46437.2021.00553, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "9325940",
                "name": "Pengchuan Zhang"
            },
            {
                "authorId": "47058148",
                "name": "Xiujun Li"
            },
            {
                "authorId": "2148941781",
                "name": "Xiaowei Hu"
            },
            {
                "authorId": "120157163",
                "name": "Jianwei Yang"
            },
            {
                "authorId": "2152828578",
                "name": "Lei Zhang"
            },
            {
                "authorId": "29957038",
                "name": "Lijuan Wang"
            },
            {
                "authorId": "1699545",
                "name": "Yejin Choi"
            },
            {
                "authorId": "48441311",
                "name": "Jianfeng Gao"
            }
        ],
        "abstract": "This paper presents a detailed study of improving visual representations for vision language (VL) tasks and develops an improved object detection model to provide object-centric representations of images. Compared to the most widely used bottom-up and top-down model [2], the new model is bigger, better-designed for VL tasks, and pre-trained on much larger training corpora that combine multiple public annotated object detection datasets. Therefore, it can generate representations of a richer collection of visual objects and concepts. While previous VL research focuses mainly on improving the vision-language fusion model and leaves the object detection model improvement untouched, we show that visual features matter significantly in VL models. In our experiments we feed the visual features generated by the new object detection model into a Transformer-based VL fusion model OSCAR [20], and utilize an improved approach OSCAR+ to pre-train the VL model and fine-tune it on a wide range of downstream VL tasks. Our results show that the new visual features significantly improve the performance across all VL tasks, creating new state-of-the-art results on seven public benchmarks. Code, models and pre-extracted features are released at https://github.com/pzzhang/VinVL."
    },
    {
        "paperId": "658a017302d29e4acf4ca789cb5d9f27983717ff",
        "url": "https://www.semanticscholar.org/paper/658a017302d29e4acf4ca789cb5d9f27983717ff",
        "title": "Masked-attention Mask Transformer for Universal Image Segmentation",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2021,
        "citationCount": 3252,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2112.01527",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2112.01527, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "50563570",
                "name": "Bowen Cheng"
            },
            {
                "authorId": "1806773",
                "name": "Ishan Misra"
            },
            {
                "authorId": "2068227",
                "name": "A. Schwing"
            },
            {
                "authorId": "144843400",
                "name": "Alexander Kirillov"
            },
            {
                "authorId": "3102850",
                "name": "Rohit Girdhar"
            }
        ],
        "abstract": "Image segmentation groups pixels with different semantics, e.g., category or instance membership. Each choice of semantics defines a task. While only the semantics of each task differ, current research focuses on designing spe-cialized architectures for each task. We present Masked- attention Mask Transformer (Mask2Former), a new archi-tecture capable of addressing any image segmentation task (panoptic, instance or semantic). Its key components in-clude masked attention, which extracts localized features by constraining cross-attention within predicted mask regions. In addition to reducing the research effort by at least three times, it outperforms the best specialized architectures by a significant margin on four popular datasets. Most no-tably, Mask2Former sets a new state-of-the-art for panoptic segmentation (57.8 PQ on COCO), instance segmentation (50.1 AP on COCO) and semantic segmentation (57.7 mIoU onADE20K)."
    },
    {
        "paperId": "70cf7c785952375e8061c92235aa20e94b02ecd4",
        "url": "https://www.semanticscholar.org/paper/70cf7c785952375e8061c92235aa20e94b02ecd4",
        "title": "Coordinate Attention for Efficient Mobile Network Design",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2021,
        "citationCount": 4173,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2103.02907",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2103.02907, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3298532",
                "name": "Qibin Hou"
            },
            {
                "authorId": "18119920",
                "name": "Daquan Zhou"
            },
            {
                "authorId": "33221685",
                "name": "Jiashi Feng"
            }
        ],
        "abstract": "Recent studies on mobile network design have demonstrated the remarkable effectiveness of channel attention (e.g., the Squeeze-and-Excitation attention) for lifting model performance, but they generally neglect the positional information, which is important for generating spatially selective attention maps. In this paper, we propose a novel attention mechanism for mobile networks by embedding positional information into channel attention, which we call \"coordinate attention\". Unlike channel attention that transforms a feature tensor to a single feature vector via 2D global pooling, the coordinate attention factorizes channel attention into two 1D feature encoding processes that aggregate features along the two spatial directions, respectively. In this way, long-range dependencies can be captured along one spatial direction and meanwhile precise positional information can be preserved along the other spatial direction. The resulting feature maps are then encoded separately into a pair of direction-aware and position-sensitive attention maps that can be complementarily applied to the input feature map to augment the representations of the objects of interest. Our coordinate attention is simple and can be flexibly plugged into classic mobile networks, such as MobileNetV2, MobileNeXt, and EfficientNet with nearly no computational overhead. Extensive experiments demonstrate that our coordinate attention is not only beneficial to ImageNet classification but more interestingly, behaves better in down-stream tasks, such as object detection and semantic segmentation. Code is available at https://github.com/Andrew-Qibin/CoordAttention."
    },
    {
        "paperId": "748c4f1945b0d994ecef38c8aac01db1c6dc7029",
        "url": "https://www.semanticscholar.org/paper/748c4f1945b0d994ecef38c8aac01db1c6dc7029",
        "title": "Model-Contrastive Federated Learning",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2021,
        "citationCount": 1409,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2103.16257",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2103.16257, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2108107022",
                "name": "Qinbin Li"
            },
            {
                "authorId": "143824511",
                "name": "Bingsheng He"
            },
            {
                "authorId": "143711382",
                "name": "D. Song"
            }
        ],
        "abstract": "Federated learning enables multiple parties to collaboratively train a machine learning model without communicating their local data. A key challenge in federated learning is to handle the heterogeneity of local data distribution across parties. Although many studies have been proposed to address this challenge, we find that they fail to achieve high performance in image datasets with deep learning models. In this paper, we propose MOON: model-contrastive federated learning. MOON is a simple and effective federated learning framework. The key idea of MOON is to utilize the similarity between model representations to correct the local training of individual parties, i.e., conducting contrastive learning in model-level. Our extensive experiments show that MOON significantly outperforms the other state-of-the-art federated learning algorithms on various image classification tasks."
    },
    {
        "paperId": "78d80c343d36baaf89f18e12d325cf6309fb6c8f",
        "url": "https://www.semanticscholar.org/paper/78d80c343d36baaf89f18e12d325cf6309fb6c8f",
        "title": "CutPaste: Self-Supervised Learning for Anomaly Detection and Localization",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2021,
        "citationCount": 1011,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2104.04015",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2104.04015, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2109497736",
                "name": "Chun-Liang Li"
            },
            {
                "authorId": "1729571",
                "name": "Kihyuk Sohn"
            },
            {
                "authorId": "2144029",
                "name": "Jinsung Yoon"
            },
            {
                "authorId": "1945962",
                "name": "Tomas Pfister"
            }
        ],
        "abstract": "We aim at constructing a high performance model for defect detection that detects unknown anomalous patterns of an image without anomalous data. To this end, we propose a two-stage framework for building anomaly detectors using normal training data only. We first learn self-supervised deep representations and then build a generative one-class classifier on learned representations. We learn representations by classifying normal data from the CutPaste, a simple data augmentation strategy that cuts an image patch and pastes at a random location of a large image. Our empirical study on MVTec anomaly detection dataset demonstrates the proposed algorithm is general to be able to detect various types of real-world defects. We bring the improvement upon previous arts by 3.1 AUCs when learning representations from scratch. By transfer learning on pretrained representations on ImageNet, we achieve a new state-of-the-art 96.6 AUC. Lastly, we extend the framework to learn and extract representations from patches to allow localizing defective areas without annotations during training."
    },
    {
        "paperId": "7c0a7419114db2209c2f386bc1537e90417cf9d4",
        "url": "https://www.semanticscholar.org/paper/7c0a7419114db2209c2f386bc1537e90417cf9d4",
        "title": "Efficient Geometry-aware 3D Generative Adversarial Networks",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2021,
        "citationCount": 1384,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2112.07945",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2112.07945, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "121028414",
                "name": "Eric Chan"
            },
            {
                "authorId": "2108720546",
                "name": "Connor Z. Lin"
            },
            {
                "authorId": "2147382797",
                "name": "Matthew A. Chan"
            },
            {
                "authorId": "1897417",
                "name": "Koki Nagano"
            },
            {
                "authorId": "52170427",
                "name": "Boxiao Pan"
            },
            {
                "authorId": "24817039",
                "name": "Shalini De Mello"
            },
            {
                "authorId": "39775678",
                "name": "Orazio Gallo"
            },
            {
                "authorId": "51352814",
                "name": "L. Guibas"
            },
            {
                "authorId": "31943350",
                "name": "Jonathan Tremblay"
            },
            {
                "authorId": "2121982",
                "name": "S. Khamis"
            },
            {
                "authorId": "2976930",
                "name": "Tero Karras"
            },
            {
                "authorId": "1731170",
                "name": "Gordon Wetzstein"
            }
        ],
        "abstract": "Unsupervised generation of high-quality multi-view-consistent images and 3D shapes using only collections of single-view 2D photographs has been a long-standing challenge. Existing 3D GANs are either compute intensive or make approximations that are not 3D-consistent; the former limits quality and resolution of the generated images and the latter adversely affects multi-view consistency and shape quality. In this work, we improve the computational efficiency and image quality of 3D GANs without overly relying on these approximations. We introduce an expressive hybrid explicit implicit network architecture that, together with other design choices, synthesizes not only high-resolution multi-view-consistent images in real time but also produces high-quality 3D geometry. By decoupling feature generation and neural rendering, our framework is able to leverage state-of-the-art 2D CNN generators, such as StyleGAN2, and inherit their efficiency and expressiveness. We demonstrate state-of-the-art 3D-aware synthesis with FFHQ and AFHQ Cats, among other experiments."
    },
    {
        "paperId": "7c3ce1b3ad598a282546e03e2dc8b52c338caed6",
        "url": "https://www.semanticscholar.org/paper/7c3ce1b3ad598a282546e03e2dc8b52c338caed6",
        "title": "Transformer Tracking",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2021,
        "citationCount": 1152,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2103.15436, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2145230123",
                "name": "Xin Chen"
            },
            {
                "authorId": "47713338",
                "name": "Bin Yan"
            },
            {
                "authorId": "2109445463",
                "name": "Jiawen Zhu"
            },
            {
                "authorId": "143987328",
                "name": "Dong Wang"
            },
            {
                "authorId": "50031004",
                "name": "Xiaoyun Yang"
            },
            {
                "authorId": "153176123",
                "name": "Huchuan Lu"
            }
        ],
        "abstract": "Correlation acts as a critical role in the tracking field, especially in recent popular Siamese-based trackers. The correlation operation is a simple fusion manner to consider the similarity between the template and the search region. However, the correlation operation itself is a local linear matching process, leading to lose semantic information and fall into local optimum easily, which may be the bottleneck of designing high-accuracy tracking algorithms. Is there any better feature fusion method than correlation? To address this issue, inspired by Transformer, this work presents a novel attention-based feature fusion network, which effectively combines the template and search region features solely using attention. Specifically, the proposed method includes an ego-context augment module based on self-attention and a cross-feature augment module based on cross-attention. Finally, we present a Transformer tracking (named TransT) method based on the Siamese-like feature extraction backbone, the designed attention-based fusion mechanism, and the classification and regression head. Experiments show that our TransT achieves very promising results on six challenging datasets, especially on large-scale LaSOT, TrackingNet, and GOT-10k benchmarks. Our tracker runs at approximatively 50 fps on GPU. Code and models are available at https://github.com/chenxin-dlut/TransT."
    },
    {
        "paperId": "800cfb3d23115cdcd4d114234b65bbdf2080f798",
        "url": "https://www.semanticscholar.org/paper/800cfb3d23115cdcd4d114234b65bbdf2080f798",
        "title": "CSWin Transformer: A General Vision Transformer Backbone with Cross-Shaped Windows",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2021,
        "citationCount": 1217,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2107.00652",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2107.00652, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "5620602",
                "name": "Xiaoyi Dong"
            },
            {
                "authorId": "9324504",
                "name": "Jianmin Bao"
            },
            {
                "authorId": "49025801",
                "name": "Dongdong Chen"
            },
            {
                "authorId": "47528018",
                "name": "Weiming Zhang"
            },
            {
                "authorId": "1708598",
                "name": "Nenghai Yu"
            },
            {
                "authorId": "145347147",
                "name": "Lu Yuan"
            },
            {
                "authorId": "47514557",
                "name": "Dong Chen"
            },
            {
                "authorId": "2261753424",
                "name": "B. Guo"
            }
        ],
        "abstract": "We present CSWin Transformer, an efficient and effective Transformer-based backbone for general-purpose vision tasks. A challenging issue in Transformer design is that global self-attention is very expensive to compute whereas local self-attention often limits the field of interactions of each token. To address this issue, we develop the Cross-Shaped Window self-attention mechanism for computing self-attention in the horizontal and vertical stripes in parallel that form a cross-shaped window, with each stripe obtained by splitting the input feature into stripes of equal width. We provide a mathematical analysis of the effect of the stripe width and vary the stripe width for different layers of the Transformer network which achieves strong modeling capability while limiting the computation cost. We also introduce Locally-enhanced Positional Encoding (LePE), which handles the local positional information better than existing encoding schemes. LePE naturally supports arbitrary input resolutions, and is thus especially effective and friendly for downstream tasks. Incorporated with these designs and a hierarchical structure, CSWin Transformer demonstrates competitive performance on common vision tasks. Specifically, it achieves 85.4% Top-1 accuracy on ImageNet-1K without any extra training data or label, 53.9 box AP and 46.4 mask AP on the COCO detection task, and 52.2 mIOU on the ADE20K semantic segmentation task, surpassing previous state-of-the-art Swin Transformer backbone by +1.2, +2.0, +1.4, and +2.0 respectively under the similar FLOPs setting. By further pretraining on the larger dataset ImageNet-21K, we achieve 87.5% Top-1 accuracy on ImageNet-1K and high segmentation performance on ADE20K with 55.7 mIoU. 11Code and pretrain model is available at https://github.com/microsoft/CSWin-Transformer"
    },
    {
        "paperId": "848eb8367785910c2fe31372605954ad8f9dfe6c",
        "url": "https://www.semanticscholar.org/paper/848eb8367785910c2fe31372605954ad8f9dfe6c",
        "title": "Ego4D: Around the World in 3,000 Hours of Egocentric Video",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2021,
        "citationCount": 1448,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2110.07058",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2110.07058, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1794409",
                "name": "K. Grauman"
            },
            {
                "authorId": "2127379149",
                "name": "Andrew Westbury"
            },
            {
                "authorId": "2132203390",
                "name": "Eugene Byrne"
            },
            {
                "authorId": "2312205852",
                "name": "Zachary Chavis"
            },
            {
                "authorId": "1792681",
                "name": "Antonino Furnari"
            },
            {
                "authorId": "3102850",
                "name": "Rohit Girdhar"
            },
            {
                "authorId": "2132855455",
                "name": "Jackson Hamburger"
            },
            {
                "authorId": "143891655",
                "name": "Hao Jiang"
            },
            {
                "authorId": "2108511234",
                "name": "Miao Liu"
            },
            {
                "authorId": "2146036705",
                "name": "Xingyu Liu"
            },
            {
                "authorId": "2110608350",
                "name": "Miguel Martin"
            },
            {
                "authorId": "38661780",
                "name": "Tushar Nagarajan"
            },
            {
                "authorId": "30407997",
                "name": "Ilija Radosavovic"
            },
            {
                "authorId": "21810992",
                "name": "Santhosh K. Ramakrishnan"
            },
            {
                "authorId": "119797486",
                "name": "Fiona Ryan"
            },
            {
                "authorId": "50857765",
                "name": "J. Sharma"
            },
            {
                "authorId": "145032628",
                "name": "Michael Wray"
            },
            {
                "authorId": "97375393",
                "name": "Mengmeng Xu"
            },
            {
                "authorId": "2065722815",
                "name": "Eric Z. Xu"
            },
            {
                "authorId": "2109529407",
                "name": "Chen Zhao"
            },
            {
                "authorId": "16936840",
                "name": "Siddhant Bansal"
            },
            {
                "authorId": "1746610",
                "name": "Dhruv Batra"
            },
            {
                "authorId": "51002409",
                "name": "Vincent Cartillier"
            },
            {
                "authorId": "36856979",
                "name": "S. Crane"
            },
            {
                "authorId": "145756059",
                "name": "Tien Do"
            },
            {
                "authorId": "2132839352",
                "name": "Morrie Doulaty"
            },
            {
                "authorId": "2132829391",
                "name": "Akshay Erapalli"
            },
            {
                "authorId": "2322150",
                "name": "Christoph Feichtenhofer"
            },
            {
                "authorId": "2078661958",
                "name": "Adriano Fragomeni"
            },
            {
                "authorId": "2153259742",
                "name": "Qichen Fu"
            },
            {
                "authorId": "39547770",
                "name": "Christian Fuegen"
            },
            {
                "authorId": "150013809",
                "name": "A. Gebreselasie"
            },
            {
                "authorId": "2086769306",
                "name": "Cristina Gonzlez"
            },
            {
                "authorId": "3325232",
                "name": "James M. Hillis"
            },
            {
                "authorId": "2118520784",
                "name": "Xuhua Huang"
            },
            {
                "authorId": "48355651",
                "name": "Yifei Huang"
            },
            {
                "authorId": "2072957749",
                "name": "Wenqi Jia"
            },
            {
                "authorId": "15626408",
                "name": "Weslie Khoo"
            },
            {
                "authorId": "30380885",
                "name": "Jchym Kolr"
            },
            {
                "authorId": "2150275",
                "name": "Satwik Kottur"
            },
            {
                "authorId": "47311290",
                "name": "Anurag Kumar"
            },
            {
                "authorId": "102333508",
                "name": "F. Landini"
            },
            {
                "authorId": "2150358103",
                "name": "Chao Li"
            },
            {
                "authorId": "2359205979",
                "name": "Yanghao Li"
            },
            {
                "authorId": "2109633863",
                "name": "Zhenqiang Li"
            },
            {
                "authorId": "11379939",
                "name": "K. Mangalam"
            },
            {
                "authorId": "2037870106",
                "name": "Raghava Modhugu"
            },
            {
                "authorId": "47077615",
                "name": "Jonathan Munro"
            },
            {
                "authorId": "1388373582",
                "name": "Tullie Murrell"
            },
            {
                "authorId": "2047039599",
                "name": "Takumi Nishiyasu"
            },
            {
                "authorId": "50065546",
                "name": "Will Price"
            },
            {
                "authorId": "1946687752",
                "name": "Paola Ruiz Puentes"
            },
            {
                "authorId": "4042496",
                "name": "Merey Ramazanova"
            },
            {
                "authorId": "2769735",
                "name": "Leda Sari"
            },
            {
                "authorId": "31604945",
                "name": "K. Somasundaram"
            },
            {
                "authorId": "7824981",
                "name": "A. Southerland"
            },
            {
                "authorId": "1751242",
                "name": "Yusuke Sugano"
            },
            {
                "authorId": "1866636284",
                "name": "Ruijie Tao"
            },
            {
                "authorId": "1500674963",
                "name": "Minh Vo"
            },
            {
                "authorId": "2108898718",
                "name": "Yuchen Wang"
            },
            {
                "authorId": "2155226697",
                "name": "Xindi Wu"
            },
            {
                "authorId": "2052544968",
                "name": "Takuma Yagi"
            },
            {
                "authorId": "2882203",
                "name": "Yunyi Zhu"
            },
            {
                "authorId": "9739979",
                "name": "P. Arbelez"
            },
            {
                "authorId": "2821130",
                "name": "David J. Crandall"
            },
            {
                "authorId": "145089978",
                "name": "D. Damen"
            },
            {
                "authorId": "1729739",
                "name": "G. Farinella"
            },
            {
                "authorId": "2931652",
                "name": "Bernard Ghanem"
            },
            {
                "authorId": "2736958",
                "name": "V. Ithapu"
            },
            {
                "authorId": "1694502",
                "name": "C. V. Jawahar"
            },
            {
                "authorId": "7996087",
                "name": "H. Joo"
            },
            {
                "authorId": "144040368",
                "name": "Kris Kitani"
            },
            {
                "authorId": "2108493029",
                "name": "Haizhou Li"
            },
            {
                "authorId": "50366818",
                "name": "Richard A. Newcombe"
            },
            {
                "authorId": "143868587",
                "name": "A. Oliva"
            },
            {
                "authorId": "2110440192",
                "name": "H. Park"
            },
            {
                "authorId": "144177248",
                "name": "James M. Rehg"
            },
            {
                "authorId": "2110962740",
                "name": "Yoichi Sato"
            },
            {
                "authorId": "46865129",
                "name": "Jianbo Shi"
            },
            {
                "authorId": "2047358650",
                "name": "Mike Zheng Shou"
            },
            {
                "authorId": "143805211",
                "name": "A. Torralba"
            },
            {
                "authorId": "1732879",
                "name": "L. Torresani"
            },
            {
                "authorId": "47901986",
                "name": "Mingfei Yan"
            },
            {
                "authorId": "153652147",
                "name": "J. Malik"
            }
        ],
        "abstract": "We introduce Ego4D, a massive-scale egocentric video dataset and benchmark suite. It offers 3,670 hours of dailylife activity video spanning hundreds of scenarios (household, outdoor, workplace, leisure, etc.) captured by 931 unique camera wearers from 74 worldwide locations and 9 different countries. The approach to collection is designed to uphold rigorous privacy and ethics standards, with consenting participants and robust de-identification procedures where relevant. Ego4D dramatically expands the volume of diverse egocentric video footage publicly available to the research community. Portions of the video are accompanied by audio, 3D meshes of the environment, eye gaze, stereo, and/or synchronized videos from multiple egocentric cameras at the same event. Furthermore, we present a host of new benchmark challenges centered around understanding the first-person visual experience in the past (querying an episodic memory), present (analyzing hand-object manipulation, audio-visual conversation, and social interactions), and future (forecasting activities). By publicly sharing this massive annotated dataset and benchmark suite, we aim to push the frontier of first-person perception. Project page: https://ego4d-data.org/"
    },
    {
        "paperId": "88e8801e4daf404d3d40f1648ef29faeb8e6d58a",
        "url": "https://www.semanticscholar.org/paper/88e8801e4daf404d3d40f1648ef29faeb8e6d58a",
        "title": "Blended Diffusion for Text-driven Editing of Natural Images",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2021,
        "citationCount": 1125,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2111.14818",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2111.14818, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2107086356",
                "name": "Omri Avrahami"
            },
            {
                "authorId": "70018371",
                "name": "D. Lischinski"
            },
            {
                "authorId": "2416503",
                "name": "Ohad Fried"
            }
        ],
        "abstract": "Natural language offers a highly intuitive interface for image editing. In this paper, we introduce the first solution for performing local (region-based) edits in generic natural images, based on a natural language description along with an ROI mask. We achieve our goal by leveraging and combining a pretrained language-image model (CLIP), to steer the edit towards a user-provided text prompt, with a denoising diffusion probabilistic model (DDPM) to generate natural-looking results. To seamlessly fuse the edited region with the unchanged parts of the image, we spatially blend noised versions of the input image with the local text-guided diffusion latent at a progression of noise levels. In addition, we show that adding augmentations to the diffusion process mitigates adversarial results. We compare against several baselines and related methods, both qualitatively and quantitatively, and show that our method outperforms these solutions in terms of overall realism, ability to preserve the background and matching the text. Finally, we show several text-driven editing applications, including adding a new object to an image, removing/replacing/altering existing objects, background replacement, and image extrapolation."
    },
    {
        "paperId": "92d50602db5746f03b91562e2cc8a98bec584e9b",
        "url": "https://www.semanticscholar.org/paper/92d50602db5746f03b91562e2cc8a98bec584e9b",
        "title": "Multi-Stage Progressive Image Restoration",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2021,
        "citationCount": 1897,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2102.02808",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2102.02808, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3323621",
                "name": "Syed Waqas Zamir"
            },
            {
                "authorId": "153150198",
                "name": "Aditya Arora"
            },
            {
                "authorId": "152973423",
                "name": "Salman Hameed Khan"
            },
            {
                "authorId": "145684318",
                "name": "Munawar Hayat"
            },
            {
                "authorId": "2358803",
                "name": "F. Khan"
            },
            {
                "authorId": "1715634",
                "name": "Ming-Hsuan Yang"
            },
            {
                "authorId": "144082425",
                "name": "Ling Shao"
            }
        ],
        "abstract": "Image restoration tasks demand a complex balance between spatial details and high-level contextualized information while recovering images. In this paper, we propose a novel synergistic design that can optimally balance these competing goals. Our main proposal is a multi-stage architecture, that progressively learns restoration functions for the degraded inputs, thereby breaking down the overall recovery process into more manageable steps. Specifically, our model first learns the contextualized features using encoder-decoder architectures and later combines them with a high-resolution branch that retains local information. At each stage, we introduce a novel per-pixel adaptive design that leverages in-situ supervised attention to reweight the local features. A key ingredient in such a multi-stage architecture is the information exchange between different stages. To this end, we propose a two-faceted approach where the information is not only exchanged sequentially from early to late stages, but lateral connections between feature processing blocks also exist to avoid any loss of information. The resulting tightly interlinked multi-stage architecture, named as MPRNet, delivers strong performance gains on ten datasets across a range of tasks including image deraining, deblurring, and denoising. The source code and pre-trained models are available at https://github.com/swz30/MPRNet."
    },
    {
        "paperId": "94eae578e6af3382f6449506965639f18aab3fa0",
        "url": "https://www.semanticscholar.org/paper/94eae578e6af3382f6449506965639f18aab3fa0",
        "title": "Video Swin Transformer",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2021,
        "citationCount": 1845,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2106.13230, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2109371439",
                "name": "Ze Liu"
            },
            {
                "authorId": "2136771175",
                "name": "Jia Ning"
            },
            {
                "authorId": "2112823372",
                "name": "Yue Cao"
            },
            {
                "authorId": "2107995927",
                "name": "Yixuan Wei"
            },
            {
                "authorId": "2148904543",
                "name": "Zheng Zhang"
            },
            {
                "authorId": "145676588",
                "name": "Stephen Lin"
            },
            {
                "authorId": "1823518756",
                "name": "Han Hu"
            }
        ],
        "abstract": "The vision community is witnessing a modeling shift from CNNs to Transformers, where pure Transformer architectures have attained top accuracy on the major video recognition benchmarks. These video models are all built on Transformer layers that globally connect patches across the spatial and temporal dimensions. In this paper, we instead advocate an inductive bias of locality in video Transformers, which leads to a better speed-accuracy trade-off compared to previous approaches which compute self-attention globally even with spatial-temporal factorization. The locality of the proposed video architecture is realized by adapting the Swin Transformer designed for the image domain, while continuing to leverage the power of pre-trained image models. Our approach achieves state-of-the-art accuracy on a broad range of video recognition benchmarks, including on action recognition (84.9 top-l accuracy on Kinetics-400 and 85.9 top-l accuracy on Kinetics-600 with ~20 less pre-training data and ~3 smaller model size) and temporal modeling (69.6 top-l accuracy on Something-Something v2)."
    },
    {
        "paperId": "988952b0e737c8ab9b6c1fbd6d54db86e299d270",
        "url": "https://www.semanticscholar.org/paper/988952b0e737c8ab9b6c1fbd6d54db86e299d270",
        "title": "Depth-supervised NeRF: Fewer Views and Faster Training for Free",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2021,
        "citationCount": 1041,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2107.02791",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2107.02791, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "151500851",
                "name": "Kangle Deng"
            },
            {
                "authorId": "2113950621",
                "name": "Andrew Liu"
            },
            {
                "authorId": "1922024303",
                "name": "Jun-Yan Zhu"
            },
            {
                "authorId": "1770537",
                "name": "Deva Ramanan"
            }
        ],
        "abstract": "A commonly observed failure mode of Neural Radiance Field (NeRF) is fitting incorrect geometries when given an insufficient number of input views. One potential reason is that standard volumetric rendering does not enforce the constraint that most of a scene's geometry consist of empty space and opaque surfaces. We formalize the above assumption through DS-NeRF (Depth-supervised Neural Radiance Fields), a loss for learning radiance fields that takes advantage of readily-available depth supervision. We leverage the fact that current NeRF pipelines require images with known camera poses that are typically estimated by running structure-from-motion (SFM). Crucially, SFM also produces sparse 3D points that can be used as free depth supervision during training: we add a loss to encourage the distribution of a ray's terminating depth matches a given 3D keypoint, incorporating depth uncertainty. DS-NeRF can render better images given fewer training views while training 2-3x faster. Further, we show that our loss is compatible with other recently proposed NeRF methods, demonstrating that depth is a cheap and easily digestible supervisory signal. And finally, we find that DS-NeRF can support other types of depth supervision such as scanned depth sensors and RGB-D reconstruction outputs."
    },
    {
        "paperId": "9c4753ef43d2928866dc5bf6cec53d03373ec2fa",
        "url": "https://www.semanticscholar.org/paper/9c4753ef43d2928866dc5bf6cec53d03373ec2fa",
        "title": "SimMIM: a Simple Framework for Masked Image Modeling",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2021,
        "citationCount": 1627,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2111.09886",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2111.09886, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2090394064",
                "name": "Zhenda Xie"
            },
            {
                "authorId": "2148904543",
                "name": "Zheng Zhang"
            },
            {
                "authorId": "2112823372",
                "name": "Yue Cao"
            },
            {
                "authorId": "51091819",
                "name": "Yutong Lin"
            },
            {
                "authorId": "9324504",
                "name": "Jianmin Bao"
            },
            {
                "authorId": "32532300",
                "name": "Zhuliang Yao"
            },
            {
                "authorId": "152464732",
                "name": "Qi Dai"
            },
            {
                "authorId": "1823518756",
                "name": "Han Hu"
            }
        ],
        "abstract": "This paper presents SimMIM, a simple framework for masked image modeling. We have simplified recently proposed relevant approaches, without the need for special designs, such as block-wise masking and tokenization via discrete VAE or clustering. To investigate what makes a masked image modeling task learn good representations, we systematically study the major components in our framework, and find that the simple designs of each component have revealed very strong representation learning performance: 1) random masking of the input image with a moderately large masked patch size (e.g., 32) makes a powerful pre-text task; 2) predicting RGB values of raw pixels by direct regression performs no worse than the patch classification approaches with complex designs; 3) the prediction head can be as light as a linear layer, with no worse performance than heavier ones. Using ViT-B, our approach achieves 83.8% top-1 fine-tuning accuracy on ImageNet-1K by pre-training also on this dataset, surpassing previous best approach by +0.6%. When applied to a larger model with about 650 million parameters, SwinV2-H, it achieves 87.1% top-1 accuracy on ImageNet-1K using only ImageNet-1K data. We also leverage this approach to address the data-hungry issue faced by large-scale model training, that a 3B model (Swin V2-G) is successfully trained to achieve state-of-the-art accuracy on four representative vision benchmarks using 40 less labelled data than that in previous practice (JFT-3B). The code is available at https://github.com/microsoft/SimMIM."
    },
    {
        "paperId": "b526c3c450d9810ae8b037b4a87bf2a22ac48b38",
        "url": "https://www.semanticscholar.org/paper/b526c3c450d9810ae8b037b4a87bf2a22ac48b38",
        "title": "Learning to Prompt for Continual Learning",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2021,
        "citationCount": 1048,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2112.08654",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2112.08654, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2135785111",
                "name": "Zifeng Wang"
            },
            {
                "authorId": "2476328",
                "name": "Zizhao Zhang"
            },
            {
                "authorId": "50521003",
                "name": "Chen-Yu Lee"
            },
            {
                "authorId": "2146204239",
                "name": "Han Zhang"
            },
            {
                "authorId": "2015991",
                "name": "Ruoxi Sun"
            },
            {
                "authorId": "49448661",
                "name": "Xiaoqi Ren"
            },
            {
                "authorId": "3164760",
                "name": "Guolong Su"
            },
            {
                "authorId": "2066252171",
                "name": "Vincent Perot"
            },
            {
                "authorId": "153140737",
                "name": "Jennifer G. Dy"
            },
            {
                "authorId": "1945962",
                "name": "Tomas Pfister"
            }
        ],
        "abstract": "The mainstream paradigm behind continual learning has been to adapt the model parameters to non-stationary data distributions, where catastrophic forgetting is the central challenge. Typical methods rely on a rehearsal buffer or known task identity at test time to retrieve learned knowl-edge and address forgetting, while this work presents a new paradigm for continual learning that aims to train a more succinct memory system without accessing task identity at test time. Our method learns to dynamically prompt (L2P) a pre-trained model to learn tasks sequen-tially under different task transitions. In our proposed framework, prompts are small learnable parameters, which are maintained in a memory space. The objective is to optimize prompts to instruct the model prediction and ex-plicitly manage task-invariant and task-specific knowledge while maintaining model plasticity. We conduct comprehen-sive experiments under popular image classification bench-marks with different challenging continual learning set-tings, where L2P consistently outperforms prior state-of-the-art methods. Surprisingly, L2P achieves competitive results against rehearsal-based methods even without a re-hearsal buffer and is directly applicable to challenging task-agnostic continual learning. Source code is available at https://github.com/google-research/12p."
    },
    {
        "paperId": "b91de7d12ec1103f6ef9eb0720d697a9e7ecc9fe",
        "url": "https://www.semanticscholar.org/paper/b91de7d12ec1103f6ef9eb0720d697a9e7ecc9fe",
        "title": "LoFTR: Detector-Free Local Feature Matching with Transformers",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2021,
        "citationCount": 1570,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2104.00680",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2104.00680, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "153552118",
                "name": "Jiaming Sun"
            },
            {
                "authorId": "1384523019",
                "name": "Zehong Shen"
            },
            {
                "authorId": "37075603",
                "name": "Yuang Wang"
            },
            {
                "authorId": "1679542",
                "name": "H. Bao"
            },
            {
                "authorId": "145453113",
                "name": "Xiaowei Zhou"
            }
        ],
        "abstract": "We present a novel method for local image feature matching. Instead of performing image feature detection, description, and matching sequentially, we propose to first establish pixel-wise dense matches at a coarse level and later refine the good matches at a fine level. In contrast to dense methods that use a cost volume to search correspondences, we use self and cross attention layers in Transformer to obtain feature descriptors that are conditioned on both images. The global receptive field provided by Transformer enables our method to produce dense matches in low-texture areas, where feature detectors usually struggle to produce repeatable interest points. The experiments on indoor and outdoor datasets show that LoFTR outperforms state-of-the-art methods by a large margin. LoFTR also ranks first on two public benchmarks of visual localization among the published methods. Code is available at our project page: https://zju3dv.github.io/loftr/."
    },
    {
        "paperId": "be0fbb810583930c071d0b9b2c5187fe260783f5",
        "url": "https://www.semanticscholar.org/paper/be0fbb810583930c071d0b9b2c5187fe260783f5",
        "title": "Swin Transformer V2: Scaling Up Capacity and Resolution",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2021,
        "citationCount": 2372,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2111.09883",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2111.09883, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2109371439",
                "name": "Ze Liu"
            },
            {
                "authorId": "1823518756",
                "name": "Han Hu"
            },
            {
                "authorId": "51091819",
                "name": "Yutong Lin"
            },
            {
                "authorId": "32532300",
                "name": "Zhuliang Yao"
            },
            {
                "authorId": "2090394064",
                "name": "Zhenda Xie"
            },
            {
                "authorId": "2107995927",
                "name": "Yixuan Wei"
            },
            {
                "authorId": "2136771175",
                "name": "Jia Ning"
            },
            {
                "authorId": "2112823372",
                "name": "Yue Cao"
            },
            {
                "authorId": "2148904543",
                "name": "Zheng Zhang"
            },
            {
                "authorId": "145307652",
                "name": "Li Dong"
            },
            {
                "authorId": "49807919",
                "name": "Furu Wei"
            },
            {
                "authorId": "2261753424",
                "name": "B. Guo"
            }
        ],
        "abstract": "We present techniques for scaling Swin Transformer [35] up to 3 billion parameters and making it capable of training with images of up to 1,536x1,536 resolution. By scaling up capacity and resolution, Swin Transformer sets new records on four representative vision benchmarks: 84.0% top-1 accuracy on ImageNet- V2 image classification, 63.1 / 54.4 box / mask mAP on COCO object detection, 59.9 mIoU on ADE20K semantic segmentation, and 86.8% top-1 accuracy on Kinetics-400 video action classification. We tackle issues of training instability, and study how to effectively transfer models pre-trained at low resolutions to higher resolution ones. To this aim, several novel technologies are proposed: 1) a residual post normalization technique and a scaled cosine attention approach to improve the stability of large vision models; 2) a log-spaced continuous position bias technique to effectively transfer models pre-trained at low-resolution images and windows to their higher-resolution counterparts. In addition, we share our crucial implementation details that lead to significant savings of GPU memory consumption and thus make it feasi-ble to train large vision models with regular GPUs. Using these techniques and self-supervised pre-training, we suc-cessfully train a strong 3 billion Swin Transformer model and effectively transfer it to various vision tasks involving high-resolution images or windows, achieving the state-of-the-art accuracy on a variety of benchmarks. Code is avail-able at https://github.com/microsoft/Swin-Transformer."
    },
    {
        "paperId": "c10075b3746a9f3dd5811970e93c8ca3ad39b39d",
        "url": "https://www.semanticscholar.org/paper/c10075b3746a9f3dd5811970e93c8ca3ad39b39d",
        "title": "High-Resolution Image Synthesis with Latent Diffusion Models",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2021,
        "citationCount": 20793,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2112.10752",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2112.10752, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1660819540",
                "name": "Robin Rombach"
            },
            {
                "authorId": "119843260",
                "name": "A. Blattmann"
            },
            {
                "authorId": "2053482699",
                "name": "Dominik Lorenz"
            },
            {
                "authorId": "35175531",
                "name": "Patrick Esser"
            },
            {
                "authorId": "1796707",
                "name": "B. Ommer"
            }
        ],
        "abstract": "By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve new state of the art scores for image inpainting and class-conditional image synthesis and highly competitive performance on various tasks, including unconditional image generation, text-to-image synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs."
    },
    {
        "paperId": "e91f73aaef155391b5b07e6612f5346dea888f64",
        "url": "https://www.semanticscholar.org/paper/e91f73aaef155391b5b07e6612f5346dea888f64",
        "title": "Plenoxels: Radiance Fields without Neural Networks",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2021,
        "citationCount": 1993,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2112.05131",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2112.05131, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2024370854",
                "name": "Alex Yu"
            },
            {
                "authorId": "1405260934",
                "name": "Sara Fridovich-Keil"
            },
            {
                "authorId": "7638730",
                "name": "Matthew Tancik"
            },
            {
                "authorId": "2152521814",
                "name": "Qinhong Chen"
            },
            {
                "authorId": "9229182",
                "name": "B. Recht"
            },
            {
                "authorId": "20615377",
                "name": "Angjoo Kanazawa"
            }
        ],
        "abstract": "We introduce Plenoxels (plenoptic voxels), a systemfor photorealistic view synthesis. Plenoxels represent a scene as a sparse 3D grid with spherical harmonics. This representation can be optimized from calibrated images via gradient methods and regularization without any neural components. On standard, benchmark tasks, Plenoxels are optimized two orders of magnitude faster than Neural Radiance Fields with no loss in visual quality. For video and code, please see https://alexyu.net/plenoxels."
    },
    {
        "paperId": "ec90ffa017a2cc6a51342509ce42b81b478aefb3",
        "url": "https://www.semanticscholar.org/paper/ec90ffa017a2cc6a51342509ce42b81b478aefb3",
        "title": "Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2021,
        "citationCount": 2197,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2111.12077",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2111.12077, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "50329510",
                "name": "J. Barron"
            },
            {
                "authorId": "2577533",
                "name": "B. Mildenhall"
            },
            {
                "authorId": "8497474",
                "name": "Dor Verbin"
            },
            {
                "authorId": "2179732",
                "name": "Pratul P. Srinivasan"
            },
            {
                "authorId": "33810877",
                "name": "Peter Hedman"
            }
        ],
        "abstract": "Though neural radiance fields (NeRF) have demon-strated impressive view synthesis results on objects and small bounded regions of space, they struggle on un-bounded scenes, where the camera may point in any di-rection and content may exist at any distance. In this set-ting, existing NeRF-like models often produce blurry or low-resolution renderings (due to the unbalanced detail and scale of nearby and distant objects), are slow to train, and may exhibit artifacts due to the inherent ambiguity of the task of reconstructing a large scene from a small set of images. We present an extension of mip-NeRF (a NeRF variant that addresses sampling and aliasing) that uses a non-linear scene parameterization, online distillation, and a novel distortion-based regularizer to overcome the chal-lenges presented by unbounded scenes. Our model, which we dub mip-NeRF 360 as we target scenes in which the camera rotates 360 degrees around a point, reduces mean-squared error by 57% compared to mip-NeRF, and is able to produce realistic synthesized views and detailed depth maps for highly intricate, unbounded real-world scenes."
    },
    {
        "paperId": "144eca44e250cc462f6fc3a172abb865978f66f5",
        "url": "https://www.semanticscholar.org/paper/144eca44e250cc462f6fc3a172abb865978f66f5",
        "title": "Multi-Concept Customization of Text-to-Image Diffusion",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2022,
        "citationCount": 1147,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2212.04488",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2212.04488, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "46373847",
                "name": "Nupur Kumari"
            },
            {
                "authorId": "2373720779",
                "name": "Bingliang Zhang"
            },
            {
                "authorId": "2109976035",
                "name": "Richard Zhang"
            },
            {
                "authorId": "2177801",
                "name": "Eli Shechtman"
            },
            {
                "authorId": "1922024303",
                "name": "Jun-Yan Zhu"
            }
        ],
        "abstract": "While generative models produce high-quality images of concepts learned from a large-scale database, a user often wishes to synthesize instantiations of their own concepts (for example, their family, pets, or items). Can we teach a model to quickly acquire a new concept, given a few examples? Furthermore, can we compose multiple new concepts together? We propose Custom Diffusion, an efficient method for augmenting existing text-to-image models. We find that only optimizing a few parameters in the text-to-image conditioning mechanism is sufficiently powerful to represent new concepts while enabling fast tuning (~ 6 minutes). Additionally, we can jointly train for multiple concepts or combine multiple fine-tuned models into one via closed-form constrained optimization. Our fine-tuned model generates variations of multiple new concepts and seamlessly composes them with existing concepts in novel settings. Our method outperforms or performs on par with several baselines and concurrent works in both qualitative and quantitative evaluations, while being memory and computationally efficient."
    },
    {
        "paperId": "16de2006e2960ba410772c6b6d460b83c0a5cc4b",
        "url": "https://www.semanticscholar.org/paper/16de2006e2960ba410772c6b6d460b83c0a5cc4b",
        "title": "Reproducible Scaling Laws for Contrastive Language-Image Learning",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2022,
        "citationCount": 1134,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2212.07143",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2212.07143, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "40063601",
                "name": "Mehdi Cherti"
            },
            {
                "authorId": "2125377840",
                "name": "R. Beaumont"
            },
            {
                "authorId": "2113839396",
                "name": "Ross Wightman"
            },
            {
                "authorId": "52193502",
                "name": "Mitchell Wortsman"
            },
            {
                "authorId": "1387994137",
                "name": "Gabriel Ilharco"
            },
            {
                "authorId": "2007745319",
                "name": "Cade Gordon"
            },
            {
                "authorId": "2137341362",
                "name": "Christoph Schuhmann"
            },
            {
                "authorId": "152772922",
                "name": "Ludwig Schmidt"
            },
            {
                "authorId": "2191688",
                "name": "J. Jitsev"
            }
        ],
        "abstract": "Scaling up neural networks has led to remarkable performance across a wide range of tasks. Moreover, performance often follows reliable scaling laws as a function of training set size, model size, and compute, which offers valuable guidance as large-scale experiments are becoming increasingly expensive. However, previous work on scaling laws has primarily used private data & models or focused on uni-modal language or vision learning. To address these limitations, we investigate scaling laws for contrastive language-image pre-training (CLIP) with the public LAION dataset and the open-source OpenCLIP repository. Our large-scale experiments involve models trained on up to two billion image-text pairs and identify power law scaling for multiple downstream tasks including zero-shot classification, retrieval, linear probing, and end-to-end fine-tuning. We find that the training distribution plays a key role in scaling laws as the OpenAI and OpenCLIP models exhibit different scaling behavior despite identical model architectures and similar training recipes. We open-source our evaluation workflow and all models, including the largest public CLIP models, to ensure reproducibility and make scaling laws research more accessible. Source code and instructions to reproduce this study is available at https://github.eom/LAION-AI/sealing-laws-openelip."
    },
    {
        "paperId": "177e957f5cd93229c9794ea652c646d2557b4a69",
        "url": "https://www.semanticscholar.org/paper/177e957f5cd93229c9794ea652c646d2557b4a69",
        "title": "A ConvNet for the 2020s",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2022,
        "citationCount": 6884,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2201.03545, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2109168016",
                "name": "Zhuang Liu"
            },
            {
                "authorId": "2053590350",
                "name": "Hanzi Mao"
            },
            {
                "authorId": "95426599",
                "name": "Chaozheng Wu"
            },
            {
                "authorId": "2322150",
                "name": "Christoph Feichtenhofer"
            },
            {
                "authorId": "1753210",
                "name": "Trevor Darrell"
            },
            {
                "authorId": "1817030",
                "name": "Saining Xie"
            }
        ],
        "abstract": "The Roaring 20s of visual recognition began with the introduction of Vision Transformers (ViTs), which quickly superseded ConvNets as the state-of-the-art image classification model. A vanilla ViT, on the other hand, faces difficulties when applied to general computer vision tasks such as object detection and semantic segmentation. It is the hierarchical Transformers (e.g., Swin Transformers) that reintroduced several ConvNet priors, making Transformers practically viable as a generic vision backbone and demonstrating remarkable performance on a wide variety of vision tasks. However, the effectiveness of such hybrid approaches is still largely credited to the intrinsic superiority of Transformers, rather than the inherent inductive biases of convolutions. In this work, we reexamine the design spaces and test the limits of what a pure ConvNet can achieve. We gradually modernize a standard ResNet toward the design of a vision Transformer, and discover several key components that contribute to the performance difference along the way. The outcome of this exploration is a family of pure ConvNet models dubbed ConvNeXt. Constructed entirely from standard ConvNet modules, ConvNeXts compete favorably with Transformers in terms of accuracy and scalability, achieving 87.8% ImageNet top-1 accuracy and outperforming Swin Transformers on COCO detection and ADE20K segmentation, while maintaining the simplicity and efficiency of standard ConvNets."
    },
    {
        "paperId": "1b31dbf44e68b698120552366df03e6e35a1e428",
        "url": "https://www.semanticscholar.org/paper/1b31dbf44e68b698120552366df03e6e35a1e428",
        "title": "Objaverse: A Universe of Annotated 3D Objects",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2022,
        "citationCount": 1335,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2212.08051",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2212.08051, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1632916259",
                "name": "Matt Deitke"
            },
            {
                "authorId": "34846449",
                "name": "Dustin Schwenk"
            },
            {
                "authorId": "145704057",
                "name": "Jordi Salvador"
            },
            {
                "authorId": "20745881",
                "name": "Luca Weihs"
            },
            {
                "authorId": "2196005933",
                "name": "Oscar Michel"
            },
            {
                "authorId": "1632920625",
                "name": "Eli VanderBilt"
            },
            {
                "authorId": "152772922",
                "name": "Ludwig Schmidt"
            },
            {
                "authorId": "2883417",
                "name": "Kiana Ehsani"
            },
            {
                "authorId": "2684226",
                "name": "Aniruddha Kembhavi"
            },
            {
                "authorId": "143787583",
                "name": "Ali Farhadi"
            }
        ],
        "abstract": "Massive data corpora like WebText, Wikipedia, Conceptual Captions, WebImageText, and LAION have propelled recent dramatic progress in AI. Large neural models trained on such datasets produce impressive results and top many of today's benchmarks. A notable omisslion within this family of large-scale datasets is 3D data. Despite considerable interest and potential applications in 3D vision, datasets of high-fidelity 3D models continue to be mid-sized with limited diversity of object categories. Addressing this gap, we present Objaverse 1.0, a large dataset of objects with 800K + (and growing) 3D models with descriptive captions, tags, and animations. Objaverse improves upon present day 3D repositories in terms of scale, number of categories, and in the visual diversity of instances within a category. We demonstrate the large potential of Objaverse via four diverse applications: training generative 3D models, improving tail category segmentation on the LVIS benchmark, training open-vocabulary object-navigation models for Embodied AI, and creating a new benchmark for robustness analysis of vision models. Objaverse can open new directions for research and enable new applications across the field of AI."
    },
    {
        "paperId": "1e91fa21b890a8f5d615578f4ddf46c3cb394691",
        "url": "https://www.semanticscholar.org/paper/1e91fa21b890a8f5d615578f4ddf46c3cb394691",
        "title": "RePaint: Inpainting using Denoising Diffusion Probabilistic Models",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2022,
        "citationCount": 1829,
        "openAccessPdf": {
            "url": "https://lirias.kuleuven.be/bitstream/20.500.12942/728757/2/Lugmayr_RePaint_Inpainting_Using_Denoising_Diffusion_Probabilistic_Models_CVPR_2022_paper.pdf",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2201.09865, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1387970700",
                "name": "Andreas Lugmayr"
            },
            {
                "authorId": "2129520569",
                "name": "Martin Danelljan"
            },
            {
                "authorId": "145848547",
                "name": "Andrs Romero"
            },
            {
                "authorId": "1807197",
                "name": "F. Yu"
            },
            {
                "authorId": "1732855",
                "name": "Radu Timofte"
            },
            {
                "authorId": "1681236",
                "name": "L. Gool"
            }
        ],
        "abstract": "Free-form inpainting is the task of adding new content to an image in the regions specified by an arbitrary binary mask. Most existing approaches train for a certain distribution of masks, which limits their generalization capabilities to unseen mask types. Furthermore, training with pixel-wise and perceptual losses often leads to simple textural extensions towards the missing areas instead of semantically meaningful generation. In this work, we propose RePaint: A Denoising Diffusion Probabilistic Model (DDPM) based inpainting approach that is applicable to even extreme masks. We employ a pretrained unconditional DDPM as the generative prior. To condition the generation process, we only alter the reverse diffusion iterations by sampling the unmasked regions using the given image infor-mation. Since this technique does not modify or condition the original DDPM network itself, the model produces high-quality and diverse output images for any inpainting form. We validate our method for both faces and general-purpose image inpainting using standard and extreme masks. Re-Paint outperforms state-of-the-art Autoregressive, and GAN approaches for at least five out of six mask distributions. Github Repository: git.io/RePaint"
    },
    {
        "paperId": "23e261a20a315059b4de5492ed071c97a20c12e7",
        "url": "https://www.semanticscholar.org/paper/23e261a20a315059b4de5492ed071c97a20c12e7",
        "title": "Imagic: Text-Based Real Image Editing with Diffusion Models",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2022,
        "citationCount": 1318,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2210.09276",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2210.09276, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2047309422",
                "name": "Bahjat Kawar"
            },
            {
                "authorId": "2145761298",
                "name": "Shiran Zada"
            },
            {
                "authorId": "49618488",
                "name": "Oran Lang"
            },
            {
                "authorId": "2047833213",
                "name": "Omer Tov"
            },
            {
                "authorId": "2146380",
                "name": "Hui-Tang Chang"
            },
            {
                "authorId": "2112779",
                "name": "Tali Dekel"
            },
            {
                "authorId": "2138834",
                "name": "Inbar Mosseri"
            },
            {
                "authorId": "144611617",
                "name": "M. Irani"
            }
        ],
        "abstract": "Text-conditioned image editing has recently attracted considerable interest. However, most methods are currently limited to one of the following: specific editing types (e.g., object overlay, style transfer), synthetically generated images, or requiring multiple input images of a common object. In this paper we demonstrate, for the very first time, the ability to apply complex (e.g., non-rigid) text-based semantic edits to a single real image. For example, we can change the posture and composition of one or multiple objects inside an image, while preserving its original characteristics. Our method can make a standing dog sit down, cause a bird to spread its wings, etc.  each within its single high-resolution user-provided natural image. Contrary to previous work, our proposed method requires only a single input image and a target text (the desired edit). It operates on real images, and does not require any additional inputs (such as image masks or additional views of the object). Our method, called Imagic, leverages a pre-trained text-to-image diffusion model for this task. It produces a text embedding that aligns with both the input image and the target text, while fine-tuning the diffusion model to capture the image-specific appearance. We demonstrate the quality and versatility of Imagic on numerous inputs from various domains, showcasing a plethora of high quality complex semantic image edits, all within a single unified framework. To better assess performance, we introduce TEdBench, a highly challenging image editing benchmark. We conduct a user study, whose findings show that human raters prefer Imagic to previous leading editing methods on TEdBench."
    },
    {
        "paperId": "3aed4648f7857c1d5e9b1da4c3afaf97463138c3",
        "url": "https://www.semanticscholar.org/paper/3aed4648f7857c1d5e9b1da4c3afaf97463138c3",
        "title": "YOLOv7: Trainable Bag-of-Freebies Sets New State-of-the-Art for Real-Time Object Detectors",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2022,
        "citationCount": 9009,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2207.02696",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2207.02696, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2423297",
                "name": "Chien-Yao Wang"
            },
            {
                "authorId": "1651204675",
                "name": "Alexey Bochkovskiy"
            },
            {
                "authorId": "1704678",
                "name": "H. Liao"
            }
        ],
        "abstract": "Real-time object detection is one of the most important research topics in computer vision. As new approaches regarding architecture optimization and training optimization are continually being developed, we have found two research topics that have spawned when dealing with these latest state-of-the-art methods. To address the topics, we propose a trainable bag-of-freebies oriented solution. We combine the flexible and efficient training tools with the proposed architecture and the compound scaling method. YOLOv7 surpasses all known object detectors in both speed and accuracy in the range from 5 FPS to 120 FPS and has the highest accuracy 56.8% AP among all known real-time object detectors with 30 FPS or higher on GPU V100. Source code is released in https://github.com/WongKinYiu/yolov7."
    },
    {
        "paperId": "5b19bf6c3f4b25cac96362c98b930cf4b37f6744",
        "url": "https://www.semanticscholar.org/paper/5b19bf6c3f4b25cac96362c98b930cf4b37f6744",
        "title": "DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2022,
        "citationCount": 3707,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2208.12242",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2208.12242, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "31601235",
                "name": "Nataniel Ruiz"
            },
            {
                "authorId": "2167749913",
                "name": "Yuanzhen Li"
            },
            {
                "authorId": "2131639924",
                "name": "Varun Jampani"
            },
            {
                "authorId": "1782328",
                "name": "Y. Pritch"
            },
            {
                "authorId": "144544291",
                "name": "Michael Rubinstein"
            },
            {
                "authorId": "3451442",
                "name": "Kfir Aberman"
            }
        ],
        "abstract": "Large text-to-image models achieved a remarkable leap in the evolution of AI, enabling high-quality and diverse synthesis of images from a given text prompt. However, these models lack the ability to mimic the appearance of subjects in a given reference set and synthesize novel renditions of them in different contexts. In this work, we present a new approach for personalization of text-to-image diffusion models. Given as input just a few images of a subject, we fine-tune a pretrained text-to-image model such that it learns to bind a unique identifier with that specific subject. Once the subject is embedded in the output domain of the model, the unique identifier can be used to synthesize novel photorealistic images of the subject contextualized in different scenes. By leveraging the semantic prior embedded in the model with a new autogenous class-specific prior preservation loss, our technique enables synthesizing the subject in diverse scenes, poses, views and lighting conditions that do not appear in the reference images. We apply our technique to several previously-unassailable tasks, including subject recontextualization, text-guided view synthesis, and artistic rendering, all while preserving the subject's key features. We also provide a new dataset and evaluation protocol for this new task of subject-driven generation. Project page: https://dreambooth.github.io/"
    },
    {
        "paperId": "a2d2bbe4c542173662a444b33b76c66992697830",
        "url": "https://www.semanticscholar.org/paper/a2d2bbe4c542173662a444b33b76c66992697830",
        "title": "InstructPix2Pix: Learning to Follow Image Editing Instructions",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2022,
        "citationCount": 2462,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2211.09800",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2211.09800, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2679394",
                "name": "Tim Brooks"
            },
            {
                "authorId": "2248172435",
                "name": "Aleksander Holynski"
            },
            {
                "authorId": "1763086",
                "name": "Alexei A. Efros"
            }
        ],
        "abstract": "We propose a method for editing images from human instructions: given an input image and a written instruction that tells the model what to do, our model follows these instructions to edit the image. To obtain training data for this problem, we combine the knowledge of two large pretrained modelsa language model (GPT-3) and a text-to-image model (Stable Diffusion)to generate a large dataset of image editing examples. Our conditional diffusion model, InstructPix2Pix, is trained on our generated data, and generalizes to real images and user-written instructions at inference time. Since it performs edits in the forward pass and does not require per-example fine-tuning or inversion, our model edits images quickly, in a matter of seconds. We show compelling editing results for a diverse collection of input images and written instructions."
    },
    {
        "paperId": "b879450f50a6113f44a5baf0bcd5b4331eeb7bbc",
        "url": "https://www.semanticscholar.org/paper/b879450f50a6113f44a5baf0bcd5b4331eeb7bbc",
        "title": "Conditional Prompt Learning for Vision-Language Models",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2022,
        "citationCount": 1840,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2203.05557",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2203.05557, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "9368124",
                "name": "Kaiyang Zhou"
            },
            {
                "authorId": "2295601",
                "name": "Jingkang Yang"
            },
            {
                "authorId": "1717179",
                "name": "Chen Change Loy"
            },
            {
                "authorId": "2117940996",
                "name": "Ziwei Liu"
            }
        ],
        "abstract": "With the rise of powerful pre-trained vision-language models like CLIP, it becomes essential to investigate ways to adapt these models to downstream datasets. A recently proposed method named Context Optimization (CoOp) introduces the concept of prompt learninga recent trend in NLPto the vision domain for adapting pre-trained vision-language models. Specifically, CoOp turns context words in a prompt into a set of learnable vectors and, with only a few labeled images for learning, can achieve huge improvements over intensively-tuned manual prompts. In our study we identify a critical problem of CoOp: the learned context is not generalizable to wider unseen classes within the same dataset, suggesting that CoOp overfits base classes observed during training. To address the problem, we propose Conditional Context Optimization (CoCoOp), which extends CoOp by further learning a lightweight neural network to generate for each image an input-conditional token (vector). Compared to CoOp's static prompts, our dynamic prompts adapt to each instance and are thus less sensitive to class shift. Extensive experiments show that CoCoOp generalizes much better than CoOp to unseen classes, even showing promising transferability beyond a single dataset; and yields stronger domain generalization performance as well. Code is available at https://github.com/KaiyangZhou/CoOp."
    },
    {
        "paperId": "bdf4af8311637c681904e71cf50f96fd0026f578",
        "url": "https://www.semanticscholar.org/paper/bdf4af8311637c681904e71cf50f96fd0026f578",
        "title": "Magic3D: High-Resolution Text-to-3D Content Creation",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2022,
        "citationCount": 1415,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2211.10440",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2211.10440, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2511674",
                "name": "Chen-Hsuan Lin"
            },
            {
                "authorId": "145200206",
                "name": "Jun Gao"
            },
            {
                "authorId": "34689393",
                "name": "Luming Tang"
            },
            {
                "authorId": "150324836",
                "name": "Towaki Takikawa"
            },
            {
                "authorId": "1751476",
                "name": "Xiaohui Zeng"
            },
            {
                "authorId": "144247007",
                "name": "Xun Huang"
            },
            {
                "authorId": "32113848",
                "name": "Karsten Kreis"
            },
            {
                "authorId": "37895334",
                "name": "S. Fidler"
            },
            {
                "authorId": "39793900",
                "name": "Ming-Yu Liu"
            },
            {
                "authorId": "2115356502",
                "name": "Tsung-Yi Lin"
            }
        ],
        "abstract": "DreamFusion [31] has recently demonstrated the utility of a pretrained text-to-image diffusion model to optimize Neural Radiance Fields (NeRF) [23], achieving remarkable text-to-3D synthesis results. However, the method has two inherent limitations: (a) extremely slow optimization of NeRF and (b) low-resolution image space supervision on NeRF, leading to low-quality 3D models with a long processing time. In this paper, we address these limitations by utilizing a two-stage optimization framework. First, we obtain a coarse model using a low-resolution diffusion prior and accelerate with a sparse 3D hash grid structure. Using the coarse representation as the initialization, we further optimize a textured 3D mesh model with an efficient differentiable renderer interacting with a high-resolution latent diffusion model. Our method, dubbed Magic3D, can create high quality 3D mesh models in 40 minutes, which is 2 faster than DreamFusion (reportedly taking 1.5 hours on average), while also achieving higher resolution. User studies show 61.7% raters to prefer our approach over DreamFusion. Together with the image-conditioned generation capabilities, we provide users with new ways to control 3D synthesis, opening up new avenues to various creative applications."
    },
    {
        "paperId": "fdd7d5b0f6b8641c356e170fd264cd11f70ba657",
        "url": "https://www.semanticscholar.org/paper/fdd7d5b0f6b8641c356e170fd264cd11f70ba657",
        "title": "Planning-oriented Autonomous Driving",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2022,
        "citationCount": 1022,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2212.10156",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2212.10156, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1452986741",
                "name": "Yi Hu"
            },
            {
                "authorId": "2184753565",
                "name": "Jiazhi Yang"
            },
            {
                "authorId": "2170764701",
                "name": "Li Chen"
            },
            {
                "authorId": "14575220",
                "name": "Keyu Li"
            },
            {
                "authorId": "2144553163",
                "name": "Chonghao Sima"
            },
            {
                "authorId": "2578924",
                "name": "Xizhou Zhu"
            },
            {
                "authorId": "2171618940",
                "name": "Siqi Chai"
            },
            {
                "authorId": "2628452",
                "name": "Senyao Du"
            },
            {
                "authorId": "2115348355",
                "name": "Tianwei Lin"
            },
            {
                "authorId": "47825073",
                "name": "Wen Wang"
            },
            {
                "authorId": "152309485",
                "name": "Lewei Lu"
            },
            {
                "authorId": "1958998899",
                "name": "Xiaosong Jia"
            },
            {
                "authorId": "2146553959",
                "name": "Qiang Liu"
            },
            {
                "authorId": "3304536",
                "name": "Jifeng Dai"
            },
            {
                "authorId": "2056924832",
                "name": "Yu Qiao"
            },
            {
                "authorId": "46382329",
                "name": "Hongyang Li"
            }
        ],
        "abstract": "Modern autonomous driving system is characterized as modular tasks in sequential order, i.e., perception, prediction, and planning. In order to perform a wide diversity of tasks and achieve advanced-level intelligence, contemporary approaches either deploy standalone models for individual tasks, or design a multi-task paradigm with separate heads. However, they might suffer from accumulative errors or deficient task coordination. Instead, we argue that a favorable framework should be devised and optimized in pursuit of the ultimate goal, i.e., planning of the self-driving car. Oriented at this, we revisit the key components within perception and prediction, and prioritize the tasks such that all these tasks contribute to planning. We introduce Unified Autonomous Driving (UniAD), a comprehensive framework up-to-date that incorporates full-stack driving tasks in one network. It is exquisitely devised to leverage advantages of each module, and provide complementary feature abstractions for agent interaction from a global perspective. Tasks are communicated with unified query interfaces to facilitate each other toward planning. We instantiate UniAD on the challenging nuScenes benchmark. With extensive ablations, the effectiveness of using such a philosophy is proven by substantially outperforming previous state-of-the-arts in all aspects. Code and models are public."
    },
    {
        "paperId": "124d4d374fbef2016fa9880489871a58a7450644",
        "url": "https://www.semanticscholar.org/paper/124d4d374fbef2016fa9880489871a58a7450644",
        "title": "Improved Baselines with Visual Instruction Tuning",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2023,
        "citationCount": 4098,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2310.03744",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.03744, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2143856368",
                "name": "Haotian Liu"
            },
            {
                "authorId": "2243126534",
                "name": "Chunyuan Li"
            },
            {
                "authorId": "1527091339",
                "name": "Yuheng Li"
            },
            {
                "authorId": "2256122200",
                "name": "Yong Jae Lee"
            }
        ],
        "abstract": "Large multimodal models (LMM) have recently shown encouraging progress with visual instruction tuning. In this paper, we present the first systematic study to investigate the design choices of LMMs in a controlled setting under the LLaVA framework. We show that the fully-connected vision-language connector in LLaVA is surprisingly power-ful and data-efficient. With simple modifications to LLa VA, namely, using CLIP- ViT-L-336px with an MLP projection and adding academic-task-oriented VQA data with response formatting prompts, we establish stronger baselines that achieve state-of-the-art across 11 benchmarks. Our final 13B checkpoint uses merely 1.2M publicly available data, and finishes full training in ~ 1 day on a single 8-AI00 node. Furthermore, we present some early exploration of open problems in LMMs, including scaling to higher resolution inputs, compositional capabilities, and model hallucination, etc. We hope this makes state-of-the-art LMM research more accessible. Code and model will be publicly available."
    },
    {
        "paperId": "2218f1713d7f721ab76801063416ec9b11c7646f",
        "url": "https://www.semanticscholar.org/paper/2218f1713d7f721ab76801063416ec9b11c7646f",
        "title": "ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2023,
        "citationCount": 1264,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2301.00808",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2301.00808, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2262209",
                "name": "Sanghyun Woo"
            },
            {
                "authorId": "3256722",
                "name": "Shoubhik Debnath"
            },
            {
                "authorId": "2874347",
                "name": "Ronghang Hu"
            },
            {
                "authorId": "39717886",
                "name": "Xinlei Chen"
            },
            {
                "authorId": "2109168016",
                "name": "Zhuang Liu"
            },
            {
                "authorId": "145017151",
                "name": "In-So Kweon"
            },
            {
                "authorId": "1817030",
                "name": "Saining Xie"
            }
        ],
        "abstract": "Driven by improved architectures and better representation learning frameworks, the field of visual recognition has enjoyed rapid modernization and performance boost in the early 2020s. For example, modern ConvNets, represented by ConvNeXt [33], have demonstrated strong performance in various scenarios. While these models were originally designed for supervised learning with ImageNet labels, they can also potentially benefit from self-supervised learning techniques such as masked autoencoders (MAE) [14]. However, we found that simply combining these two approaches leads to subpar performance. In this paper, we propose a fully convolutional masked autoencoder framework and a new Global Response Normalization (GRN) layer that can be added to the ConvNeXt architecture to enhance inter-channel feature competition. This co-design of self-supervised learning techniques and architectural improvement results in a new model family called ConvNeXt V2, which significantly improves the performance of pure ConvNets on various recognition benchmarks, including ImageNet classification, COCO detection, and ADE20K segmentation. We also provide pre-trained ConvNeXt V2 models of various sizes, ranging from an efficient 3.7M-parameter Atto model with 76.7% top-1 accuracy on ImageNet, to a 650M Huge model that achieves a state-of-the-art 88.9% accuracy using only public training data."
    },
    {
        "paperId": "395bfdae59f1a5fde16213cade43d2587e4565df",
        "url": "https://www.semanticscholar.org/paper/395bfdae59f1a5fde16213cade43d2587e4565df",
        "title": "4D Gaussian Splatting for Real-Time Dynamic Scene Rendering",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2023,
        "citationCount": 1025,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2310.08528",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.08528, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2257429038",
                "name": "Guanjun Wu"
            },
            {
                "authorId": "2167029536",
                "name": "Taoran Yi"
            },
            {
                "authorId": "50882585",
                "name": "Jiemin Fang"
            },
            {
                "authorId": "3041937",
                "name": "Lingxi Xie"
            },
            {
                "authorId": "2257369199",
                "name": "Xiaopeng Zhang"
            },
            {
                "authorId": "2257698239",
                "name": "Wei Wei"
            },
            {
                "authorId": "2238154287",
                "name": "Wenyu Liu"
            },
            {
                "authorId": "2257417852",
                "name": "Qi Tian"
            },
            {
                "authorId": "2257846814",
                "name": "Xinggang Wang"
            }
        ],
        "abstract": "Representing and rendering dynamic scenes has been an important but challenging task. Especially, to accurately model complex motions, high efficiency is usually hard to guarantee. To achieve real-time dynamic scene rendering while also enjoying high training and storage efficiency, we propose 4D Gaussian Splatting (4D-GS) as a holistic representation for dynamic scenes rather than applying 3D-GS for each individual frame. In 4D-GS, a novel explicit representation containing both 3D Gaussians and 4D neural voxels is proposed. A decomposed neural voxel encoding algorithm inspired by HexPlane is proposed to efficiently build Gaussian features from 4D neural voxels and then a lightweight MLP is applied to predict Gaussian deformations at novel timestamps. Our 4D-GS method achieves real-time rendering under high resolutions, 82 FPS at an 800x800 resolution on an RTX 3090 GPU while maintaining comparable or better quality than previous state- of-the-art methods. More demos and code are available at https://guanjunwu.github.io/4dgs/."
    },
    {
        "paperId": "3a6c423cc0f687fc43a671e55dce06f681df7476",
        "url": "https://www.semanticscholar.org/paper/3a6c423cc0f687fc43a671e55dce06f681df7476",
        "title": "DETRs Beat YOLOs on Real-time Object Detection",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2023,
        "citationCount": 2245,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2304.08069",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2304.08069, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2080026760",
                "name": "Wenyu Lv"
            },
            {
                "authorId": "1492024199",
                "name": "Shangliang Xu"
            },
            {
                "authorId": "2291854678",
                "name": "Yian Zhao"
            },
            {
                "authorId": "30284284",
                "name": "Guanzhong Wang"
            },
            {
                "authorId": "2214577904",
                "name": "Jinman Wei"
            },
            {
                "authorId": "2052274972",
                "name": "Cheng Cui"
            },
            {
                "authorId": "2867809",
                "name": "Yuning Du"
            },
            {
                "authorId": "2066154565",
                "name": "Qingqing Dang"
            },
            {
                "authorId": "2192345899",
                "name": "Yi Liu"
            }
        ],
        "abstract": "The YOLO series has become the most popular frame-work for real-time object detection due to its reasonable trade-off between speed and accuracy. However, we observe that the speed and accuracy of YOLOs are negatively affected by the NMS. Recently, end-to-end Transformer-based detectors (DETRs) have provided an alternative to eliminating NMS. Nevertheless, the high computational cost limits their practicality and hinders them from fully exploiting the advantage of excluding NMS. In this paper, we propose the Real-Time DEtection TRansformer (RT-DETR), the first real-time end-to-end object detector to our best knowledge that addresses the above dilemma. We build RT-DETR in two steps, drawing on the advanced DETR: first we focus on maintaining accuracy while improving speed, followed by maintaining speed while improving accuracy. Specifically, we design an efficient hybrid encoder to expeditiously process multi-scale features by decoupling intra-scale interaction and cross-scale fusion to improve speed. Then, we propose the uncertainty-minimal query selection to provide high-quality initial queries to the decoder, thereby improving accuracy. In addition, RT-DETR supports flexible speed tuning by adjusting the number of decoder layers to adapt to various scenarios without retraining. Our RT-DETR-R50 /R101 achieves 53.1% 154.3% AP on COCO and 108 /74 FPS on T4 GPU, outperforming previously advanced YOLOs in both speed and accuracy. Furthermore, RT-DETR-R50 outperforms DINO-R50 by 2.2% AP in accuracy and about 21 times in FPS. After pre-training with Objects365, RT-DETR-R50 / R101 achieves 55.3% 156.2% AP. The project page: https://zhao-yian.github.io/RTDEtr."
    },
    {
        "paperId": "6a33e58ef961a3a0a5657518b2be86395eb7c8d0",
        "url": "https://www.semanticscholar.org/paper/6a33e58ef961a3a0a5657518b2be86395eb7c8d0",
        "title": "Intern VL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2023,
        "citationCount": 2139,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2312.14238",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2312.14238, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "66350249",
                "name": "Zhe Chen"
            },
            {
                "authorId": "2109215916",
                "name": "Jiannan Wu"
            },
            {
                "authorId": "2257133501",
                "name": "Wenhai Wang"
            },
            {
                "authorId": "2276207009",
                "name": "Weijie Su"
            },
            {
                "authorId": "2155229619",
                "name": "Guo Chen"
            },
            {
                "authorId": "2191075284",
                "name": "Sen Xing"
            },
            {
                "authorId": "2276203785",
                "name": "Zhong Muyan"
            },
            {
                "authorId": "2276279994",
                "name": "Qinglong Zhang"
            },
            {
                "authorId": "2578924",
                "name": "Xizhou Zhu"
            },
            {
                "authorId": "152309485",
                "name": "Lewei Lu"
            },
            {
                "authorId": "2218579598",
                "name": "Bin Li"
            },
            {
                "authorId": "2253674868",
                "name": "Ping Luo"
            },
            {
                "authorId": "2276323159",
                "name": "Tong Lu"
            },
            {
                "authorId": "2258755556",
                "name": "Yu Qiao"
            },
            {
                "authorId": "3304536",
                "name": "Jifeng Dai"
            }
        ],
        "abstract": "The exponential growth of large language models (LLMs) has opened up numerous possibilities for multi-modal AGI systems. However, the progress in vision and vision-language foundation models, which are also critical elements of multi-modal AGI, has not kept pace with LLMs. In this work, we design a large-scale vision-language foun-dation model (Intern VL), which scales up the vision foun-dation model to 6 billion parameters and progressively aligns it with the LLM, using web-scale image-text data from various sources. This model can be broadly applied to and achieve state-of-the-art performance on 32 generic visual-linguistic benchmarks including visual perception tasks such as image-level or pixel-level recognition, vision-language tasks such as zero-shot image/video classification, zero-shot image/video-text retrieval, and link with LLMs to create multi-modal dialogue systems. It has powerful visual capabilities and can be a good alternative to the ViT-22B. We hope that our research could contribute to the development of multi-modal large models."
    },
    {
        "paperId": "7dc6da87eaa6f830354feb2db14023cab8678c91",
        "url": "https://www.semanticscholar.org/paper/7dc6da87eaa6f830354feb2db14023cab8678c91",
        "title": "ImageBind One Embedding Space to Bind Them All",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2023,
        "citationCount": 1285,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/2305.05665",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.05665, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3102850",
                "name": "Rohit Girdhar"
            },
            {
                "authorId": "1388811741",
                "name": "Alaaeldin El-Nouby"
            },
            {
                "authorId": "2109168016",
                "name": "Zhuang Liu"
            },
            {
                "authorId": "152964870",
                "name": "Mannat Singh"
            },
            {
                "authorId": "3085301",
                "name": "Kalyan Vasudev Alwala"
            },
            {
                "authorId": "2319608",
                "name": "Armand Joulin"
            },
            {
                "authorId": "1806773",
                "name": "Ishan Misra"
            }
        ],
        "abstract": "We present ImageBind, an approach to learn a joint embedding across six different modalities - images, text, audio, depth, thermal, and IMU data. We show that all combinations of paired data are not necessary to train such a joint embedding, and only image-paired data is sufficient to bind the modalities together. ImageBind can leverage recent large scale vision-language models, and extends their zero-shot capabilities to new modalities just by using their natural pairing with images. It enables novel emergent applications out-of-the-box including cross-modal retrieval, composing modalities with arithmetic, cross-modal detection and generation. The emergent capabilities improve with the strength of the image encoder and we set a new state-of-the-art on emergent zero-shot recognition tasks across modalities, outperforming specialist supervised models. Finally, we show strong few-shot recognition results outperforming prior work, and that ImageBind serves as a new way to evaluate vision models for visual and non-visual tasks."
    },
    {
        "paperId": "a3aa1323a7f08c40207eaa359041e5bd72b25b27",
        "url": "https://www.semanticscholar.org/paper/a3aa1323a7f08c40207eaa359041e5bd72b25b27",
        "title": "Run, Don't Walk: Chasing Higher FLOPS for Faster Neural Networks",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2023,
        "citationCount": 1444,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2303.03667",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2303.03667, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2155098595",
                "name": "Jierun Chen"
            },
            {
                "authorId": "2210967799",
                "name": "Shiu-hong Kao"
            },
            {
                "authorId": "2110434779",
                "name": "Hao He"
            },
            {
                "authorId": "37515287",
                "name": "Weipeng Zhuo"
            },
            {
                "authorId": "1902406591",
                "name": "Song Wen"
            },
            {
                "authorId": "2183657085",
                "name": "Chul-Ho Lee"
            },
            {
                "authorId": "2118784964",
                "name": "Shueng-Han Gary Chan"
            }
        ],
        "abstract": "To design fast neural networks, many works have been focusing on reducing the number of floating-point operations (FLOPs). We observe that such reduction in FLOPs, however, does not necessarily lead to a similar level of re-duction in latency. This mainly stems from inefficiently low floating-point operations per second (FLOPS). To achieve faster networks, we revisit popular operators and demonstrate that such low FLOPS is mainly due to frequent memory access of the operators, especially the depthwise con-volution. We hence propose a novel partial convolution (PConv) that extracts spatial features more efficiently, by cutting down redundant computation and memory access simultaneously. Building upon our PConv, we further propose FasterNet, a new family of neural networks, which attains substantially higher running speed than others on a wide range of devices, without compromising on accuracy for various vision tasks. For example, on ImageNet-lk, our tiny FasterNet-TO is 2.8, 3.3, and 2.4 faster than MobileViT-XXS on GPU, CPU, and ARM processors, respectively, while being 2.9% more accurate. Our large FasterNet-L achieves impressive 83.5% top-1 accuracy, on par with the emerging Swin-B, while having 36% higher inference throughput on GPU, as well as saving 37% compute time on CPU. Code is available at https://github.com/JierunChen/FasterNet."
    },
    {
        "paperId": "b50d19c5c298f6562c3b3c6c3822a351bdc89260",
        "url": "https://www.semanticscholar.org/paper/b50d19c5c298f6562c3b3c6c3822a351bdc89260",
        "title": "MMMU: A Massive Multi-Discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2023,
        "citationCount": 1585,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2311.16502",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.16502, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2268495181",
                "name": "Xiang Yue"
            },
            {
                "authorId": "2268493966",
                "name": "Yuansheng Ni"
            },
            {
                "authorId": "145086492",
                "name": "Kai Zhang"
            },
            {
                "authorId": "2268491856",
                "name": "Tianyu Zheng"
            },
            {
                "authorId": "2268706116",
                "name": "Ruoqi Liu"
            },
            {
                "authorId": "2143853895",
                "name": "Ge Zhang"
            },
            {
                "authorId": "1480968444",
                "name": "Samuel Stevens"
            },
            {
                "authorId": "2197076899",
                "name": "Dongfu Jiang"
            },
            {
                "authorId": "2268493042",
                "name": "Weiming Ren"
            },
            {
                "authorId": "2253808926",
                "name": "Yuxuan Sun"
            },
            {
                "authorId": "2268683835",
                "name": "Cong Wei"
            },
            {
                "authorId": "2754371",
                "name": "Botao Yu"
            },
            {
                "authorId": "2032236274",
                "name": "Ruibin Yuan"
            },
            {
                "authorId": "2268818200",
                "name": "Renliang Sun"
            },
            {
                "authorId": "2053888252",
                "name": "Ming Yin"
            },
            {
                "authorId": "2112670551",
                "name": "Boyuan Zheng"
            },
            {
                "authorId": "2218306363",
                "name": "Zhenzhu Yang"
            },
            {
                "authorId": "2348575598",
                "name": "Yibo Liu"
            },
            {
                "authorId": "2239245627",
                "name": "Wenhao Huang"
            },
            {
                "authorId": "2239158325",
                "name": "Huan Sun"
            },
            {
                "authorId": "1758652",
                "name": "Yu Su"
            },
            {
                "authorId": "2253811180",
                "name": "Wenhu Chen"
            }
        ],
        "abstract": "We introduce MMMU: a new benchmark designed to evaluate multimodal models on massive multi-discipline tasks demanding college-level subject knowledge and deliberate reasoning. MMMU includes 11.5K meticulously collected multimodal questions from college exams, quizzes, and text-books, covering six core disciplines: Art & Design, Busi-ness, Science, Health & Medicine, Humanities & Social Science, and Tech & Engineering. These questions span 30 subjects and 183 subfields, comprising 30 highly het-erogeneous image types, such as charts, diagrams, maps, tables, music sheets, and chemical structures. Unlike existing benchmarks, MMMU focuses on advanced perception and reasoning with domain-specific knowledge, challenging models to perform tasks akin to those faced by experts. The evaluation of 28 open-source LMMs as well as the propri-etary GPT-4V(ision) and Gemini highlights the substantial challenges posed by MMMU. Even the advanced GPT-4V and Gemini Ultra only achieve accuracies of 56% and 59% respectively, indicating significant room for improvement. We believe MMMU will stimulate the community to build next-generation multimodal foundation models towards expert artificial general intelligence."
    },
    {
        "paperId": "f5a0c57f90c6abe31482e9f320ccac5ee789b135",
        "url": "https://www.semanticscholar.org/paper/f5a0c57f90c6abe31482e9f320ccac5ee789b135",
        "title": "Align Your Latents: High-Resolution Video Synthesis with Latent Diffusion Models",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2023,
        "citationCount": 1407,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2304.08818",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2304.08818, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "119843260",
                "name": "A. Blattmann"
            },
            {
                "authorId": "1660819540",
                "name": "Robin Rombach"
            },
            {
                "authorId": "18900686",
                "name": "Huan Ling"
            },
            {
                "authorId": "102541178",
                "name": "Tim Dockhorn"
            },
            {
                "authorId": "2596437",
                "name": "Seung Wook Kim"
            },
            {
                "authorId": "37895334",
                "name": "S. Fidler"
            },
            {
                "authorId": "32113848",
                "name": "Karsten Kreis"
            }
        ],
        "abstract": "Latent Diffusion Models (LDMs) enable high-quality image synthesis while avoiding excessive compute demands by training a diffusion model in a compressed lower-dimensional latent space. Here, we apply the LDM paradigm to high-resolution video generation, a particularly resource-intensive task. We first pre-train an LDM on images only; then, we turn the image generator into a video generator by introducing a temporal dimension to the latent space diffusion model and finetuning on encoded image sequences, i.e., videos. Similarly, we temporally align diffusion model upsamplers, turning them into temporally consistent video super resolution models. We focus on two relevant real-world applications: Simulation of in-the-wild driving data and creative content creation with text-to-video modeling. In particular, we validate our Video LDM on real driving videos of resolution $512 \\times 1024$, achieving state-of-the-art performance. Furthermore, our approach can easily leverage off-the-shelf pretrained image LDMs, as we only need to train a temporal alignment model in that case. Doing so, we turn the publicly available, state-of-the-art text-to-image LDM Stable Diffusion into an efficient and expressive text-to-video model with resolution up to $1280 \\times 2048$. We show that the temporal layers trained in this way generalize to different finetuned text-to-image LDMs. Utilizing this property, we show the first results for personalized text-to-video generation, opening exciting directions for future content creation. Project page: https://nv-tlabs.github.io/VideoLDM/"
    },
    {
        "paperId": "afea54c7f17f4fac0f71bebed6bc782ed69d8bd0",
        "url": "https://www.semanticscholar.org/paper/afea54c7f17f4fac0f71bebed6bc782ed69d8bd0",
        "title": "Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2024,
        "citationCount": 1373,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2401.10891",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2401.10891, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2268796616",
                "name": "Lihe Yang"
            },
            {
                "authorId": "2261363653",
                "name": "Bingyi Kang"
            },
            {
                "authorId": "2276315322",
                "name": "Zilong Huang"
            },
            {
                "authorId": "2261385713",
                "name": "Xiaogang Xu"
            },
            {
                "authorId": "2276580610",
                "name": "Jiashi Feng"
            },
            {
                "authorId": "2237588370",
                "name": "Hengshuang Zhao"
            }
        ],
        "abstract": "This work presents Depth Anything11While the grammatical soundness of this name may be questionable, we treat it as a whole and pay homage to Segment Anything [26]., a highly practical solution for robust monocular depth estimation. Without pursuing novel technical modules, we aim to build a simple yet powerful foundation model dealing with any images under any circumstances. To this end, we scale up the dataset by designing a data engine to collect and automatically annotate large-scale unlabeled data (~62M), which significantly enlarges the data coverage and thus is able to reduce the generalization error. We investigate two simple yet effective strategies that make data scaling-up promising. First, a more challenging optimization target is created by leveraging data augmentation tools. It compels the model to actively seek extra visual knowledge and acquire robust representations. Second, an auxiliary supervision is developed to enforce the model to inherit rich semantic priors from pre-trained encoders. We evaluate its zero-shot capabilities extensively, including six public datasets and randomly captured photos. It demonstrates impressive generalization ability (Figure 1). Further, through fine-tuning it with metric depth information from NYUv2 and KITTI, new SOTAs are set. Our better depth model also results in a better depth-conditioned ControlNet. Our models are released here."
    },
    {
        "paperId": "afea54c7f17f4fac0f71bebed6bc782ed69d8bd0",
        "url": "https://www.semanticscholar.org/paper/afea54c7f17f4fac0f71bebed6bc782ed69d8bd0",
        "title": "Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data",
        "venue": "Computer Vision and Pattern Recognition",
        "year": 2024,
        "citationCount": 1373,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/2401.10891",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2401.10891, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2268796616",
                "name": "Lihe Yang"
            },
            {
                "authorId": "2261363653",
                "name": "Bingyi Kang"
            },
            {
                "authorId": "2276315322",
                "name": "Zilong Huang"
            },
            {
                "authorId": "2261385713",
                "name": "Xiaogang Xu"
            },
            {
                "authorId": "2276580610",
                "name": "Jiashi Feng"
            },
            {
                "authorId": "2237588370",
                "name": "Hengshuang Zhao"
            }
        ],
        "abstract": "This work presents Depth Anything11While the grammatical soundness of this name may be questionable, we treat it as a whole and pay homage to Segment Anything [26]., a highly practical solution for robust monocular depth estimation. Without pursuing novel technical modules, we aim to build a simple yet powerful foundation model dealing with any images under any circumstances. To this end, we scale up the dataset by designing a data engine to collect and automatically annotate large-scale unlabeled data (~62M), which significantly enlarges the data coverage and thus is able to reduce the generalization error. We investigate two simple yet effective strategies that make data scaling-up promising. First, a more challenging optimization target is created by leveraging data augmentation tools. It compels the model to actively seek extra visual knowledge and acquire robust representations. Second, an auxiliary supervision is developed to enforce the model to inherit rich semantic priors from pre-trained encoders. We evaluate its zero-shot capabilities extensively, including six public datasets and randomly captured photos. It demonstrates impressive generalization ability (Figure 1). Further, through fine-tuning it with metric depth information from NYUv2 and KITTI, new SOTAs are set. Our better depth model also results in a better depth-conditioned ControlNet. Our models are released here."
    },
    {
        "paperId": "e54d8b07ef659f9ee2671441c4355e414e408836",
        "url": "https://www.semanticscholar.org/paper/e54d8b07ef659f9ee2671441c4355e414e408836",
        "title": "OntoNotes: The 90% Solution",
        "venue": "North American Chapter of the Association for Computational Linguistics",
        "year": 2006,
        "citationCount": 1033,
        "openAccessPdf": {
            "url": "https://dl.acm.org/doi/pdf/10.5555/1614049.1614064",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/N06-2015, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "144547315",
                "name": "E. Hovy"
            },
            {
                "authorId": "1734174",
                "name": "Mitchell P. Marcus"
            },
            {
                "authorId": "145755155",
                "name": "Martha Palmer"
            },
            {
                "authorId": "1744313",
                "name": "L. Ramshaw"
            },
            {
                "authorId": "1732071",
                "name": "R. Weischedel"
            }
        ],
        "abstract": "We describe the OntoNotes methodology and its result, a large multilingual richly-annotated corpus constructed at 90% interannotator agreement. An initial portion (300K words of English newswire and 250K words of Chinese newswire) will be made available to the community during 2007."
    },
    {
        "paperId": "8e31f3c7e70e9a5f8afafd86cebc004d5eca8c2b",
        "url": "https://www.semanticscholar.org/paper/8e31f3c7e70e9a5f8afafd86cebc004d5eca8c2b",
        "title": "Automatic Evaluation of Topic Coherence",
        "venue": "North American Chapter of the Association for Computational Linguistics",
        "year": 2010,
        "citationCount": 1065,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://aclanthology.org/N10-1012, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "67057961",
                "name": "D. Newman"
            },
            {
                "authorId": "1800564",
                "name": "Jey Han Lau"
            },
            {
                "authorId": "2635108",
                "name": "Karl Grieser"
            },
            {
                "authorId": "145465286",
                "name": "Timothy Baldwin"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "7b5e31257f01aba987f16e175a3e49e00a5bd3bb",
        "url": "https://www.semanticscholar.org/paper/7b5e31257f01aba987f16e175a3e49e00a5bd3bb",
        "title": "A Simple, Fast, and Effective Reparameterization of IBM Model 2",
        "venue": "North American Chapter of the Association for Computational Linguistics",
        "year": 2013,
        "citationCount": 1091,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://aclanthology.org/N13-1073, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1745899",
                "name": "Chris Dyer"
            },
            {
                "authorId": "2730654",
                "name": "Victor Chahuneau"
            },
            {
                "authorId": "144365875",
                "name": "Noah A. Smith"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "c4fd9c86b2b41df51a6fe212406dda81b1997fd4",
        "url": "https://www.semanticscholar.org/paper/c4fd9c86b2b41df51a6fe212406dda81b1997fd4",
        "title": "Linguistic Regularities in Continuous Space Word Representations",
        "venue": "North American Chapter of the Association for Computational Linguistics",
        "year": 2013,
        "citationCount": 3845,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://aclanthology.org/N13-1090, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2047446108",
                "name": "Tomas Mikolov"
            },
            {
                "authorId": "144105277",
                "name": "Wen-tau Yih"
            },
            {
                "authorId": "1681543",
                "name": "G. Zweig"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "651e5bcc14f14605a879303e97572a27ea8c7956",
        "url": "https://www.semanticscholar.org/paper/651e5bcc14f14605a879303e97572a27ea8c7956",
        "title": "A Diversity-Promoting Objective Function for Neural Conversation Models",
        "venue": "North American Chapter of the Association for Computational Linguistics",
        "year": 2015,
        "citationCount": 2562,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/N16-1014.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1510.03055, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2386020962",
                "name": "Jiwei Li"
            },
            {
                "authorId": "1947267",
                "name": "Michel Galley"
            },
            {
                "authorId": "3125776",
                "name": "Chris Brockett"
            },
            {
                "authorId": "1800422",
                "name": "Jianfeng Gao"
            },
            {
                "authorId": "83415753",
                "name": "W. Dolan"
            }
        ],
        "abstract": "Sequence-to-sequence neural network models for generation of conversational responses tend to generate safe, commonplace responses (e.g., \"I don't know\") regardless of the input. We suggest that the traditional objective function, i.e., the likelihood of output (response) given input (message) is unsuited to response generation tasks. Instead we propose using Maximum Mutual Information (MMI) as the objective function in neural models. Experimental results demonstrate that the proposed MMI models produce more diverse, interesting, and appropriate responses, yielding substantive gains in BLEU scores on two conversational datasets and in human evaluations."
    },
    {
        "paperId": "455afd748e8834ef521e4b67c7c056d3c33429e2",
        "url": "https://www.semanticscholar.org/paper/455afd748e8834ef521e4b67c7c056d3c33429e2",
        "title": "Hierarchical Attention Networks for Document Classification",
        "venue": "North American Chapter of the Association for Computational Linguistics",
        "year": 2016,
        "citationCount": 4671,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/N16-1174.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/N16-1174, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "8387085",
                "name": "Zichao Yang"
            },
            {
                "authorId": "2022168",
                "name": "Diyi Yang"
            },
            {
                "authorId": "1745899",
                "name": "Chris Dyer"
            },
            {
                "authorId": "144137069",
                "name": "Xiaodong He"
            },
            {
                "authorId": "46234526",
                "name": "Alex Smola"
            },
            {
                "authorId": "144547315",
                "name": "E. Hovy"
            }
        ],
        "abstract": "We propose a hierarchical attention network for document classification. Our model has two distinctive characteristics: (i) it has a hierarchical structure that mirrors the hierarchical structure of documents; (ii) it has two levels of attention mechanisms applied at the wordand sentence-level, enabling it to attend differentially to more and less important content when constructing the document representation. Experiments conducted on six large scale text classification tasks demonstrate that the proposed architecture outperform previous methods by a substantial margin. Visualization of the attention layers illustrates that the model selects qualitatively informative words and sentences."
    },
    {
        "paperId": "c0883f5930a232a9c1ad601c978caede29155979",
        "url": "https://www.semanticscholar.org/paper/c0883f5930a232a9c1ad601c978caede29155979",
        "title": "Why Should I Trust You?: Explaining the Predictions of Any Classifier",
        "venue": "North American Chapter of the Association for Computational Linguistics",
        "year": 2016,
        "citationCount": 19543,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/N16-3020.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1602.04938, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "78846919",
                "name": "Marco Tulio Ribeiro"
            },
            {
                "authorId": "34650964",
                "name": "Sameer Singh"
            },
            {
                "authorId": "1730156",
                "name": "Carlos Guestrin"
            }
        ],
        "abstract": "Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally varound the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted."
    },
    {
        "paperId": "df704cca917666dace4e42b4d3a50f65597b8f06",
        "url": "https://www.semanticscholar.org/paper/df704cca917666dace4e42b4d3a50f65597b8f06",
        "title": "Hateful Symbols or Hateful People? Predictive Features for Hate Speech Detection on Twitter",
        "venue": "North American Chapter of the Association for Computational Linguistics",
        "year": 2016,
        "citationCount": 1754,
        "openAccessPdf": {
            "url": "https://doi.org/10.18653/v1/n16-2013",
            "status": "BRONZE",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/N16-2013, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2138053020",
                "name": "Zeerak Talat"
            },
            {
                "authorId": "2022288",
                "name": "Dirk Hovy"
            }
        ],
        "abstract": "Hate speech in the form of racist and sexist remarks are a common occurrence on social media. For that reason, many social media services address the problem of identifying hate speech, but the definition of hate speech varies markedly and is largely a manual effort (BBC, 2015; Lomas, 2015). We provide a list of criteria founded in critical race theory, and use them to annotate a publicly available corpus of more than 16k tweets. We analyze the impact of various extra-linguistic features in conjunction with character n-grams for hatespeech detection. We also present a dictionary based the most indicative words in our data."
    },
    {
        "paperId": "f5a7da72496e2ca8edcd9f9123773012c010cfc6",
        "url": "https://www.semanticscholar.org/paper/f5a7da72496e2ca8edcd9f9123773012c010cfc6",
        "title": "Neural Architectures for Named Entity Recognition",
        "venue": "North American Chapter of the Association for Computational Linguistics",
        "year": 2016,
        "citationCount": 4198,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/N16-1030.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1603.01360, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1830914",
                "name": "Guillaume Lample"
            },
            {
                "authorId": "143668305",
                "name": "Miguel Ballesteros"
            },
            {
                "authorId": "50324141",
                "name": "Sandeep Subramanian"
            },
            {
                "authorId": "2189948",
                "name": "Kazuya Kawakami"
            },
            {
                "authorId": "1745899",
                "name": "Chris Dyer"
            }
        ],
        "abstract": "Comunicacio presentada a la 2016 Conference of the North American Chapter of the Association for Computational Linguistics, celebrada a San Diego (CA, EUA) els dies 12 a 17 de juny 2016."
    },
    {
        "paperId": "5ded2b8c64491b4a67f6d39ce473d4b9347a672e",
        "url": "https://www.semanticscholar.org/paper/5ded2b8c64491b4a67f6d39ce473d4b9347a672e",
        "title": "A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference",
        "venue": "North American Chapter of the Association for Computational Linguistics",
        "year": 2017,
        "citationCount": 4818,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/N18-1101.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1704.05426, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "81840293",
                "name": "Adina Williams"
            },
            {
                "authorId": "10666396",
                "name": "Nikita Nangia"
            },
            {
                "authorId": "3644767",
                "name": "Samuel R. Bowman"
            }
        ],
        "abstract": "This paper introduces the Multi-Genre Natural Language Inference (MultiNLI) corpus, a dataset designed for use in the development and evaluation of machine learning models for sentence understanding. At 433k examples, this resource is one of the largest corpora available for natural language inference (a.k.a. recognizing textual entailment), improving upon available resources in both its coverage and difficulty. MultiNLI accomplishes this by offering data from ten distinct genres of written and spoken English, making it possible to evaluate systems on nearly the full complexity of the language, while supplying an explicit setting for evaluating cross-genre domain adaptation. In addition, an evaluation using existing machine learning models designed for the Stanford NLI corpus shows that it represents a substantially more difficult task than does that corpus, despite the two showing similar levels of inter-annotator agreement."
    },
    {
        "paperId": "0be19fd9896e5d40222c690cc3ff553adc7c0e27",
        "url": "https://www.semanticscholar.org/paper/0be19fd9896e5d40222c690cc3ff553adc7c0e27",
        "title": "Gender Bias in Coreference Resolution: Evaluation and Debiasing Methods",
        "venue": "North American Chapter of the Association for Computational Linguistics",
        "year": 2018,
        "citationCount": 1074,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/N18-2003.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1804.06876, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "33524946",
                "name": "Jieyu Zhao"
            },
            {
                "authorId": "1785372925",
                "name": "Tianlu Wang"
            },
            {
                "authorId": "2064210",
                "name": "Mark Yatskar"
            },
            {
                "authorId": "2004053",
                "name": "Vicente Ordonez"
            },
            {
                "authorId": "2782886",
                "name": "Kai-Wei Chang"
            }
        ],
        "abstract": "In this paper, we introduce a new benchmark for co-reference resolution focused on gender bias, WinoBias. Our corpus contains Winograd-schema style sentences with entities corresponding to people referred by their occupation (e.g. the nurse, the doctor, the carpenter). We demonstrate that a rule-based, a feature-rich, and a neural coreference system all link gendered pronouns to pro-stereotypical entities with higher accuracy than anti-stereotypical entities, by an average difference of 21.1 in F1 score. Finally, we demonstrate a data-augmentation approach that, in combination with existing word-embedding debiasing techniques, removes the bias demonstrated by these systems in WinoBias without significantly affecting their performance on existing datasets."
    },
    {
        "paperId": "2997b26ffb8c291ce478bd8a6e47979d5a55c466",
        "url": "https://www.semanticscholar.org/paper/2997b26ffb8c291ce478bd8a6e47979d5a55c466",
        "title": "Annotation Artifacts in Natural Language Inference Data",
        "venue": "North American Chapter of the Association for Computational Linguistics",
        "year": 2018,
        "citationCount": 1228,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/N18-2017.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1803.02324, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "40895369",
                "name": "Suchin Gururangan"
            },
            {
                "authorId": "2705113",
                "name": "Swabha Swayamdipta"
            },
            {
                "authorId": "39455775",
                "name": "Omer Levy"
            },
            {
                "authorId": "2279023325",
                "name": "Roy Schwartz"
            },
            {
                "authorId": "3644767",
                "name": "Samuel R. Bowman"
            },
            {
                "authorId": "144365875",
                "name": "Noah A. Smith"
            }
        ],
        "abstract": "Large-scale datasets for natural language inference are created by presenting crowd workers with a sentence (premise), and asking them to generate three new sentences (hypotheses) that it entails, contradicts, or is logically neutral with respect to. We show that, in a significant portion of such data, this protocol leaves clues that make it possible to identify the label by looking only at the hypothesis, without observing the premise. Specifically, we show that a simple text categorization model can correctly classify the hypothesis alone in about 67% of SNLI (Bowman et. al, 2015) and 53% of MultiNLI (Williams et. al, 2017). Our analysis reveals that specific linguistic phenomena such as negation and vagueness are highly correlated with certain inference classes. Our findings suggest that the success of natural language inference models to date has been overestimated, and that the task remains a hard open problem."
    },
    {
        "paperId": "3febb2bed8865945e7fddc99efd791887bb7e14f",
        "url": "https://www.semanticscholar.org/paper/3febb2bed8865945e7fddc99efd791887bb7e14f",
        "title": "Deep Contextualized Word Representations",
        "venue": "North American Chapter of the Association for Computational Linguistics",
        "year": 2018,
        "citationCount": 11955,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/N18-1202.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1802.05365, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "39139825",
                "name": "Matthew E. Peters"
            },
            {
                "authorId": "50043859",
                "name": "Mark Neumann"
            },
            {
                "authorId": "2136562",
                "name": "Mohit Iyyer"
            },
            {
                "authorId": "40642935",
                "name": "Matt Gardner"
            },
            {
                "authorId": "143997772",
                "name": "Christopher Clark"
            },
            {
                "authorId": "2544107",
                "name": "Kenton Lee"
            },
            {
                "authorId": "1982950",
                "name": "Luke Zettlemoyer"
            }
        ],
        "abstract": "We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals."
    },
    {
        "paperId": "b1d24e8e08435b7c52335485a0d635abf9bc604c",
        "url": "https://www.semanticscholar.org/paper/b1d24e8e08435b7c52335485a0d635abf9bc604c",
        "title": "FEVER: a Large-scale Dataset for Fact Extraction and VERification",
        "venue": "North American Chapter of the Association for Computational Linguistics",
        "year": 2018,
        "citationCount": 1944,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/N18-1074.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1803.05355, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "144603330",
                "name": "James Thorne"
            },
            {
                "authorId": "2064056928",
                "name": "Andreas Vlachos"
            },
            {
                "authorId": "2718039",
                "name": "Christos Christodoulopoulos"
            },
            {
                "authorId": "2008596",
                "name": "Arpit Mittal"
            }
        ],
        "abstract": "In this paper we introduce a new publicly available dataset for verification against textual sources, FEVER: Fact Extraction and VERification. It consists of 185,445 claims generated by altering sentences extracted from Wikipedia and subsequently verified without knowledge of the sentence they were derived from. The claims are classified as Supported, Refuted or NotEnoughInfo by annotators achieving 0.6841 in Fleiss kappa. For the first two classes, the annotators also recorded the sentence(s) forming the necessary evidence for their judgment. To characterize the challenge of the dataset presented, we develop a pipeline approach and compare it to suitably designed oracles. The best accuracy we achieve on labeling a claim accompanied by the correct evidence is 31.87%, while if we ignore the evidence we achieve 50.91%. Thus we believe that FEVER is a challenging testbed that will help stimulate progress on claim verification against textual sources."
    },
    {
        "paperId": "c8efcc854d97dfc2a42b83316a2109f9d166e43f",
        "url": "https://www.semanticscholar.org/paper/c8efcc854d97dfc2a42b83316a2109f9d166e43f",
        "title": "Self-Attention with Relative Position Representations",
        "venue": "North American Chapter of the Association for Computational Linguistics",
        "year": 2018,
        "citationCount": 2607,
        "openAccessPdf": {
            "url": "https://www.aclweb.org/anthology/N18-2074.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1803.02155, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "38759328",
                "name": "Peter Shaw"
            },
            {
                "authorId": "39328010",
                "name": "Jakob Uszkoreit"
            },
            {
                "authorId": "40348417",
                "name": "Ashish Vaswani"
            }
        ],
        "abstract": "Relying entirely on an attention mechanism, the Transformer introduced by Vaswani et al. (2017) achieves state-of-the-art results for machine translation. In contrast to recurrent and convolutional neural networks, it does not explicitly model relative or absolute position information in its structure. Instead, it requires adding representations of absolute positions to its inputs. In this work we present an alternative approach, extending the self-attention mechanism to efficiently consider representations of the relative positions, or distances between sequence elements. On the WMT 2014 English-to-German and English-to-French translation tasks, this approach yields improvements of 1.3 BLEU and 0.3 BLEU over absolute position representations, respectively. Notably, we observe that combining relative and absolute position representations yields no further improvement in translation quality. We describe an efficient implementation of our method and cast it as an instance of relation-aware self-attention mechanisms that can generalize to arbitrary graph-labeled inputs."
    },
    {
        "paperId": "1e83c20def5c84efa6d4a0d80aa3159f55cb9c3f",
        "url": "https://www.semanticscholar.org/paper/1e83c20def5c84efa6d4a0d80aa3159f55cb9c3f",
        "title": "Attention is not Explanation",
        "venue": "North American Chapter of the Association for Computational Linguistics",
        "year": 2019,
        "citationCount": 1509,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1902.10186, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "49837811",
                "name": "Sarthak Jain"
            },
            {
                "authorId": "1912476",
                "name": "Byron C. Wallace"
            }
        ],
        "abstract": "Attention mechanisms have seen wide adoption in neural NLP models. In addition to improving predictive performance, these are often touted as affording transparency: models equipped with attention provide a distribution over attended-to input units, and this is often presented (at least implicitly) as communicating the relative importance of inputs. However, it is unclear what relationship exists between attention weights and model outputs. In this work we perform extensive experiments across a variety of NLP tasks that aim to assess the degree to which attention weights provide meaningful explanations for predictions. We find that they largely do not. For example, learned attention weights are frequently uncorrelated with gradient-based measures of feature importance, and one can identify very different attention distributions that nonetheless yield equivalent predictions. Our findings show that standard attention modules do not provide meaningful explanations and should not be treated as though they do."
    },
    {
        "paperId": "455a8838cde44f288d456d01c76ede95b56dc675",
        "url": "https://www.semanticscholar.org/paper/455a8838cde44f288d456d01c76ede95b56dc675",
        "title": "A Structural Probe for Finding Syntax in Word Representations",
        "venue": "North American Chapter of the Association for Computational Linguistics",
        "year": 2019,
        "citationCount": 1227,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/N19-1419, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "145430120",
                "name": "John Hewitt"
            },
            {
                "authorId": "144783904",
                "name": "Christopher D. Manning"
            }
        ],
        "abstract": "Recent work has improved our ability to detect linguistic knowledge in word representations. However, current methods for detecting syntactic knowledge do not test whether syntax trees are represented in their entirety. In this work, we propose a structural probe, which evaluates whether syntax trees are embedded in a linear transformation of a neural networks word representation space. The probe identifies a linear transformation under which squared L2 distance encodes the distance between words in the parse tree, and one in which squared L2 norm encodes depth in the parse tree. Using our probe, we show that such transformations exist for both ELMo and BERT but not in baselines, providing evidence that entire syntax trees are embedded implicitly in deep models vector geometry."
    },
    {
        "paperId": "9770fff7379a7ab9006b48939462354dda9a2053",
        "url": "https://www.semanticscholar.org/paper/9770fff7379a7ab9006b48939462354dda9a2053",
        "title": "BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions",
        "venue": "North American Chapter of the Association for Computational Linguistics",
        "year": 2019,
        "citationCount": 2013,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1905.10044, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "143997772",
                "name": "Christopher Clark"
            },
            {
                "authorId": "2544107",
                "name": "Kenton Lee"
            },
            {
                "authorId": "1744179",
                "name": "Ming-Wei Chang"
            },
            {
                "authorId": "15652489",
                "name": "T. Kwiatkowski"
            },
            {
                "authorId": "123052390",
                "name": "Michael Collins"
            },
            {
                "authorId": "3259253",
                "name": "Kristina Toutanova"
            }
        ],
        "abstract": "In this paper we study yes/no questions that are naturally occurring  meaning that they are generated in unprompted and unconstrained settings. We build a reading comprehension dataset, BoolQ, of such questions, and show that they are unexpectedly challenging. They often query for complex, non-factoid information, and require difficult entailment-like inference to solve. We also explore the effectiveness of a range of transfer learning baselines. We find that transferring from entailment data is more effective than transferring from paraphrase or extractive QA data, and that it, surprisingly, continues to be very beneficial even when starting from massive pre-trained language models such as BERT. Our best method trains BERT on MultiNLI and then re-trains it on our train set. It achieves 80.4% accuracy compared to 90% accuracy of human annotators (and 62% majority-baseline), leaving a significant gap for future work."
    },
    {
        "paperId": "c21a4d70d83e0f6eb2a9e1c41d034842dd561e47",
        "url": "https://www.semanticscholar.org/paper/c21a4d70d83e0f6eb2a9e1c41d034842dd561e47",
        "title": "CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge",
        "venue": "North American Chapter of the Association for Computational Linguistics",
        "year": 2019,
        "citationCount": 2125,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1811.00937, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "12371246",
                "name": "Alon Talmor"
            },
            {
                "authorId": "47426264",
                "name": "Jonathan Herzig"
            },
            {
                "authorId": "35219984",
                "name": "Nicholas Lourie"
            },
            {
                "authorId": "1750652",
                "name": "Jonathan Berant"
            }
        ],
        "abstract": "When answering a question, people often draw upon their rich world knowledge in addition to the particular context. Recent work has focused primarily on answering questions given some relevant document or context, and required very little general background. To investigate question answering with prior knowledge, we present CommonsenseQA: a challenging new dataset for commonsense question answering. To capture common sense beyond associations, we extract from ConceptNet (Speer et al., 2017) multiple target concepts that have the same semantic relation to a single source concept. Crowd-workers are asked to author multiple-choice questions that mention the source concept and discriminate in turn between each of the target concepts. This encourages workers to create questions with complex semantics that often require prior knowledge. We create 12,247 questions through this procedure and demonstrate the difficulty of our task with a large number of strong baselines. Our best baseline is based on BERT-large (Devlin et al., 2018) and obtains 56% accuracy, well below human performance, which is 89%."
    },
    {
        "paperId": "dda6fb309f62e2557a071522354d8c2c897a2805",
        "url": "https://www.semanticscholar.org/paper/dda6fb309f62e2557a071522354d8c2c897a2805",
        "title": "DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs",
        "venue": "North American Chapter of the Association for Computational Linguistics",
        "year": 2019,
        "citationCount": 1137,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1903.00161, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "33546336",
                "name": "Dheeru Dua"
            },
            {
                "authorId": "1705260",
                "name": "Yizhong Wang"
            },
            {
                "authorId": "2697425",
                "name": "Pradeep Dasigi"
            },
            {
                "authorId": "2157025",
                "name": "Gabriel Stanovsky"
            },
            {
                "authorId": "34650964",
                "name": "Sameer Singh"
            },
            {
                "authorId": "40642935",
                "name": "Matt Gardner"
            }
        ],
        "abstract": "Reading comprehension has recently seen rapid progress, with systems matching humans on the most popular datasets for the task. However, a large body of work has highlighted the brittleness of these systems, showing that there is much work left to be done. We introduce a new reading comprehension benchmark, DROP, which requires Discrete Reasoning Over the content of Paragraphs. In this crowdsourced, adversarially-created, 55k-question benchmark, a system must resolve references in a question, perhaps to multiple input positions, and perform discrete operations over them (such as addition, counting, or sorting). These operations require a much more comprehensive understanding of the content of paragraphs, as they remove the paraphrase-and-entity-typing shortcuts available in prior datasets. We apply state-of-the-art methods from both the reading comprehension and semantic parsing literatures on this dataset and show that the best systems only achieve 38.4% F1 on our generalized accuracy metric, while expert human performance is 96%. We additionally present a new model that combines reading comprehension methods with simple numerical reasoning to achieve 51% F1."
    },
    {
        "paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992",
        "url": "https://www.semanticscholar.org/paper/df2b0e26d0599ce3e70df8a9da02e51594e0e992",
        "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
        "venue": "North American Chapter of the Association for Computational Linguistics",
        "year": 2019,
        "citationCount": 107607,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1810.04805, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "39172707",
                "name": "Jacob Devlin"
            },
            {
                "authorId": "1744179",
                "name": "Ming-Wei Chang"
            },
            {
                "authorId": "2544107",
                "name": "Kenton Lee"
            },
            {
                "authorId": "3259253",
                "name": "Kristina Toutanova"
            }
        ],
        "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."
    },
    {
        "paperId": "faadd7d081c8d67e8c2567e8a5579e46cd6b2280",
        "url": "https://www.semanticscholar.org/paper/faadd7d081c8d67e8c2567e8a5579e46cd6b2280",
        "title": "fairseq: A Fast, Extensible Toolkit for Sequence Modeling",
        "venue": "North American Chapter of the Association for Computational Linguistics",
        "year": 2019,
        "citationCount": 3315,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/1904.01038",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1904.01038, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "40511414",
                "name": "Myle Ott"
            },
            {
                "authorId": "2068070",
                "name": "Sergey Edunov"
            },
            {
                "authorId": "51428394",
                "name": "Alexei Baevski"
            },
            {
                "authorId": "144270981",
                "name": "Angela Fan"
            },
            {
                "authorId": "39793298",
                "name": "Sam Gross"
            },
            {
                "authorId": "144055124",
                "name": "Nathan Ng"
            },
            {
                "authorId": "2529182",
                "name": "David Grangier"
            },
            {
                "authorId": "2325985",
                "name": "Michael Auli"
            }
        ],
        "abstract": "fairseq is an open-source sequence modeling toolkit that allows researchers and developers to train custom models for translation, summarization, language modeling, and other text generation tasks. The toolkit is based on PyTorch and supports distributed training across multiple GPUs and machines. We also support fast mixed-precision training and inference on modern GPUs. A demo video can be found at https://www.youtube.com/watch?v=OtgDdWtHvto"
    },
    {
        "paperId": "74276a37bfa50f90dfae37f767b2b67784bd402a",
        "url": "https://www.semanticscholar.org/paper/74276a37bfa50f90dfae37f767b2b67784bd402a",
        "title": "mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer",
        "venue": "North American Chapter of the Association for Computational Linguistics",
        "year": 2020,
        "citationCount": 2903,
        "openAccessPdf": {
            "url": "https://aclanthology.org/2021.naacl-main.41.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2010.11934, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2692973",
                "name": "Linting Xue"
            },
            {
                "authorId": "40832517",
                "name": "Noah Constant"
            },
            {
                "authorId": "145625142",
                "name": "Adam Roberts"
            },
            {
                "authorId": "26688118",
                "name": "Mihir Kale"
            },
            {
                "authorId": "1388360943",
                "name": "Rami Al-Rfou"
            },
            {
                "authorId": "9356387",
                "name": "Aditya Siddhant"
            },
            {
                "authorId": "20407480",
                "name": "Aditya Barua"
            },
            {
                "authorId": "2402716",
                "name": "Colin Raffel"
            }
        ],
        "abstract": "The recent Text-to-Text Transfer Transformer (T5) leveraged a unified text-to-text format and scale to attain state-of-the-art results on a wide variety of English-language NLP tasks. In this paper, we introduce mT5, a multilingual variant of T5 that was pre-trained on a new Common Crawl-based dataset covering 101 languages. We detail the design and modified training of mT5 and demonstrate its state-of-the-art performance on many multilingual benchmarks. We also describe a simple technique to prevent accidental translation in the zero-shot setting, where a generative model chooses to (partially) translate its prediction into the wrong language. All of the code and model checkpoints used in this work are publicly available."
    },
    {
        "paperId": "f30444fbb6ad806168e2564db4815cd27faa7fd9",
        "url": "https://www.semanticscholar.org/paper/f30444fbb6ad806168e2564db4815cd27faa7fd9",
        "title": "Its Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners",
        "venue": "North American Chapter of the Association for Computational Linguistics",
        "year": 2020,
        "citationCount": 1052,
        "openAccessPdf": {
            "url": "https://aclanthology.org/2021.naacl-main.185.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2009.07118, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "32246932",
                "name": "Timo Schick"
            },
            {
                "authorId": "144418438",
                "name": "Hinrich Schtze"
            }
        ],
        "abstract": "When scaled to hundreds of billions of parameters, pretrained language models such as GPT-3 (Brown et al., 2020) achieve remarkable few-shot performance. However, enormous amounts of compute are required for training and applying such big models, resulting in a large carbon footprint and making it difficult for researchers and practitioners to use them. We show that performance similar to GPT-3 can be obtained with language models that are much greener in that their parameter count is several orders of magnitude smaller. This is achieved by converting textual inputs into cloze questions that contain a task description, combined with gradient-based optimization; exploiting unlabeled data gives further improvements. We identify key factors required for successful natural language understanding with small language models."
    },
    {
        "paperId": "13c4e5a6122f3fa2663f63e49537091da6532f35",
        "url": "https://www.semanticscholar.org/paper/13c4e5a6122f3fa2663f63e49537091da6532f35",
        "title": "Are NLP Models really able to Solve Simple Math Word Problems?",
        "venue": "North American Chapter of the Association for Computational Linguistics",
        "year": 2021,
        "citationCount": 1056,
        "openAccessPdf": {
            "url": "https://aclanthology.org/2021.naacl-main.168.pdf",
            "status": "HYBRID",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2103.07191, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "1443788809",
                "name": "Arkil Patel"
            },
            {
                "authorId": "92954142",
                "name": "S. Bhattamishra"
            },
            {
                "authorId": "144260125",
                "name": "Navin Goyal"
            }
        ],
        "abstract": "The problem of designing NLP solvers for math word problems (MWP) has seen sustained research activity and steady gains in the test accuracy. Since existing solvers achieve high performance on the benchmark datasets for elementary level MWPs containing one-unknown arithmetic word problems, such problems are often considered solved with the bulk of research attention moving to more complex MWPs. In this paper, we restrict our attention to English MWPs taught in grades four and lower. We provide strong evidence that the existing MWP solvers rely on shallow heuristics to achieve high performance on the benchmark datasets. To this end, we show that MWP solvers that do not have access to the question asked in the MWP can still solve a large fraction of MWPs. Similarly, models that treat MWPs as bag-of-words can also achieve surprisingly high accuracy. Further, we introduce a challenge dataset, SVAMP, created by applying carefully chosen variations over examples sampled from existing datasets. The best accuracy achieved by state-of-the-art models is substantially lower on SVAMP, thus showing that much remains to be done even for the simplest of the MWPs."
    }
]